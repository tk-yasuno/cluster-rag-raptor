# HDBSCANç‰ˆRAPTOR ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

## ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# HDBSCANã‚’è¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install hdbscan

# ã¾ãŸã¯å…¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
```

## ğŸš€ 5åˆ†ã§å§‹ã‚ã‚‹

### Step 1: Ollamaãƒ¢ãƒ‡ãƒ«ã®æº–å‚™

```bash
# LLM (è¦ç´„ç”¨)
ollama pull granite-code:8b

# Embeddings (ãƒ™ã‚¯ãƒˆãƒ«åŒ–ç”¨)
ollama pull mxbai-embed-large
```

### Step 2: åŸºæœ¬çš„ãªå®Ÿè¡Œ

```python
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor_hdbscan import RAPTORRetrieverHDBSCAN

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
llm = ChatOllama(model="granite-code:8b", base_url="http://localhost:11434")
embeddings = OllamaEmbeddings(model="mxbai-embed-large", base_url="http://localhost:11434")

# HDBSCANç‰ˆRAPTOR
raptor = RAPTORRetrieverHDBSCAN(
    embeddings_model=embeddings,
    llm=llm,
    min_cluster_size=15,  # â­ é‡è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    exclude_noise=True     # ãƒã‚¤ã‚ºé™¤å»ON
)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ & æ¤œç´¢
raptor.index("test.txt")
results = raptor.retrieve("your query")
```

### Step 3: æ¯”è¼ƒå®Ÿé¨“

```bash
python example_hdbscan_comparison.py
```

## ğŸ¯ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ—©è¦‹è¡¨

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | æ¨å¥¨å€¤ | èª¬æ˜ |
|-----------|--------|------|
| `min_cluster_size` | 10-20 | ã‚¯ãƒ©ã‚¹ã‚¿ã®æœ€å°ã‚µã‚¤ã‚º |
| `min_samples` | 5-7 | `min_cluster_size/3`ãŒç›®å®‰ |
| `metric` | `'cosine'` | æ„å‘³çš„embeddingså‘ã‘ |
| `exclude_noise` | `True` | ãƒã‚¤ã‚ºé™¤å»ã‚’æœ‰åŠ¹åŒ– |
| `max_depth` | 2-3 | ãƒ„ãƒªãƒ¼ã®æ·±ã• |

## ğŸ”§ ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹

### ç´°ã‹ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

```python
raptor = RAPTORRetrieverHDBSCAN(
    min_cluster_size=5,   # å°ã•ã
    min_samples=2,
    metric='cosine'
)
```

### ä¿å®ˆçš„ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

```python
raptor = RAPTORRetrieverHDBSCAN(
    min_cluster_size=25,  # å¤§ãã
    min_samples=8,
    metric='euclidean'
)
```

## ğŸ“Š çµæœã®ç¢ºèª

```python
# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¾Œ
print(f"é™¤å»ã•ã‚ŒãŸãƒã‚¤ã‚º: {raptor.noise_stats['total_noise_chunks']}")
print(f"æ·±ã•åˆ¥ãƒã‚¤ã‚º: {raptor.noise_stats['noise_by_depth']}")

# æ¤œç´¢çµæœ
for i, doc in enumerate(results):
    print(f"{i+1}. é¡ä¼¼åº¦: {doc.metadata.get('similarity')}")
    print(f"   å†…å®¹: {doc.page_content[:100]}...")
```

## ğŸ†š ä»–æ‰‹æ³•ã¨ã®é•ã„

```python
# K-means (å›ºå®šã‚¯ãƒ©ã‚¹ã‚¿æ•°)
from raptor import RAPTORRetriever
raptor_kmeans = RAPTORRetriever(max_clusters=3)

# GMM + BIC (è‡ªå‹•æœ€é©åŒ–)
from raptor_gmm import RAPTORRetrieverGMM
raptor_gmm = RAPTORRetrieverGMM(use_bic=True)

# HDBSCAN (è‡ªå‹• + ãƒã‚¤ã‚ºé™¤å»)
from raptor_hdbscan import RAPTORRetrieverHDBSCAN
raptor_hdbscan = RAPTORRetrieverHDBSCAN(exclude_noise=True)
```

## ğŸ’¡ ã‚ˆãã‚ã‚‹è³ªå•

**Q: min_cluster_sizeã¯ã©ã†æ±ºã‚ã‚‹ï¼Ÿ**  
A: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®1-3%ã‚’ç›®å®‰ã«ã€‚1000æ–‡å­—ãƒãƒ£ãƒ³ã‚¯ãªã‚‰10-20ã€‚

**Q: ãƒã‚¤ã‚ºãŒå¤šã™ãã‚‹/å°‘ãªã™ãã‚‹ï¼Ÿ**  
A: `min_cluster_size`ã‚’èª¿æ•´ã€‚å¤§ããã™ã‚‹ã¨ãƒã‚¤ã‚ºå¢—ã€å°ã•ãã™ã‚‹ã¨ãƒã‚¤ã‚ºæ¸›ã€‚

**Q: ã©ã®è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ä½¿ã†ï¼Ÿ**  
A: æ„å‘³çš„embeddingsï¼ˆmxbaiç­‰ï¼‰ã¯`cosine`ã€æ±ç”¨ã¯`euclidean`ã€‚

**Q: å®Ÿè¡ŒãŒé…ã„ï¼Ÿ**  
A: `max_depth`ã‚’æ¸›ã‚‰ã™ã€`chunk_size`ã‚’å¤§ããã™ã‚‹ã€‚

## ğŸ“– è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [HDBSCAN_GUIDE.md](HDBSCAN_GUIDE.md) - å®Œå…¨ã‚¬ã‚¤ãƒ‰
- [example_hdbscan_comparison.py](example_hdbscan_comparison.py) - æ¯”è¼ƒå®Ÿé¨“ã‚³ãƒ¼ãƒ‰

## ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. âœ… åŸºæœ¬å®Ÿè¡Œã‚’è©¦ã™
2. âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´
3. âœ… æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œ
4. âœ… è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§æ¤œè¨¼

---

Happy Clustering! ğŸš€
