"""
RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) Implementation
with HDBSCAN (Hierarchical Density-Based Spatial Clustering)

æ”¹è‰¯ç‚¹:
- K-means/GMM â†’ HDBSCAN ã«å¤‰æ›´
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®è‡ªå‹•æ±ºå®šï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ï¼‰
- ãƒã‚¤ã‚ºæ¤œå‡ºæ©Ÿèƒ½ã«ã‚ˆã‚Šæ„å‘³ã®è–„ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–
- éšå±¤æ€§ã‚’æŒã¤condensed treeã®æ´»ç”¨

åˆ©ç‚¹:
âœ… ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®è‡ªå‹•æ±ºå®š
âœ… ãƒã‚¤ã‚ºï¼ˆæ„å‘³ã®è–„ã„ãƒãƒ£ãƒ³ã‚¯ï¼‰ã®æ¤œå‡ºãƒ»é™¤å¤–
âœ… å¯†åº¦ãƒ™ãƒ¼ã‚¹ã§è‡ªç„¶ãªã‚¯ãƒ©ã‚¹ã‚¿å½¢æˆ
âœ… é«˜æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ï¼ˆmxbai-embed-largeç­‰ï¼‰ã¨ç›¸æ€§è‰¯å¥½
âœ… condensed treeã«ã‚ˆã‚‹çœŸã®éšå±¤æ€§

Required packages:
pip install -U langchain langchain-community langchain-ollama scikit-learn numpy hdbscan
"""

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
import hdbscan
import numpy as np
from typing import List, Dict, Optional, Tuple
import uuid


class RAPTORRetrieverHDBSCAN:
    """
    RAPTOR with HDBSCAN: å¯†åº¦ãƒ™ãƒ¼ã‚¹ã®éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    ãƒã‚¤ã‚ºæ¤œå‡ºã«ã‚ˆã‚‹æ„å‘³ã®è–„ã„ãƒãƒ£ãƒ³ã‚¯ã®é™¤å¤–æ©Ÿèƒ½ä»˜ã
    """
    
    def __init__(
        self,
        embeddings_model,
        llm,
        min_cluster_size: int = 15,
        min_samples: int = 5,
        max_depth: int = 3,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        cluster_selection_method: str = 'eom',  # 'eom' or 'leaf'
        metric: str = 'euclidean',  # 'euclidean' or 'cosine'
        exclude_noise: bool = True
    ):
        """
        Args:
            embeddings_model: Embeddings model for vectorization
            llm: LLM for summarization
            min_cluster_size: HDBSCANã®æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚ºï¼ˆé‡è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
            min_samples: å¯†åº¦æ¨å®šã®ãŸã‚ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
            max_depth: Maximum tree depth
            chunk_size: Document chunk size
            chunk_overlap: Overlap between chunks
            cluster_selection_method: 'eom' (Excess of Mass) or 'leaf'
            metric: Distance metric ('euclidean' or 'cosine')
            exclude_noise: ãƒã‚¤ã‚ºãƒãƒ£ãƒ³ã‚¯ã‚’é™¤å¤–ã™ã‚‹ã‹
        """
        self.embeddings_model = embeddings_model
        self.llm = llm
        self.min_cluster_size = min_cluster_size
        self.min_samples = min_samples
        self.max_depth = max_depth
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.cluster_selection_method = cluster_selection_method
        self.metric = metric
        self.exclude_noise = exclude_noise
        self.tree_structure = {}
        self.noise_stats = {
            'total_noise_chunks': 0,
            'noise_by_depth': {}
        }
        
    def load_and_split_documents(self, file_path: str, encoding: str = "utf-8") -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        loader = TextLoader(file_path, encoding=encoding)
        docs = loader.load()
        
        print(f"Loaded document length: {len(docs[0].page_content)} characters")
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        chunks = splitter.split_documents(docs)
        print(f"Split into {len(chunks)} chunks")
        
        return chunks
    
    def embed_documents(self, documents: List[Document]) -> np.ndarray:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        texts = [doc.page_content for doc in documents]
        embeddings = self.embeddings_model.embed_documents(texts)
        return np.array(embeddings)
    
    def cluster_documents_hdbscan(
        self, 
        embeddings: np.ndarray
    ) -> Tuple[np.ndarray, Dict[str, any]]:
        """
        HDBSCANã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        
        Returns:
            cluster_labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«ï¼ˆ-1ã¯ãƒã‚¤ã‚ºï¼‰
            stats: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆæƒ…å ±
        """
        n_samples = len(embeddings)
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã™ãã‚‹å ´åˆ
        if n_samples < self.min_cluster_size:
            print(f"âš ï¸  Sample size ({n_samples}) < min_cluster_size ({self.min_cluster_size})")
            print(f"   Creating single cluster with all documents")
            return np.zeros(n_samples, dtype=int), {
                'n_clusters': 1,
                'n_noise': 0,
                'noise_ratio': 0.0
            }
        
        # ã‚³ã‚µã‚¤ãƒ³è·é›¢ã‚’ä½¿ã†å ´åˆã¯ã€åŸ‹ã‚è¾¼ã¿ã‚’æ­£è¦åŒ–ã—ã¦ã‹ã‚‰Euclideanè·é›¢ã‚’ä½¿ç”¨
        # ã“ã‚Œã«ã‚ˆã‚Šã‚³ã‚µã‚¤ãƒ³è·é›¢ã¨åŒç­‰ã®çµæœãŒå¾—ã‚‰ã‚Œã‚‹
        if self.metric == 'cosine':
            print(f"ğŸ”„ Normalizing embeddings for cosine similarity...")
            # L2æ­£è¦åŒ–
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            embeddings_normalized = embeddings / (norms + 1e-10)  # ã‚¼ãƒ­é™¤ç®—å›é¿
            metric_to_use = 'euclidean'
            embeddings_to_use = embeddings_normalized
        else:
            metric_to_use = self.metric
            embeddings_to_use = embeddings
        
        # HDBSCANå®Ÿè¡Œ
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=self.min_cluster_size,
            min_samples=self.min_samples,
            metric=metric_to_use,
            cluster_selection_method=self.cluster_selection_method,
            core_dist_n_jobs=-1  # ä¸¦åˆ—å‡¦ç†
        )
        
        cluster_labels = clusterer.fit_predict(embeddings_to_use)
        
        # çµ±è¨ˆæƒ…å ±ã‚’åé›†
        unique_labels = set(cluster_labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        noise_ratio = n_noise / n_samples if n_samples > 0 else 0
        
        stats = {
            'n_clusters': n_clusters,
            'n_noise': n_noise,
            'noise_ratio': noise_ratio,
            'cluster_sizes': {},
            'clusterer': clusterer  # condensed treeã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ç”¨
        }
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚µã‚¤ã‚º
        for label in unique_labels:
            if label != -1:
                stats['cluster_sizes'][label] = list(cluster_labels).count(label)
        
        print(f"\nğŸ” HDBSCAN Clustering Results:")
        print(f"   Clusters found: {n_clusters}")
        print(f"   Noise points: {n_noise} ({noise_ratio*100:.1f}%)")
        if n_clusters > 0:
            print(f"   Cluster sizes: {stats['cluster_sizes']}")
        
        return cluster_labels, stats
    
    def summarize_cluster(self, documents: List[Document]) -> str:
        """ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦ç´„"""
        # è¤‡æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’çµåˆ
        combined_text = "\n\n".join([doc.page_content for doc in documents])
        
        # è¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = ChatPromptTemplate.from_template(
            "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ä¿æŒã—ãªãŒã‚‰ã€"
            "å…¨ä½“ã®å†…å®¹ã‚’200-300æ–‡å­—ç¨‹åº¦ã§ã¾ã¨ã‚ã¦ãã ã•ã„:\n\n{text}"
        )
        
        chain = prompt | self.llm | StrOutputParser()
        summary = chain.invoke({"text": combined_text[:4000]})  # ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾ç­–
        
        return summary
    
    def build_tree(self, documents: List[Document], depth: int = 0) -> Dict:
        """å†å¸°çš„ã«ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’æ§‹ç¯‰ï¼ˆHDBSCANç‰ˆï¼‰"""
        print(f"\n{'='*80}")
        print(f"Building tree at depth {depth} with {len(documents)} documents")
        print(f"{'='*80}")
        
        if depth >= self.max_depth or len(documents) < self.min_cluster_size:
            print(f"âœ‹ Reached max depth or insufficient documents. Creating leaf node.")
            return {
                'depth': depth,
                'documents': documents,
                'is_leaf': True
            }
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        embeddings = self.embed_documents(documents)
        
        # HDBSCANã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        cluster_labels, stats = self.cluster_documents_hdbscan(embeddings)
        
        # ãƒã‚¤ã‚ºçµ±è¨ˆã‚’è¨˜éŒ²
        if depth not in self.noise_stats['noise_by_depth']:
            self.noise_stats['noise_by_depth'][depth] = 0
        self.noise_stats['noise_by_depth'][depth] += stats['n_noise']
        self.noise_stats['total_noise_chunks'] += stats['n_noise']
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã†
        if stats['n_clusters'] == 0:
            print(f"âš ï¸  No clusters found. Creating leaf node.")
            return {
                'depth': depth,
                'documents': documents,
                'is_leaf': True
            }
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å‡¦ç†
        clusters = {}
        summaries = []
        
        unique_labels = set(cluster_labels)
        
        for cluster_id in unique_labels:
            # ãƒã‚¤ã‚ºã‚’é™¤å¤–
            if cluster_id == -1:
                if self.exclude_noise:
                    print(f"ğŸ—‘ï¸  Excluding {stats['n_noise']} noise points")
                    continue
                else:
                    print(f"âš ï¸  Including {stats['n_noise']} noise points in separate cluster")
            
            cluster_docs = [doc for i, doc in enumerate(documents) if cluster_labels[i] == cluster_id]
            
            if len(cluster_docs) == 0:
                continue
            
            cluster_label = "noise" if cluster_id == -1 else str(cluster_id)
            print(f"\nğŸ“¦ Cluster {cluster_label}: {len(cluster_docs)} documents")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ç´„ã‚’ç”Ÿæˆ
            summary = self.summarize_cluster(cluster_docs)
            summary_doc = Document(
                page_content=summary,
                metadata={
                    'type': 'summary',
                    'depth': depth,
                    'cluster_id': cluster_id,
                    'num_source_docs': len(cluster_docs),
                    'is_noise': cluster_id == -1
                }
            )
            summaries.append(summary_doc)
            
            # å†å¸°çš„ã«å­ãƒãƒ¼ãƒ‰ã‚’æ§‹ç¯‰
            clusters[cluster_id] = {
                'summary': summary_doc,
                'documents': cluster_docs,
                'children': self.build_tree(cluster_docs, depth + 1)
            }
        
        return {
            'depth': depth,
            'clusters': clusters,
            'summaries': summaries,
            'is_leaf': False,
            'hdbscan_stats': stats
        }
    
    def search_tree(self, tree: Dict, query: str, top_k: int = 3) -> List[Document]:
        """ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’æ¤œç´¢"""
        if tree.get('is_leaf', False):
            # ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰: æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿”ã™
            docs = tree['documents']
            if len(docs) == 0:
                return []
            
            # ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
            query_embedding = np.array(self.embeddings_model.embed_query(query))
            doc_embeddings = self.embed_documents(docs)
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
            similarities = np.dot(doc_embeddings, query_embedding) / (
                np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_embedding)
            )
            
            # Top-k ã‚’é¸æŠ
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                doc = docs[idx]
                doc.metadata['similarity'] = float(similarities[idx])
                results.append(doc)
            
            return results
        
        # å†…éƒ¨ãƒãƒ¼ãƒ‰: è¦ç´„ã‚’æ¤œç´¢ã—ã€æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ã«é€²ã‚€
        summaries = tree.get('summaries', [])
        if not summaries:
            return []
        
        # ã‚¯ã‚¨ãƒªã¨è¦ç´„ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        query_embedding = np.array(self.embeddings_model.embed_query(query))
        summary_embeddings = self.embed_documents(summaries)
        
        similarities = np.dot(summary_embeddings, query_embedding) / (
            np.linalg.norm(summary_embeddings, axis=1) * np.linalg.norm(query_embedding)
        )
        
        # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠ
        best_cluster_idx = np.argmax(similarities)
        cluster_id = summaries[best_cluster_idx].metadata['cluster_id']
        
        # é¸æŠã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ã®å­ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢
        best_cluster = tree['clusters'][cluster_id]
        return self.search_tree(best_cluster['children'], query, top_k)
    
    def index(self, file_path: str, encoding: str = "utf-8"):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ RAPTOR with HDBSCAN - Document Indexing")
        print(f"{'='*80}")
        print(f"ğŸ“„ File: {file_path}")
        print(f"ğŸ“Š Parameters:")
        print(f"   - min_cluster_size: {self.min_cluster_size}")
        print(f"   - min_samples: {self.min_samples}")
        print(f"   - max_depth: {self.max_depth}")
        print(f"   - metric: {self.metric}")
        print(f"   - exclude_noise: {self.exclude_noise}")
        print(f"{'='*80}")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã¨åˆ†å‰²
        documents = self.load_and_split_documents(file_path, encoding)
        
        # ãƒ„ãƒªãƒ¼æ§‹é€ ã®æ§‹ç¯‰
        self.tree_structure = self.build_tree(documents)
        
        # ãƒã‚¤ã‚ºçµ±è¨ˆã‚’è¡¨ç¤º
        print(f"\n{'='*80}")
        print(f"ğŸ—‘ï¸  Noise Statistics")
        print(f"{'='*80}")
        print(f"   Total noise chunks excluded: {self.noise_stats['total_noise_chunks']}")
        print(f"   Noise by depth:")
        for depth, count in sorted(self.noise_stats['noise_by_depth'].items()):
            print(f"     Depth {depth}: {count} chunks")
        print(f"{'='*80}")
        
        print(f"\nâœ… Indexing complete!")
    
    def retrieve(self, query: str, top_k: int = 3) -> List[Document]:
        """ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢"""
        if not self.tree_structure:
            raise ValueError("No documents indexed. Call index() first.")
        
        print(f"\nğŸ” Query: {query}")
        results = self.search_tree(self.tree_structure, query, top_k)
        print(f"âœ… Found {len(results)} results")
        
        return results


if __name__ == "__main__":
    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    print("RAPTOR with HDBSCAN - Demo")
    print("=" * 80)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    llm = ChatOllama(
        model="granite-code:8b",
        base_url="http://localhost:11434",
        temperature=0
    )
    
    embeddings_model = OllamaEmbeddings(
        model="mxbai-embed-large",
        base_url="http://localhost:11434"
    )
    
    # RAPTOR with HDBSCAN
    raptor = RAPTORRetrieverHDBSCAN(
        embeddings_model=embeddings_model,
        llm=llm,
        min_cluster_size=15,
        min_samples=5,
        max_depth=2,
        chunk_size=1000,
        chunk_overlap=200,
        exclude_noise=True
    )
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
    raptor.index("test.txt")
    
    # æ¤œç´¢
    results = raptor.retrieve("philosophy", top_k=3)
    
    print("\n" + "=" * 80)
    print("Search Results:")
    print("=" * 80)
    for i, doc in enumerate(results, 1):
        similarity = doc.metadata.get('similarity', 'N/A')
        print(f"\n{i}. Similarity: {similarity}")
        print(f"   Content: {doc.page_content[:200]}...")
    print("\n" + "=" * 80)
