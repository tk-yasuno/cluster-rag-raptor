# HDBSCANç‰ˆRAPTOR å®Ÿè£…ãƒãƒ¼ãƒˆ

## ğŸ“‹ å®Ÿè£…ã®è¦ç‚¹

### ä¸»è¦ãªå¤‰æ›´ç‚¹

1. **ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **
   ```python
   # å¾“æ¥: K-means (å›ºå®šã‚¯ãƒ©ã‚¹ã‚¿æ•°)
   kmeans = KMeans(n_clusters=max_clusters)
   labels = kmeans.fit_predict(embeddings)
   
   # HDBSCANç‰ˆ: å¯†åº¦ãƒ™ãƒ¼ã‚¹ (è‡ªå‹•ã‚¯ãƒ©ã‚¹ã‚¿æ•°)
   clusterer = hdbscan.HDBSCAN(
       min_cluster_size=15,
       min_samples=5,
       metric='cosine'
   )
   labels = clusterer.fit_predict(embeddings)
   # labels == -1 ã¯ãƒã‚¤ã‚º
   ```

2. **ãƒã‚¤ã‚ºå‡¦ç†**
   ```python
   # ãƒã‚¤ã‚ºé™¤å¤–
   for label in unique_labels:
       if label == -1:
           if self.exclude_noise:
               print(f"ğŸ—‘ï¸  Excluding {n_noise} noise points")
               continue
   ```

3. **çµ±è¨ˆæƒ…å ±ã®è¿½è·¡**
   ```python
   self.noise_stats = {
       'total_noise_chunks': 0,
       'noise_by_depth': {}
   }
   ```

## ğŸ”¬ æŠ€è¡“çš„è©³ç´°

### HDBSCANãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿

#### `min_cluster_size`
- **å°ã•ã„å€¤ (5-10)**: å¤šãã®å°ã‚¯ãƒ©ã‚¹ã‚¿ã€å°‘ãªã„ãƒã‚¤ã‚º
- **ä¸­é–“å€¤ (10-20)**: ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆæ¨å¥¨ï¼‰
- **å¤§ãã„å€¤ (20-30)**: å°‘ãªã„å¤§ã‚¯ãƒ©ã‚¹ã‚¿ã€å¤šã„ãƒã‚¤ã‚º

#### `min_samples`
- **å…¬å¼æ¨å¥¨**: `min_cluster_size`ã¨åŒã˜å€¤
- **ä¸€èˆ¬çš„**: `min_cluster_size / 3`
- **åŠ¹æœ**: ãƒã‚¤ã‚ºåˆ¤å®šã®å³ã—ã•ã‚’èª¿æ•´

#### `metric`
- **`'euclidean'`**: L2è·é›¢ã€æ±ç”¨çš„
- **`'cosine'`**: ã‚³ã‚µã‚¤ãƒ³è·é›¢ã€æ–¹å‘æ€§é‡è¦–
- **`'manhattan'`**: L1è·é›¢ã€å¤–ã‚Œå€¤ã«å¼·ã„

### è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯é¸æŠã®ç†è«–

```python
# mxbai-embed-largeã®ã‚ˆã†ãªæ„å‘³çš„embeddings
# â†’ æ–¹å‘æ€§ãŒé‡è¦ â†’ cosineæ¨å¥¨
metric = 'cosine'

# ä¸€èˆ¬çš„ãªæ•°å€¤ç‰¹å¾´é‡
# â†’ è·é›¢ãŒé‡è¦ â†’ euclideanæ¨å¥¨
metric = 'euclidean'
```

**ç†ç”±**: æ„å‘³çš„embeddingsã¯æ­£è¦åŒ–ã•ã‚Œã¦ãŠã‚Šã€ãƒ™ã‚¯ãƒˆãƒ«ã®æ–¹å‘ãŒæ„å‘³ã‚’è¡¨ã™ã€‚
é•·ã•ã¯é‡è¦ã§ãªã„ â†’ ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãŒé©åˆ‡ã€‚

## ğŸ¯ å®Ÿè£…ä¸Šã®å·¥å¤«

### 1. ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³ã¸ã®å¯¾å¿œ

```python
if n_samples < self.min_cluster_size:
    print(f"âš ï¸  Sample size too small")
    return np.zeros(n_samples, dtype=int), {
        'n_clusters': 1,
        'n_noise': 0
    }
```

### 2. å†å¸°çµ‚äº†æ¡ä»¶ã®èª¿æ•´

```python
# K-meansç‰ˆ: max_clustersã¨æ¯”è¼ƒ
if len(documents) <= self.max_clusters:
    return leaf_node

# HDBSCANç‰ˆ: min_cluster_sizeã¨æ¯”è¼ƒ
if len(documents) < self.min_cluster_size:
    return leaf_node
```

### 3. ã‚¯ãƒ©ã‚¹ã‚¿0å€‹ã®å‡¦ç†

```python
if stats['n_clusters'] == 0:
    print(f"âš ï¸  No clusters found. Creating leaf node.")
    return {
        'depth': depth,
        'documents': documents,
        'is_leaf': True
    }
```

## ğŸ“Š æ€§èƒ½æœ€é©åŒ–

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›

```python
# condensed treeã¯å¤§ãã„ã®ã§ã€å¿…è¦æ™‚ã®ã¿ä¿æŒ
# stats['clusterer'] ã«æ ¼ç´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
if need_condensed_tree:
    stats['clusterer'] = clusterer
```

### ä¸¦åˆ—å‡¦ç†ã®æ´»ç”¨

```python
clusterer = hdbscan.HDBSCAN(
    core_dist_n_jobs=-1  # å…¨CPUã‚³ã‚¢ã‚’ä½¿ç”¨
)
```

## ğŸ§ª å®Ÿé¨“è¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### æ¯”è¼ƒå®Ÿé¨“ã®ãƒã‚¤ãƒ³ãƒˆ

1. **åŒä¸€ãƒ‡ãƒ¼ã‚¿ã§è¤‡æ•°æ‰‹æ³•ã‚’æ¯”è¼ƒ**
   ```python
   methods = ['kmeans', 'gmm_bic', 'hdbscan']
   for method in methods:
       build_time = measure_build(method)
       search_quality = measure_search(method)
   ```

2. **è©•ä¾¡æŒ‡æ¨™**
   - ãƒ“ãƒ«ãƒ‰æ™‚é–“
   - ã‚¯ã‚¨ãƒªæ™‚é–“
   - æ¤œç´¢ç²¾åº¦ï¼ˆtop-k similarityï¼‰
   - ãƒã‚¤ã‚ºé™¤å»æ•°
   - ã‚¯ãƒ©ã‚¹ã‚¿æ•°

3. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ**
   ```python
   min_cluster_sizes = [5, 10, 15, 20]
   metrics = ['euclidean', 'cosine']
   
   for size in min_cluster_sizes:
       for metric in metrics:
           test_configuration(size, metric)
   ```

## ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ³•

#### å•é¡Œ1: ã™ã¹ã¦ãŒãƒã‚¤ã‚ºã«ãªã‚‹
```python
# åŸå› : min_cluster_sizeãŒå¤§ãã™ãã‚‹
# è§£æ±º: å€¤ã‚’æ¸›ã‚‰ã™
min_cluster_size = 5  # ã‚ˆã‚Šå°ã•ã
```

#### å•é¡Œ2: ã‚¯ãƒ©ã‚¹ã‚¿ãŒ1ã¤ã ã‘
```python
# åŸå› : min_cluster_sizeãŒå°ã•ã™ãã‚‹ or ãƒ‡ãƒ¼ã‚¿ãŒå‡è³ª
# è§£æ±º: å€¤ã‚’å¢—ã‚„ã™ or metricã‚’å¤‰æ›´
min_cluster_size = 20
metric = 'cosine'  # euclideanã‹ã‚‰å¤‰æ›´
```

#### å•é¡Œ3: å®Ÿè¡ŒãŒé…ã„
```python
# åŸå› : condensed treeè¨ˆç®—ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
# è§£æ±º: ãƒ‡ãƒ¼ã‚¿ã‚’æ¸›ã‚‰ã™ or max_depthã‚’ä¸‹ã’ã‚‹
max_depth = 1
chunk_size = 2000  # ã‚ˆã‚Šå¤§ãã
```

## ğŸ’» ã‚³ãƒ¼ãƒ‰æ§‹é€ 

### ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

```
RAPTORRetrieverHDBSCAN
â”œâ”€â”€ __init__()               # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
â”œâ”€â”€ cluster_documents_hdbscan()  # HDBSCANã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
â”œâ”€â”€ build_tree()             # å†å¸°çš„ãƒ„ãƒªãƒ¼æ§‹ç¯‰
â”œâ”€â”€ search_tree()            # ãƒ„ãƒªãƒ¼æ¤œç´¢
â”œâ”€â”€ index()                  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
â””â”€â”€ retrieve()               # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
```

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

```
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    â†“ load_and_split_documents()
ãƒãƒ£ãƒ³ã‚¯
    â†“ embed_documents()
åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
    â†“ cluster_documents_hdbscan()
ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ« (-1=ãƒã‚¤ã‚º)
    â†“ build_tree() (å†å¸°)
ãƒ„ãƒªãƒ¼æ§‹é€ 
    â†“ search_tree()
æ¤œç´¢çµæœ
```

## ğŸ”® ä»Šå¾Œã®æ‹¡å¼µã‚¢ã‚¤ãƒ‡ã‚¢

### 1. Condensed Treeã®æ´»ç”¨

```python
# condensed treeã‹ã‚‰æœ€é©ãªåˆ‡æ–­ãƒ¬ãƒ™ãƒ«ã‚’é¸æŠ
tree = clusterer.condensed_tree_
persistence = tree.to_pandas()
optimal_clusters = select_by_stability(persistence)
```

### 2. Soft Clustering

```python
# ç¢ºç‡çš„ãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—ã®æ´»ç”¨
soft_clusters = hdbscan.all_points_membership_vectors(clusterer)
# è¤‡æ•°ã‚¯ãƒ©ã‚¹ã‚¿ã«æ‰€å±ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†
```

### 3. å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

```python
# æ·±ã•ã«å¿œã˜ã¦min_cluster_sizeã‚’å¤‰æ›´
def adaptive_min_cluster_size(depth, base_size=15):
    return max(5, base_size - depth * 5)
```

### 4. ãƒã‚¤ã‚ºãƒãƒ£ãƒ³ã‚¯ã®å†åˆ©ç”¨

```python
# ãƒã‚¤ã‚ºã‚’åˆ¥é€”ä¿å­˜ã—ã€ç‰¹æ®Šãªã‚¯ã‚¨ãƒªã§æ´»ç”¨
noise_chunks = [doc for i, doc in enumerate(documents) 
                if labels[i] == -1]
self.noise_index = create_separate_index(noise_chunks)
```

## ğŸ“š ç†è«–çš„èƒŒæ™¯

### Mutual Reachability Distance

HDBSCANã®æ ¸å¿ƒ:

```
d_mreach(a, b) = max(core_distance(a), core_distance(b), d(a, b))
```

- `core_distance(a)`: ç‚¹aã® k-nearest neighborè·é›¢
- `d(a, b)`: ç‚¹a, bé–“ã®å…ƒã®è·é›¢
- ã“ã‚Œã«ã‚ˆã‚Šãƒã‚¤ã‚ºã«é ‘å¥ãªè·é›¢ã‚’å®šç¾©

### Excess of Mass (EoM)

ã‚¯ãƒ©ã‚¹ã‚¿é¸æŠåŸºæº–:

```
EoM(C) = âˆ« (Î» - Î»_min) dÎ»
```

- ã‚¯ãƒ©ã‚¹ã‚¿ã®ã€Œå®‰å®šæ€§ã€ã‚’æ¸¬å®š
- ã‚ˆã‚Šé•·ãå­˜åœ¨ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å„ªå…ˆ

## âœ… å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [x] HDBSCANã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®å®Ÿè£…
- [x] ãƒã‚¤ã‚ºæ¤œå‡ºãƒ»é™¤å¤–æ©Ÿèƒ½
- [x] çµ±è¨ˆæƒ…å ±ã®è¿½è·¡
- [x] è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯é¸æŠï¼ˆeuclidean/cosineï¼‰
- [x] ã‚µãƒ³ãƒ—ãƒ«æ•°ä¸è¶³ã¸ã®å¯¾å¿œ
- [x] å†å¸°çµ‚äº†æ¡ä»¶ã®èª¿æ•´
- [x] æ¯”è¼ƒå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- [x] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½
- [x] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
- [ ] Condensed treeæ´»ç”¨ï¼ˆå°†æ¥çš„ï¼‰
- [ ] Soft clusteringï¼ˆå°†æ¥çš„ï¼‰
- [ ] å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼ˆå°†æ¥çš„ï¼‰

## ğŸ“ å‚è€ƒå®Ÿè£…

### K-meansç‰ˆã¨ã®å¯¾æ¯”

```python
# K-meansç‰ˆ
def cluster_documents(embeddings, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters)
    return kmeans.fit_predict(embeddings)

# HDBSCANç‰ˆ
def cluster_documents_hdbscan(embeddings):
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=15,
        metric='cosine'
    )
    labels = clusterer.fit_predict(embeddings)
    # ãƒã‚¤ã‚ºå‡¦ç†ã‚’è¿½åŠ 
    n_noise = list(labels).count(-1)
    return labels, {'n_noise': n_noise}
```

---

**å®Ÿè£…è€…:** Takato Yasuno  
**å®Ÿè£…æ—¥:** 2025å¹´10æœˆ16æ—¥  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³:** 1.0
