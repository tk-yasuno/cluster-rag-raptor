"""
RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) Implementation
with GMM (Gaussian Mixture Model) and BIC-based optimal cluster selection

æ”¹è‰¯ç‚¹:
- K-means â†’ GMM (Gaussian Mixture Model) ã«å¤‰æ›´
- BIC (Bayesian Information Criterion) ã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®è‡ªå‹•é¸æŠ
- ã‚ˆã‚ŠæŸ”è»Ÿãªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ¥•å††å½¢ã‚¯ãƒ©ã‚¹ã‚¿ã«å¯¾å¿œï¼‰

Required packages:
pip install -U langchain langchain-community langchain-ollama scikit-learn numpy
"""

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans
import numpy as np
from typing import List, Dict, Optional, Tuple
import uuid


class RAPTORRetrieverGMM:
    """
    RAPTOR with GMM: Gaussian Mixture Model based clustering
    BICã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®è‡ªå‹•é¸æŠæ©Ÿèƒ½ä»˜ã
    """
    
    def __init__(
        self,
        embeddings_model,
        llm,
        max_clusters: int = 5,
        min_clusters: int = 2,
        max_depth: int = 3,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        use_bic: bool = True,
        clustering_method: str = "gmm"  # "gmm" or "kmeans"
    ):
        """
        Args:
            embeddings_model: Embeddings model for vectorization
            llm: LLM for summarization
            max_clusters: Maximum number of clusters to consider
            min_clusters: Minimum number of clusters to consider
            max_depth: Maximum tree depth
            chunk_size: Document chunk size
            chunk_overlap: Overlap between chunks
            use_bic: If True, use BIC to automatically select optimal cluster count
            clustering_method: "gmm" (Gaussian Mixture) or "kmeans" (K-means)
        """
        self.embeddings_model = embeddings_model
        self.llm = llm
        self.max_clusters = max_clusters
        self.min_clusters = min_clusters
        self.max_depth = max_depth
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_bic = use_bic
        self.clustering_method = clustering_method
        self.tree_structure = {}
        
    def load_and_split_documents(self, file_path: str, encoding: str = "utf-8") -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        loader = TextLoader(file_path, encoding=encoding)
        docs = loader.load()
        
        print(f"Loaded document length: {len(docs[0].page_content)} characters")
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        chunks = splitter.split_documents(docs)
        print(f"Split into {len(chunks)} chunks")
        
        return chunks
    
    def embed_documents(self, documents: List[Document]) -> np.ndarray:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        texts = [doc.page_content for doc in documents]
        embeddings = self.embeddings_model.embed_documents(texts)
        return np.array(embeddings)
    
    def find_optimal_clusters_bic(self, embeddings: np.ndarray) -> Tuple[int, List[float]]:
        """
        BIC (Bayesian Information Criterion) ã‚’ä½¿ã£ã¦æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’é¸æŠ
        
        BICãŒæœ€å°ã¨ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒæœ€é©
        BIC = -2 * log-likelihood + k * log(n)
        ã“ã“ã§ã€k = ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€n = ã‚µãƒ³ãƒ—ãƒ«æ•°
        
        Returns:
            optimal_n_clusters: æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°
            bic_scores: å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã§ã®BICã‚¹ã‚³ã‚¢
        """
        n_samples = len(embeddings)
        
        # æœ€å°ãƒ»æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®åˆ¶ç´„
        min_k = max(self.min_clusters, 2)
        max_k = min(self.max_clusters, n_samples - 1)
        
        if max_k < min_k:
            print(f"âš ï¸  Sample size ({n_samples}) too small for clustering. Using 1 cluster.")
            return 1, [0.0]
        
        print(f"\nğŸ” Searching for optimal cluster count using BIC...")
        print(f"   Range: {min_k} to {max_k} clusters")
        
        bic_scores = []
        cluster_range = range(min_k, max_k + 1)
        
        for n_clusters in cluster_range:
            try:
                if self.clustering_method == "gmm":
                    gmm = GaussianMixture(
                        n_components=n_clusters,
                        covariance_type='full',
                        random_state=42,
                        n_init=3
                    )
                    gmm.fit(embeddings)
                    bic = gmm.bic(embeddings)
                else:  # kmeans doesn't have native BIC, so we approximate
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    kmeans.fit(embeddings)
                    # Approximate BIC for K-means
                    labels = kmeans.labels_
                    centers = kmeans.cluster_centers_
                    # Calculate within-cluster sum of squares
                    wcss = sum([np.sum((embeddings[labels == i] - centers[i]) ** 2) 
                               for i in range(n_clusters)])
                    # Approximate log-likelihood
                    log_likelihood = -wcss
                    # BIC approximation: -2*log(L) + k*log(n)
                    k = n_clusters * embeddings.shape[1]  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
                    bic = -2 * log_likelihood + k * np.log(n_samples)
                
                bic_scores.append(bic)
                print(f"   k={n_clusters}: BIC={bic:.2f}")
                
            except Exception as e:
                print(f"   k={n_clusters}: Failed ({e})")
                bic_scores.append(float('inf'))
        
        # BICãŒæœ€å°ã¨ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’é¸æŠ
        optimal_idx = np.argmin(bic_scores)
        optimal_n_clusters = list(cluster_range)[optimal_idx]
        
        print(f"\nâœ… Optimal cluster count: {optimal_n_clusters} (BIC={bic_scores[optimal_idx]:.2f})")
        
        return optimal_n_clusters, bic_scores
    
    def cluster_documents_gmm(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:
        """
        Gaussian Mixture Model ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        
        GMMã®åˆ©ç‚¹:
        - ã‚½ãƒ•ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆç¢ºç‡çš„å‰²ã‚Šå½“ã¦ï¼‰
        - æ¥•å††å½¢ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«å¯¾å¿œ
        - å…±åˆ†æ•£è¡Œåˆ—ã‚’è€ƒæ…®
        """
        # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚ˆã‚Šå¤šã„å ´åˆã¯èª¿æ•´
        n_clusters = min(n_clusters, len(embeddings))
        if n_clusters < 2:
            return np.zeros(len(embeddings), dtype=int)
        
        gmm = GaussianMixture(
            n_components=n_clusters,
            covariance_type='full',  # 'full', 'tied', 'diag', 'spherical'
            random_state=42,
            n_init=3
        )
        cluster_labels = gmm.fit_predict(embeddings)
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ç¢ºç‡ã‚‚å–å¾—å¯èƒ½
        # probabilities = gmm.predict_proba(embeddings)
        
        return cluster_labels
    
    def cluster_documents_kmeans(self, embeddings: np.ndarray, n_clusters: int) -> np.ndarray:
        """K-meansã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰"""
        n_clusters = min(n_clusters, len(embeddings))
        if n_clusters < 2:
            return np.zeros(len(embeddings), dtype=int)
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(embeddings)
        return cluster_labels
    
    def cluster_documents(self, embeddings: np.ndarray, n_clusters: Optional[int] = None) -> np.ndarray:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        
        Args:
            embeddings: Document embeddings
            n_clusters: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ï¼ˆNoneã®å ´åˆã¯BICã§è‡ªå‹•é¸æŠï¼‰
        """
        # BICã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®è‡ªå‹•é¸æŠ
        if n_clusters is None and self.use_bic:
            n_clusters, bic_scores = self.find_optimal_clusters_bic(embeddings)
        elif n_clusters is None:
            n_clusters = self.max_clusters
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        if self.clustering_method == "gmm":
            return self.cluster_documents_gmm(embeddings, n_clusters)
        else:
            return self.cluster_documents_kmeans(embeddings, n_clusters)
    
    def summarize_cluster(self, documents: List[Document]) -> str:
        """ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦ç´„"""
        combined_text = "\n\n".join([doc.page_content for doc in documents])
        
        prompt = ChatPromptTemplate.from_template(
            "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ä¿æŒã—ãªãŒã‚‰ã€"
            "å…¨ä½“ã®å†…å®¹ã‚’200-300æ–‡å­—ç¨‹åº¦ã§ã¾ã¨ã‚ã¦ãã ã•ã„:\n\n{text}"
        )
        
        chain = prompt | self.llm | StrOutputParser()
        summary = chain.invoke({"text": combined_text[:4000]})
        
        return summary
    
    def build_tree(self, documents: List[Document], depth: int = 0) -> Dict:
        """å†å¸°çš„ã«ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’æ§‹ç¯‰"""
        print(f"\n=== Building tree at depth {depth} with {len(documents)} documents ===")
        
        if depth >= self.max_depth or len(documents) <= self.max_clusters:
            print(f"Reached max depth or minimal documents. Creating leaf node.")
            return {
                "type": "leaf",
                "documents": documents,
                "depth": depth
            }
        
        # Embeddingsç”Ÿæˆ
        embeddings = self.embed_documents(documents)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆBICã«ã‚ˆã‚‹æœ€é©æ•°é¸æŠï¼‰
        cluster_labels = self.cluster_documents(embeddings, n_clusters=None)
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã«å¯¾ã—ã¦å†å¸°çš„ã«å‡¦ç†
        clusters = {}
        for cluster_id in np.unique(cluster_labels):
            cluster_docs = [documents[i] for i in range(len(documents)) 
                           if cluster_labels[i] == cluster_id]
            
            print(f"Cluster {cluster_id}: {len(cluster_docs)} documents")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ç´„ã‚’ç”Ÿæˆ
            if len(cluster_docs) > 1:
                summary = self.summarize_cluster(cluster_docs)
                summary_doc = Document(
                    page_content=summary,
                    metadata={"type": "summary", "cluster_id": cluster_id, "depth": depth}
                )
            else:
                summary_doc = cluster_docs[0]
            
            # å­ãƒãƒ¼ãƒ‰ã‚’æ§‹ç¯‰
            child_node = self.build_tree(cluster_docs, depth + 1)
            clusters[cluster_id] = {
                "summary": summary_doc,
                "child": child_node
            }
        
        return {
            "type": "internal",
            "clusters": clusters,
            "depth": depth
        }
    
    def index(self, file_path: str):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–"""
        print("\n=== Starting RAPTOR Indexing (GMM-BIC) ===")
        print(f"Clustering method: {self.clustering_method.upper()}")
        print(f"BIC optimization: {'ON' if self.use_bic else 'OFF'}")
        
        documents = self.load_and_split_documents(file_path)
        self.tree_structure = self.build_tree(documents)
        print("\n=== RAPTOR Tree Construction Complete ===")
    
    def search_tree(self, query: str, node: Dict, depth: int = 0) -> List[Document]:
        """ãƒ„ãƒªãƒ¼ã‚’æ¤œç´¢"""
        if node["type"] == "leaf":
            return node["documents"]
        
        # ã‚¯ã‚¨ãƒªã®embeddingã‚’å–å¾—
        query_embedding = np.array(self.embeddings_model.embed_query(query))
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ç´„ã¨ã‚¯ã‚¨ãƒªã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        best_similarity = -1
        best_cluster = None
        
        for cluster_id, cluster_data in node["clusters"].items():
            summary_embedding = np.array(
                self.embeddings_model.embed_query(cluster_data["summary"].page_content)
            )
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            similarity = np.dot(query_embedding, summary_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(summary_embedding)
            )
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_cluster = cluster_id
        
        print(f"Selected cluster {best_cluster} at depth {depth} (similarity: {best_similarity:.4f})")
        
        # æœ€ã‚‚é¡ä¼¼ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ã‚’å†å¸°çš„ã«æ¤œç´¢
        return self.search_tree(
            query, 
            node["clusters"][best_cluster]["child"],
            depth + 1
        )
    
    def retrieve(self, query: str, top_k: int = 3) -> List[Document]:
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢"""
        print(f"\n=== Searching for: '{query}' ===")
        
        # ãƒ„ãƒªãƒ¼ã‚’æ¢ç´¢
        candidate_docs = self.search_tree(query, self.tree_structure)
        
        # ã‚¯ã‚¨ãƒªã¨ã®é¡ä¼¼åº¦ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        query_embedding = np.array(self.embeddings_model.embed_query(query))
        doc_embeddings = self.embed_documents(candidate_docs)
        
        similarities = []
        for doc_emb in doc_embeddings:
            sim = np.dot(query_embedding, doc_emb) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)
            )
            similarities.append(sim)
        
        # Top-k ã‚’è¿”ã™
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        results = [candidate_docs[i] for i in top_indices]
        
        # é¡ä¼¼åº¦ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        for i, idx in enumerate(top_indices):
            results[i].metadata['similarity'] = similarities[idx]
        
        return results
