"""
RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) Implementation
with CLASSIX (Clustering via Approximate Supervised Similarity Index)

CLASSIXã®åˆ©ç‚¹:
âœ… ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®è‡ªå‹•æ±ºå®š
âœ… é«˜é€Ÿãƒ»è»½é‡ï¼ˆHDBSCANã‚ˆã‚Šé«˜é€Ÿï¼‰
âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãèª¿æ•´ãŒå®¹æ˜“
âœ… è·é›¢ãƒ™ãƒ¼ã‚¹ã§æ„å‘³çš„åŸ‹ã‚è¾¼ã¿ã«å¯¾å¿œ
âœ… ãƒã‚¤ã‚ºé™¤å»æ©Ÿèƒ½

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
- radius: ã‚¯ãƒ©ã‚¹ã‚¿ã®åŠå¾„ï¼ˆå°ã•ã„ã»ã©ç´°ã‹ãåˆ†å‰²ï¼‰
- minPts: ã‚¯ãƒ©ã‚¹ã‚¿ã®æœ€å°ã‚µã‚¤ã‚º

Required packages:
pip install -U langchain langchain-community langchain-ollama scikit-learn numpy classix
"""

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from classix import CLASSIX
import numpy as np
from typing import List, Dict, Optional, Tuple
import uuid


class RAPTORRetrieverCLASSIX:
    """
    RAPTOR with CLASSIX: é«˜é€Ÿã§èª¿æ•´ãŒå®¹æ˜“ãªå¯†åº¦ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    """
    
    def __init__(
        self,
        embeddings_model,
        llm,
        radius: float = 0.5,
        minPts: int = 5,
        max_depth: int = 3,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        use_cosine: bool = True
    ):
        """
        Args:
            embeddings_model: Embeddings model for vectorization
            llm: LLM for summarization
            radius: ã‚¯ãƒ©ã‚¹ã‚¿ã®åŠå¾„ï¼ˆ0.3-0.8æ¨å¥¨ã€å°ã•ã„ã»ã©ç´°ã‹ãåˆ†å‰²ï¼‰
            minPts: ã‚¯ãƒ©ã‚¹ã‚¿ã®æœ€å°ã‚µã‚¤ã‚ºï¼ˆ3-10æ¨å¥¨ï¼‰
            max_depth: Maximum tree depth
            chunk_size: Document chunk size
            chunk_overlap: Overlap between chunks
            use_cosine: ã‚³ã‚µã‚¤ãƒ³è·é›¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆTrueæ¨å¥¨ï¼‰
        """
        self.embeddings_model = embeddings_model
        self.llm = llm
        self.radius = radius
        self.minPts = minPts
        self.max_depth = max_depth
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_cosine = use_cosine
        self.tree_structure = {}
        self.cluster_stats = {
            'total_clusters_by_depth': {},
            'noise_by_depth': {},
            'params': {
                'radius': radius,
                'minPts': minPts,
                'use_cosine': use_cosine
            }
        }
        
        print(f"ğŸš€ RAPTOR with CLASSIX initialized")
        print(f"   Parameters: radius={radius}, minPts={minPts}, use_cosine={use_cosine}")
        
    def load_and_split_documents(self, file_path: str, encoding: str = "utf-8") -> List[Document]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        loader = TextLoader(file_path, encoding=encoding)
        docs = loader.load()
        
        print(f"Loaded document length: {len(docs[0].page_content)} characters")
        
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        chunks = splitter.split_documents(docs)
        print(f"Split into {len(chunks)} chunks")
        
        return chunks
    
    def embed_documents(self, documents: List[Document]) -> np.ndarray:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–"""
        texts = [doc.page_content for doc in documents]
        embeddings = self.embeddings_model.embed_documents(texts)
        return np.array(embeddings)
    
    def cluster_documents_classix(
        self, 
        embeddings: np.ndarray
    ) -> Tuple[np.ndarray, Dict[str, any]]:
        """
        CLASSIXã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        
        Returns:
            cluster_labels: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ©ãƒ™ãƒ«ï¼ˆ-1ã¯ãƒã‚¤ã‚ºï¼‰
            stats: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆæƒ…å ±
        """
        n_samples = len(embeddings)
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã™ãã‚‹å ´åˆ
        if n_samples < self.minPts:
            print(f"âš ï¸  Sample size ({n_samples}) < minPts ({self.minPts})")
            print(f"   Creating single cluster with all documents")
            return np.zeros(n_samples, dtype=int), {
                'n_clusters': 1,
                'n_noise': 0,
                'noise_ratio': 0.0
            }
        
        # ã‚³ã‚µã‚¤ãƒ³è·é›¢ã‚’ä½¿ã†å ´åˆã¯æ­£è¦åŒ–
        if self.use_cosine:
            print(f"ğŸ”„ Normalizing embeddings for cosine similarity...")
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            embeddings_normalized = embeddings / (norms + 1e-10)
            embeddings_to_use = embeddings_normalized
        else:
            embeddings_to_use = embeddings
        
        # CLASSIXå®Ÿè¡Œ
        print(f"âš¡ Running CLASSIX clustering (radius={self.radius}, minPts={self.minPts})...")
        
        clusterer = CLASSIX(
            radius=self.radius,
            minPts=self.minPts,
            verbose=0
        )
        
        clusterer.fit(embeddings_to_use)
        cluster_labels = clusterer.labels_
        
        # çµ±è¨ˆæƒ…å ±ã‚’åé›†
        unique_labels = set(cluster_labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = int(np.sum(cluster_labels == -1))
        noise_ratio = n_noise / n_samples if n_samples > 0 else 0
        
        stats = {
            'n_clusters': n_clusters,
            'n_noise': n_noise,
            'noise_ratio': noise_ratio,
            'cluster_sizes': {}
        }
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚µã‚¤ã‚º
        for label in unique_labels:
            if label != -1:
                stats['cluster_sizes'][int(label)] = int(np.sum(cluster_labels == label))
        
        print(f"\nğŸ” CLASSIX Clustering Results:")
        print(f"   Clusters found: {n_clusters}")
        print(f"   Noise points: {n_noise} ({noise_ratio*100:.1f}%)")
        if n_clusters > 0:
            cluster_sizes_list = list(stats['cluster_sizes'].values())
            print(f"   Cluster size range: {min(cluster_sizes_list)} - {max(cluster_sizes_list)}")
            print(f"   Average cluster size: {np.mean(cluster_sizes_list):.1f}")
        
        return cluster_labels, stats
    
    def summarize_cluster(self, documents: List[Document]) -> str:
        """ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¦ç´„"""
        combined_text = "\n\n".join([doc.page_content for doc in documents])
        
        prompt = ChatPromptTemplate.from_template(
            "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ä¿æŒã—ãªãŒã‚‰ã€"
            "å…¨ä½“ã®å†…å®¹ã‚’200-300æ–‡å­—ç¨‹åº¦ã§ã¾ã¨ã‚ã¦ãã ã•ã„:\n\n{text}"
        )
        
        chain = prompt | self.llm | StrOutputParser()
        summary = chain.invoke({"text": combined_text[:4000]})
        
        return summary
    
    def build_tree(self, documents: List[Document], depth: int = 0) -> Dict:
        """å†å¸°çš„ã«ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’æ§‹ç¯‰ï¼ˆCLASSIXç‰ˆï¼‰"""
        print(f"\n{'='*80}")
        print(f"Building tree at depth {depth} with {len(documents)} documents")
        print(f"{'='*80}")
        
        if depth >= self.max_depth or len(documents) < self.minPts:
            print(f"âœ‹ Reached max depth or insufficient documents. Creating leaf node.")
            return {
                'depth': depth,
                'documents': documents,
                'is_leaf': True
            }
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        embeddings = self.embed_documents(documents)
        
        # CLASSIXã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        cluster_labels, stats = self.cluster_documents_classix(embeddings)
        
        # çµ±è¨ˆã‚’è¨˜éŒ²
        if depth not in self.cluster_stats['total_clusters_by_depth']:
            self.cluster_stats['total_clusters_by_depth'][depth] = 0
            self.cluster_stats['noise_by_depth'][depth] = 0
        
        self.cluster_stats['total_clusters_by_depth'][depth] += stats['n_clusters']
        self.cluster_stats['noise_by_depth'][depth] += stats['n_noise']
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã†
        if stats['n_clusters'] == 0:
            print(f"âš ï¸  No clusters found. Creating leaf node.")
            return {
                'depth': depth,
                'documents': documents,
                'is_leaf': True
            }
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å‡¦ç†
        clusters = {}
        summaries = []
        
        unique_labels = set(cluster_labels)
        
        for cluster_id in unique_labels:
            # ãƒã‚¤ã‚ºã‚’é™¤å¤–
            if cluster_id == -1:
                print(f"ğŸ—‘ï¸  Excluding {stats['n_noise']} noise points")
                continue
            
            cluster_docs = [doc for i, doc in enumerate(documents) if cluster_labels[i] == cluster_id]
            
            if len(cluster_docs) == 0:
                continue
            
            print(f"\nğŸ“¦ Cluster {cluster_id}: {len(cluster_docs)} documents")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ç´„ã‚’ç”Ÿæˆ
            summary = self.summarize_cluster(cluster_docs)
            summary_doc = Document(
                page_content=summary,
                metadata={
                    'type': 'summary',
                    'depth': depth,
                    'cluster_id': int(cluster_id),
                    'num_source_docs': len(cluster_docs)
                }
            )
            summaries.append(summary_doc)
            
            # å†å¸°çš„ã«å­ãƒãƒ¼ãƒ‰ã‚’æ§‹ç¯‰
            clusters[int(cluster_id)] = {
                'summary': summary_doc,
                'documents': cluster_docs,
                'children': self.build_tree(cluster_docs, depth + 1)
            }
        
        return {
            'depth': depth,
            'clusters': clusters,
            'summaries': summaries,
            'is_leaf': False,
            'classix_stats': stats
        }
    
    def search_tree(self, tree: Dict, query: str, top_k: int = 3) -> List[Document]:
        """ãƒ„ãƒªãƒ¼æ§‹é€ ã‚’æ¤œç´¢"""
        if tree.get('is_leaf', False):
            docs = tree['documents']
            if len(docs) == 0:
                return []
            
            # ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
            query_embedding = np.array(self.embeddings_model.embed_query(query))
            doc_embeddings = self.embed_documents(docs)
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
            similarities = np.dot(doc_embeddings, query_embedding) / (
                np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_embedding)
            )
            
            # Top-k ã‚’é¸æŠ
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                doc = docs[idx]
                doc.metadata['similarity'] = float(similarities[idx])
                results.append(doc)
            
            return results
        
        # å†…éƒ¨ãƒãƒ¼ãƒ‰
        summaries = tree.get('summaries', [])
        if not summaries:
            return []
        
        query_embedding = np.array(self.embeddings_model.embed_query(query))
        summary_embeddings = self.embed_documents(summaries)
        
        similarities = np.dot(summary_embeddings, query_embedding) / (
            np.linalg.norm(summary_embeddings, axis=1) * np.linalg.norm(query_embedding)
        )
        
        best_cluster_idx = np.argmax(similarities)
        cluster_id = summaries[best_cluster_idx].metadata['cluster_id']
        
        best_cluster = tree['clusters'][cluster_id]
        return self.search_tree(best_cluster['children'], query, top_k)
    
    def index(self, file_path: str, encoding: str = "utf-8"):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ RAPTOR with CLASSIX - Document Indexing")
        print(f"{'='*80}")
        print(f"ğŸ“„ File: {file_path}")
        print(f"ğŸ“Š Parameters:")
        print(f"   - radius: {self.radius}")
        print(f"   - minPts: {self.minPts}")
        print(f"   - max_depth: {self.max_depth}")
        print(f"   - use_cosine: {self.use_cosine}")
        print(f"{'='*80}")
        
        documents = self.load_and_split_documents(file_path, encoding)
        self.tree_structure = self.build_tree(documents)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Clustering Statistics")
        print(f"{'='*80}")
        print(f"   Total clusters by depth:")
        for depth, count in sorted(self.cluster_stats['total_clusters_by_depth'].items()):
            noise = self.cluster_stats['noise_by_depth'].get(depth, 0)
            print(f"     Depth {depth}: {count} clusters, {noise} noise points")
        print(f"{'='*80}")
        
        print(f"\nâœ… Indexing complete!")
    
    def retrieve(self, query: str, top_k: int = 3) -> List[Document]:
        """ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢"""
        if not self.tree_structure:
            raise ValueError("No documents indexed. Call index() first.")
        
        print(f"\nğŸ” Query: {query}")
        results = self.search_tree(self.tree_structure, query, top_k)
        print(f"âœ… Found {len(results)} results")
        
        return results


if __name__ == "__main__":
    print("RAPTOR with CLASSIX - Demo")
    print("=" * 80)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    llm = ChatOllama(
        model="granite-code:8b",
        base_url="http://localhost:11434",
        temperature=0
    )
    
    embeddings_model = OllamaEmbeddings(
        model="mxbai-embed-large",
        base_url="http://localhost:11434"
    )
    
    # CLASSIXç‰ˆRAPTOR
    raptor = RAPTORRetrieverCLASSIX(
        embeddings_model=embeddings_model,
        llm=llm,
        radius=0.5,         # ã‚¯ãƒ©ã‚¹ã‚¿ã®åŠå¾„
        minPts=5,           # æœ€å°ãƒã‚¤ãƒ³ãƒˆæ•°
        max_depth=2,
        chunk_size=1000,
        chunk_overlap=200,
        use_cosine=True     # ã‚³ã‚µã‚¤ãƒ³è·é›¢ä½¿ç”¨
    )
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
    raptor.index("test.txt")
    
    # æ¤œç´¢
    results = raptor.retrieve("philosophy", top_k=3)
    
    print("\n" + "=" * 80)
    print("Search Results:")
    print("=" * 80)
    for i, doc in enumerate(results, 1):
        similarity = doc.metadata.get('similarity', 'N/A')
        print(f"\n{i}. Similarity: {similarity}")
        print(f"   Content: {doc.page_content[:200]}...")
    print("\n" + "=" * 80)
