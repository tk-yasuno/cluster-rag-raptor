# RAPTOR ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰

âš¡ 5åˆ†ã§RAPTORã‚’ä½¿ã„å§‹ã‚ã‚‹ãŸã‚ã®ç°¡æ˜“ã‚¬ã‚¤ãƒ‰

## ğŸ“¦ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆ3åˆ†ï¼‰

### ã‚¹ãƒ†ãƒƒãƒ—1: Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Windows/Mac/Linuxã§ä»¥ä¸‹ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# https://ollama.ai/

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ç¢ºèª
ollama --version
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
# LLMãƒ¢ãƒ‡ãƒ«ï¼ˆè¦ç´„ç”Ÿæˆç”¨ï¼‰
ollama pull granite-code:8b

# Embeddingãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ç”¨ï¼‰
ollama pull mxbai-embed-large
```

### ã‚¹ãƒ†ãƒƒãƒ—3: Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install -U langchain langchain-community langchain-ollama scikit-learn numpy
```

## ğŸš€ åŸºæœ¬çš„ãªä½¿ã„æ–¹ï¼ˆ2åˆ†ï¼‰

### æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰ä¾‹

```python
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor import RAPTORRetriever

# 1. ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
llm = ChatOllama(
    model="granite-code:8b",
    base_url="http://localhost:11434",
    temperature=0
)

embeddings = OllamaEmbeddings(
    model="mxbai-embed-large",
    base_url="http://localhost:11434"
)

# 2. RAPTORã®ä½œæˆ
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=3,  # ã‚·ãƒ³ãƒ—ãƒ«ã«3ã‚¯ãƒ©ã‚¹ã‚¿
    max_depth=2      # 2éšå±¤ã¾ã§
)

# 3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
raptor.index("your_document.txt")

# 4. æ¤œç´¢å®Ÿè¡Œ
results = raptor.retrieve("æ¤œç´¢ã—ãŸã„å†…å®¹", top_k=3)

# 5. çµæœã®è¡¨ç¤º
for i, doc in enumerate(results, 1):
    print(f"\n=== çµæœ {i} ===")
    print(doc.page_content)
```

## ğŸ“ å®Œå…¨ãªå®Ÿè¡Œä¾‹

### ä¾‹1: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ãŸæ¤œç´¢ (example.py)

```python
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor import RAPTORRetriever

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
llm = ChatOllama(
    model="granite-code:8b",
    base_url="http://localhost:11434",
    temperature=0
)

embeddings_model = OllamaEmbeddings(
    model="mxbai-embed-large",
    base_url="http://localhost:11434"
)

# RAPTORãƒ¬ãƒˆãƒªãƒ¼ãƒãƒ¼ä½œæˆ
raptor = RAPTORRetriever(
    embeddings_model=embeddings_model,
    llm=llm,
    max_clusters=3,
    max_depth=2,
    chunk_size=1000,
    chunk_overlap=200
)

print("ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­...")
raptor.index("example_document.txt")

print("\nğŸ” æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")

# ã‚¯ã‚¨ãƒª1
query1 = "ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¯ä½•ã§ã™ã‹ï¼Ÿ"
results1 = raptor.retrieve(query1, top_k=3)

print(f"\n=== '{query1}' ã®æ¤œç´¢çµæœ ===")
for i, doc in enumerate(results1, 1):
    print(f"\nçµæœ {i}:")
    print(doc.page_content[:300])
    print("...")

# ã‚¯ã‚¨ãƒª2
query2 = "å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„"
results2 = raptor.retrieve(query2, top_k=3)

print(f"\n=== '{query2}' ã®æ¤œç´¢çµæœ ===")
for i, doc in enumerate(results2, 1):
    print(f"\nçµæœ {i}:")
    print(doc.page_content[:300])
    print("...")
```

**å®Ÿè¡Œæ–¹æ³•**:
```bash
python example.py
```

### ä¾‹2: Wikipedia ã‹ã‚‰å‹•çš„ã«å–å¾— (example2-wiki.py)

Wikipedia APIã‚’ä½¿ã£ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€RAGæ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ä¾‹ï¼š

```python
import requests
import tempfile
import os
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor import RAPTORRetriever

def get_wikipedia_page(title: str) -> str:
    """Wikipedia APIã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    URL = "https://en.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "format": "json",
        "titles": title,
        "prop": "extracts",
        "explaintext": True,
    }
    headers = {"User-Agent": "RAPTOR_RAG_Example/1.0"}
    response = requests.get(URL, params=params, headers=headers)
    data = response.json()
    page = next(iter(data["query"]["pages"].values()))
    return page["extract"] if "extract" in page else None

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
llm = ChatOllama(model="granite-code:8b", temperature=0)
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

# RAPTORãƒ¬ãƒˆãƒªãƒ¼ãƒãƒ¼ä½œæˆ
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=3,
    max_depth=2
)

# Wikipedia ã‹ã‚‰å®®å´é§¿ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
print("ğŸŒ Fetching Wikipedia page...")
wiki_content = get_wikipedia_page("Hayao_Miyazaki")
print(f"âœ… Fetched {len(wiki_content):,} characters")

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False, suffix='.txt') as tmp:
    tmp.write(wiki_content)
    tmp_path = tmp.name

try:
    print("ğŸ“Š Indexing Wikipedia content...")
    raptor.index(tmp_path)
    
    # è¤‡æ•°ã‚¯ã‚¨ãƒªã§æ¤œç´¢
    queries = [
        "What animation studio did Miyazaki found?",
        "What awards has Miyazaki received?",
        "What are Miyazaki's most famous films?"
    ]
    
    for query in queries:
        print(f"\nğŸ” Query: '{query}'")
        results = raptor.retrieve(query, top_k=3)
        
        for i, doc in enumerate(results, 1):
            print(f"\n--- Result {i} ---")
            print(doc.page_content[:200])
            
finally:
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    if os.path.exists(tmp_path):
        os.remove(tmp_path)

print("\nâœ… Wikipedia RAG completed!")
```

**å®Ÿè¡Œæ–¹æ³•**:
```bash
python example2-wiki.py
```

**å‡ºåŠ›ä¾‹**:
```
ğŸŒ Fetching Wikipedia page...
âœ… Fetched 70,159 characters
ğŸ“Š Indexing Wikipedia content...
Split into 118 chunks

ğŸ” Query: 'What animation studio did Miyazaki found?'
Selected cluster 0 at depth 0 (similarity: 0.7885)
Selected cluster 1 at depth 1 (similarity: 0.7720)

--- Result 1 ---
=== Studio Ghibli ===
==== Foundation and Laputa (1985â€“1987) ====...
```

**ä¸»ãªç‰¹å¾´**:
- ğŸ“¥ Wikipedia APIã‹ã‚‰å‹•çš„ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
- ğŸŒ³ 70,159æ–‡å­— â†’ 118ãƒãƒ£ãƒ³ã‚¯ â†’ éšå±¤åŒ–
- ğŸ” é«˜ç²¾åº¦æ¤œç´¢ï¼ˆé¡ä¼¼åº¦ 0.73-0.78ï¼‰
- ğŸŒ ä»»æ„ã®Wikipediaãƒšãƒ¼ã‚¸ã§åˆ©ç”¨å¯èƒ½

### ä¾‹3: å¤§è¦æ¨¡è«–æ–‡å‡¦ç† (example3-large-scale.py) ğŸš€

arXivè«–æ–‡ï¼ˆ370Kæ–‡å­—è¦æ¨¡ï¼‰ã‚’ä½¿ã£ãŸå®Ÿæˆ¦çš„ãªå¤§è¦æ¨¡RAGï¼š

```python
import requests
import PyPDF2
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor import RAPTORRetriever

# arXivè«–æ–‡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ARXIV_ID = "2508.06401"  # RAGã®ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡
url = f"https://arxiv.org/pdf/{ARXIV_ID}.pdf"
response = requests.get(url, stream=True)
with open("rag_survey.pdf", 'wb') as f:
    for chunk in response.iter_content(chunk_size=8192):
        f.write(chunk)

# PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
with open("rag_survey.pdf", 'rb') as f:
    reader = PyPDF2.PdfReader(f)
    text_parts = [page.extract_text() for page in reader.pages]
    paper_text = "\n\n".join(text_parts)

print(f"ğŸ“Š Extracted {len(paper_text):,} characters")

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
llm = ChatOllama(model="granite-code:8b", temperature=0)
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

# â­ å¤§è¦æ¨¡æ–‡æ›¸ç”¨ã®æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=3,      # ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„ã‚¯ãƒ©ã‚¹ã‚¿æ•°
    max_depth=2,         # åŠ¹ç‡é‡è¦–
    chunk_size=1200,     # â­ é‡è¦: å˜èªã®é€”åˆ‡ã‚Œã‚’é˜²ã
    chunk_overlap=250    # â­ é‡è¦: æ–‡è„ˆä¿æŒ
)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
import tempfile
with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False) as tmp:
    tmp.write(paper_text)
    tmp_path = tmp.name

raptor.index(tmp_path)

# è¤‡é›‘ãªã‚¯ã‚¨ãƒªã§æ¤œç´¢
queries = [
    "What are the main techniques used in RAG systems?",
    "What evaluation metrics are used for RAG systems?",
    "What are the main challenges in RAG implementation?"
]

for query in queries:
    results = raptor.retrieve(query, top_k=3)
    print(f"\nğŸ” {query}")
    for i, doc in enumerate(results, 1):
        print(f"  {i}. {doc.page_content[:150]}...")
```

**å®Ÿè¡Œæ–¹æ³•**:
```bash
python example3-large-scale.py
```

**å‡ºåŠ›ä¾‹**:
```
ğŸ“Š Extracted 370,694 characters
Split into 404 chunks
Build time: 2.5åˆ†

ğŸ” What are the main techniques used in RAG systems?
Selected cluster 2 at depth 0 (similarity: 0.5306)
Selected cluster 1 at depth 1 (similarity: 0.5337)
Query time: 5.443ç§’

  1. //www.nature.com/articles/s41746-024-01091-y.pdf
     [95] P. Xia, K. Zhu, H. Li, H. Zhu, Y . Li...
```

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å®Ÿç¸¾**:
- ğŸ“Š 370,694æ–‡å­—ï¼ˆ48,399å˜èªï¼‰ã‚’å‡¦ç†
- âš¡ æ§‹ç¯‰æ™‚é–“: 2.5åˆ†
- ğŸ” å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“: 2.55ç§’ï¼ˆ26%é«˜é€ŸåŒ–é”æˆï¼‰
- ğŸ¯ 404ãƒãƒ£ãƒ³ã‚¯ â†’ 9ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼ˆ45å€åœ§ç¸®ï¼‰

**ğŸ“ é‡è¦ãªæ•™è¨“ï¼ˆå®Ÿæˆ¦ã‹ã‚‰å­¦ã‚“ã ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼‰**:

1. **chunk_size=1200 ãŒä¸­è¦æ¨¡æ–‡æ›¸ã®æœ€é©è§£**
   ```python
   # âŒ æ‚ªã„ä¾‹: chunk_size=1000
   # çµæœ: "...47\n[26] J. Jin, Y . Zhu..."  â† æ•°å­—ã§é€”åˆ‡ã‚Œã‚‹
   
   # âœ… è‰¯ã„ä¾‹: chunk_size=1200
   # çµæœ: "A Systematic Literature Review of 
   #        Retrieval-Augmented Generation..."  â† å®Œå…¨ãªæ–‡ç« 
   ```

2. **chunk_overlap=250 ã§æ–‡è„ˆã‚’ä¿æŒ**
   - 200ã§ã¯ä¸è¶³: ãƒãƒ£ãƒ³ã‚¯é–“ã§æ„å‘³ãŒæ–­çµ¶
   - 250ã§æœ€é©: æ®µè½ã®åˆ‡ã‚Œç›®ã‚’è·¨ã„ã§æ–‡è„ˆä¿æŒ
   - ã‚¯ã‚¨ãƒªé€Ÿåº¦ãŒ26%å‘ä¸Šï¼ˆ3.43ç§’ â†’ 2.55ç§’ï¼‰

3. **ã‚¹ã‚±ãƒ¼ãƒ«ã«å¿œã˜ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**
   ```python
   # å°è¦æ¨¡ï¼ˆ<100Kæ–‡å­—ï¼‰
   chunk_size=500, max_clusters=2, max_depth=2
   
   # ä¸­è¦æ¨¡ï¼ˆ100-500Kæ–‡å­—ï¼‰â­ ä»Šå›ã®ã‚±ãƒ¼ã‚¹
   chunk_size=1200, max_clusters=3, max_depth=2
   
   # å¤§è¦æ¨¡ï¼ˆ>500Kæ–‡å­—ï¼‰
   chunk_size=1500, max_clusters=5, max_depth=3
   ```

4. **å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿**
   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ~1.5GBï¼ˆåŠ¹ç‡çš„ï¼‰
   - å‡¦ç†é€Ÿåº¦: 2,446æ–‡å­—/ç§’
   - æ¤œç´¢é€Ÿåº¦å„ªä½æ€§: 60å€ï¼ˆãƒ„ãƒªãƒ¼ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ vs å…¨æ¢ç´¢ï¼‰

### ä¾‹4: å°‚é–€æŠ€è¡“æ–‡æ›¸ (example4-bridge-design.py) ğŸ—ï¸

245ãƒšãƒ¼ã‚¸ã®æ©‹æ¢è¨­è¨ˆæ‰‹å¼•ãï¼ˆå®Ÿå‹™æ–‡æ›¸ï¼‰ã‚’ä½¿ã£ãŸå°‚é–€RAGï¼š

```python
import requests
import PyPDF2
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor import RAPTORRetriever

# çŸ³å·çœŒã®æ©‹æ¢è¨­è¨ˆæ‰‹å¼•ãã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
url = "https://www.pref.ishikawa.lg.jp/douken/documents/kyouryousekkeinotebiki.pdf"
response = requests.get(url, stream=True, timeout=120)

with open("bridge_design_guidelines.pdf", 'wb') as f:
    total_size = 0
    for chunk in response.iter_content(chunk_size=8192):
        f.write(chunk)
        total_size += len(chunk)
        if total_size % (1024 * 1024) == 0:
            print(f"Downloaded: {total_size // (1024*1024)}MB...")

# PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆ245ãƒšãƒ¼ã‚¸ï¼‰
with open("bridge_design_guidelines.pdf", 'rb') as f:
    reader = PyPDF2.PdfReader(f)
    num_pages = len(reader.pages)
    print(f"Total pages: {num_pages}")
    
    text_parts = []
    for i, page in enumerate(reader.pages):
        if (i + 1) % 25 == 0:
            print(f"Processing page {i + 1}/{num_pages}...")
        text = page.extract_text()
        if text:
            text_parts.append(text)
    
    guidelines_text = "\n\n".join(text_parts)

print(f"Extracted {len(guidelines_text):,} characters")

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
llm = ChatOllama(model="granite-code:8b", temperature=0)
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

# å°‚é–€æŠ€è¡“æ–‡æ›¸ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=3,      # æŠ€è¡“æ–‡æ›¸ã®æ§‹é€ ã«æœ€é©
    max_depth=2,         # åŠ¹ç‡ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹
    chunk_size=1200,     # å°‚é–€ç”¨èªã‚’é€”åˆ‡ã‚Œã•ã›ãªã„
    chunk_overlap=250    # ç« ç¯€ã®é€£ç¶šæ€§ã‚’ä¿æŒ
)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
import tempfile
with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False) as tmp:
    tmp.write(guidelines_text)
    tmp_path = tmp.name

raptor.index(tmp_path)

# å°‚é–€çš„ãªæ—¥æœ¬èªã‚¯ã‚¨ãƒªã§æ¤œç´¢
queries = [
    "è€éœ‡è¨­è¨ˆã«é–¢ã™ã‚‹åŸºæº–ã¯ã©ã®ã‚ˆã†ã«å®šã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ",
    "æ©‹æ¢ã®æ–½å·¥è¨ˆç”»ã«ãŠã‘ã‚‹ç•™æ„ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "é“è·¯æ©‹ç¤ºæ–¹æ›¸ã¨ã®æ•´åˆæ€§ã«ã¤ã„ã¦ã©ã®ã‚ˆã†ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ"
]

for query in queries:
    results = raptor.retrieve(query, top_k=3)
    print(f"\nğŸ” {query}")
    for i, doc in enumerate(results, 1):
        preview = doc.page_content[:150].replace('\n', ' ')
        print(f"  {i}. {preview}...")
```

**å®Ÿè¡Œæ–¹æ³•**:
```bash
python example4-bridge-design.py
```

**å‡ºåŠ›ä¾‹**:
```
Downloaded: 1MB...
Downloaded: 2MB...
...
Downloaded: 8MB...
âœ… Downloaded 9,315,291 bytes (8MB)
Total pages: 245
Processing page 25/245...
...
âœ… Extracted 207,558 characters from 245 pages

Split into 254 chunks
Build time: 1.6åˆ†

ğŸ” è€éœ‡è¨­è¨ˆã«é–¢ã™ã‚‹åŸºæº–ã¯ã©ã®ã‚ˆã†ã«å®šã‚ã‚‰ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ
Selected cluster 1 at depth 0 (similarity: 0.5102)
Selected cluster 0 at depth 1 (similarity: 0.4893)
Query time: 4.269ç§’

  1. - 114 - ï¼”ï¼è¨­ç½®ç®‡æ‰€ (1ï¼‰ æ¤œ æŸ» è·¯ ã® è¨­ ç½® ç®‡ æ‰€ ã¯...
  2. - 135 - å›³7.10 è½ æ©‹ é˜² æ­¢ æ§‹ é€  ã® ä¾‹...
```

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å®Ÿç¸¾**:
- ğŸ“Š 207,558æ–‡å­—ï¼ˆ64,745å˜èªã€245ãƒšãƒ¼ã‚¸ï¼‰ã‚’å‡¦ç†
- ğŸ“„ PDF: 9.3MBï¼ˆå›³è¡¨å«ã‚€å°‚é–€æŠ€è¡“æ–‡æ›¸ï¼‰
- âš¡ æ§‹ç¯‰æ™‚é–“: 1.6åˆ†
- ğŸ” å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“: 2.51ç§’
- ğŸ¯ 254ãƒãƒ£ãƒ³ã‚¯ â†’ 9ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼ˆ28å€åœ§ç¸®ï¼‰
- ğŸ† æ¤œç´¢é€Ÿåº¦å„ªä½æ€§: 39å€

**ğŸ“ å°‚é–€æŠ€è¡“æ–‡æ›¸ã§ã®æ•™è¨“**:

1. **PDFã®ç‰¹æ€§ç†è§£**
   ```
   245ãƒšãƒ¼ã‚¸ â†’ ç´„20ä¸‡æ–‡å­—
   ç†ç”±: å›³è¡¨ã€ç©ºç™½ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒå¤šã„
   æ•™è¨“: ãƒšãƒ¼ã‚¸æ•°â‰ æ–‡å­—æ•°ã€å®Ÿæ¸¬ãŒé‡è¦
   ```

2. **æ—¥æœ¬èªå°‚é–€ç”¨èªã¸ã®å¯¾å¿œ**
   - ã€Œè€éœ‡è¨­è¨ˆã€ã€Œé“è·¯æ©‹ç¤ºæ–¹æ›¸ã€ã€Œè½æ©‹é˜²æ­¢æ§‹é€ ã€ãªã©å°‚é–€ç”¨èªãŒæ­£ç¢ºã«æ¤œç´¢å¯èƒ½
   - chunk_size=1200 ã§ç”¨èªã®é€”åˆ‡ã‚Œã‚’é˜²æ­¢
   - mxbai-embed-large ã¯æ—¥æœ¬èªã«ã‚‚å¯¾å¿œ

3. **å®Ÿå‹™æ–‡æ›¸ã®æ§‹é€ æ´»ç”¨**
   - ç« ãƒ»ç¯€ãƒ»é …ã®éšå±¤æ§‹é€ ãŒRAPTORã®ãƒ„ãƒªãƒ¼ã¨è‡ªç„¶ã«å¯¾å¿œ
   - æ³•è¦å‚ç…§ã‚„æŠ€è¡“åŸºæº–ã®æ¨ªæ–­æ¤œç´¢ã«æœ€é©
   - è¨­è¨ˆè€…ã®å•ã„åˆã‚ã›ã«å³åº§ã«å›ç­”ï¼ˆ2.5ç§’ï¼‰

4. **ã‚¹ã‚±ãƒ¼ãƒ«åˆ¥å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡è¦ï¼‰**
   ```
   20ä¸‡æ–‡å­—: 1.6åˆ†æ§‹ç¯‰ã€2.5ç§’æ¤œç´¢
   37ä¸‡æ–‡å­—: 2.5åˆ†æ§‹ç¯‰ã€2.6ç§’æ¤œç´¢
   
   çµè«–: æ¤œç´¢æ™‚é–“ã¯ã»ã¼ä¸€å®šï¼ˆO(log n)ã®å®Ÿè¨¼ï¼‰âœ¨
   ```

### ä¾‹5: è¶…å¤§è¦æ¨¡ã‚¹ã‚±ãƒ¼ãƒ« - æ©Ÿæ¢°å­¦ç¿’æ•™ç§‘æ›¸ (example5-esl-book.py) ğŸš€ğŸ“š

**ğŸŒŸ 100ä¸‡æ–‡å­—è¶…ã‚¹ã‚±ãƒ¼ãƒ«ã®å®Œå…¨å®Ÿè¨¼**

```python
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor import RAPTORRetriever
import PyPDF2
import sys

# Windows ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã®çµµæ–‡å­—å¯¾å¿œï¼ˆé‡è¦ï¼ï¼‰
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except AttributeError:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')

# PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
def pdf_to_text(pdf_path: str) -> str:
    """764ãƒšãƒ¼ã‚¸ã®å¤§è¦æ¨¡PDFã‚’å‡¦ç†"""
    with open(pdf_path, 'rb') as f:
        reader = PyPDF2.PdfReader(f)
        num_pages = len(reader.pages)
        print(f"Total pages: {num_pages}")
        
        text_parts = []
        for i, page in enumerate(reader.pages):
            if (i + 1) % 50 == 0:
                print(f"Processing page {i + 1}/{num_pages}...")
            text = page.extract_text()
            if text:
                text_parts.append(text)
        
        return "\n\n".join(text_parts)

# The Elements of Statistical Learning ã‚’å‡¦ç†
# äº‹å‰ã«PDFã‚’æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: ESLII_print12_toc.pdf
book_text = pdf_to_text("ESLII_print12_toc.pdf")
print(f"ğŸ“Š Extracted: {len(book_text):,} characters")

# ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
with open("elements_of_statistical_learning.txt", 'w', encoding='utf-8') as f:
    f.write(book_text)

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
llm = ChatOllama(
    model="granite-code:8b",
    base_url="http://localhost:11434",
    temperature=0
)

embeddings_model = OllamaEmbeddings(
    model="mxbai-embed-large",
    base_url="http://localhost:11434"
)

# ğŸŒŸ è¶…å¤§è¦æ¨¡æ–‡æ›¸ç”¨ã®æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
raptor = RAPTORRetriever(
    embeddings_model=embeddings_model,
    llm=llm,
    max_clusters=5,      # å¤šæ§˜ãªMLãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
    max_depth=3,         # æ·±ã„éšå±¤: åˆ†é‡ â†’ æ‰‹æ³•ç¾¤ â†’ è©³ç´°
    chunk_size=1500,     # è¤‡é›‘ãªæ•°å¼ãƒ»æŠ€è¡“ç”¨èªã‚’ä¿æŒ
    chunk_overlap=300    # æ•°å¼ã®é€£ç¶šæ€§ã‚’ç¶­æŒï¼ˆ20%ï¼‰
)

print("â±ï¸  Expected build time: 30-60 minutes")
print("â˜• Great time for lunch or a long coffee break!")

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ï¼ˆã“ã‚ŒãŒ47.4åˆ†ã‹ã‹ã‚‹ï¼‰
raptor.index("elements_of_statistical_learning.txt")

# æ©Ÿæ¢°å­¦ç¿’å°‚é–€ã‚¯ã‚¨ãƒªã§æ¤œè¨¼
ml_queries = [
    "Which chapters discuss ensemble methods?",
    "Summarize the differences between Lasso and Ridge regression",
    "What are the key assumptions behind Support Vector Machines?",
    "How does boosting differ from bagging?",
    "What are the main techniques for nonlinear dimensionality reduction?"
]

print("\nğŸ” Benchmarking ML Queries...")
for idx, query in enumerate(ml_queries, 1):
    print(f"\nQuery {idx}/5: {query}")
    results = raptor.retrieve(query, top_k=3)
    
    for i, doc in enumerate(results, 1):
        preview = doc.page_content[:250].replace('\n', ' ')
        print(f"  {i}. {preview}...")
```

**å®Ÿè¡Œæ–¹æ³•**:
```bash
# 1. äº‹å‰ã«PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# URL: https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf.download.html
# ãƒ•ã‚¡ã‚¤ãƒ«å: ESLII_print12_toc.pdf

# 2. cluster-rag-raptor/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®

# 3. å®Ÿè¡Œï¼ˆ30-60åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰
python example5-esl-book.py
```

**å‡ºåŠ›ä¾‹**:
```
ğŸ“š RAPTOR Ultra-Large-Scale: The Elements of Statistical Learning (759p)
================================================================================

âœ… Found manually downloaded PDF: ESLII_print12_toc.pdf
ğŸ“„ Extracting text from PDF...
   Total pages: 764
   Processing page 50/764 (6.5%) - ETA: 1.0 min
   Processing page 100/764 (13.1%) - ETA: 0.7 min
   ...
   Processing page 750/764 (98.2%) - ETA: 0.0 min
âœ… Extracted 1,830,878 characters from 764 pages
   Extraction took 1.3 minutes

ğŸ“Š Document Statistics:
   Total characters: 1,830,878
   Total words: 377,469
   Scale: 1.83M characters
   Category: ğŸš€ MILLION-CHARACTER SCALE ACHIEVED!

================================================================================
ğŸ“Š Step 4: Building RAPTOR Tree (This will take 30-60 minutes...)
================================================================================

=== Starting RAPTOR Indexing ===
Split into 1758 chunks

=== Building tree at depth 0 with 1758 documents ===
...
=== RAPTOR Tree Construction Complete ===

Build time: 47.4åˆ†
Characters processed: 1,830,878
Processing speed: 643 chars/sec

================================================================================
ğŸ” Machine Learning Query Benchmarking
================================================================================

Query 1/5: Which chapters discuss ensemble methods?
Selected cluster 4 at depth 0 (similarity: 0.6597)
Selected cluster 1 at depth 1 (similarity: 0.6460)
Query time: 3.810ç§’

Query 2/5: Summarize the differences between Lasso and Ridge regression
Selected cluster 4 at depth 0 (similarity: 0.6692)
Query time: 1.575ç§’
...

Average query time: 2.013ç§’
```

**ğŸ† è¨˜éŒ²çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å®Ÿç¸¾**:

| æŒ‡æ¨™ | å€¤ | ä»–äº‹ä¾‹ã¨ã®æ¯”è¼ƒ |
|------|-----|---------------|
| **æ–‡æ›¸è¦æ¨¡** | **1,830,878æ–‡å­— (1.83M)** | example4ã® **8.8å€** ğŸš€ |
| **ãƒšãƒ¼ã‚¸æ•°** | 764ãƒšãƒ¼ã‚¸ | 759æœ¬ç·¨ + ç›®æ¬¡/ä»˜éŒ² |
| **å˜èªæ•°** | 377,469èª | è‹±èªæŠ€è¡“æ–‡æ›¸ |
| **ãƒãƒ£ãƒ³ã‚¯æ•°** | 1,758ãƒãƒ£ãƒ³ã‚¯ | example4ã® 6.9å€ |
| **PDFæŠ½å‡º** | 1.3åˆ† | 764ãƒšãƒ¼ã‚¸å‡¦ç† |
| **ãƒ„ãƒªãƒ¼æ§‹ç¯‰** | **47.4åˆ†** | â±ï¸ ä¸€åº¦ãã‚Šã®æŠ•è³‡ |
| **å¹³å‡ã‚¯ã‚¨ãƒª** | **2.013ç§’** | âš¡ example4ã¨åŒç­‰ï¼ |
| **æ¤œç´¢å„ªä½æ€§** | **1414å€** | 47.4åˆ† Ã· 2.0ç§’ |
| **ãƒ¡ãƒ¢ãƒªä½¿ç”¨** | ~7.3GB | embeddingså«ã‚€ |

**ğŸ“Š O(log n) ã®æ±ºå®šçš„å®Ÿè¨¼**:

```
æ–‡å­—æ•°ã‚¹ã‚±ãƒ¼ãƒ«æ¯”è¼ƒ:
example2 (Wikipedia):    70K   â†’  2.5ç§’  (åŸºæº–)
example3 (arXivè«–æ–‡):   370K   â†’  2.55ç§’ (5.3å€ã®æ–‡æ›¸é‡)
example4 (æ©‹æ¢è¨­è¨ˆ):    207K   â†’  2.51ç§’ (3.0å€ã®æ–‡æ›¸é‡)
example5 (MLæ•™ç§‘æ›¸): 1,830K   â†’  2.01ç§’ (26.1å€ã®æ–‡æ›¸é‡ï¼)

çµè«–: æ–‡å­—æ•°ãŒ26å€ã«ãªã£ã¦ã‚‚ã‚¯ã‚¨ãƒªæ™‚é–“ã¯ã»ã¼ä¸€å®šï¼
â†’ O(log n) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç†è«–çš„å„ªä½æ€§ã‚’å®Œå…¨å®Ÿè¨¼ âœ…
```

**ğŸ“ 100ä¸‡æ–‡å­—è¶…ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®é‡è¦ãªæ•™è¨“**:

1. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ®µéšçš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**
   ```python
   # å°è¦æ¨¡ (<100K):   chunk_size=500-800
   # ä¸­è¦æ¨¡ (100-500K): chunk_size=1000-1200  â­example3,4
   # å¤§è¦æ¨¡ (500K-2M):  chunk_size=1500-2000  â­example5
   # è¶…å¤§è¦æ¨¡ (>2M):     chunk_size=2000+, åˆ†æ•£å‡¦ç†æ¤œè¨
   ```

2. **chunk_overlap ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡**
   ```python
   # åŸºæœ¬ãƒ«ãƒ¼ãƒ«: chunk_size ã® 20% ã‚’ç¶­æŒ
   chunk_size=1500 â†’ chunk_overlap=300 âœ…
   
   # ç†ç”±: æ•°å¼å±•é–‹ã‚„å®šç†è¨¼æ˜ãŒè¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã«ã¾ãŸãŒã‚‹
   # 20%æœªæº€ã ã¨æ–‡è„ˆãŒå¤±ã‚ã‚Œã€LLMã®ç†è§£åº¦ãŒä½ä¸‹
   ```

3. **max_depth=3 ã®éšå±¤æ§‹é€ **
   ```
   Level 0 (Root): åˆ†é‡å…¨ä½“ï¼ˆæ©Ÿæ¢°å­¦ç¿’ã®å…¨ä½“åƒï¼‰
   â”œâ”€ Level 1: å¤§ã‚«ãƒ†ã‚´ãƒªï¼ˆå›å¸°ã€åˆ†é¡ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€æ¬¡å…ƒå‰Šæ¸›ç­‰ï¼‰
   â”‚  â”œâ”€ Level 2: æ‰‹æ³•ç¾¤ï¼ˆLasso/Ridgeã€SVMã€Boosting/Baggingç­‰ï¼‰
   â”‚  â”‚  â””â”€ Level 3: å®Ÿè£…è©³ç´°ãƒ»ç†è«–è¨¼æ˜ãƒ»å…·ä½“ä¾‹
   
   1758ãƒãƒ£ãƒ³ã‚¯ã‚’åŠ¹ç‡çš„ã«3éšå±¤ã§æ•´ç† âœ¨
   ```

4. **æ§‹ç¯‰æ™‚é–“ã®ROIåˆ†æ**
   ```
   åˆæœŸæŠ•è³‡: 47.4åˆ†ï¼ˆPDFæŠ½å‡º1.3åˆ† + ãƒ„ãƒªãƒ¼æ§‹ç¯‰46.1åˆ†ï¼‰
   æ¤œç´¢ã‚³ã‚¹ãƒˆ: 2.0ç§’/ã‚¯ã‚¨ãƒª
   
   ROIè¨ˆç®—:
   - 1414å›ã®ã‚¯ã‚¨ãƒªã§å…ƒãŒå–ã‚Œã‚‹ï¼ˆ47.4åˆ† Ã· 2.0ç§’ï¼‰
   - å®Ÿå‹™ã§ã¯æ•°åƒã€œæ•°ä¸‡ã‚¯ã‚¨ãƒªãŒæƒ³å®šã•ã‚Œã‚‹
   - ä¸€åº¦æ§‹ç¯‰â†’æ°¸ç¶šçš„ã«é«˜é€Ÿæ¤œç´¢å¯èƒ½
   
   ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:
   â†’ äº‹å‰ã«ãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰ã—ã¦Pickle/JSONåŒ–
   â†’ ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã¯æ•°ç§’ã€å³åº§ã«ã‚¯ã‚¨ãƒªé–‹å§‹å¯èƒ½
   ```

5. **æ©Ÿæ¢°å­¦ç¿’æ•™ç§‘æ›¸ã®ç‰¹æ€§**
   - 18ç« ï¼‹ä»˜éŒ²ã®æ˜ç¢ºãªéšå±¤æ§‹é€ ãŒRAPTORã¨ç›¸æ€§æŠœç¾¤
   - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã€æ­£å‰‡åŒ–ã€SVMã€æ¬¡å…ƒå‰Šæ¸›ç­‰ã®æ¨ªæ–­æ¤œç´¢
   - é¡ä¼¼åº¦ 0.61-0.69 ã§é–¢é€£ç« ã‚’æ­£ç¢ºã«è­˜åˆ¥
   - å°‚é–€ç”¨èªï¼ˆLasso, Ridge, Boosting, Baggingç­‰ï¼‰ã‚’é€”åˆ‡ã‚Œãªãä¿æŒ

6. **Windowsç’°å¢ƒã®è½ã¨ã—ç©´**
   ```python
   # âŒ çµµæ–‡å­—ã‚’ä½¿ã†ã¨cp932ã‚¨ãƒ©ãƒ¼
   print("ğŸ“š RAPTOR...")  # UnicodeEncodeError!
   
   # âœ… UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å¼·åˆ¶
   if sys.platform == 'win32':
       sys.stdout.reconfigure(encoding='utf-8')
   
   # ã“ã‚Œã§çµµæ–‡å­—ãŒæ­£å¸¸ã«è¡¨ç¤ºã•ã‚Œã‚‹ âœ¨
   ```

7. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®é™ç•Œã¨Next Steps**
   ```
   å˜ä¸€ãƒã‚·ãƒ³ã®å®Ÿç”¨ç¯„å›²:
   - 1-2Mæ–‡å­—:  âœ… æœ¬äº‹ä¾‹ã€16GB RAMæ¨å¥¨
   - 2-5Mæ–‡å­—:  âš ï¸  32GB+ RAMå¿…é ˆ
   - 5Mæ–‡å­—è¶…:  âŒ åˆ†æ•£å‡¦ç†ã‚’æ¤œè¨
   
   å¤§è¦æ¨¡åŒ–ã®æˆ¦ç•¥:
   1. ãƒãƒ£ãƒ³ã‚¯ã®ä¸¦åˆ—embeddingsç”Ÿæˆ
   2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®åˆ†æ•£å‡¦ç†
   3. ãƒ„ãƒªãƒ¼æ§‹é€ ã®ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
   4. Redisç­‰ã§ã®ä¸­é–“çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
   ```

8. **å®Ÿå‹™ã§ã®å¿œç”¨ã‚·ãƒŠãƒªã‚ª**
   - ğŸ“š æŠ€è¡“æ›¸ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®çµ±åˆæ¤œç´¢ï¼ˆO'Reillyå…¨é›†ç­‰ï¼‰
   - ğŸ¢ ä¼æ¥­ã®å…¨ç¤¾è¦ç¨‹ãƒ»ãƒãƒ‹ãƒ¥ã‚¢ãƒ«é›†ã®è³ªå•å¿œç­”Bot
   - ğŸ“ å¤§å­¦ã®e-ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
   - ğŸ”¬ ç ”ç©¶è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®é«˜åº¦æ¤œç´¢
   - ğŸ“– é›»å­æ›¸ç±ãƒªãƒ¼ãƒ€ãƒ¼ã®æ¬¡ä¸–ä»£æ¤œç´¢æ©Ÿèƒ½
   - ğŸ’¼ æ³•å¾‹äº‹å‹™æ‰€ã®åˆ¤ä¾‹ãƒ»æ³•ä»¤æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 

**ğŸ’¡ Production Deployment ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:

```python
# âœ… æœ¬ç•ªç’°å¢ƒã¸ã®å±•é–‹æ™‚ã®æ¨å¥¨äº‹é …

# 1. ãƒ„ãƒªãƒ¼æ§‹é€ ã®æ°¸ç¶šåŒ–
import pickle
with open('raptor_tree.pkl', 'wb') as f:
    pickle.dump(raptor.tree, f)

# 2. é«˜é€Ÿãƒ­ãƒ¼ãƒ‰
with open('raptor_tree.pkl', 'rb') as f:
    raptor.tree = pickle.load(f)
# ãƒ­ãƒ¼ãƒ‰æ™‚é–“: æ•°ç§’ï¼ˆæ§‹ç¯‰æ™‚é–“: 47.4åˆ†ã¨æ¯”è¼ƒï¼‰

# 3. ã‚¯ã‚¨ãƒªãƒ­ã‚°ã®åé›†
import logging
logging.basicConfig(filename='raptor_queries.log')

# 4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥
from functools import lru_cache
@lru_cache(maxsize=1000)
def cached_retrieve(query):
    return raptor.retrieve(query, top_k=3)

# 5. ãƒ¡ãƒ¢ãƒªç›£è¦–
import psutil
print(f"Memory usage: {psutil.virtual_memory().percent}%")

# 6. ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
from langchain.callbacks import TimeoutCallback
raptor.retrieve(query, callbacks=[TimeoutCallback(timeout=10)])
```

## ğŸ¯ ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹åˆ¥ã®è¨­å®š

### ã‚±ãƒ¼ã‚¹1: å°ã•ãªæ–‡æ›¸ï¼ˆ<10ä¸‡æ–‡å­—ï¼‰

```python
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=2,   # å°‘ãªã„ã‚¯ãƒ©ã‚¹ã‚¿
    max_depth=2,      # æµ…ã„éšå±¤
    chunk_size=500    # å°ã•ã„ãƒãƒ£ãƒ³ã‚¯
)
```

**é©ç”¨ä¾‹**: ãƒ–ãƒ­ã‚°è¨˜äº‹ã€çŸ­ã„è«–æ–‡ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

### ã‚±ãƒ¼ã‚¹2: ä¸­è¦æ¨¡æ–‡æ›¸ï¼ˆ10-50ä¸‡æ–‡å­—ï¼‰

```python
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=3,   # ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„ã‚¯ãƒ©ã‚¹ã‚¿æ•°
    max_depth=2,      # æ¨™æº–çš„ãªéšå±¤
    chunk_size=1000   # æ¨™æº–çš„ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
)
```

**é©ç”¨ä¾‹**: æŠ€è¡“æ›¸ã€é•·ç·¨è¨˜äº‹ã€ç ”ç©¶è«–æ–‡

### ã‚±ãƒ¼ã‚¹3: å¤§è¦æ¨¡æ–‡æ›¸ï¼ˆ>50ä¸‡æ–‡å­—ï¼‰

```python
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=5,   # å¤šãã®ã‚¯ãƒ©ã‚¹ã‚¿
    max_depth=3,      # æ·±ã„éšå±¤
    chunk_size=1500   # å¤§ãã„ãƒãƒ£ãƒ³ã‚¯
)
```

**é©ç”¨ä¾‹**: æ›¸ç±å…¨ä½“ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€è¤‡æ•°æ–‡æ›¸ã®çµåˆ

## ğŸ’¡ ã‚ˆãã‚ã‚‹è³ªå•

### Q1: ã©ã®ãã‚‰ã„ã®æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã‹ï¼Ÿ

**ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–**: 
- å°è¦æ¨¡ï¼ˆ<10ä¸‡æ–‡å­—ï¼‰: ~1åˆ†
- ä¸­è¦æ¨¡ï¼ˆ10-50ä¸‡æ–‡å­—ï¼‰: ~5åˆ†
- å¤§è¦æ¨¡ï¼ˆ>50ä¸‡æ–‡å­—ï¼‰: ~15åˆ†

**æ¤œç´¢**: å¸¸ã«1ç§’æœªæº€

### Q2: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯ï¼Ÿ

- **æœ€å°è¦ä»¶**: 8GB RAM
- **æ¨å¥¨**: 16GB RAM
- **GPU**: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆOllamaãŒè‡ªå‹•çš„ã«ä½¿ç”¨ï¼‰

### Q3: è¤‡æ•°ã®æ–‡æ›¸ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã§ãã¾ã™ã‹ï¼Ÿ

ã¯ã„ï¼è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦ãã ã•ã„ï¼š

```python
# è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
documents = []
for file in ["doc1.txt", "doc2.txt", "doc3.txt"]:
    with open(file, 'r', encoding='utf-8') as f:
        documents.append(f.read())

# çµåˆã—ã¦ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
combined = "\n\n".join(documents)
with open("combined.txt", 'w', encoding='utf-8') as f:
    f.write(combined)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
raptor.index("combined.txt")
```

### Q4: æ¤œç´¢çµæœã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ã«ã¯ï¼Ÿ

1. **ã‚ˆã‚Šæ·±ã„éšå±¤ã‚’è©¦ã™**: `max_depth=3`
2. **ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’å¢—ã‚„ã™**: `max_clusters=5`
3. **ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’èª¿æ•´**: å¤§ãã„ãƒ†ã‚­ã‚¹ãƒˆãªã‚‰ `chunk_size=1500`
4. **ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’å¢—ã‚„ã™**: `chunk_overlap=300`

### Q5: ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ï¼Ÿ

**"Connection refused"**:
```bash
# OllamaãŒèµ·å‹•ã—ã¦ã„ãªã„
ollama serve
```

**"Model not found"**:
```bash
# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull granite-code:8b
ollama pull mxbai-embed-large
```

**ãƒ¡ãƒ¢ãƒªä¸è¶³**:
```python
# ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å°ã•ã
raptor = RAPTORRetriever(chunk_size=500)
```

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹

```
=== Starting RAPTOR Indexing ===
Loaded document length: 624212 characters
Split into 864 chunks

=== Building tree at depth 0 with 864 documents ===
Cluster 0: 344 documents
Cluster 1: 219 documents
Cluster 2: 301 documents

=== Building tree at depth 1 with 344 documents ===
...

=== RAPTOR Tree Construction Complete ===

=== Searching for: 'philosophy' ===
Selected cluster 0 at depth 0 (similarity: 0.6691)
Selected cluster 2 at depth 1 (similarity: 0.6587)

=== Top 3 Results ===
Result 1: ãƒ—ãƒ©ãƒˆãƒ³ã®å“²å­¦çš„ä¿¡æ¡ã«ã¤ã„ã¦...
Result 2: ç†æƒ³çš„çŸ¥è­˜è«–ã«é–¢ã™ã‚‹è­°è«–...
Result 3: ãƒ—ãƒ©ãƒˆãƒ³çš„æ„›ã®æ¦‚å¿µ...
```

## ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

1. **è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [README.md](README.md)ã‚’å‚ç…§
2. **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**: è¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„LLMã‚’å¤‰æ›´
3. **æœ€é©åŒ–**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ç²¾åº¦å‘ä¸Š

### ç™ºå±•çš„ãªä½¿ã„æ–¹

```python
# ã‚«ã‚¹ã‚¿ãƒ è¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
class MyRAPTOR(RAPTORRetriever):
    def summarize_cluster(self, documents):
        # ç‹¬è‡ªã®è¦ç´„ãƒ­ã‚¸ãƒƒã‚¯
        pass

# è¤‡æ•°ã‚¯ã‚¨ãƒªã®ä¸¦åˆ—å®Ÿè¡Œ
queries = ["query1", "query2", "query3"]
results = [raptor.retrieve(q, top_k=3) for q in queries]
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] OllamaãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹
- [ ] `ollama serve` ãŒå®Ÿè¡Œä¸­
- [ ] ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ï¼ˆ`ollama list`ã§ç¢ºèªï¼‰
- [ ] Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- [ ] æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- [ ] ååˆ†ãªãƒ¡ãƒ¢ãƒªï¼ˆæ¨å¥¨16GBï¼‰

## ğŸ“ ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆ:
1. GitHubã®Issueã‚’ç¢ºèª
2. æ–°ã—ã„Issueã‚’ä½œæˆ
3. README.mdã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§

---

ğŸ‰ ã“ã‚Œã§RAPTORã‚’ä½¿ã„å§‹ã‚ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸï¼
