# RAPTOR ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰

âš¡ 5åˆ†ã§RAPTORã‚’ä½¿ã„å§‹ã‚ã‚‹ãŸã‚ã®ç°¡æ˜“ã‚¬ã‚¤ãƒ‰

## ğŸ“¦ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆ3åˆ†ï¼‰

### ã‚¹ãƒ†ãƒƒãƒ—1: Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Windows/Mac/Linuxã§ä»¥ä¸‹ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# https://ollama.ai/

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¾Œã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ç¢ºèª
ollama --version
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
# LLMãƒ¢ãƒ‡ãƒ«ï¼ˆè¦ç´„ç”Ÿæˆç”¨ï¼‰
ollama pull granite-code:8b

# Embeddingãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ç”¨ï¼‰
ollama pull mxbai-embed-large
```

### ã‚¹ãƒ†ãƒƒãƒ—3: Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install -U langchain langchain-community langchain-ollama scikit-learn numpy
```

## ğŸš€ åŸºæœ¬çš„ãªä½¿ã„æ–¹ï¼ˆ2åˆ†ï¼‰

### æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰ä¾‹

```python
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor import RAPTORRetriever

# 1. ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
llm = ChatOllama(
    model="granite-code:8b",
    base_url="http://localhost:11434",
    temperature=0
)

embeddings = OllamaEmbeddings(
    model="mxbai-embed-large",
    base_url="http://localhost:11434"
)

# 2. RAPTORã®ä½œæˆ
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=3,  # ã‚·ãƒ³ãƒ—ãƒ«ã«3ã‚¯ãƒ©ã‚¹ã‚¿
    max_depth=2      # 2éšå±¤ã¾ã§
)

# 3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
raptor.index("your_document.txt")

# 4. æ¤œç´¢å®Ÿè¡Œ
results = raptor.retrieve("æ¤œç´¢ã—ãŸã„å†…å®¹", top_k=3)

# 5. çµæœã®è¡¨ç¤º
for i, doc in enumerate(results, 1):
    print(f"\n=== çµæœ {i} ===")
    print(doc.page_content)
```

## ğŸ“ å®Œå…¨ãªå®Ÿè¡Œä¾‹

### ä¾‹1: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ãŸæ¤œç´¢ (example.py)

```python
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor import RAPTORRetriever

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
llm = ChatOllama(
    model="granite-code:8b",
    base_url="http://localhost:11434",
    temperature=0
)

embeddings_model = OllamaEmbeddings(
    model="mxbai-embed-large",
    base_url="http://localhost:11434"
)

# RAPTORãƒ¬ãƒˆãƒªãƒ¼ãƒãƒ¼ä½œæˆ
raptor = RAPTORRetriever(
    embeddings_model=embeddings_model,
    llm=llm,
    max_clusters=3,
    max_depth=2,
    chunk_size=1000,
    chunk_overlap=200
)

print("ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­...")
raptor.index("example_document.txt")

print("\nğŸ” æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")

# ã‚¯ã‚¨ãƒª1
query1 = "ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã¯ä½•ã§ã™ã‹ï¼Ÿ"
results1 = raptor.retrieve(query1, top_k=3)

print(f"\n=== '{query1}' ã®æ¤œç´¢çµæœ ===")
for i, doc in enumerate(results1, 1):
    print(f"\nçµæœ {i}:")
    print(doc.page_content[:300])
    print("...")

# ã‚¯ã‚¨ãƒª2
query2 = "å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„"
results2 = raptor.retrieve(query2, top_k=3)

print(f"\n=== '{query2}' ã®æ¤œç´¢çµæœ ===")
for i, doc in enumerate(results2, 1):
    print(f"\nçµæœ {i}:")
    print(doc.page_content[:300])
    print("...")
```

**å®Ÿè¡Œæ–¹æ³•**:
```bash
python example.py
```

### ä¾‹2: Wikipedia ã‹ã‚‰å‹•çš„ã«å–å¾— (example2-wiki.py)

Wikipedia APIã‚’ä½¿ã£ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€RAGæ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ä¾‹ï¼š

```python
import requests
import tempfile
import os
from langchain_ollama import ChatOllama, OllamaEmbeddings
from raptor import RAPTORRetriever

def get_wikipedia_page(title: str) -> str:
    """Wikipedia APIã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    URL = "https://en.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "format": "json",
        "titles": title,
        "prop": "extracts",
        "explaintext": True,
    }
    headers = {"User-Agent": "RAPTOR_RAG_Example/1.0"}
    response = requests.get(URL, params=params, headers=headers)
    data = response.json()
    page = next(iter(data["query"]["pages"].values()))
    return page["extract"] if "extract" in page else None

# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
llm = ChatOllama(model="granite-code:8b", temperature=0)
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

# RAPTORãƒ¬ãƒˆãƒªãƒ¼ãƒãƒ¼ä½œæˆ
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=3,
    max_depth=2
)

# Wikipedia ã‹ã‚‰å®®å´é§¿ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
print("ğŸŒ Fetching Wikipedia page...")
wiki_content = get_wikipedia_page("Hayao_Miyazaki")
print(f"âœ… Fetched {len(wiki_content):,} characters")

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False, suffix='.txt') as tmp:
    tmp.write(wiki_content)
    tmp_path = tmp.name

try:
    print("ğŸ“Š Indexing Wikipedia content...")
    raptor.index(tmp_path)
    
    # è¤‡æ•°ã‚¯ã‚¨ãƒªã§æ¤œç´¢
    queries = [
        "What animation studio did Miyazaki found?",
        "What awards has Miyazaki received?",
        "What are Miyazaki's most famous films?"
    ]
    
    for query in queries:
        print(f"\nğŸ” Query: '{query}'")
        results = raptor.retrieve(query, top_k=3)
        
        for i, doc in enumerate(results, 1):
            print(f"\n--- Result {i} ---")
            print(doc.page_content[:200])
            
finally:
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    if os.path.exists(tmp_path):
        os.remove(tmp_path)

print("\nâœ… Wikipedia RAG completed!")
```

**å®Ÿè¡Œæ–¹æ³•**:
```bash
python example2-wiki.py
```

**å‡ºåŠ›ä¾‹**:
```
ğŸŒ Fetching Wikipedia page...
âœ… Fetched 70,159 characters
ğŸ“Š Indexing Wikipedia content...
Split into 118 chunks

ğŸ” Query: 'What animation studio did Miyazaki found?'
Selected cluster 0 at depth 0 (similarity: 0.7885)
Selected cluster 1 at depth 1 (similarity: 0.7720)

--- Result 1 ---
=== Studio Ghibli ===
==== Foundation and Laputa (1985â€“1987) ====...
```

**ä¸»ãªç‰¹å¾´**:
- ğŸ“¥ Wikipedia APIã‹ã‚‰å‹•çš„ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
- ğŸŒ³ 70,159æ–‡å­— â†’ 118ãƒãƒ£ãƒ³ã‚¯ â†’ éšå±¤åŒ–
- ğŸ” é«˜ç²¾åº¦æ¤œç´¢ï¼ˆé¡ä¼¼åº¦ 0.73-0.78ï¼‰
- ğŸŒ ä»»æ„ã®Wikipediaãƒšãƒ¼ã‚¸ã§åˆ©ç”¨å¯èƒ½

## ğŸ¯ ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹åˆ¥ã®è¨­å®š

### ã‚±ãƒ¼ã‚¹1: å°ã•ãªæ–‡æ›¸ï¼ˆ<10ä¸‡æ–‡å­—ï¼‰

```python
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=2,   # å°‘ãªã„ã‚¯ãƒ©ã‚¹ã‚¿
    max_depth=2,      # æµ…ã„éšå±¤
    chunk_size=500    # å°ã•ã„ãƒãƒ£ãƒ³ã‚¯
)
```

**é©ç”¨ä¾‹**: ãƒ–ãƒ­ã‚°è¨˜äº‹ã€çŸ­ã„è«–æ–‡ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

### ã‚±ãƒ¼ã‚¹2: ä¸­è¦æ¨¡æ–‡æ›¸ï¼ˆ10-50ä¸‡æ–‡å­—ï¼‰

```python
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=3,   # ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„ã‚¯ãƒ©ã‚¹ã‚¿æ•°
    max_depth=2,      # æ¨™æº–çš„ãªéšå±¤
    chunk_size=1000   # æ¨™æº–çš„ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
)
```

**é©ç”¨ä¾‹**: æŠ€è¡“æ›¸ã€é•·ç·¨è¨˜äº‹ã€ç ”ç©¶è«–æ–‡

### ã‚±ãƒ¼ã‚¹3: å¤§è¦æ¨¡æ–‡æ›¸ï¼ˆ>50ä¸‡æ–‡å­—ï¼‰

```python
raptor = RAPTORRetriever(
    embeddings_model=embeddings,
    llm=llm,
    max_clusters=5,   # å¤šãã®ã‚¯ãƒ©ã‚¹ã‚¿
    max_depth=3,      # æ·±ã„éšå±¤
    chunk_size=1500   # å¤§ãã„ãƒãƒ£ãƒ³ã‚¯
)
```

**é©ç”¨ä¾‹**: æ›¸ç±å…¨ä½“ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€è¤‡æ•°æ–‡æ›¸ã®çµåˆ

## ğŸ’¡ ã‚ˆãã‚ã‚‹è³ªå•

### Q1: ã©ã®ãã‚‰ã„ã®æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã‹ï¼Ÿ

**ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–**: 
- å°è¦æ¨¡ï¼ˆ<10ä¸‡æ–‡å­—ï¼‰: ~1åˆ†
- ä¸­è¦æ¨¡ï¼ˆ10-50ä¸‡æ–‡å­—ï¼‰: ~5åˆ†
- å¤§è¦æ¨¡ï¼ˆ>50ä¸‡æ–‡å­—ï¼‰: ~15åˆ†

**æ¤œç´¢**: å¸¸ã«1ç§’æœªæº€

### Q2: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯ï¼Ÿ

- **æœ€å°è¦ä»¶**: 8GB RAM
- **æ¨å¥¨**: 16GB RAM
- **GPU**: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆOllamaãŒè‡ªå‹•çš„ã«ä½¿ç”¨ï¼‰

### Q3: è¤‡æ•°ã®æ–‡æ›¸ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã§ãã¾ã™ã‹ï¼Ÿ

ã¯ã„ï¼è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦ãã ã•ã„ï¼š

```python
# è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
documents = []
for file in ["doc1.txt", "doc2.txt", "doc3.txt"]:
    with open(file, 'r', encoding='utf-8') as f:
        documents.append(f.read())

# çµåˆã—ã¦ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
combined = "\n\n".join(documents)
with open("combined.txt", 'w', encoding='utf-8') as f:
    f.write(combined)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
raptor.index("combined.txt")
```

### Q4: æ¤œç´¢çµæœã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ã«ã¯ï¼Ÿ

1. **ã‚ˆã‚Šæ·±ã„éšå±¤ã‚’è©¦ã™**: `max_depth=3`
2. **ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’å¢—ã‚„ã™**: `max_clusters=5`
3. **ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’èª¿æ•´**: å¤§ãã„ãƒ†ã‚­ã‚¹ãƒˆãªã‚‰ `chunk_size=1500`
4. **ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’å¢—ã‚„ã™**: `chunk_overlap=300`

### Q5: ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ï¼Ÿ

**"Connection refused"**:
```bash
# OllamaãŒèµ·å‹•ã—ã¦ã„ãªã„
ollama serve
```

**"Model not found"**:
```bash
# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull granite-code:8b
ollama pull mxbai-embed-large
```

**ãƒ¡ãƒ¢ãƒªä¸è¶³**:
```python
# ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å°ã•ã
raptor = RAPTORRetriever(chunk_size=500)
```

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹

```
=== Starting RAPTOR Indexing ===
Loaded document length: 624212 characters
Split into 864 chunks

=== Building tree at depth 0 with 864 documents ===
Cluster 0: 344 documents
Cluster 1: 219 documents
Cluster 2: 301 documents

=== Building tree at depth 1 with 344 documents ===
...

=== RAPTOR Tree Construction Complete ===

=== Searching for: 'philosophy' ===
Selected cluster 0 at depth 0 (similarity: 0.6691)
Selected cluster 2 at depth 1 (similarity: 0.6587)

=== Top 3 Results ===
Result 1: ãƒ—ãƒ©ãƒˆãƒ³ã®å“²å­¦çš„ä¿¡æ¡ã«ã¤ã„ã¦...
Result 2: ç†æƒ³çš„çŸ¥è­˜è«–ã«é–¢ã™ã‚‹è­°è«–...
Result 3: ãƒ—ãƒ©ãƒˆãƒ³çš„æ„›ã®æ¦‚å¿µ...
```

## ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### å­¦ç¿’ãƒªã‚½ãƒ¼ã‚¹

1. **è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [README.md](README.md)ã‚’å‚ç…§
2. **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**: è¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„LLMã‚’å¤‰æ›´
3. **æœ€é©åŒ–**: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ç²¾åº¦å‘ä¸Š

### ç™ºå±•çš„ãªä½¿ã„æ–¹

```python
# ã‚«ã‚¹ã‚¿ãƒ è¦ç´„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
class MyRAPTOR(RAPTORRetriever):
    def summarize_cluster(self, documents):
        # ç‹¬è‡ªã®è¦ç´„ãƒ­ã‚¸ãƒƒã‚¯
        pass

# è¤‡æ•°ã‚¯ã‚¨ãƒªã®ä¸¦åˆ—å®Ÿè¡Œ
queries = ["query1", "query2", "query3"]
results = [raptor.retrieve(q, top_k=3) for q in queries]
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] OllamaãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹
- [ ] `ollama serve` ãŒå®Ÿè¡Œä¸­
- [ ] ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ï¼ˆ`ollama list`ã§ç¢ºèªï¼‰
- [ ] Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- [ ] æ–‡æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- [ ] ååˆ†ãªãƒ¡ãƒ¢ãƒªï¼ˆæ¨å¥¨16GBï¼‰

## ğŸ“ ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆ:
1. GitHubã®Issueã‚’ç¢ºèª
2. æ–°ã—ã„Issueã‚’ä½œæˆ
3. README.mdã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§

---

ğŸ‰ ã“ã‚Œã§RAPTORã‚’ä½¿ã„å§‹ã‚ã‚‹æº–å‚™ãŒã§ãã¾ã—ãŸï¼
