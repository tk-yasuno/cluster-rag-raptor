"""
GMM+BIC vs CLASSIX æ¯”è¼ƒå®Ÿé¨“ (å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿)

ç›®çš„:
- GMM+BIC ã¨ CLASSIX ã®æ€§èƒ½ã‚’å…¬å¹³ã«æ¯”è¼ƒ
- ãƒ“ãƒ«ãƒ‰æ™‚é–“ã€ã‚¯ã‚¨ãƒªæ™‚é–“ã€æ¤œç´¢ç²¾åº¦ã‚’è©•ä¾¡
- ä¸¡æ‰‹æ³•ã®é•·æ‰€ãƒ»çŸ­æ‰€ã‚’æ˜ç¢ºåŒ–

ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: elements_of_statistical_learning.txt (ç´„190ä¸‡æ–‡å­—)
"""

from raptor_gmm import RAPTORRetrieverGMM
from raptor_classix import RAPTORRetrieverCLASSIX
from langchain_ollama import OllamaEmbeddings, ChatOllama
import time
from datetime import timedelta

print("=" * 80)
print("ğŸ”¬ GMM+BIC vs CLASSIX æ¯”è¼ƒå®Ÿé¨“ (å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿)")
print("=" * 80)
print()

# å…±é€šè¨­å®š
FILE_PATH = "elements_of_statistical_learning.txt"
QUERY = "statistical learning"  # ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ãŸã‚¯ã‚¨ãƒª
TOP_K = 5

# ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆä¸¡æ‰‹æ³•ã§å…±é€šï¼‰
embeddings_model = OllamaEmbeddings(model="mxbai-embed-large")
llm = ChatOllama(model="granite-code:8b", temperature=0.0)

print("ğŸ“Š å…±é€šè¨­å®š:")
print(f"   ãƒ‡ãƒ¼ã‚¿: {FILE_PATH}")
print(f"   ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: ç´„190ä¸‡æ–‡å­— (æ¨å®š3,660 chunks, chunk_size=500)")
print(f"   ã‚¯ã‚¨ãƒª: '{QUERY}'")
print(f"   Top-K: {TOP_K}")
print(f"   LLM: granite-code:8b")
print(f"   Embeddings: mxbai-embed-large (context length: 512)")
print(f"   âš ï¸  å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚å‡¦ç†æ™‚é–“ãŒé•·ããªã‚Šã¾ã™ (æ¨å®š10-20åˆ†)")
print()

# ================================================================================
# ãƒ†ã‚¹ãƒˆ1: GMM+BIC (BICè‡ªå‹•é¸æŠ)
# ================================================================================

print("=" * 80)
print("ğŸ“Š Test 1: GMM+BIC (BICè‡ªå‹•é¸æŠ)")
print("=" * 80)
print()

raptor_gmm = RAPTORRetrieverGMM(
    embeddings_model=embeddings_model,
    llm=llm,
    max_clusters=5,
    min_clusters=2,
    max_depth=2,
    chunk_size=500,      # mxbai-embed-large 512ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œ
    chunk_overlap=50,
    use_bic=True,
    clustering_method="gmm"
)

print("ğŸš€ GMM+BIC initialized")
print(f"   Parameters: max_clusters=5, min_clusters=2, max_depth=2")
print()

# ãƒ“ãƒ«ãƒ‰æ™‚é–“æ¸¬å®š
start_time = time.time()
raptor_gmm.index(FILE_PATH)
gmm_build_time = time.time() - start_time

print(f"\nâœ… GMM+BIC Indexing complete!")
print(f"   Build time: {gmm_build_time:.2f}ç§’ ({timedelta(seconds=int(gmm_build_time))})")
print()

# ã‚¯ã‚¨ãƒªæ™‚é–“æ¸¬å®š
start_time = time.time()
gmm_results = raptor_gmm.retrieve(QUERY, top_k=TOP_K)
gmm_query_time = time.time() - start_time

print(f"ğŸ” Query: {QUERY}")
print(f"âœ… Found {len(gmm_results)} results")
print(f"   Query time: {gmm_query_time:.3f}ç§’")
print()

# çµæœè¡¨ç¤º
if gmm_results:
    gmm_top_similarity = gmm_results[0].metadata.get('similarity', 0.0)
    print(f"   Top similarity: {gmm_top_similarity}")
    print()
    print("   Top 3 Results:")
    for i, doc in enumerate(gmm_results[:3], 1):
        score = doc.metadata.get('similarity', 0.0)
        print(f"   {i}. Similarity: {score}")
        print(f"      Preview: {doc.page_content[:100]}...")
        print()
else:
    gmm_top_similarity = 0.0
    print("   No results found")
    print()

# ================================================================================
# ãƒ†ã‚¹ãƒˆ2: CLASSIX (radius=1.0, æœ€é©è¨­å®š)
# ================================================================================

print("=" * 80)
print("ğŸ“Š Test 2: CLASSIX (radius=1.0, æœ€é©è¨­å®š)")
print("=" * 80)
print()

raptor_classix = RAPTORRetrieverCLASSIX(
    embeddings_model=embeddings_model,
    llm=llm,
    radius=1.0,          # å®Ÿé¨“ã§æ¤œè¨¼æ¸ˆã¿æœ€é©å€¤
    minPts=3,
    max_depth=2,
    chunk_size=500,      # mxbai-embed-large 512ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œ
    chunk_overlap=50,
    use_cosine=True
)

print("ğŸš€ CLASSIX initialized")
print(f"   Parameters: radius=1.0, minPts=3, max_depth=2")
print()

# ãƒ“ãƒ«ãƒ‰æ™‚é–“æ¸¬å®š
start_time = time.time()
raptor_classix.index(FILE_PATH)
classix_build_time = time.time() - start_time

print(f"\nâœ… CLASSIX Indexing complete!")
print(f"   Build time: {classix_build_time:.2f}ç§’ ({timedelta(seconds=int(classix_build_time))})")
print()

# ã‚¯ã‚¨ãƒªæ™‚é–“æ¸¬å®š
start_time = time.time()
classix_results = raptor_classix.retrieve(QUERY, top_k=TOP_K)
classix_query_time = time.time() - start_time

print(f"ğŸ” Query: {QUERY}")
print(f"âœ… Found {len(classix_results)} results")
print(f"   Query time: {classix_query_time:.3f}ç§’")
print()

# çµæœè¡¨ç¤º
if classix_results:
    classix_top_similarity = classix_results[0].metadata.get('similarity', 0.0)
    print(f"   Top similarity: {classix_top_similarity}")
    print()
    print("   Top 3 Results:")
    for i, doc in enumerate(classix_results[:3], 1):
        score = doc.metadata.get('similarity', 0.0)
        print(f"   {i}. Similarity: {score}")
        print(f"      Preview: {doc.page_content[:100]}...")
        print()
else:
    classix_top_similarity = 0.0
    print("   No results found")
    print()

# ================================================================================
# ãƒ†ã‚¹ãƒˆ3: CLASSIX (radius=0.5, ãƒãƒ©ãƒ³ã‚¹å‹) - ã‚¹ã‚­ãƒƒãƒ—
# ================================================================================
# ç†ç”±: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§éå‰°ãªç´°åˆ†åŒ–ãŒç™ºç”Ÿã—ã€å‡¦ç†æ™‚é–“ãŒæ¥µç«¯ã«é•·ã„
# (2380 chunks â†’ æ•°ç™¾ã®1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ â†’ å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§è¦ç´„ç”Ÿæˆ)

print("=" * 80)
print("ğŸ“Š Test 3: CLASSIX (radius=0.5, ãƒãƒ©ãƒ³ã‚¹å‹) - âš ï¸ ã‚¹ã‚­ãƒƒãƒ—")
print("=" * 80)
print()
print("âš ï¸  å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§radius=0.5ã¯éå‰°ãªç´°åˆ†åŒ–ã«ã‚ˆã‚ŠéåŠ¹ç‡")
print("   (æ¨å®šå‡¦ç†æ™‚é–“: 30-60åˆ†)")
print("   Test 1, 2ã®çµæœã‹ã‚‰ç·åˆè©•ä¾¡ã‚’å®Ÿæ–½ã—ã¾ã™")
print()

# ãƒ€ãƒŸãƒ¼å€¤ã‚’è¨­å®š
classix_balanced_build_time = 0.0
classix_balanced_query_time = 0.0
classix_balanced_top_similarity = 0.0
classix_balanced_results = []

# ================================================================================
# æ¯”è¼ƒã‚µãƒãƒªãƒ¼
# ================================================================================

print("=" * 80)
print("ğŸ“Š æ¯”è¼ƒã‚µãƒãƒªãƒ¼")
print("=" * 80)
print()

# è¡¨å½¢å¼ã§æ¯”è¼ƒ
print("| Method | Buildæ™‚é–“ | Queryæ™‚é–“ | Topé¡ä¼¼åº¦ | ç·æ™‚é–“ |")
print("|--------|-----------|-----------|----------|--------|")
print(f"| GMM+BIC | {gmm_build_time:.2f}ç§’ | {gmm_query_time:.3f}ç§’ | {gmm_top_similarity:.5f} | {gmm_build_time + gmm_query_time:.2f}ç§’ |")
print(f"| CLASSIX (r=1.0) | {classix_build_time:.2f}ç§’ | {classix_query_time:.3f}ç§’ | {classix_top_similarity:.5f} | {classix_build_time + classix_query_time:.2f}ç§’ |")
print(f"| CLASSIX (r=0.5) | ã‚¹ã‚­ãƒƒãƒ— | ã‚¹ã‚­ãƒƒãƒ— | ã‚¹ã‚­ãƒƒãƒ— | ã‚¹ã‚­ãƒƒãƒ— |")
print()
print("âš ï¸  æ³¨: CLASSIX r=0.5ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§éå‰°ç´°åˆ†åŒ–ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
print()

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
print("ğŸ† ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ:")
print()

# æœ€é€Ÿãƒ“ãƒ«ãƒ‰ (r=0.5ã‚’é™¤å¤–)
fastest_build = min(gmm_build_time, classix_build_time)
if fastest_build == gmm_build_time:
    fastest_build_method = "GMM+BIC"
else:
    fastest_build_method = "CLASSIX (r=1.0)"
print(f"âœ… æœ€é€Ÿãƒ“ãƒ«ãƒ‰: {fastest_build_method} ({fastest_build:.2f}ç§’)")

# æœ€é€Ÿã‚¯ã‚¨ãƒª (r=0.5ã‚’é™¤å¤–)
fastest_query = min(gmm_query_time, classix_query_time)
if fastest_query == gmm_query_time:
    fastest_query_method = "GMM+BIC"
else:
    fastest_query_method = "CLASSIX (r=1.0)"
print(f"âœ… æœ€é€Ÿã‚¯ã‚¨ãƒª: {fastest_query_method} ({fastest_query:.3f}ç§’)")

# æœ€é«˜ç²¾åº¦ (r=0.5ã‚’é™¤å¤–)
best_similarity = max(gmm_top_similarity, classix_top_similarity)
if best_similarity == gmm_top_similarity:
    best_similarity_method = "GMM+BIC"
else:
    best_similarity_method = "CLASSIX (r=1.0)"
print(f"âœ… æœ€é«˜ç²¾åº¦: {best_similarity_method} ({best_similarity:.5f})")
print()

# é€Ÿåº¦å·®ã®è¨ˆç®—
print("ğŸ“ˆ é€Ÿåº¦æ¯”è¼ƒ:")
print()
print(f"   GMM+BIC vs CLASSIX (r=1.0):")
if classix_build_time > 0:
    build_speedup = gmm_build_time / classix_build_time
    if build_speedup > 1:
        print(f"      Build: CLASSIX ãŒ {build_speedup:.2f}x é€Ÿã„")
    else:
        print(f"      Build: GMM+BIC ãŒ {1/build_speedup:.2f}x é€Ÿã„")
if classix_query_time > 0:
    query_speedup = gmm_query_time / classix_query_time
    if query_speedup > 1:
        print(f"      Query: CLASSIX ãŒ {query_speedup:.2f}x é€Ÿã„")
    else:
        print(f"      Query: GMM+BIC ãŒ {1/query_speedup:.2f}x é€Ÿã„")
print()

# ç²¾åº¦æ¯”è¼ƒ
print("ğŸ¯ ç²¾åº¦æ¯”è¼ƒ:")
print()
print(f"   GMM+BIC:          {gmm_top_similarity:.5f}")
print(f"   CLASSIX (r=1.0):  {classix_top_similarity:.5f} (å·®: {classix_top_similarity - gmm_top_similarity:+.5f})")
print()

# ç·åˆè©•ä¾¡
print("=" * 80)
print("ğŸ’¡ ç·åˆè©•ä¾¡")
print("=" * 80)
print()

# ã©ã¡ã‚‰ãŒå„ªã‚Œã¦ã„ã‚‹ã‹åˆ¤å®š
total_gmm = gmm_build_time + gmm_query_time
total_classix = classix_build_time + classix_query_time

print("âš–ï¸ ç·åˆã‚¹ã‚³ã‚¢ (é€Ÿåº¦ + ç²¾åº¦):")
print()

# æ­£è¦åŒ–ã‚¹ã‚³ã‚¢è¨ˆç®—
min_time = min(total_gmm, total_classix)
max_time = max(total_gmm, total_classix)
min_sim = min(gmm_top_similarity, classix_top_similarity)
max_sim = max(gmm_top_similarity, classix_top_similarity)

# ã‚¹ã‚³ã‚¢è¨ˆç®— (æ™‚é–“ã¯å°ã•ã„ã»ã©è‰¯ã„ã€é¡ä¼¼åº¦ã¯å¤§ãã„ã»ã©è‰¯ã„)
def calculate_score(build_time, query_time, similarity):
    total_time = build_time + query_time
    # æ™‚é–“ã‚¹ã‚³ã‚¢: 0-50ç‚¹ (é€Ÿã„ã»ã©é«˜å¾—ç‚¹)
    if max_time > min_time:
        time_score = 50 * (1 - (total_time - min_time) / (max_time - min_time))
    else:
        time_score = 50
    # ç²¾åº¦ã‚¹ã‚³ã‚¢: 0-50ç‚¹ (é«˜ã„ã»ã©é«˜å¾—ç‚¹)
    if max_sim > min_sim:
        accuracy_score = 50 * (similarity - min_sim) / (max_sim - min_sim)
    else:
        accuracy_score = 50
    return time_score + accuracy_score

gmm_score = calculate_score(gmm_build_time, gmm_query_time, gmm_top_similarity)
classix_score = calculate_score(classix_build_time, classix_query_time, classix_top_similarity)

print(f"   GMM+BIC:          {gmm_score:.1f}/100")
print(f"   CLASSIX (r=1.0):  {classix_score:.1f}/100")
print()

# æ¨å¥¨
if classix_score > gmm_score:
    print("ğŸ† æ¨å¥¨: CLASSIX (radius=1.0)")
    print("   ç†ç”±: æœ€é€Ÿãƒ“ãƒ«ãƒ‰ & é«˜ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹")
else:
    print("ğŸ† æ¨å¥¨: GMM+BIC")
    print("   ç†ç”±: è‡ªå‹•ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é¸æŠã®æŸ”è»Ÿæ€§")
print()

print("=" * 80)
print("å®Ÿé¨“å®Œäº†ï¼")
print("=" * 80)
