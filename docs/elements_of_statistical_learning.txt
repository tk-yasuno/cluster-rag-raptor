Springer Series in Statistics
Trevor Hastie
Robert TibshiraniJerome FriedmanSpringer Series in Statistics
The Elements of
Statistical Learning
Data Mining, Inference, and Prediction
The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-
nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.
This major new edition features many topics not covered in the original, including graphical
models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at
Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.
›springer.comSTATISTICS
isbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman
The Elements of Statictical Learning
Hastie • Tibshirani • Friedman
Second Edition

This is page v
Printer: Opaque this
To our parents:
Valerie and Patrick Hastie
Vera and Sami Tibshirani
Florence and Harry Friedman
and to our families:
Samantha, Timothy, and Lynda
Charlie, Ryan, Julie, and Cheryl
Melanie, Dora, Monika, and Ildiko

vi

This is page vii
Printer: Opaque this
Preface to the Second Edition
In God we trust, all others bring data.
–William Edwards Deming (1900-1993)1
We have been gratiﬁed by the popularity of the ﬁrst edition of The
Elements of Statistical Learning. This, along with the fast pace of research
in the statistical learning ﬁeld, motivated us to update our book with a
second edition.
We have added four new chapters and updated some of the existi ng
chapters. Because many readers are familiar with the layout of the ﬁrst
edition, we have tried to change it as little as possible. Her e is a summary
of the main changes:
1On the Web, this quote has been widely attributed to both Deming and R obert W.
Hayden; however Professor Hayden told us that he can claim no credit for th is quote,
and ironically we could ﬁnd no “data” conﬁrming that Deming actual ly said this.

viii Preface to the Second Edition
Chapter What’s new
1.Introduction
2.Overview of Supervised Learning
3.Linear Methods for Regression LAR algorithm and generaliza tions
of the lasso
4.Linear Methods for Classiﬁcation Lasso path for logistic re gression
5.Basis Expansions and Regulariza-
tionAdditional illustrations of RKHS
6.Kernel Smoothing Methods
7.Model Assessment and Selection Strengths and pitfalls of cr oss-
validation
8.Model Inference and Averaging
9.Additive Models, Trees, and
Related Methods
10.Boosting and Additive Trees New example from ecology; some
material split oﬀ to Chapter 16.
11.Neural Networks Bayesian neural nets and the NIPS
2003 challenge
12.Support Vector Machines and
Flexible DiscriminantsPath algorithm for SVM classiﬁer
13.Prototype Methods and
Nearest-Neighbors
14.Unsupervised Learning Spectral clustering, kernel PCA,
sparse PCA, non-negative matrix
factorization archetypal analysis,
nonlinear dimension reduction,
Google page rank algorithm, a
direct approach to ICA
15.Random Forests New
16.Ensemble Learning New
17.Undirected Graphical Models New
18.High-Dimensional Problems New
Some further notes:
•Our ﬁrst edition was unfriendly to colorblind readers; in pa rticular,
we tended to favor red/greencontrasts which are particularly trou-
blesome. We have changed the color palette in this edition to a large
extent, replacing the above with an orange/bluecontrast.
•We have changed the name of Chapter 6 from “Kernel Methods” to
“Kernel Smoothing Methods”, to avoid confusion with the mac hine-
learningkernelmethodthatisdiscussedinthecontextofsu pportvec-
tor machines (Chapter 12) and more generally in Chapters 5 an d 14.
•In the ﬁrst edition, the discussion of error-rate estimatio n in Chap-
ter 7 was sloppy, as we did not clearly diﬀerentiate the notio ns of
conditional error rates (conditional on the training set) a nd uncondi-
tional rates. We have ﬁxed this in the new edition.

Preface to the Second Edition ix
•Chapters 15 and 16 follow naturally from Chapter 10, and the c hap-
ters are probably best read in that order.
•In Chapter 17, we have not attempted a comprehensive treatme nt
of graphical models, and discuss only undirected models and some
new methods for their estimation. Due to a lack of space, we ha ve
speciﬁcally omitted coverage of directed graphical models .
•Chapter 18 explores the “ p≫N” problem, which is learning in high-
dimensional feature spaces. These problems arise in many ar eas, in-
cluding genomic and proteomic studies, and document classi ﬁcation.
We thank the many readers who have found the (too numerous) er rors in
the ﬁrst edition. We apologize for those and have done our bes t to avoid er-
rorsinthisnewedition.WethankMarkSegal,BalaRajaratna m,andLarry
Wasserman for comments on some of the new chapters, and many S tanford
graduate and post-doctoral students who oﬀered comments, i n particular
Mohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Male ki, Donal
McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and
Hui Zou. We thank John Kimmel for his patience in guiding us th rough this
new edition. RT dedicates this edition to the memory of Anna M cPhee.
Trevor Hastie
Robert Tibshirani
Jerome Friedman
Stanford, California
August 2008

x Preface to the Second Edition

This is page xi
Printer: Opaque this
Preface to the First Edition
We are drowning in information and starving for knowledge.
–Rutherford D. Roger
The ﬁeld of Statistics is constantly challenged by the probl ems that science
andindustrybringstoitsdoor.Intheearlydays,theseprob lemsoftencame
from agricultural and industrial experiments and were rela tively small in
scope. With the advent of computers and the information age, statistical
problems have exploded both in size and complexity. Challen ges in the
areas of data storage, organization and searching have led t o the new ﬁeld
of “data mining”; statistical and computational problems i n biology and
medicine have created “bioinformatics.” Vast amounts of da ta are being
generated in many ﬁelds, and the statistician’s job is to mak e sense of it
all: to extract important patterns and trends, and understa nd “what the
data says.” We call this learning from data .
The challenges in learning from data have led to a revolution in the sta-
tisticalsciences.Sincecomputationplayssuchakeyrole, itisnotsurprising
that much of this new development has been done by researcher s in other
ﬁelds such as computer science and engineering.
The learning problems that we consider can be roughly catego rized as
eithersupervised orunsupervised . In supervised learning, the goal is to pre-
dict the value of an outcome measure based on a number of input measures;
in unsupervised learning, there is no outcome measure, and t he goal is to
describe the associations and patterns among a set of input m easures.

xii Preface to the First Edition
This book is our attempt to bring together many of the importa nt new
ideas in learning, and explain them in a statistical framewo rk. While some
mathematical details are needed, we emphasize the methods a nd their con-
ceptual underpinnings rather than their theoretical prope rties. As a result,
we hope that this book will appeal not just to statisticians b ut also to
researchers and practitioners in a wide variety of ﬁelds.
Just as we have learned a great deal from researchers outside of the ﬁeld
of statistics, our statistical viewpoint may help others to better understand
diﬀerent aspects of learning:
There is no true interpretation of anything; interpretatio n is a
vehicle in the service of human comprehension. The value of
interpretation is in enabling others to fruitfully think ab out an
idea.
–Andreas Buja
We would like to acknowledge the contribution of many people to the
conception and completion of this book. David Andrews, Leo B reiman,
Andreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton , Werner
Stuetzle, and John Tukey have greatly inﬂuenced our careers . Balasub-
ramanian Narasimhan gave us advice and help on many computat ional
problems, and maintained an excellent computing environme nt. Shin-Ho
Bang helped in the production of a number of the ﬁgures. Lee Wi lkinson
gavevaluabletipsoncolorproduction.IlanaBelitskaya,E vaCantoni,Maya
Gupta,MichaelJordan,ShantiGopatam,RadfordNeal,Jorge Picazo,Bog-
dan Popescu, Olivier Renaud, Saharon Rosset, John Storey, J i Zhu, Mu
Zhu, two reviewers and many students read parts of the manusc ript and
oﬀered helpful suggestions. John Kimmel was supportive, pa tient and help-
ful at every phase; MaryAnn Brickner and Frank Ganz headed a s uperb
production team at Springer. Trevor Hastie would like to tha nk the statis-
tics department at the University of Cape Town for their hosp itality during
the ﬁnal stages of this book. We gratefully acknowledge NSF a nd NIH for
their support of this work. Finally, we would like to thank ou r families and
our parents for their love and support.
Trevor Hastie
Robert Tibshirani
Jerome Friedman
Stanford, California
May 2001
The quiet statisticians have changed our world; not by disco v-
ering new facts or technical developments, but by changing t he
ways that we reason, experiment and form our opinions ....
–Ian Hacking

This is page xiii
Printer: Opaque this
Contents
Preface to the Second Edition vii
Preface to the First Edition xi
1 Introduction 1
2 Overview of Supervised Learning 9
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9
2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9
2.3 Two Simple Approaches to Prediction:
Least Squares and Nearest Neighbors . . . . . . . . . . . 11
2.3.1 Linear Models and Least Squares . . . . . . . . 11
2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14
2.3.3 From Least Squares to Nearest Neighbors . . . . 16
2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18
2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22
2.6 Statistical Models, Supervised Learning
and Function Approximation . . . . . . . . . . . . . . . . 28
2.6.1 A Statistical Model
for the Joint Distribution Pr( X,Y) . . . . . . . 28
2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29
2.6.3 Function Approximation . . . . . . . . . . . . . 29
2.7 Structured Regression Models . . . . . . . . . . . . . . . 32
2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32

xiv Contents
2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33
2.8.1 Roughness Penalty and Bayesian Methods . . . 34
2.8.2 Kernel Methods and Local Regression . . . . . . 34
2.8.3 Basis Functions and Dictionary Methods . . . . 35
2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3 Linear Methods for Regression 43
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43
3.2 Linear Regression Models and Least Squares . . . . . . . 44
3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49
3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51
3.2.3 Multiple Regression
from Simple Univariate Regression . . . . . . . . 52
3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56
3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57
3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57
3.3.2 Forward- and Backward-Stepwise Selection . . . 58
3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60
3.3.4 Prostate Cancer Data Example (Continued) . . 61
3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61
3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61
3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68
3.4.3 Discussion: Subset Selection, Ridge Regression
and the Lasso . . . . . . . . . . . . . . . . . . . 69
3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73
3.5 Methods Using Derived Input Directions . . . . . . . . . 79
3.5.1 Principal Components Regression . . . . . . . . 79
3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80
3.6 Discussion: A Comparison of the Selection
and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82
3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84
3.8 More on the Lasso and Related Path Algorithms . . . . . 86
3.8.1 Incremental Forward Stagewise Regression . . . 86
3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89
3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89
3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90
3.8.5 Further Properties of the Lasso . . . . . . . . . . 91
3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92
3.9 Computational Considerations . . . . . . . . . . . . . . . 93
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94

Contents xv
4 Linear Methods for Classiﬁcation 101
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101
4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103
4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106
4.3.1 Regularized Discriminant Analysis . . . . . . . . 112
4.3.2 Computations for LDA . . . . . . . . . . . . . . 113
4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113
4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119
4.4.1 Fitting Logistic Regression Models . . . . . . . . 120
4.4.2 Example: South African Heart Disease . . . . . 122
4.4.3 Quadratic Approximations and Inference . . . . 124
4.4.4L1Regularized Logistic Regression . . . . . . . . 125
4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127
4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129
4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130
4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
5 Basis Expansions and Regularization 139
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139
5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141
5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144
5.2.2 Example:SouthAfricanHeartDisease(Continued)146
5.2.3 Example: Phoneme Recognition . . . . . . . . . 148
5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150
5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151
5.4.1 Degrees of Freedom and Smoother Matrices . . . 153
5.5 Automatic Selection of the Smoothing Parameters . . . . 15 6
5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158
5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158
5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161
5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162
5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167
5.8.1 Spaces of Functions Generated by Kernels . . . 168
5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170
5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174
5.9.1 Wavelet Bases and the Wavelet Transform . . . 176
5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
Appendix: Computational Considerations for Splines . . . . . . 186
Appendix:B-splines . . . . . . . . . . . . . . . . . . . . . 186
Appendix: Computations for Smoothing Splines . . . . . 189

xvi Contents
6 Kernel Smoothing Methods 191
6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192
6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194
6.1.2 Local Polynomial Regression . . . . . . . . . . . 197
6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198
6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200
6.4 Structured Local Regression Models in IRp. . . . . . . . 201
6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203
6.4.2 Structured Regression Functions . . . . . . . . . 203
6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205
6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 20 8
6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208
6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210
6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210
6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212
6.8 Mixture Models for Density Estimation and Classiﬁcatio n 214
6.9 Computational Considerations . . . . . . . . . . . . . . . 216
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
7 Model Assessment and Selection 219
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219
7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219
7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223
7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226
7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228
7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230
7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232
7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233
7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235
7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237
7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239
7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241
7.10.1K-Fold Cross-Validation . . . . . . . . . . . . . 241
7.10.2 The Wrong and Right Way
to Do Cross-validation . . . . . . . . . . . . . . . 245
7.10.3 Does Cross-Validation Really Work? . . . . . . . 247
7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249
7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252
7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
8 Model Inference and Averaging 261
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261

Contents xvii
8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261
8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261
8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265
8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267
8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267
8.4 Relationship Between the Bootstrap
and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271
8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272
8.5.1 Two-Component Mixture Model . . . . . . . . . 272
8.5.2 The EM Algorithm in General . . . . . . . . . . 276
8.5.3 EM as a Maximization–Maximization Procedure 277
8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279
8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
8.7.1 Example: Trees with Simulated Data . . . . . . 283
8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288
8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
9 Additive Models, Trees, and Related Methods 295
9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295
9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297
9.1.2 Example: Additive Logistic Regression . . . . . 299
9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304
9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305
9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305
9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307
9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308
9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310
9.2.5 Spam Example (Continued) . . . . . . . . . . . 313
9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317
9.3.1 Spam Example (Continued) . . . . . . . . . . . 320
9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 3 21
9.4.1 Spam Example (Continued) . . . . . . . . . . . 326
9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327
9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328
9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329
9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332
9.7 Computational Considerations . . . . . . . . . . . . . . . 334
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
10 Boosting and Additive Trees 337
10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337
10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340

xviii Contents
10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341
10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342
10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343
10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345
10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346
10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 35 0
10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352
10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353
10.10 Numerical Optimization via Gradient Boosting . . . . . . 358
10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358
10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359
10.10.3 Implementations of Gradient Boosting . . . . . . 360
10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361
10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364
10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364
10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365
10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367
10.13.1 Relative Importance of Predictor Variables . . . 367
10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369
10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371
10.14.1 California Housing . . . . . . . . . . . . . . . . . 371
10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375
10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
11 Neural Networks 389
11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389
11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389
11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392
11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395
11.5 Some Issues in Training Neural Networks . . . . . . . . . 397
11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397
11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398
11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398
11.5.4 Number of Hidden Units and Layers . . . . . . . 400
11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400
11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401
11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404
11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408
11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409
11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410
11.9.2 Performance Comparisons . . . . . . . . . . . . 412
11.10 Computational Considerations . . . . . . . . . . . . . . . 414
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415

Contents xix
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
12 Support Vector Machines and
Flexible Discriminants 417
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417
12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417
12.2.1 Computing the Support Vector Classiﬁer . . . . 420
12.2.2 Mixture Example (Continued) . . . . . . . . . . 421
12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423
12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423
12.3.2 The SVM as a Penalization Method . . . . . . . 426
12.3.3 Function Estimation and Reproducing Kernels . 428
12.3.4 SVMs and the Curse of Dimensionality . . . . . 431
12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432
12.3.6 Support Vector Machines for Regression . . . . . 434
12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436
12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438
12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 4 38
12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440
12.5.1 Computing the FDA Estimates . . . . . . . . . . 444
12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446
12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449
12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
13 Prototype Methods and Nearest-Neighbors 459
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459
13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459
13.2.1K-means Clustering . . . . . . . . . . . . . . . . 460
13.2.2 Learning Vector Quantization . . . . . . . . . . 462
13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463
13.3k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463
13.3.1 Example: A Comparative Study . . . . . . . . . 468
13.3.2 Example: k-Nearest-Neighbors
and Image Scene Classiﬁcation . . . . . . . . . . 470
13.3.3 Invariant Metrics and Tangent Distance . . . . . 471
13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475
13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478
13.4.2 Global Dimension Reduction
for Nearest-Neighbors . . . . . . . . . . . . . . . 479
13.5 Computational Considerations . . . . . . . . . . . . . . . 480
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481

xx Contents
14 Unsupervised Learning 485
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485
14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487
14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488
14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489
14.2.3 Example: Market Basket Analysis . . . . . . . . 492
14.2.4 Unsupervised as Supervised Learning . . . . . . 495
14.2.5 Generalized Association Rules . . . . . . . . . . 497
14.2.6 Choice of Supervised Learning Method . . . . . 499
14.2.7 Example: Market Basket Analysis (Continued) . 499
14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501
14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503
14.3.2 Dissimilarities Based on Attributes . . . . . . . 503
14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505
14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507
14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507
14.3.6K-means . . . . . . . . . . . . . . . . . . . . . . 509
14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510
14.3.8 Example: Human Tumor Microarray Data . . . 512
14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514
14.3.10K-medoids . . . . . . . . . . . . . . . . . . . . . 515
14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518
14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520
14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528
14.5 Principal Components, Curves and Surfaces . . . . . . . . 53 4
14.5.1 Principal Components . . . . . . . . . . . . . . . 534
14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541
14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544
14.5.4 Kernel Principal Components . . . . . . . . . . . 547
14.5.5 Sparse Principal Components . . . . . . . . . . . 550
14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553
14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554
14.7 Independent Component Analysis
and Exploratory Projection Pursuit . . . . . . . . . . . . 557
14.7.1 Latent Variables and Factor Analysis . . . . . . 558
14.7.2 Independent Component Analysis . . . . . . . . 560
14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565
14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565
14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570
14.9 Nonlinear Dimension Reduction
and Local Multidimensional Scaling . . . . . . . . . . . . 572
14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579

Contents xxi
15 Random Forests 587
15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587
15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587
15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592
15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592
15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593
15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595
15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596
15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597
15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597
15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600
15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603
16 Ensemble Learning 605
16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605
16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607
16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607
16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610
16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613
16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616
16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617
16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
17 Undirected Graphical Models 625
17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625
17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627
17.3 Undirected Graphical Models for Continuous Variables . 630
17.3.1 Estimation of the Parameters
when the Graph Structure is Known . . . . . . . 631
17.3.2 Estimation of the Graph Structure . . . . . . . . 635
17.4 Undirected Graphical Models for Discrete Variables . . . 638
17.4.1 Estimation of the Parameters
when the Graph Structure is Known . . . . . . . 639
17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641
17.4.3 Estimation of the Graph Structure . . . . . . . . 642
17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645
18 High-Dimensional Problems: p≫N 649
18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649

xxii Contents
18.2 Diagonal Linear Discriminant Analysis
and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651
18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654
18.3.1 Regularized Discriminant Analysis . . . . . . . . 656
18.3.2 Logistic Regression
with Quadratic Regularization . . . . . . . . . . 657
18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657
18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658
18.3.5 Computational Shortcuts When p≫N. . . . . 659
18.4 Linear Classiﬁers with L1Regularization . . . . . . . . . 661
18.4.1 Application of Lasso
to Protein Mass Spectroscopy . . . . . . . . . . 664
18.4.2 The Fused Lasso for Functional Data . . . . . . 666
18.5 Classiﬁcation When Features are Unavailable . . . . . . . 6 68
18.5.1 Example: String Kernels
and Protein Classiﬁcation . . . . . . . . . . . . . 668
18.5.2 Classiﬁcation and Other Models Using
Inner-Product Kernels and Pairwise Distances . 670
18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672
18.6 High-Dimensional Regression:
Supervised Principal Components . . . . . . . . . . . . . 674
18.6.1 Connection to Latent-Variable Modeling . . . . 678
18.6.2 Relationship with Partial Least Squares . . . . . 680
18.6.3 Pre-Conditioning for Feature Selection . . . . . 681
18.7 Feature Assessment and the Multiple-Testing Problem . . 683
18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687
18.7.2 Asymmetric Cutpoints and the SAM Procedure 690
18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692
18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694
References 699
Author Index 729
Index 737

This is page 1
Printer: Opaque this
1
Introduction
Statistical learning plays a key role in many areas of science, ﬁnance and
industry. Here are some examples of learning problems:
•Predict whether a patient, hospitalized due to a heart attac k, will
have a second heart attack. The prediction is to be based on de mo-
graphic, diet and clinical measurements for that patient.
•Predict the price of a stock in 6 months from now, on the basis o f
company performance measures and economic data.
•Identify the numbers in a handwritten ZIP code, from a digiti zed
image.
•Estimate the amount of glucose in the blood of a diabetic pers on,
from the infrared absorption spectrum of that person’s bloo d.
•Identify the risk factors for prostate cancer, based on clin ical and
demographic variables.
The science of learning plays a key role in the ﬁelds of statis tics, data
mining and artiﬁcial intelligence, intersecting with area s of engineering and
other disciplines.
This book is about learning from data. In a typical scenario, we have
an outcome measurement, usually quantitative (such as a sto ck price) or
categorical (such as heart attack/no heart attack), that we wish to predict
based on a set of features (such as diet and clinical measurements). We
have atraining set of data, in which we observe the outcome and feature

2 1. Introduction
TABLE 1.1. Average percentage of words or characters in an email messag e
equal to the indicated word or character. We have chosen the w ords and characters
showing the largest diﬀerence between spamandemail.
george you your hp free hpl ! our re edu remove
spam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28
email 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01
measurements for a set of objects (such as people). Using thi s data we build
a prediction model, or learner, which will enable us to predict the outcome
for new unseen objects. A good learner is one that accurately predicts such
an outcome.
The examples above describe what is called the supervised learning prob-
lem. It is called “supervised” because of the presence of the outcome vari-
able to guide the learning process. In the unsupervised learning problem ,
we observe only the features and have no measurements of the o utcome.
Our task is rather to describe how the data are organized or cl ustered. We
devote most of this book to supervised learning; the unsuper vised problem
is less developed in the literature, and is the focus of Chapt er 14.
Here are some examples of real learning problems that are dis cussed in
this book.
Example 1: Email Spam
The data for this example consists of information from 4601 e mail mes-
sages, in a study to try to predict whether the email was junk e mail, or
“spam.” The objective was to design an automatic spam detect or that
could ﬁlter out spam before clogging the users’ mailboxes. F or all 4601
email messages, the true outcome (email type) emailorspamis available,
along with the relative frequencies of 57 of the most commonl y occurring
words and punctuation marks in the email message. This is a su pervised
learning problem, with the outcome the class variable email/spam. It is also
called aclassiﬁcation problem.
Table 1.1 lists the words and characters showing the largest average
diﬀerence between spamandemail.
Our learning method has to decide which features to use and ho w: for
example, we might use a rule such as
if (%george<0.6) & (%you>1.5) then spam
elseemail.
Another form of a rule might be:
if (0.2·%you−0.3·%george)>0 then spam
elseemail.

1. Introduction 3
lpsa−1 1 2 3 4
ooo ooo ooo oo o ooooo o oooo oooooooooo ooooooooooooooo oo ooooooooooooo oo oooo ooooooooooooooooooooooooooooo
oo o ooo ooo oo o oooooooooooooooooooo ooooo o ooooooooo oo ooooooo oooooo oo oooo ooooooooooooooooooooooooooooo40 50 60 70 80
oo o ooo ooo oo o oooooo o ooooooooooooo ooooooooooooooo o o o oooooo oo oooo oo oooo oooooooooooooo ooooooooooooooo
oo o ooo o o o oo ooooo o oo o o o ooooo ooo oo o oo o oo oo oo o o ooo o o oo ooo o o o ooooo o o o ooo o o o o oooo oo oo ooooo o o oo o o oooooo0.0 0.4 0.8
oo o ooo ooo oo o oooooooooooooooooooo oooooo o ooooooo o oo oooooooooooo o o o oooo oo o o oooo oo o ooo o oo o o ooo o oooooo
oo o ooo ooo oo o ooooooooo o o o oooooo oo oooo o o oooooooo o oo ooooo ooooooo o oo oooo ooo ooooooo o ooooooo oooo ooooooo6.0 7.0 8.0 9.0
oo o ooo ooo oo o oooooooooooooooooooo oooooooo o ooooo o oo ooooooooooooo oo oooo ooooo o ooooooooo o ooooooooooooo
0 1 2 3 4 5oo o ooo ooo oo o ooooooooooo o oo o o o ooo oooooooo o ooooo o oo o o o oooooooooo oo oo o o oo ooo o ooo o oooooooo oooo oooooo o−1 1 2 3 4ooo
oo
ooo
ooo
ooooo
oo
oooo
oo
oo
o
ooo
ooo
oooo
oo
ooo
ooooo
oo
ooo
ooo
oo
ooo
ooo
oo
ooo
ooo
ooooo
oooo
oooo
oo
ooo
oo
ooo
ooo
lcavol
ooo
oo
ooo
ooo
ooooo
oo
oooo
oo
oo
o
ooo
ooo
oooo
oo
ooo
ooooo
oo
ooo
ooo
oo
ooo
ooo
oo
ooo
ooo
ooooo
oooo
oooo
oo
ooo
oo
ooo
ooo
ooo
oo
ooo
ooo
ooooo
oo
oooo
oo
oo
o
ooo
ooo
oooo
oo
ooo
ooooo
oo
ooo
ooo
oo
o oo
ooo
oo
ooo
ooo
ooooo
oooo
oo oo
oo
ooo
oo
ooo
ooo
ooo
oo
ooo
ooo
ooooo
oo
oooo
oo
oo
o
ooo
ooo
ooo o
oo
o oo
ooooo
oo
o oo
ooo
oo
ooo
ooo
o o
ooo
ooo
o oooo
oo oo
oooo
oo
ooo
oo
ooo
o oo
ooo
oo
ooo
ooo
ooooo
oo
oooo
oo
oo
o
ooo
ooo
oooo
oo
ooo
ooooo
oo
ooo
ooo
oo
ooo
ooo
o o
ooo
ooo
o oooo
oo o o
oo o o
oo
ooo
oo
ooo
ooo
ooo
oo
ooo
ooo
ooooo
oo
oooo
oo
oo
o
ooo
ooo
ooo o
oo
ooo
ooooo
oo
ooo
ooo
oo
ooo
ooo
o o
ooo
ooo
ooooo
oo o o
oooo
oo
ooo
oo
ooo
ooo
ooo
oo
ooo
ooo
ooooo
oo
oooo
oo
oo
o
ooo
ooo
oooo
oo
o oo
ooooo
oo
ooo
ooo
oo
ooo
ooo
oo
ooo
ooo
ooooo
oooo
ooo o
oo
ooo
oo
ooo
ooo
ooo
oo
ooo
ooo
ooooo
oo
oooo
oo
oo
o
ooo
ooo
oooo
oo
o oo
ooooo
oo
o oo
ooo
oo
ooo
ooo
o o
oo o
ooo
ooooo
oo oo
oooo
oo
ooo
oo
ooo
o oo
oo
oooooo o
ooo
ooo
ooo
oo
ooooo
ooo
ooo
o
oooo
o
oo
ooooo
oooo
oooo
oo
oo
ooo
oo
ooo
oooooo
o
oo
oo
ooo
oo
ooooooo
oo
oo
ooo
ooo
oo
oooooo o
ooo
ooo
ooo
oo
ooooo
ooo
ooo
o
oooo
o
oo
ooooo
oooo
oooo
oo
oo
ooo
oo
ooo
oooooo
o
oo
oo
ooo
oo
ooooooo
oo
oo
ooo
ooolweight
oo
oooooo o
ooo
ooo
ooo
oo
ooooo
ooo
ooo
o
oooo
o
oo
ooooo
oooo
o ooo
oo
oo
ooo
oo
ooo
oooooo
o
oo
oo
ooo
oo
ooooooo
oo
oo
oo o
ooo
oo
oooooo o
oo o
ooo
ooo
oo
o oooo
ooo
ooo
o
oooo
o
oo
oooo o
oooo
o ooo
oo
oo
ooo
oo
ooo
oooooo
o
oo
oo
ooo
oo
ooooooo
oo
oo
ooo
ooo
oo
oooooo o
ooo
ooo
ooo
oo
ooooo
ooo
ooo
o
oooo
o
oo
ooooo
ooo o
oooo
oo
oo
ooo
oo
ooo
oooooo
o
oo
oo
ooo
oo
ooo oooo
oo
oo
ooo
ooo
oo
oooooo o
ooo
ooo
ooo
oo
o oo oo
ooo
ooo
o
oooo
o
oo
ooooo
ooo o
oooo
oo
oo
ooo
oo
ooo
oooooo
o
oo
oo
ooo
oo
ooooooo
oo
oo
ooo
ooo
oo
oooooo o
ooo
ooo
ooo
oo
ooooo
ooo
ooo
o
oooo
o
oo
ooooo
ooo o
oooo
oo
oo
ooo
oo
ooo
oooooo
o
oo
oo
ooo
oo
ooo oooo
oo
oo
ooo
ooo
2.5 3.5 4.5oo
oooooo o
ooo
ooo
ooo
oo
ooo oo
ooo
ooo
o
oooo
o
oo
ooooo
ooo o
oooo
oo
oo
ooo
oo
ooo
oo oooo
o
oo
oo
oo o
oo
ooooooo
oo
oo
ooo
ooo40 50 60 70 80ooo
oo
oo
o
oooo oo
ooo
o
oo
oooooooo oo o oo
oooo
oo
ooo
ooo
oo
o
ooo
o oo
oo
ooo
oo
oo
ooooooo
oo
o
ooooo
o
oo
oo
o
oo
oooo
o
oo
ooo o
ooo
oo
oo
o
oooo oo
ooo
o
oo
oo oooooo oo o oo
oooo
oo
ooo
ooo
oo
o
ooo
o oo
oo
ooo
oo
oo
ooooooo
oo
o
ooooo
o
oo
oo
o
oo
oooo
o
oo
ooo o
ooo
oo
oo
o
oooo oo
ooo
o
oo
oooooooo oo o oo
oooo
oo
ooo
ooo
oo
o
ooo
o oo
oo
ooo
oo
oo
ooooooo
oo
o
ooooo
o
oo
oo
o
oo
oooo
o
oo
oooo
age
ooo
oo
oo
o
oooo oo
ooo
o
oo
o o oooooo oo o oo
oo oo
oo
ooo
o oo
oo
o
ooo
o oo
oo
ooo
oo
oo
ooooooo
oo
o
ooooo
o
oo
oo
o
oo
oooo
o
oo
ooo o
ooo
oo
oo
o
oooo oo
ooo
o
oo
oooooooo oo o oo
oooo
oo
ooo
ooo
oo
o
ooo
o oo
oo
ooo
oo
oo
ooooooo
oo
o
ooooo
o
oo
oo
o
oo
oooo
o
oo
ooo o
ooo
oo
oo
o
oooo oo
ooo
o
oo
o o oooooo oo o oo
oooo
oo
ooo
ooo
oo
o
ooo
o oo
oo
ooo
oo
oo
ooooooo
oo
o
ooooo
o
oo
oo
o
oo
oooo
o
oo
ooo o
ooo
oo
oo
o
oooo oo
ooo
o
oo
oooooooo oo o oo
oooo
oo
ooo
ooo
oo
o
ooo
o oo
oo
ooo
oo
oo
ooooooo
oo
o
ooooo
o
oo
oo
o
oo
oooo
o
oo
ooo o
ooo
oo
oo
o
oooo oo
ooo
o
oo
oooooooo oo o oo
oooo
oo
ooo
ooo
oo
o
ooo
o oo
oo
ooo
oo
oo
ooooooo
oo
o
ooooo
o
oo
oo
o
oo
oooo
o
oo
ooo o
oo o ooooo
o ooo
o o o oo
o oo
oo
ooo
o
oo
o
ooo
o
o oo
o
oo
o
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
oo
oooo
oo
oo
oo
oooo
o ooo
ooo
oo
ooo
oo
oooo
o
o oo o o ooo
o o oo
o o ooo
o oo
oo
ooo
o
oo
o
ooo
o
o oo
o
oo
o
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
oo
oooo
oo
oo
oo
oooo
o ooo
ooo
oo
o oo
oo
o o oo
o
o o o oo ooo
o o oo
o o o oo
o oo
oo
ooo
o
oo
o
ooo
o
o oo
o
oo
o
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
oo
oooo
oo
oo
oo
oooo
o ooo
ooo
oo
o oo
oo
o o oo
o
o o o o o ooo
o ooo
o o o oo
o oo
oo
ooo
o
oo
o
ooo
o
o oo
o
oo
o
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
oo
oooo
oo
oo
oo
oooo
o ooo
ooo
oo
ooo
oo
o o oo
olbph
o o o o o ooo
o o oo
o o o oo
o oo
oo
ooo
o
oo
o
ooo
o
o oo
o
oo
o
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
oo
oooo
oo
oo
oo
oooo
o ooo
ooo
oo
o oo
oo
o o oo
o
o o o o o ooo
o o oo
o o o oo
o oo
oo
ooo
o
oo
o
ooo
o
o oo
o
oo
o
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
oo
oooo
oo
oo
oo
oooo
o ooo
ooo
oo
o oo
oo
o ooo
o
o o o o o ooo
o o oo
o o o oo
o oo
oo
ooo
o
oo
o
ooo
o
o oo
o
oo
o
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
oo
oooo
oo
oo
oo
oooo
o ooo
ooo
oo
o oo
oo
o o oo
o
−1 0 1 2o o o o o ooo
o o oo
o o o oo
o oo
oo
ooo
o
oo
o
ooo
o
o oo
o
oo
o
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
oo
oooo
oo
oo
oo
oooo
o ooo
ooo
oo
o oo
oo
o o oo
o0.0 0.4 0.8oo o ooo o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o oo
o o o o o o oo
o o o o o o o o o o o o o oo
oo
o o o o o oo
oo oo o
o oo
o o oo
o oo
ooo o
oooooo o
o oo o o o o o o o o o o o oo o oo o o o o o o o o o o o o o o o o o o oo
o o o o o ooo
o o o o o o o o o o o o o oo
oo
o o oo o oo
oo o o o
o oo
o o oo
o oo
oo o o
ooo o o o o
o o o oo o o o o o o o o o o o oo o o oo o oo o oo o o o o oo o o ooo
oo oo o o oo
o o o o o o o o oo o o o oo
oo
oo oo o oo
oo o o o
o oo
o o oo
o oo
oo o o
ooo o o oo
o o o o o o o o o oo o o o o o o oo o o o o o o o oo o o o o o o o o o oo
o o o o o o oo
o o o o o o o o o oo o o oo
oo
o o o o ooo
oo o o o
o oo
oo oo
o oo
ooo o
oo oo o o o
o o o o o o o oo o o oo o o o oo o oo oo o o o o o o o o o oo o oo oo
o o o oo o oo
o o oo o ooo oo o o o oo
oo
o o o oo oo
oo o o o
ooo
o o oo
o oo
oo o o
oo o o o o o
svi
o o o o o o o o o o o o o o o o o o o o o oo oo o o o o oo o o o o o ooo
o o o o o o oo
o o o o o o oo o o o o o oo
oo
o o o o o oo
oo o o o
o oo
o o oo
o oo
oo o o
oo o oo o o
o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oo
o oo o o o oo
o o o o o o o o o o o o o oo
oo
o o o o o oo
oo oo o
o oo
o o oo
o oo
oo o o
oo o o o o o
o o o o o o o o o o o o o o o o o o o o o o o oo o o o oo o o o o oo o oo
o oo oo o oo
o o o o o o o o o o o o o oo
oo
o o oo ooo
oo oo o
o oo
o o oo
o oo
oo o o
oo o o o oo
oo o ooo o o o oo oo
oo
ooo
o o oo
oo
o oo
ooo
o
o o oo
oo
oo
o
o ooo
o
oo
o
oo
o ooo
o oo
o o o oooo
ooo
o
ooo
ooooo
o
oo
ooo
oo
oo
ooo
o
o oooo
oo
o oo o o o o o o o o oo
oo
ooo
o o oo
oo
o oo
ooo
o
o o oo
oo
oo
o
o ooo
o
oo
o
oo
o ooo
o oo
o o o oooo
ooo
o
ooo
ooooo
o
oo
ooo
oo
oo
ooo
o
o oooo
oo
o o o oo o o o o o o oo
oo
ooo
o o oo
oo
o oo
ooo
o
o ooo
oo
oo
o
o ooo
o
oo
o
oo
o ooo
o oo
o o o oooo
ooo
o
ooo
ooooo
o
oo
ooo
oo
oo
ooo
o
o oooo
oo
o o o o o o o o o oo oo
oo
ooo
o o oo
oo
o oo
ooo
o
o o oo
oo
oo
o
o ooo
o
oo
o
oo
o ooo
o oo
o o o oooo
ooo
o
ooo
ooooo
o
oo
ooo
oo
oo
ooo
o
o oooo
oo
o o o o o o o oo o o oo
oo
ooo
o ooo
oo
o oo
ooo
o
o o oo
oo
oo
o
o ooo
o
oo
o
oo
o ooo
o oo
o o o oooo
ooo
o
ooo
ooooo
o
oo
o oo
oo
oo
ooo
o
o oooo
oo
o o o o o o o o o o o oo
oo
ooo
o o oo
oo
o oo
ooo
o
o o oo
oo
oo
o
o ooo
o
oo
o
oo
o ooo
o oo
o o o oooo
ooo
o
ooo
ooooo
o
oo
ooo
oo
oo
ooo
o
o oooo
oo
lcp
o o o o o o o o o o o oo
oo
ooo
o o oo
oo
o oo
ooo
o
o o oo
oo
oo
o
o ooo
o
oo
o
oo
o ooo
o oo
o o o oooo
ooo
o
ooo
ooooo
o
oo
ooo
oo
oo
ooo
o
o oooo
oo
−1 0 1 2 3o o o o o o o o o o o oo
oo
ooo
o o oo
oo
o oo
ooo
o
o o oo
oo
oo
o
o ooo
o
oo
o
oo
o ooo
o oo
o o o oooo
ooo
o
ooo
ooooo
o
oo
ooo
oo
oo
ooo
o
o oooo
oo6.0 7.0 8.0 9.0ooo
ooo o o o oo oo o o
oo
o o o oo
oo
o oo o o
o o o o o ooo
o o oo
o
oo o oo
o
o oo
oo o o o o
oo o
ooo
o
oo o o
oo o o oo
o o o o o o o o oo
o o
ooo o
oooooo o
o oo
o o o o o o o o oo o o
oo
o o o oo
oo
o oo o o
o o o o o ooo
o o oo
o
oo ooo
o
o oo
oo o o o o
oo o
ooo
o
oo oo
oo o o oo
o o o o o o o o oo
o o
oo o o
ooo o o o o
o oo
oo o o o o o o oo o o
oo
o o o oo
oo
o ooo o
o o o oo ooo
o o oo
o
oo o oo
o
o oo
oo o o oo
oo o
ooo
o
oo oo
oo o o oo
o o o o o o o o oo
o o
oo o o
ooo o o oo
o oo
o o o o o o oo oo o o
oo
o o o oo
oo
o ooo o
o o o o o ooo
o o oo
o
oo o oo
o
o oo
oo o o o o
oo o
ooo
o
oo o o
oo o o oo
o o o o o oo o oo
o o
ooo o
oo oo o o o
o oo
o o o o oo o o oo o o
oo
o o ooo
oo
o oo o o
o o o oo ooo
o o oo
o
oo o oo
o
o oo
ooo o oo
oo o
ooo
o
oo o o
oo o ooo
o o ooo o o o oo
o o
oo o o
oo o o o o o
o oo
o o o o o o o o oo o o
oo
o o o oo
oo
o oo o o
o o o o o ooo
o ooo
o
oo o oo
o
o oo
oo o o o o
oo o
ooo
o
oo o o
oo oo oo
o o o o oo o o oo
o o
oo o o
oo o o o o o
o oo
o o o o o o o o oo o o
oo
o o o oo
oo
o oo o o
o o o o o ooo
o o oo
o
oo o oo
o
o oo
oo oo o o
oo o
ooo
o
oo o o
oo oo oo
o o o o o o o o oo
o o
oo o o
oo o oo o ogleason
o oo
o o o o o o o o oo o o
oo
o o o oo
oo
o oo o o
o o o o o ooo
o o oo
o
oo o oo
o
o oo
oo o o o o
oo o
ooo
o
oo oo
oo o o oo
o o o o o o o o oo
o o
oo o o
oo o o o oo
0 1 2 3 4 5ooo
ooo o o o oo oo
o ooo
o o o oo
oo
o oo
oo
o o o o o ooo oo
oo
o
ooooo
o
o oo
oo
o
oo
oooo
ooo
o
ooo
o
ooo
ooo
ooo
oo
o
oo
oo
oo
oooo
ooo
o
oo
o
o oo
o o o o o o o o oo
o ooo
o o o oo
oo
o oo
oo
o o o o o ooo oo
oo
o
ooooo
o
o oo
oo
o
oo
oooo
ooo
o
ooo
o
ooo
ooo
ooo
oo
o
oo
oo
oo
oooo
ooo
o
oo
o
2.5 3.5 4.5o oo
oo o o o o o o oo
o ooo
o o o oo
oo
o oo
oo
o o o oo ooo oo
oo
o
ooooo
o
o oo
oo
o
oo
oooo
ooo
o
ooo
o
ooo
ooo
ooo
oo
o
oo
oo
oo
oooo
ooo
o
oo
o
o oo
o o o o o o oo oo
o ooo
o o o oo
oo
o oo
oo
o o o o o ooo oo
oo
o
ooooo
o
o oo
oo
o
oo
oooo
ooo
o
ooo
o
ooo
ooo
ooo
oo
o
oo
oo
oo
oooo
ooo
o
oo
o
−1 0 1 2o oo
o o o o oo o o oo
o ooo
o o ooo
oo
o oo
oo
o o o oo ooo oo
oo
o
ooooo
o
o oo
oo
o
oo
oooo
ooo
o
ooo
o
ooo
ooo
ooo
oo
o
oo
oo
oo
oooo
ooo
o
oo
o
o oo
o o o o o o o o oo
o ooo
o o o oo
oo
o oo
oo
o o o o o ooo oo
oo
o
ooooo
o
o oo
oo
o
oo
oooo
ooo
o
ooo
o
ooo
ooo
ooo
oo
o
oo
oo
oo
oooo
ooo
o
oo
o
−1 0 1 2 3o oo
o o o o o o o o oo
o ooo
o o o oo
oo
o oo
oo
o o o o o ooo oo
oo
o
ooooo
o
o oo
oo
o
oo
oooo
ooo
o
ooo
o
ooo
ooo
ooo
oo
o
oo
oo
oo
oooo
ooo
o
oo
o
o oo
o o o o o o o o oo
o ooo
o o o oo
oo
o oo
oo
o o o o o ooo oo
oo
o
ooooo
o
o oo
oo
o
oo
oooo
ooo
o
ooo
o
ooo
ooo
ooo
oo
o
oo
oo
oo
oooo
ooo
o
oo
o
0 20 60 100
0 20 60 100pgg45
FIGURE 1.1. Scatterplot matrix of the prostate cancer data. The ﬁrst row s hows
the response against each of the predictors in turn. Two of th e predictors, sviand
gleason, are categorical.
For this problem not all errors are equal; we want to avoid ﬁlt ering out
good email, while letting spam get through is not desirable b ut less serious
in its consequences. We discuss a number of diﬀerent methods for tackling
this learning problem in the book.
Example 2: Prostate Cancer
The data for this example, displayed in Figure 1.11, come from a study
by Stamey et al. (1989) that examined the correlation betwee n the level of
1There was an error in these data in the ﬁrst edition of this book. Subj ect 32 had
a value of 6.1 for lweight, which translates to a 449 gm prostate! The correct value is
44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.

4 1. Introduction
FIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.
prostate speciﬁc antigen (PSA) and a number of clinical meas ures, in 97
men who were about to receive a radical prostatectomy.
The goal is to predict the log of PSA ( lpsa) from a number of measure-
ments including log cancer volume ( lcavol), log prostate weight lweight,
age, log of benign prostatic hyperplasia amount lbph, seminal vesicle in-
vasionsvi, log of capsular penetration lcp, Gleason score gleason, and
percent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix
of the variables. Some correlations with lpsaare evident, but a good pre-
dictive model is diﬃcult to construct by eye.
This is a supervised learning problem, known as a regression problem ,
because the outcome measurement is quantitative.
Example 3: Handwritten Digit Recognition
The data from this example come from the handwritten ZIP code s on
envelopes from U.S. postal mail. Each image is a segment from a ﬁve digit
ZIP code, isolating a single digit. The images are 16 ×16 eight-bit grayscale
maps, with each pixel ranging in intensity from 0 to 255. Some sample
images are shown in Figure 1.2.
The images have been normalized to have approximately the sa me size
and orientation. The task is to predict, from the 16 ×16 matrix of pixel
intensities, the identity of each image (0 ,1,...,9) quickly and accurately. If
it is accurate enough, the resulting algorithm would be used as part of an
automatic sorting procedure for envelopes. This is a classi ﬁcation problem
for which the error rate needs to be kept very low to avoid misd irection of

1. Introduction 5
mail. In order to achieve this low error rate, some objects ca n be assigned
to a “don’t know” category, and sorted instead by hand.
Example 4: DNA Expression Microarrays
DNA stands for deoxyribonucleic acid, and is the basic mater ial that makes
up human chromosomes. DNA microarrays measure the expressi on of a
gene in a cell by measuring the amount of mRNA (messenger ribo nucleic
acid) present for that gene. Microarrays are considered a br eakthrough
technology in biology, facilitating the quantitative stud y of thousands of
genes simultaneously from a single sample of cells.
Here is how a DNA microarray works. The nucleotide sequences for a few
thousand genes are printed on a glass slide. A target sample a nd a reference
sample are labeled with red and green dyes, and each are hybri dized with
the DNA on the slide. Through ﬂuoroscopy, the log (red/green ) intensities
of RNA hybridizing at each site is measured. The result is a fe w thousand
numbers, typically ranging from say −6to6, measuring the expressionlevel
of each gene in the target relative to the reference sample. P ositive values
indicate higher expression in the target versus the referen ce, and vice versa
for negative values.
A gene expression dataset collects together the expression values from a
series of DNA microarray experiments, with each column repr esenting an
experiment.Therearethereforeseveralthousandrowsrepr esentingindivid-
ual genes, and tens of columns representing samples: in the p articular ex-
ample of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns),
although for clarity only a random sample of 100 rows are show n. The ﬁg-
ure displays the data set as a heat map, ranging from green (ne gative) to
red (positive). The samples are 64 cancer tumors from diﬀere nt patients.
The challenge here is to understand how the genes and samples are or-
ganized. Typical questions include the following:
(a) which samples are most similar to each other, in terms of t heir expres-
sion proﬁles across genes?
(b) which genes are most similar to each other, in terms of the ir expression
proﬁles across samples?
(c) do certain genes show very high (or low) expression for ce rtain cancer
samples?
We could view this task as a regression problem, with two cate gorical
predictor variables—genes and samples—with the response var iable being
the level of expression. However, it is probably more useful to view it as
unsupervised learning problem. For example, for question (a) above, we
think of the samples as points in 6830–dimensional space, wh ich we want
toclustertogether in some way.

6 1. Introduction
SID42354SID31984SID301902SIDW128368SID375990SID360097SIDW325120ESTsChr.10SIDW365099SID377133SID381508SIDW308182SID380265SIDW321925ESTsChr.15SIDW362471SIDW417270SIDW298052SID381079SIDW428642TUPLE1TUP1ERLUMENSIDW416621SID43609ESTsSID52979SIDW357197SIDW366311ESTsSMALLNUCSIDW486740ESTsSID297905SID485148SID284853ESTsChr.15SID200394SIDW322806ESTsChr.2SIDW257915SID46536SIDW488221ESTsChr.5SID280066SIDW376394ESTsChr.15SIDW321854WASWiskottHYPOTHETICALSIDW376776SIDW205716SID239012SIDW203464HLACLASSISIDW510534SIDW279664SIDW201620SID297117SID377419SID114241ESTsCh31SIDW376928SIDW310141SIDW298203PTPRCSID289414SID127504ESTsChr.3SID305167SID488017SIDW296310ESTsChr.6SID47116MITOCHONDRIAL60ChrSIDW376586HomosapiensSIDW487261SIDW470459SID167117SIDW31489SID375812DNAPOLYMERSID377451ESTsChr.1MYBPROTOSID471915ESTsSIDW469884HumanmRNASIDW377402ESTsSID207172RASGTPASESID325394H.sapiensmRNAGNALSID73161SIDW380102SIDW299104BREAST
RENAL
MELANOMAMELANOMA
MCF7D-repro
COLONCOLON
K562B-repro
COLON
NSCLC
LEUKEMIA
RENAL
MELANOMA
BREAST
CNSCNS
RENAL
MCF7A-repro
NSCLC
K562A-repro
COLON
CNS
NSCLCNSCLC
LEUKEMIA
CNS
OVARIAN
BREAST
LEUKEMIA
MELANOMAMELANOMA
OVARIANOVARIAN
NSCLC
RENAL
BREAST
MELANOMA
OVARIANOVARIAN
NSCLC
RENAL
BREAST
MELANOMA
LEUKEMIA
COLON
BREAST
LEUKEMIA
COLON
CNS
MELANOMA
NSCLC
PROSTATE
NSCLC
RENALRENAL
NSCLC
RENAL
LEUKEMIA
OVARIAN
PROSTATE
COLON
BREAST
RENAL
UNKNOWNFIGURE 1.3. DNA microarray data: expression matrix of 6830genes (rows)
and64samples (columns), for the human tumor data. Only a random sample
of100rows are shown. The display is a heat map, ranging from bright g reen
(negative, under expressed) to bright red (positive, over e xpressed). Missing values
are gray. The rows and columns are displayed in a randomly chosen order.

1. Introduction 7
Who Should Read this Book
This book is designed for researchers and students in a broad variety of
ﬁelds: statistics, artiﬁcial intelligence, engineering, ﬁnance and others. We
expect that the reader will have had at least one elementary c ourse in
statistics, covering basic topics including linear regres sion.
We have not attempted to write a comprehensive catalog of lea rning
methods, but rather to describe some of the most important te chniques.
Equally notable, we describe the underlying concepts and co nsiderations
by which a researcher can judge a learning method. We have tri ed to write
this book in an intuitive fashion, emphasizing concepts rat her than math-
ematical details.
Asstatisticians,ourexpositionwillnaturallyreﬂectour backgroundsand
areas of expertise. However in the past eight years we have be en attending
conferences in neural networks, data mining and machine lea rning, and our
thinking has been heavily inﬂuenced by these exciting ﬁelds . This inﬂuence
is evident in our current research, and in this book.
How This Book is Organized
Our view is that one must understand simple methods before tr ying to
grasp more complex ones. Hence, after giving an overview of t he supervis-
ing learning problem in Chapter 2 , we discuss linear methods for regression
and classiﬁcation in Chapters 3 and4. InChapter 5 we describe splines,
wavelets and regularization/penalization methods for a si ngle predictor,
whileChapter 6 covers kernel methods and local regression. Both of these
sets of methods are important building blocks for high-dime nsional learn-
ing techniques. Model assessment and selection is the topic ofChapter 7 ,
covering the concepts of bias and variance, overﬁtting and m ethods such as
cross-validation for choosing models. Chapter 8 discusses model inference
and averaging, including an overview of maximum likelihood , Bayesian in-
ference and the bootstrap, the EM algorithm, Gibbs sampling and bagging,
A related procedure called boosting is the focus of Chapter 10 .
InChapters 9–13 we describe a series of structured methods for su-
pervised learning, with Chapters 9 and 11 covering regression and Chap-
ters 12 and 13 focusing on classiﬁcation. Chapter 14 describes methods for
unsupervised learning. Two recently proposed techniques, random forests
and ensemble learning, are discussed in Chapters 15 and 16 . We describe
undirected graphical models in Chapter 17 and ﬁnally we study high-
dimensional problems in Chapter 18 .
At the end of each chapter we discuss computational considerations im-
portant for data mining applications, including how the com putations scale
with the number of observations and predictors. Each chapte r ends with
Bibliographic Notes giving background references for the material.

8 1. Introduction
We recommend that Chapters 1–4 be ﬁrst read in sequence. Chap ter 7
should also be considered mandatory, as it covers central co ncepts that
pertain to all learning methods. With this in mind, the rest o f the book
can be read sequentially, or sampled, depending on the reade r’s interest.
The symbol
 indicates a technically diﬃcult section, one that can
be skipped without interrupting the ﬂow of the discussion.
Book Website
The website for this book is located at
http://www-stat.stanford.edu/ElemStatLearn
It contains a number of resources, including many of the data sets used in
this book.
Note for Instructors
We have successively used the ﬁrst edition of this book as the basis for a
two-quartercourse,andwiththeadditionalmaterialsinth issecondedition,
itcouldevenbeusedforathree-quartersequence.Exercise sareprovidedat
the end of each chapter. It is important for students to have a ccess to good
software tools for these topics. We used the R and S-PLUS prog ramming
languages in our courses.

This is page 9
Printer: Opaque this
2
Overview of Supervised Learning
2.1 Introduction
The ﬁrst three examples described in Chapter 1 have several c omponents
in common. For each there is a set of variables that might be de noted as
inputs, which are measured or preset. These have some inﬂuence on on e or
moreoutputs. For each example the goal is to use the inputs to predict the
values of the outputs. This exercise is called supervised learning .
We have used the more modern language of machine learning. In the
statistical literature the inputs are often called the predictors , a term we
will use interchangeably with inputs, and more classically theindependent
variables .Inthepatternrecognitionliteraturetheterm featuresispreferred,
which we use as well. The outputs are called the responses , or classically
thedependent variables .
2.2 Variable Types and Terminology
The outputs vary in nature among the examples. In the glucose prediction
example, the output is a quantitative measurement, where some measure-
ments are bigger than others, and measurements close in valu e are close
in nature. In the famous Iris discrimination example due to R . A. Fisher,
the output is qualitative (species of Iris) and assumes values in a ﬁnite set
G={Virginica ,SetosaandVersicolor}. In the handwritten digit example
the output is one of 10 diﬀerent digit classes:G={0,1,...,9}. In both of

10 2. Overview of Supervised Learning
these there is no explicit ordering in the classes, and in fac t often descrip-
tive labels rather than numbers are used to denote the classe s. Qualitative
variables are also referred to as categorical ordiscretevariables as well as
factors.
For both types of outputs it makes sense to think of using the i nputs to
predict the output. Given some speciﬁc atmospheric measure ments today
and yesterday, we want to predict the ozone level tomorrow. G iven the
grayscale values for the pixels of the digitized image of the handwritten
digit, we want to predict its class label.
This distinction in output type has led to a naming conventio n for the
prediction tasks: regression when we predict quantitative outputs, and clas-
siﬁcation when we predict qualitative outputs. We will see that these t wo
tasks have a lot in common, and in particular both can be viewe d as a task
in function approximation.
Inputs also vary in measurement type; we can have some of each of qual-
itative and quantitative input variables. These have also l ed to distinctions
in the types of methods that are used for prediction: some met hods are
deﬁned most naturally for quantitative inputs, some most na turally for
qualitative and some for both.
A third variable type is ordered categorical , such as small, medium and
large, where there is an ordering between the values, but no metric notion
is appropriate (the diﬀerence between medium and small need not be the
same as that between large and medium). These are discussed f urther in
Chapter 4.
Qualitative variables are typically represented numerica lly by codes. The
easiest case is when there are only two classes or categories , such as “suc-
cess” or “failure,” “survived” or “died.” These are often re presented by a
single binary digit or bit as 0 or 1, or else by −1 and 1. For reasons that will
become apparent, such numeric codes are sometimes referred to astargets.
When there are more than two categories, several alternativ es are available.
The most useful and commonly used coding is via dummy variables . Here a
K-level qualitative variable is represented by a vector of Kbinary variables
or bits, only one of which is “on” at a time. Although more comp act coding
schemes are possible, dummy variables are symmetric in the l evels of the
factor.
We will typically denote an input variable by the symbol X. IfXis
a vector, its components can be accessed by subscripts Xj. Quantitative
outputs will be denoted by Y, and qualitative outputs by G(for group).
We use uppercase letters such as X,YorGwhen referring to the generic
aspects of a variable. Observed values are written in lowerc ase; hence the
ith observed value of Xis written as xi(wherexiis again a scalar or
vector). Matrices are represented by bold uppercase letter s; for example, a
set ofNinputp-vectorsxi, i= 1,...,Nwould be represented by the N×p
matrixX. In general, vectors will not be bold, except when they have N
components; this convention distinguishes a p-vector of inputs xifor the

2.3 Least Squares and Nearest Neighbors 11
ith observation from the N-vectorxjconsisting of all the observations on
variableXj. Since all vectors are assumed to be column vectors, the ith
row ofXisxT
i, the vector transpose of xi.
For the moment we can loosely state the learning task as follo ws: given
the value of an input vector X, make a good prediction of the output Y,
denoted by ˆY(pronounced “y-hat”). If Ytakes values in IR then so should
ˆY; likewise for categorical outputs, ˆGshould take values in the same set G
associated with G.
For a two-class G, one approach is to denote the binary coded target
asY, and then treat it as a quantitative output. The predictions ˆYwill
typically lie in [0 ,1], and we can assign to ˆGthe class label according to
whether ˆy >0.5. This approach generalizes to K-level qualitative outputs
as well.
We need data to construct prediction rules, often a lot of it. We thus
suppose we have available a set of measurements ( xi,yi) or (xi,gi), i=
1,...,N,knownasthe training data ,withwhichtoconstructourprediction
rule.
2.3 Two Simple Approaches to Prediction: Least
Squares and Nearest Neighbors
In this section we develop two simple but powerful predictio n methods: the
linear model ﬁt by least squares and the k-nearest-neighbor prediction rule.
Thelinearmodelmakeshugeassumptionsaboutstructureand yieldsstable
but possibly inaccurate predictions. The method of k-nearest neighbors
makes very mild structural assumptions: its predictions ar e often accurate
but can be unstable.
2.3.1 Linear Models and Least Squares
The linear model has been a mainstay of statistics for the pas t 30 years
and remains one of our most important tools. Given a vector of inputs
XT= (X1,X2,...,X p), we predict the output Yvia the model
ˆY=ˆβ0+p/summationdisplay
j=1Xjˆβj. (2.1)
The term ˆβ0is the intercept, also known as the biasin machine learning.
Often it is convenient to include the constant variable 1 in X, include ˆβ0in
the vector of coeﬃcients ˆβ, and then write the linear model in vector form
as an inner product
ˆY=XTˆβ, (2.2)

12 2. Overview of Supervised Learning
whereXTdenotes vector or matrix transpose ( Xbeing a column vector).
Here we are modeling a single output, so ˆYis a scalar; in general ˆYcan be
aK–vector, in which case βwould be a p×Kmatrix of coeﬃcients. In the
(p+ 1)-dimensional input–output space, ( X,ˆY) represents a hyperplane.
If the constant is included in X, then the hyperplane includes the origin
and is a subspace; if not, it is an aﬃne set cutting the Y-axis at the point
(0,ˆβ0). From now on we assume that the intercept is included in ˆβ.
Viewed as a function over the p-dimensional input space, f(X) =XTβ
is linear, and the gradient f′(X) =βis a vector in input space that points
in the steepest uphill direction.
How do we ﬁt the linear model to a set of training data? There ar e
many diﬀerent methods, but by far the most popular is the meth od of
least squares . In this approach, we pick the coeﬃcients βto minimize the
residual sum of squares
RSS(β) =N/summationdisplay
i=1(yi−xT
iβ)2. (2.3)
RSS(β) is a quadratic function of the parameters, and hence its min imum
always exists, but may not be unique. The solution is easiest to characterize
in matrix notation. We can write
RSS(β) = (y−Xβ)T(y−Xβ), (2.4)
whereXis anN×pmatrix with each row an input vector, and yis an
N-vector of the outputs in the training set. Diﬀerentiating w .r.t.βwe get
thenormal equations
XT(y−Xβ) = 0. (2.5)
IfXTXis nonsingular, then the unique solution is given by
ˆβ= (XTX)−1XTy, (2.6)
and the ﬁtted value at the ith inputxiis ˆyi= ˆy(xi) =xT
iˆβ. At an arbi-
trary input x0the prediction is ˆ y(x0) =xT
0ˆβ. The entire ﬁtted surface is
characterized by the pparameters ˆβ. Intuitively, it seems that we do not
need a very large data set to ﬁt such a model.
Let’s look at an example of the linear model in a classiﬁcatio n context.
Figure 2.1 shows a scatterplot of training data on a pair of in putsX1and
X2. The data are simulated, and for the present the simulation m odel is
not important. The output class variable Ghas the values BLUEorORANGE,
and is represented as such in the scatterplot. There are 100 p oints in each
of the two classes. The linear regression model was ﬁt to thes e data, with
the response Ycoded as 0 for BLUEand 1 for ORANGE. The ﬁtted values ˆY
are converted to a ﬁtted class variable ˆGaccording to the rule
ˆG=/braceleftigg
ORANGE ifˆY >0.5,
BLUE ifˆY≤0.5.(2.7)

2.3 Least Squares and Nearest Neighbors 13
Linear Regression of 0/1 Response
.. . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
FIGURE 2.1. A classiﬁcation example in two dimensions. The classes are code d
as a binary variable ( BLUE= 0,ORANGE= 1), and then ﬁt by linear regression.
The line is the decision boundary deﬁned by xTˆβ= 0.5. The orange shaded region
denotes that part of input space classiﬁed as ORANGE, while the blue region is
classiﬁed as BLUE.
The set of points in IR2classiﬁed as ORANGEcorresponds to{x:xTˆβ >0.5},
indicated in Figure 2.1, and the two predicted classes are se parated by the
decision boundary {x:xTˆβ= 0.5}, which is linear in this case. We see
that for these data there are several misclassiﬁcations on b oth sides of the
decisionboundary.Perhapsourlinearmodelistoorigid—or aresucherrors
unavoidable? Remember that these are errors on the training data itself,
and we have not said where the constructed data came from. Con sider the
two possible scenarios:
Scenario 1: The training data in each class were generated from bivariat e
Gaussian distributions with uncorrelated components and d iﬀerent
means.
Scenario 2: Thetrainingdataineachclasscamefromamixtureof10low-
variance Gaussian distributions, with individual means th emselves
distributed as Gaussian.
A mixture of Gaussians is best described in terms of the gener ative
model. One ﬁrst generates a discrete variable that determin es which of

14 2. Overview of Supervised Learning
the component Gaussians to use, and then generates an observ ation from
the chosen density. In the case of one Gaussian per class, we w ill see in
Chapter 4 that a linear decision boundary is the best one can d o, and that
our estimate is almost optimal. The region of overlap is inev itable, and
future data to be predicted will be plagued by this overlap as well.
In the case of mixtures of tightly clustered Gaussians the st ory is dif-
ferent. A linear decision boundary is unlikely to be optimal , and in fact is
not. The optimal decision boundary is nonlinear and disjoin t, and as such
will be much more diﬃcult to obtain.
We now look at another classiﬁcation and regression procedu re that is
in some sense at the opposite end of the spectrum to the linear model, and
far better suited to the second scenario.
2.3.2 Nearest-Neighbor Methods
Nearest-neighbor methods use those observations in the tra ining setTclos-
est in input space to xto form ˆY. Speciﬁcally, the k-nearest neighbor ﬁt
forˆYis deﬁned as follows:
ˆY(x) =1
k/summationdisplay
xi∈Nk(x)yi, (2.8)
whereNk(x) is the neighborhood of xdeﬁned by the kclosest points xiin
the training sample. Closeness implies a metric, which for t he moment we
assume is Euclidean distance. So, in words, we ﬁnd the kobservations with
xiclosest toxin input space, and average their responses.
In Figure 2.2 we use the same training data as in Figure 2.1, an d use
15-nearest-neighbor averaging of the binary coded respons e as the method
of ﬁtting. Thus ˆYis the proportion of ORANGE’s in the neighborhood, and
so assigning class ORANGEtoˆGifˆY >0.5 amounts to a majority vote in
the neighborhood. The colored regions indicate all those po ints in input
space classiﬁed as BLUEorORANGEby such a rule, in this case found by
evaluating the procedure on a ﬁne grid in input space. We see t hat the
decision boundaries that separate the BLUEfrom the ORANGEregions are far
more irregular, and respond to local clusters where one clas s dominates.
Figure 2.3 shows the results for 1-nearest-neighbor classi ﬁcation: ˆYis
assigned the value yℓof the closest point xℓtoxin the training data. In
this case the regions of classiﬁcation can be computed relat ively easily, and
correspond to a Voronoi tessellation of the training data. Each point xi
has an associated tile bounding the region for which it is the closest input
point. For all points xin the tile, ˆG(x) =gi. The decision boundary is even
more irregular than before.
The method of k-nearest-neighbor averaging is deﬁned in exactly the
same way for regression of a quantitative output Y, althoughk= 1 would
be an unlikely choice.

2.3 Least Squares and Nearest Neighbors 15
15-Nearest Neighbor Classifier
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .
... .. .. .. .. . .. . .. . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
FIGURE 2.2. The same classiﬁcation example in two dimensions as in Fig-
ure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE= 1)and
then ﬁt by 15-nearest-neighbor averaging as in (2.8). The predicted clas s is hence
chosen by majority vote amongst the 15-nearest neighbors.
In Figure 2.2 we see that far fewer training observations are misclassiﬁed
than in Figure 2.1. This should not give us too much comfort, t hough, since
in Figure 2.3 noneof the training data are misclassiﬁed. A little thought
suggests that for k-nearest-neighbor ﬁts, the error on the training data
should be approximately an increasing function of k, and will always be 0
fork= 1. An independent test set would give us a more satisfactory means
for comparing the diﬀerent methods.
It appears that k-nearest-neighbor ﬁts have a single parameter, the num-
ber of neighbors k, compared to the pparameters in least-squares ﬁts. Al-
though this is the case, we will see that the eﬀective number of parameters
ofk-nearest neighbors is N/kand is generally bigger than p, and decreases
with increasing k. To get an idea of why, note that if the neighborhoods
were nonoverlapping, there would be N/kneighborhoods and we would ﬁt
one parameter (a mean) in each neighborhood.
It is also clear that we cannot use sum-of-squared errors on t he training
set as a criterion for picking k, since we would always pick k= 1! It would
seem thatk-nearest-neighbor methods would be more appropriate for th e
mixture Scenario 2 described above, while for Gaussian data the decision
boundaries of k-nearest neighbors would be unnecessarily noisy.

16 2. Overview of Supervised Learning
1−Nearest Neighbor Classifier
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
ooooo
ooo
o
o
ooo
oooooo
oo
o
oo
ooo
o
FIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-
ure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE= 1), and
then predicted by 1-nearest-neighbor classiﬁcation.
2.3.3 From Least Squares to Nearest Neighbors
The linear decision boundary from least squares is very smoo th, and ap-
parently stable to ﬁt. It does appear to rely heavily on the as sumption
that a linear decision boundary is appropriate. In language we will develop
shortly, it has low variance and potentially high bias.
On the other hand, the k-nearest-neighbor procedures do not appear to
relyonanystringentassumptionsabouttheunderlyingdata ,andcanadapt
to any situation. However, any particular subregion of the d ecision bound-
ary depends on a handful of input points and their particular positions,
and is thus wiggly and unstable—high variance and low bias.
Each method has its own situations for which it works best; in particular
linear regression is more appropriate for Scenario 1 above, while nearest
neighbors are more suitable for Scenario 2. The time has come to expose
the oracle! The data in fact were simulated from a model somew here be-
tween the two, but closer to Scenario 2. First we generated 10 meansmk
from a bivariate Gaussian distribution N((1,0)T,I) and labeled this class
BLUE. Similarly, 10 more were drawn from N((0,1)T,I) and labeled class
ORANGE. Then for each class we generated 100 observations as follow s: for
each observation, we picked an mkat random with probability 1 /10, and

2.3 Least Squares and Nearest Neighbors 17
Degrees of Freedom − N/kTest Error
0.10 0.15 0.20 0.25 0.30
  2   3   5   8  12  18  29  67 200151 101  69  45  31  21  11   7   5   3   1
Train
Test
Bayesk −  Number of Nearest Neighbors
Linear
FIGURE 2.4. Misclassiﬁcation curves for the simulation example used in Fig -
ures 2.1, 2.2 and 2.3. A single training sample of size 200was used, and a test
sample of size 10,000. The orange curves are test and the blue are training er-
ror fork-nearest-neighbor classiﬁcation. The results for linear regr ession are the
bigger orange and blue squares at three degrees of freedom. Th e purple line is the
optimal Bayes error rate.
then generated a N(mk,I/5), thus leading to a mixture of Gaussian clus-
ters for each class. Figure 2.4 shows the results of classify ing 10,000 new
observations generated from the model. We compare the resul ts for least
squares and those for k-nearest neighbors for a range of values of k.
A large subset of the most popular techniques in use today are variants of
these two simple procedures. In fact 1-nearest-neighbor, t he simplest of all,
captures a large percentage of the market for low-dimension al problems.
The following list describes some ways in which these simple procedures
have been enhanced:
•Kernel methods use weights that decrease smoothly to zero wi th dis-
tancefromthetargetpoint,ratherthantheeﬀective0 /1weightsused
byk-nearest neighbors.
•In high-dimensional spaces the distance kernels are modiﬁe d to em-
phasize some variable more than others.

18 2. Overview of Supervised Learning
•Local regression ﬁts linear models by locally weighted leas t squares,
rather than ﬁtting constants locally.
•Linear models ﬁt to a basis expansion of the original inputs a llow
arbitrarily complex models.
•Projection pursuit and neural network models consist of sum s of non-
linearly transformed linear models.
2.4 Statistical Decision Theory
In this section we develop a small amount of theory that provi des a frame-
work for developing models such as those discussed informal ly so far. We
ﬁrst consider the case of a quantitative output, and place ou rselves in the
world of random variables and probability spaces. Let X∈IRpdenote a
real valued random input vector, and Y∈IR a real valued random out-
put variable, with joint distribution Pr( X,Y). We seek a function f(X)
for predicting Ygiven values of the input X. This theory requires a loss
functionL(Y,f(X)) for penalizing errors in prediction, and by far the most
common and convenient is squared error loss :L(Y,f(X)) = (Y−f(X))2.
This leads us to a criterion for choosing f,
EPE(f) = E(Y−f(X))2(2.9)
=/integraldisplay
[y−f(x)]2Pr(dx,dy), (2.10)
the expected (squared) prediction error . By conditioning1onX, we can
write EPE as
EPE(f) = EXEY|X/parenleftbig
[Y−f(X)]2|X/parenrightbig
(2.11)
and we see that it suﬃces to minimize EPE pointwise:
f(x) = argmincEY|X/parenleftbig
[Y−c]2|X=x/parenrightbig
. (2.12)
The solution is
f(x) = E(Y|X=x), (2.13)
the conditional expectation, also known as the regression function. Thus
the best prediction of Yat any point X=xis the conditional mean, when
best is measured by average squared error.
The nearest-neighbor methods attempt to directly implemen t this recipe
using the training data. At each point x, we might ask for the average of all
1Conditioning here amounts to factoring the joint density Pr( X,Y) = Pr(Y|X)Pr(X)
where Pr( Y|X) = Pr(Y,X)/Pr(X), and splitting up the bivariate integral accordingly.

2.4 Statistical Decision Theory 19
thoseyis with input xi=x. Since there is typically at most one observation
at any point x, we settle for
ˆf(x) = Ave(yi|xi∈Nk(x)), (2.14)
where “Ave” denotes average, and Nk(x) is the neighborhood containing
thekpoints inTclosest tox. Two approximations are happening here:
•expectation is approximated by averaging over sample data;
•conditioning at a point is relaxed to conditioning on some re gion
“close” to the target point.
For large training sample size N, the points in the neighborhood are likely
to be close to x, and askgets large the average will get more stable.
In fact, under mild regularity conditions on the joint proba bility distri-
bution Pr(X,Y), one can show that as N,k→∞such thatk/N→0,
ˆf(x)→E(Y|X=x). In light of this, why look further, since it seems
we have a universal approximator? We often do not have very la rge sam-
ples. If the linear or some more structured model is appropri ate, then we
can usually get a more stable estimate than k-nearest neighbors, although
such knowledge has to be learned from the data as well. There a re other
problems though, sometimes disastrous. In Section 2.5 we se e that as the
dimensionpgets large, so does the metric size of the k-nearest neighbor-
hood. So settling for nearest neighborhood as a surrogate fo r conditioning
will fail us miserably. The convergence above still holds, b ut therateof
convergence decreases as the dimension increases.
How does linear regressionﬁt intothis framework? Thesimpl est explana-
tion is that one assumes that the regression function f(x) is approximately
linear in its arguments:
f(x)≈xTβ. (2.15)
Thisisamodel-basedapproach—wespecifyamodelfortheregr essionfunc-
tion. Plugging this linear model for f(x) into EPE (2.9) and diﬀerentiating
we can solve for βtheoretically:
β= [E(XXT)]−1E(XY). (2.16)
Note we have notconditioned on X; rather we have used our knowledge
of the functional relationship to poolover values of X. The least squares
solution (2.6) amounts to replacing the expectation in (2.1 6) by averages
over the training data.
So bothk-nearest neighbors and least squares end up approximating
conditional expectations by averages. But they diﬀer drama tically in terms
of model assumptions:
•Least squares assumes f(x) is well approximated by a globally linear
function.

20 2. Overview of Supervised Learning
•k-nearest neighbors assumes f(x) is well approximated by a locally
constant function.
Although the latter seems more palatable, we have already se en that we
may pay a price for this ﬂexibility.
Many of the more modern techniques described in this book are model
based, although far more ﬂexible than the rigid linear model . For example,
additive models assume that
f(X) =p/summationdisplay
j=1fj(Xj). (2.17)
Thisretainstheadditivity ofthelinearmodel,buteachcoo rdinatefunction
fjis arbitrary. It turns out that the optimal estimate for the a dditive model
uses techniques such as k-nearest neighbors to approximate univariate con-
ditional expectations simultaneously for each of the coordinate functions.
Thus the problems of estimating a conditional expectation i n high dimen-
sionsaresweptawayinthiscasebyimposingsome(oftenunre alistic)model
assumptions, in this case additivity.
Are we happy with the criterion (2.11)? What happens if we rep lace the
L2loss function with the L1: E|Y−f(X)|? The solution in this case is the
conditional median,
ˆf(x) = median( Y|X=x), (2.18)
which is a diﬀerent measure of location, and its estimates ar e more robust
than those for the conditional mean. L1criteria have discontinuities in
their derivatives, which have hindered their widespread us e. Other more
resistant loss functions will be mentioned in later chapter s, but squared
error is analytically convenient and the most popular.
What do we do when the output is a categorical variable G? The same
paradigm works here, except we need a diﬀerent loss function for penalizing
prediction errors. An estimate ˆGwill assume values in G, the set of possible
classes. Our loss function can be represented by a K×KmatrixL, where
K= card(G).Lwill be zero on the diagonal and nonnegative elsewhere,
whereL(k,ℓ) is the price paid for classifying an observation belonging to
classGkasGℓ. Most often we use the zero–one loss function, where all
misclassiﬁcations are charged a single unit. The expected p rediction error
is
EPE = E[L(G,ˆG(X))], (2.19)
where again the expectation is taken with respect to the join t distribution
Pr(G,X). Again we condition, and can write EPE as
EPE = E XK/summationdisplay
k=1L[Gk,ˆG(X)]Pr(Gk|X) (2.20)

2.4 Statistical Decision Theory 21
Bayes Optimal Classifier
... .. . . .. . . . .. . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
FIGURE 2.5. The optimal Bayes decision boundary for the simulation examp le
of Figures 2.1, 2.2 and 2.3. Since the generating density is k nown for each class,
this boundary can be calculated exactly (Exercise 2.2).
and again it suﬃces to minimize EPE pointwise:
ˆG(x) = argming∈GK/summationdisplay
k=1L(Gk,g)Pr(Gk|X=x). (2.21)
With the 0–1 loss function this simpliﬁes to
ˆG(x) = argming∈G[1−Pr(g|X=x)] (2.22)
or simply
ˆG(x) =Gkif Pr(Gk|X=x) = max
g∈GPr(g|X=x).(2.23)
This reasonable solution is known as the Bayes classiﬁer , and says that
we classify to the most probable class, using the conditiona l (discrete) dis-
tribution Pr( G|X). Figure 2.5 shows the Bayes-optimal decision boundary
for our simulation example. The error rate of the Bayes class iﬁer is called
theBayes rate .

22 2. Overview of Supervised Learning
Again we see that the k-nearest neighbor classiﬁer directly approximates
this solution—a majority vote in a nearest neighborhood amou nts to ex-
actly this, except that conditional probability at a point i s relaxed to con-
ditional probability within a neighborhood of a point, and p robabilities are
estimated by training-sample proportions.
Suppose for a two-class problem we had taken the dummy-varia ble ap-
proach and coded Gvia a binary Y, followed by squared error loss estima-
tion. Then ˆf(X) = E(Y|X) = Pr(G=G1|X) ifG1corresponded to Y= 1.
Likewise for a K-class problem, E( Yk|X) = Pr(G=Gk|X). This shows
that our dummy-variable regression procedure, followed by classiﬁcation to
the largest ﬁtted value, is another way of representing the B ayes classiﬁer.
Although this theory is exact, in practice problems can occu r, depending
on the regression model used. For example, when linear regre ssion is used,
ˆf(X) need not be positive, and we might be suspicious about using it as
an estimate of a probability. We will discuss a variety of app roaches to
modeling Pr( G|X) in Chapter 4.
2.5 Local Methods in High Dimensions
We have examined two learning techniques for prediction so f ar: the stable
but biased linear model and the less stable but apparently le ss biased class
ofk-nearest-neighbor estimates. It would seem that with a reas onably large
set of training data, we could always approximate the theore tically optimal
conditional expectation by k-nearest-neighbor averaging, since we should
be able to ﬁnd a fairly large neighborhood of observations cl ose to any x
and average them. This approach and our intuition breaks dow n in high
dimensions, and the phenomenon is commonly referred to as th ecurse
of dimensionality (Bellman, 1961). There are many manifestations of this
problem, and we will examine a few here.
Considerthenearest-neighborprocedureforinputsunifor mlydistributed
in ap-dimensional unit hypercube, as in Figure 2.6. Suppose we se nd out a
hypercubical neighborhood about a target point to capture a fractionrof
the observations. Since this corresponds to a fraction rof the unit volume,
theexpectededgelengthwillbe ep(r) =r1/p.Intendimensions e10(0.01) =
0.63 ande10(0.1) = 0.80, while the entire range for each input is only 1 .0.
So to capture 1% or 10% of the data to form a local average, we mu st cover
63% or 80% of the range of each input variable. Such neighborh oods are no
longer “local.” Reducing rdramatically does not help much either, since
the fewer observations we average, the higher is the varianc e of our ﬁt.
Another consequence of the sparse sampling in high dimensio ns is that
all sample points are close to an edge of the sample. Consider Ndata points
uniformly distributed in a p-dimensional unit ball centered at the origin.
Suppose we consider a nearest-neighbor estimate at the orig in. The median

2.5 Local Methods in High Dimensions 23
1
10Unit Cube
Fraction of VolumeDistance
0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0p=1p=2p=3p=10
Neighborhood
FIGURE 2.6. The curse of dimensionality is well illustrated by a subcubica l
neighborhood for uniform data in a unit cube. The ﬁgure on the right shows the
side-length of the subcube needed to capture a fraction rof the volume of the data,
for diﬀerent dimensions p. In ten dimensions we need to cover 80%of the range
of each coordinate to capture 10%of the data.
distance from the origin to the closest data point is given by the expression
d(p,N) =/parenleftig
1−1
21/N/parenrightig1/p
(2.24)
(Exercise 2.3). A more complicated expression exists for th e mean distance
to the closest point. For N= 500,p= 10 ,d(p,N)≈0.52, more than
halfwaytotheboundary.Hencemostdatapointsarecloserto theboundary
of the sample space than to any other data point. The reason th at this
presents a problem is that prediction is much more diﬃcult ne ar the edges
of the training sample. One must extrapolate from neighbori ng sample
points rather than interpolate between them.
Another manifestation of the curse is that the sampling dens ity is pro-
portional to N1/p, wherepis the dimension of the input space and Nis the
sample size. Thus, if N1= 100 represents a dense sample for a single input
problem, then N10= 10010is the sample size required for the same sam-
pling density with 10 inputs. Thus in high dimensions all fea sible training
samples sparsely populate the input space.
Let us construct another uniform example. Suppose we have 10 00 train-
ing examples xigenerated uniformly on [ −1,1]p. Assume that the true
relationship between XandYis
Y=f(X) =e−8||X||2,
without any measurement error. We use the 1-nearest-neighb or rule to
predicty0at the test-point x0= 0. Denote the training set by T. We can

24 2. Overview of Supervised Learning
compute the expected prediction error at x0for our procedure, averaging
over all such samples of size 1000. Since the problem is deter ministic, this
is the mean squared error (MSE) for estimating f(0):
MSE(x0) = E T[f(x0)−ˆy0]2
= ET[ˆy0−ET(ˆy0)]2+[ET(ˆy0)−f(x0)]2
= Var T(ˆy0)+Bias2(ˆy0). (2.25)
Figure 2.7 illustrates the setup. We have broken down the MSE into two
components that will become familiar as we proceed: varianc e and squared
bias.Suchadecompositionisalwayspossibleandoftenusef ul,andisknown
as thebias–variance decomposition . Unless the nearest neighbor is at 0,
ˆy0will be smaller than f(0) in this example, and so the average estimate
will be biased downward. The variance is due to the sampling v ariance of
the 1-nearest neighbor. In low dimensions and with N= 1000, the nearest
neighbor is very close to 0, and so both the bias and variance a re small. As
the dimension increases, the nearest neighbor tends to stra y further from
the target point, and both bias and variance are incurred. By p= 10, for
more than 99% of the samples the nearest neighbor is a distanc e greater
than 0.5 from the origin. Thus as pincreases, the estimate tends to be 0
more often than not, and hence the MSE levels oﬀ at 1 .0, as does the bias,
and the variance starts dropping (an artifact of this exampl e).
Although this is a highly contrived example, similar phenom ena occur
more generally. The complexity of functions of many variabl es can grow
exponentially with the dimension, and if we wish to be able to estimate
such functions with the same accuracy as function in low dime nsions, then
we need the size of our training set to grow exponentially as w ell. In this
example, the function is a complex interaction of all pvariables involved.
The dependence of the bias term on distance depends on the tru th, and
it need not always dominate with 1-nearest neighbor. For exa mple, if the
function always involves only a few dimensions as in Figure 2 .8, then the
variance can dominate instead.
Suppose, on the other hand, that we know that the relationshi p between
YandXis linear,
Y=XTβ+ε, (2.26)
whereε∼N(0,σ2) and we ﬁt the model by least squares to the train-
ing data. For an arbitrary test point x0, we have ˆy0=xT
0ˆβ, which can
be written as ˆ y0=xT
0β+/summationtextN
i=1ℓi(x0)εi, whereℓi(x0) is theith element
ofX(XTX)−1x0. Since under this model the least squares estimates are

2.5 Local Methods in High Dimensions 25
Xf(X)
-1.0 -0.5 0.0 0.5 1.00.0 0.2 0.4 0.6 0.8 1.0•1-NN in One Dimension
X1X2
-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•••
•••
••
•
•••
•••
••
••
•
•
•••
••1-NN in One vs. Two Dimensions
DimensionAverage Distance to Nearest Neighbor
2 4 6 8 100.0 0.2 0.4 0.6 0.8••••••••••Distance to 1-NN vs. Dimension
DimensionMse
2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0• •••••••••
• •••••••• • • •••••••••MSE vs. Dimension
•MSE
•Variance
•Sq. Bias
FIGURE 2.7. A simulation example, demonstrating the curse of dimensional-
ity and its eﬀect on MSE, bias and variance. The input feature s are uniformly
distributed in [−1,1]pforp= 1,...,10The top left panel shows the target func-
tion (no noise) in IR:f(X) =e−8||X||2, and demonstrates the error that 1-nearest
neighbor makes in estimating f(0). The training point is indicated by the blue tick
mark. The top right panel illustrates why the radius of the 1-nearest neighborhood
increases with dimension p. The lower left panel shows the average radius of the
1-nearest neighborhoods. The lower-right panel shows the MSE , squared bias and
variance curves as a function of dimension p.

26 2. Overview of Supervised Learning
Xf(X)
-1.0 -0.5 0.0 0.5 1.00 1 2 3 4•1-NN in One Dimension
DimensionMSE
2 4 6 8 100.0 0.05 0.10 0.15 0.20 0.25••••••••••
••••••••••
• •• • ••••••MSE  vs. Dimension
•MSE
•Variance
•Sq. Bias
FIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here
the function is constant in all but one dimension: f(X) =1
2(X1+ 1)3. The
variance dominates.
unbiased, we ﬁnd that
EPE(x0) = E y0|x0ET(y0−ˆy0)2
= Var(y0|x0)+ET[ˆy0−ETˆy0]2+[ETˆy0−xT
0β]2
= Var(y0|x0)+Var T(ˆy0)+Bias2(ˆy0)
=σ2+ETxT
0(XTX)−1x0σ2+02. (2.27)
Here we have incurred an additional variance σ2in the prediction error,
since our target is not deterministic. There is no bias, and t he variance
depends on x0. IfNis large andTwere selected at random, and assuming
E(X) = 0, then XTX→NCov(X) and
Ex0EPE(x0)∼Ex0xT
0Cov(X)−1x0σ2/N+σ2
= trace[Cov( X)−1Cov(x0)]σ2/N+σ2
=σ2(p/N)+σ2. (2.28)
Here we see that the expected EPE increases linearly as a func tion ofp,
with slope σ2/N. IfNis large and/or σ2is small, this growth in vari-
ance is negligible (0 in the deterministic case). By imposin g some heavy
restrictions on the class of models being ﬁtted, we have avoi ded the curse
of dimensionality. Some of the technical details in (2.27) a nd (2.28) are
derived in Exercise 2.5.
Figure 2.9 compares 1-nearest neighbor vs. least squares in two situa-
tions, both of which have the form Y=f(X) +ε,Xuniform as before,
andε∼N(0,1). The sample size is N= 500. For the orange curve, f(x)

2.5 Local Methods in High Dimensions 27
DimensionEPE Ratio
2 4 6 8 101.6 1.7 1.8 1.9 2.0 2.1••••••••••••••••••••Expected Prediction Error of 1NN vs. OLS
•Linear
•Cubic
FIGURE 2.9. The curves show the expected prediction error (at x0= 0) for
1-nearest neighbor relative to least squares for the model Y=f(X)+ε. For the
orange curve, f(x) =x1, while for the blue curve f(x) =1
2(x1+1)3.
is linear in the ﬁrst coordinate, for the blue curve, cubic as in Figure 2.8.
Shown is the relative EPE of 1-nearest neighbor to least squa res, which
appears to start at around 2 for the linear case. Least square s is unbiased
in this case, and as discussed above the EPE is slightly above σ2= 1.
The EPE for 1-nearest neighbor is always above 2, since the va riance of
ˆf(x0) in this case is at least σ2, and the ratio increases with dimension as
the nearest neighbor strays from the target point. For the cu bic case, least
squares is biased, which moderates the ratio. Clearly we cou ld manufacture
examples where the bias of least squares would dominate the v ariance, and
the 1-nearest neighbor would come out the winner.
By relying on rigid assumptions, the linear model has no bias at all and
negligible variance, while the error in 1-nearest neighbor is substantially
larger. However, if the assumptions are wrong, all bets are o ﬀ and the
1-nearest neighbor may dominate. We will see that there is a w hole spec-
trum of models between the rigid linear models and the extrem ely ﬂexible
1-nearest-neighbor models, each with their own assumption s and biases,
which have been proposed speciﬁcally to avoid the exponenti al growth in
complexity of functions in high dimensions by drawing heavi ly on these
assumptions.
Before we delve more deeply, let us elaborate a bit on the conc ept of
statistical models and see how they ﬁt into the prediction framework.

28 2. Overview of Supervised Learning
2.6 Statistical Models, Supervised Learning and
Function Approximation
Our goal is to ﬁnd a useful approximation ˆf(x) to the function f(x) that
underlies the predictive relationship between the inputs a nd outputs. In the
theoretical setting of Section 2.4, we saw that squared erro r loss lead us
to the regression function f(x) = E(Y|X=x) for a quantitative response.
The class of nearest-neighbor methods can be viewed as direc t estimates
of this conditional expectation, but we have seen that they c an fail in at
least two ways:
•if the dimension of the input space is high, the nearest neigh bors need
not be close to the target point, and can result in large error s;
•if special structure is known to exist, this can be used to red uce both
the bias and the variance of the estimates.
We anticipate using other classes of models for f(x), in many cases specif-
ically designed to overcome the dimensionality problems, a nd here we dis-
cuss a framework for incorporating them into the prediction problem.
2.6.1 A Statistical Model for the Joint Distribution Pr(X,Y)
Suppose in fact that our data arose from a statistical model
Y=f(X)+ε, (2.29)
where the random error εhas E(ε) = 0 and is independent of X. Note that
for this model, f(x) = E(Y|X=x), and in fact the conditional distribution
Pr(Y|X) depends on Xonlythrough the conditional mean f(x).
The additive error model is a useful approximation to the tru th. For
most systems the input–output pairs ( X,Y) will not have a deterministic
relationship Y=f(X). Generally there will be other unmeasured variables
thatalsocontributeto Y,includingmeasurementerror.Theadditivemodel
assumes that we can capture all these departures from a deter ministic re-
lationship via the error ε.
For some problems a deterministic relationship does hold. M any of the
classiﬁcation problems studied in machine learning are of t his form, where
the response surface can be thought of as a colored map deﬁned in IRp.
The training data consist of colored examples from the map {xi,gi}, and
the goal is to be able to color any point. Here the function is d eterministic,
and the randomness enters through the xlocation of the training points.
For the moment we will not pursue such problems, but will see t hat they
can be handled by techniques appropriate for the error-base d models.
The assumption in (2.29) that the errors are independent and identically
distributedisnotstrictlynecessary,butseemstobeatthe backofourmind

2.6 Statistical Models, Supervised Learning and Function Ap proximation 29
when we average squared errors uniformly in our EPE criterio n. With such
a model it becomes natural to use least squares as a data crite rion for
model estimation as in (2.1). Simple modiﬁcations can be mad e to avoid
the independence assumption; for example, we can have Var( Y|X=x) =
σ(x), and now both the mean and variance depend on X. In general the
conditional distribution Pr( Y|X) can depend on Xin complicated ways,
but the additive error model precludes these.
So far we have concentrated on the quantitative response. Ad ditive error
models are typically not used for qualitative outputs G; in this case the tar-
get function p(X)isthe conditional density Pr( G|X), and this is modeled
directly. For example, for two-class data, it is often reaso nable to assume
that the data arise from independent binary trials, with the probability of
one particular outcome being p(X), and the other 1 −p(X). Thus ifYis
the 0–1 coded version of G, then E(Y|X=x) =p(x), but the variance
depends on xas well: Var( Y|X=x) =p(x)[1−p(x)].
2.6.2 Supervised Learning
Before we launch into more statistically oriented jargon, w e present the
function-ﬁtting paradigm from a machine learning point of v iew. Suppose
for simplicity that the errors are additive and that the mode lY=f(X)+ε
is a reasonable assumption. Supervised learning attempts t o learnfby
example through a teacher. One observes the system under study, both
the inputs and outputs, and assembles a training set of observations T=
(xi,yi), i= 1,...,N. The observed input values to the system xiare also
fed into an artiﬁcial system, known as a learning algorithm ( usually a com-
puter program), which also produces outputs ˆf(xi) in response to the in-
puts. The learning algorithm has the property that it can mod ify its in-
put/output relationship ˆfin response to diﬀerences yi−ˆf(xi) between the
original and generated outputs. This process is known as learning by exam-
ple. Upon completion of the learning process the hope is that the artiﬁcial
and real outputs will be close enough to be useful for all sets of inputs likely
to be encountered in practice.
2.6.3 Function Approximation
The learning paradigm of the previous section has been the mo tivation
for research into the supervised learning problem in the ﬁel ds of machine
learning (with analogies to human reasoning) and neural net works (with
biological analogies to the brain). The approach taken in ap plied mathe-
matics and statistics has been from the perspective of funct ion approxima-
tion and estimation. Here the data pairs {xi,yi}are viewed as points in a
(p+1)-dimensional Euclidean space. The function f(x) has domain equal
to thep-dimensional input subspace, and is related to the data via a model

30 2. Overview of Supervised Learning
such asyi=f(xi)+εi. For convenience in this chapter we will assume the
domain is IRp, ap-dimensional Euclidean space, although in general the
inputs can be of mixed type. The goal is to obtain a useful appr oximation
tof(x) for allxin some region of IRp, given the representations in T.
Although somewhat less glamorous than the learning paradig m, treating
supervised learning as a problem in function approximation encourages the
geometrical concepts of Euclidean spaces and mathematical concepts of
probabilistic inference to be applied to the problem. This i s the approach
taken in this book.
Many of the approximations we will encounter have associate d a set of
parameters θthat can be modiﬁed to suit the data at hand. For example,
the linear model f(x) =xTβhasθ=β. Another class of useful approxi-
mators can be expressed as linear basis expansions
fθ(x) =K/summationdisplay
k=1hk(x)θk, (2.30)
where thehkare a suitable set of functions or transformations of the inp ut
vectorx. Traditional examples are polynomial and trigonometric ex pan-
sions, where for example hkmight bex2
1,x1x2
2, cos(x1) and so on. We
also encounter nonlinear expansions, such as the sigmoid tr ansformation
common to neural network models,
hk(x) =1
1+exp(−xTβk). (2.31)
We can use least squares to estimate the parameters θinfθas we did
for the linear model, by minimizing the residual sum-of-squ ares
RSS(θ) =N/summationdisplay
i=1(yi−fθ(xi))2(2.32)
as a function of θ. This seems a reasonable criterion for an additive error
model. In terms of function approximation, we imagine our pa rameterized
function as a surface in p+ 1 space, and what we observe are noisy re-
alizations from it. This is easy to visualize when p= 2 and the vertical
coordinate is the output y, as in Figure 2.10. The noise is in the output
coordinate, so we ﬁnd the set of parameters such that the ﬁtte d surface
gets as close to the observed points as possible, where close is measured by
the sum of squared vertical errors in RSS( θ).
For the linear model we get a simple closed form solution to th e mini-
mization problem. This is also true for the basis function me thods, if the
basis functions themselves do not have any hidden parameter s. Otherwise
the solution requires either iterative methods or numerica l optimization.
While least squares is generally very convenient, it is not t he only crite-
rion used and in some cases would not make much sense. A more ge neral

2.6 Statistical Models, Supervised Learning and Function Ap proximation 31
•••
•••
••••
••
•••
••••
•••
••
•••
•
•
••
••• ••••
••
•••
•••
•
••
••
•••
•
••
•••
•
•••
••••
•••
•
••
FIGURE 2.10. Least squares ﬁtting of a function of two inputs. The paramet ers
offθ(x)are chosen so as to minimize the sum-of-squared vertical err ors.
principle for estimation is maximum likelihood estimation . Suppose we have
a random sample yi, i= 1,...,Nfrom a density Pr θ(y) indexed by some
parameters θ. The log-probability of the observed sample is
L(θ) =N/summationdisplay
i=1logPrθ(yi). (2.33)
The principle of maximum likelihood assumes that the most re asonable
values forθare those for which the probability of the observed sample is
largest. Least squares for the additive error model Y=fθ(X) +ε, with
ε∼N(0,σ2), is equivalent to maximum likelihood using the conditiona l
likelihood
Pr(Y|X,θ) =N(fθ(X),σ2). (2.34)
So although the additional assumption of normality seems mo re restrictive,
the results are the same. The log-likelihood of the data is
L(θ) =−N
2log(2π)−Nlogσ−1
2σ2N/summationdisplay
i=1(yi−fθ(xi))2,(2.35)
and the only term involving θis the last, which is RSS( θ) up to a scalar
negative multiplier.
A more interesting example is the multinomial likelihood fo r the regres-
sion function Pr( G|X) for a qualitative output G. Suppose we have a model
Pr(G=Gk|X=x) =pk,θ(x), k= 1,...,Kfor the conditional probabil-
ity of each class given X, indexed by the parameter vector θ. Then the

32 2. Overview of Supervised Learning
log-likelihood (also referred to as the cross-entropy) is
L(θ) =N/summationdisplay
i=1logpgi,θ(xi), (2.36)
and when maximized it delivers values of θthat best conform with the data
in this likelihood sense.
2.7 Structured Regression Models
Wehaveseenthatalthoughnearest-neighborandotherlocal methodsfocus
directly on estimating the function at a point, they face pro blems in high
dimensions. They may also be inappropriate even in low dimen sions in
cases where more structured approaches can make more eﬃcien t use of the
data. This section introduces classes of such structured ap proaches. Before
we proceed, though, we discuss further the need for such clas ses.
2.7.1 Diﬃculty of the Problem
Consider the RSS criterion for an arbitrary function f,
RSS(f) =N/summationdisplay
i=1(yi−f(xi))2. (2.37)
Minimizing (2.37) leads to inﬁnitely many solutions: any fu nctionˆfpassing
through the training points ( xi,yi) is a solution. Any particular solution
chosen might be a poor predictor at test points diﬀerent from the training
points. If there are multiple observation pairs xi,yiℓ, ℓ= 1,...,N iat each
value ofxi, the risk is limited. In this case, the solutions pass throug h
the average values of the yiℓat eachxi; see Exercise 2.6. The situation is
similar to the one we have already visited in Section 2.4; ind eed, (2.37) is
the ﬁnite sample version of (2.11) on page 18. If the sample si zeNwere
suﬃciently large such that repeats were guaranteed and dens ely arranged,
it would seem that these solutions might all tend to the limit ing conditional
expectation.
In order to obtain useful results for ﬁnite N, we must restrict the eligible
solutions to (2.37) to a smaller set of functions. How to deci de on the
nature of the restrictions is based on considerations outsi de of the data.
Theserestrictions aresometimes encodedviatheparametri crepresentation
offθ, or may be built into the learning method itself, either impl icitly or
explicitly. These restricted classes of solutions are the m ajor topic of this
book. One thing should be clear, though. Any restrictions im posed onf
that lead to a unique solution to (2.37) do not really remove t he ambiguity

2.8 Classes of Restricted Estimators 33
caused by the multiplicity of solutions. There are inﬁnitel y many possible
restrictions, each leading to a unique solution, so the ambi guity has simply
been transferred to the choice of constraint.
In general the constraints imposed by most learning methods can be
described as complexity restrictions of one kind or another. This usually
means some kind of regular behavior in small neighborhoods o f the input
space. That is, for all input points xsuﬃciently close to each other in
some metric, ˆfexhibits some special structure such as nearly constant,
linear or low-order polynomial behavior. The estimator is t hen obtained by
averaging or polynomial ﬁtting in that neighborhood.
The strength of the constraint is dictated by the neighborho od size. The
larger the size of the neighborhood, the stronger the constr aint, and the
more sensitive the solution is to the particular choice of co nstraint. For
example, local constant ﬁts in inﬁnitesimally small neighb orhoods is no
constraint at all; local linear ﬁts in very large neighborho ods is almost a
globally linear model, and is very restrictive.
The nature of the constraint depends on the metric used. Some methods,
such as kernel and local regression and tree-based methods, directly specify
the metric and size of the neighborhood. The nearest-neighb or methods
discussed so far are based on the assumption that locally the function is
constant;closetoatargetinput x0,thefunctiondoesnotchangemuch,and
so close outputs can be averaged to produce ˆf(x0). Other methods such
as splines, neural networks and basis-function methods imp licitly deﬁne
neighborhoods of local behavior. In Section 5.4.1 we discus s the concept
of anequivalent kernel (see Figure 5.8 on page 157), which describes this
local dependence for any method linear in the outputs. These equivalent
kernels in many cases look just like the explicitly deﬁned we ighting kernels
discussed above—peaked at the target point and falling away s moothly
away from it.
One fact should be clear by now. Any method that attempts to pr o-
duce locally varying functions in small isotropic neighbor hoods will run
into problems in high dimensions—again the curse of dimensio nality. And
conversely, all methods that overcome the dimensionality p roblems have an
associated—andoftenimplicitoradaptive—metricformeasur ingneighbor-
hoods, which basically does not allow the neighborhood to be simultane-
ously small in all directions.
2.8 Classes of Restricted Estimators
The variety of nonparametric regressiontechniques or lear ning methods fall
intoanumberofdiﬀerentclassesdependingonthenatureoft herestrictions
imposed. These classes are not distinct, and indeed some met hods fall in
several classes. Here we give a brief summary, since detaile d descriptions

34 2. Overview of Supervised Learning
are given in later chapters. Each of the classes has associat ed with it one
or more parameters, sometimes appropriately called smoothing parameters,
that control the eﬀective size of the local neighborhood. He re we describe
three broad classes.
2.8.1 Roughness Penalty and Bayesian Methods
Here the class of functions is controlled by explicitly pena lizing RSS( f)
with a roughness penalty
PRSS(f;λ) = RSS(f)+λJ(f). (2.38)
The user-selected functional J(f) will be large for functions fthat vary too
rapidly over small regions of input space. For example, the p opularcubic
smoothing spline for one-dimensional inputs is the solution to the penalized
least-squares criterion
PRSS(f;λ) =N/summationdisplay
i=1(yi−f(xi))2+λ/integraldisplay
[f′′(x)]2dx. (2.39)
The roughness penalty here controls large values of the seco nd derivative
off, and the amount of penalty is dictated by λ≥0. Forλ= 0 no penalty
is imposed, and any interpolating function will do, while fo rλ=∞only
functions linear in xare permitted.
Penalty functionals Jcan be constructed for functions in any dimension,
and special versions can be created to impose special struct ure. For ex-
ample, additive penalties J(f) =/summationtextp
j=1J(fj) are used in conjunction with
additive functions f(X) =/summationtextp
j=1fj(Xj) to create additive models with
smooth coordinate functions. Similarly, projection pursuit regression mod-
els havef(X) =/summationtextM
m=1gm(αT
mX) for adaptively chosen directions αm, and
the functions gmcan each have an associated roughness penalty.
Penalty function, or regularization methods, express our prior belief that
the type of functions we seek exhibit a certain type of smooth behavior, and
indeed can usually be cast in a Bayesian framework. The penal tyJcorre-
sponds to a log-prior, and PRSS( f;λ) the log-posterior distribution, and
minimizing PRSS( f;λ) amounts to ﬁnding the posterior mode. We discuss
roughness-penalty approaches in Chapter 5 and the Bayesian paradigm in
Chapter 8.
2.8.2 Kernel Methods and Local Regression
Thesemethodscanbethoughtofasexplicitlyprovidingesti matesofthere-
gression function or conditional expectation by specifyin g the nature of the
local neighborhood, and of the class of regular functions ﬁt ted locally. The
local neighborhood is speciﬁed by a kernel function Kλ(x0,x) which assigns

2.8 Classes of Restricted Estimators 35
weights to points xin a region around x0(see Figure 6.1 on page 192). For
example, the Gaussian kernel has a weight function based on t he Gaussian
density function
Kλ(x0,x) =1
λexp/bracketleftbigg
−||x−x0||2
2λ/bracketrightbigg
(2.40)
and assigns weights to points that die exponentially with th eir squared
Euclidean distance from x0. The parameter λcorresponds to the variance
of the Gaussian density, and controls the width of the neighb orhood. The
simplest form of kernel estimate is the Nadaraya–Watson wei ghted average
ˆf(x0) =/summationtextN
i=1Kλ(x0,xi)yi/summationtextN
i=1Kλ(x0,xi). (2.41)
In general we can deﬁne a local regression estimate of f(x0) asfˆθ(x0),
whereˆθminimizes
RSS(fθ,x0) =N/summationdisplay
i=1Kλ(x0,xi)(yi−fθ(xi))2, (2.42)
andfθis some parameterized function, such as a low-order polynom ial.
Some examples are:
•fθ(x) =θ0, the constant function; this results in the Nadaraya–
Watson estimate in (2.41) above.
•fθ(x) =θ0+θ1xgives the popular local linear regression model.
Nearest-neighbor methods can be thought of as kernel method s having a
more data-dependent metric. Indeed, the metric for k-nearest neighbors is
Kk(x,x0) =I(||x−x0||≤||x(k)−x0||),
wherex(k)is the training observation ranked kth in distance from x0, and
I(S) is the indicator of the set S.
Thesemethodsofcourseneedtobemodiﬁedinhighdimensions ,toavoid
the curse of dimensionality. Various adaptations are discu ssed in Chapter 6.
2.8.3 Basis Functions and Dictionary Methods
This class of methods includes the familiar linear and polyn omial expan-
sions, but more importantly a wide variety of more ﬂexible mo dels. The
model forfis a linear expansion of basis functions
fθ(x) =M/summationdisplay
m=1θmhm(x), (2.43)

36 2. Overview of Supervised Learning
where each of the hmis a function of the input x, and the term linear here
refers to the action of the parameters θ. This class covers a wide variety of
methods. In some cases the sequence of basis functions is pre scribed, such
as a basis for polynomials in xof total degree M.
Forone-dimensional x,polynomialsplinesofdegree Kcanberepresented
by an appropriate sequence of Mspline basis functions, determined in turn
byM−K−1knots.Theseproducefunctionsthatarepiecewisepolynomials
of degreeKbetween the knots, and joined up with continuity of degree
K−1 at the knots. As an example consider linear splines, or piec ewise
linear functions. One intuitively satisfying basis consis ts of the functions
b1(x) = 1,b2(x) =x, andbm+2(x) = (x−tm)+,m= 1,...,M−2,
wheretmis themth knot, and z+denotes positive part. Tensor products
of spline bases can be used for inputs with dimensions larger than one
(see Section 5.2, and the CART and MARS models in Chapter 9.) T he
parameterMcontrols the degree of the polynomial or the number of knots
in the case of splines.
Radial basis functions are symmetric p-dimensional kernels located at
particular centroids,
fθ(x) =M/summationdisplay
m=1Kλm(µm,x)θm; (2.44)
for example, the Gaussian kernel Kλ(µ,x) =e−||x−µ||2/2λis popular.
Radial basis functions have centroids µmand scales λmthat have to
be determined. The spline basis functions have knots. In gen eral we would
like the data to dictate them as well. Including these as para meters changes
the regression problem from a straightforward linear probl em to a combi-
natorially hard nonlinear problem. In practice, shortcuts such as greedy
algorithms or two stage processes are used. Section 6.7 desc ribes some such
approaches.
A single-layer feed-forward neural network model with line ar output
weights can be thought of as an adaptive basis function metho d. The model
has the form
fθ(x) =M/summationdisplay
m=1βmσ(αT
mx+bm), (2.45)
whereσ(x) = 1/(1 +e−x) is known as the activation function. Here, as
in the projection pursuit model, the directions αmand thebiastermsbm
havetobedetermined,andtheirestimationisthemeatofthe computation.
Details are given in Chapter 11.
These adaptively chosen basis function methods are also kno wn asdictio-
narymethods, where one has available a possibly inﬁnite set or di ctionary
Dof candidate basis functions from which to choose, and model s are built
up by employing some kind of search mechanism.

2.9 Model Selection and the Bias–Variance Tradeoﬀ 37
2.9 Model Selection and the Bias–Variance
Tradeoﬀ
All the models described above and many others discussed in l ater chapters
have asmoothing orcomplexity parameter that has to be determined:
•the multiplier of the penalty term;
•the width of the kernel;
•or the number of basis functions.
Inthecaseofthesmoothingspline,theparameter λindexesmodelsranging
from a straight line ﬁt to the interpolating model. Similarl y a local degree-
mpolynomial model ranges between a degree- mglobal polynomial when
the window size is inﬁnitely large, to an interpolating ﬁt wh en the window
size shrinks to zero. This means that we cannot use residual s um-of-squares
on the training data to determine these parameters as well, s ince we would
always pick those that gave interpolating ﬁts and hence zero residuals. Such
a model is unlikely to predict future data well at all.
Thek-nearest-neighbor regression ﬁt ˆfk(x0) usefully illustrates the com-
peting forces that aﬀect the predictive ability of such appr oximations. Sup-
pose the data arise from a model Y=f(X) +ε, with E(ε) = 0 and
Var(ε) =σ2. For simplicity here we assume that the values of xiin the
sample are ﬁxed in advance (nonrandom). The expected predic tion error
atx0, also known as testorgeneralization error, can be decomposed:
EPEk(x0) = E[(Y−ˆfk(x0))2|X=x0]
=σ2+[Bias2(ˆfk(x0))+Var T(ˆfk(x0))] (2.46)
=σ2+/bracketleftig
f(x0)−1
kk/summationdisplay
ℓ=1f(x(ℓ))/bracketrightig2
+σ2
k.(2.47)
The subscripts in parentheses ( ℓ) indicate the sequence of nearest neighbors
tox0.
There are three terms in this expression. The ﬁrst term σ2is their-
reducible error—the variance of the new test target—and is beyond our
control, even if we know the true f(x0).
The second and third terms are under our control, and make up t he
mean squared error ofˆfk(x0) in estimating f(x0), which is broken down
into a bias component and a variance component. The bias term is the
squared diﬀerence between the true mean f(x0) and the expected value of
the estimate—[E T(ˆfk(x0))−f(x0)]2—where the expectation averages the
randomness in the training data. This term will most likely i ncrease with
k, if the true function is reasonably smooth. For small kthe few closest
neighbors will have values f(x(ℓ)) close tof(x0), so their average should

38 2. Overview of Supervised Learning
High Bias
Low VarianceLow Bias
High VariancePrediction Error
Model ComplexityTraining SampleTest Sample
Low High
FIGURE 2.11. Test and training error as a function of model complexity.
be close to f(x0). Askgrows, the neighbors are further away, and then
anything can happen.
The variance term is simply the variance of an average here, a nd de-
creases as the inverse of k. So askvaries, there is a bias–variance tradeoﬀ.
More generally, as the model complexity of our procedureis increased, the
variance tends to increase and the squared bias tends to decr ease. The op-
posite behavior occurs as the model complexity is decreased . Fork-nearest
neighbors, the model complexity is controlled by k.
Typically we would like to choose our model complexity to tra de bias
oﬀ with variance in such a way as to minimize the test error. An obvious
estimate of test error is the training error1
N/summationtext
i(yi−ˆyi)2. Unfortunately
training error is not a good estimate of test error, as it does not properly
account for model complexity.
Figure 2.11 shows the typical behavior of the test and traini ng error, as
model complexity is varied. The training error tends to decr ease whenever
we increase the model complexity, that is, whenever we ﬁt the data harder.
However with too much ﬁtting, the model adapts itself too clo sely to the
training data, and will not generalize well (i.e., have larg e test error). In
that case the predictions ˆf(x0) will have large variance, as reﬂected in the
last term of expression (2.46). In contrast, if the model is n ot complex
enough, it will underﬁt and may have large bias, again resulting in poor
generalization. In Chapter 7 we discuss methods for estimat ing the test
error of a prediction method, and hence estimating the optim al amount of
model complexity for a given prediction method and training set.

Exercises 39
Bibliographic Notes
Some good general books on the learning problem are Duda et al . (2000),
Bishop(1995),(Bishop, 2006),Ripley(1996),Cherkasskya ndMulier(2007)
and Vapnik (1996). Parts of this chapter are based on Friedma n (1994b).
Exercises
Ex. 2.1Suppose each of K-classes has an associated target tk, which is a
vector of all zeros, except a one in the kth position. Show that classifying to
the largest element of ˆ yamounts to choosing the closest target, min k||tk−
ˆy||, if the elements of ˆ ysum to one.
Ex. 2.2Show how to compute the Bayes decision boundary for the simul a-
tion example in Figure 2.5.
Ex. 2.3Derive equation (2.24).
Ex. 2.4The edge eﬀect problem discussed on page 23 is not peculiar to
uniform sampling from bounded domains. Consider inputs dra wn from a
spherical multinormal distribution X∼N(0,Ip). The squared distance
from any sample point to the origin has a χ2
pdistribution with mean p.
Consider a prediction point x0drawn from this distribution, and let a=
x0/||x0||be an associated unit vector. Let zi=aTxibe the projection of
each of the training points on this direction.
Show that the ziare distributed N(0,1) with expected squared distance
from the origin 1, while the target point has expected square d distancep
from the origin.
Hence forp= 10, a randomly drawn test point is about 3 .1 standard
deviations from the origin, while all the training points ar e on average
one standard deviation along direction a. So most prediction points see
themselves as lying on the edge of the training set.
Ex. 2.5
(a) Derive equation (2.27). The last line makes use of (3.8) t hrough a
conditioning argument.
(b) Derive equation (2.28), making use of the cyclicproperty of the trace
operator [trace( AB) = trace(BA)], and its linearity (which allows us
to interchange the order of trace and expectation).
Ex. 2.6Consider a regression problem with inputs xiand outputs yi, and a
parameterized model fθ(x) to be ﬁt by least squares. Show that if there are
observations with tiedoridentical values ofx, then the ﬁt can be obtained
from a reduced weighted least squares problem.

40 2. Overview of Supervised Learning
Ex. 2.7Suppose we have a sample of Npairsxi,yidrawn i.i.d. from the
distribution characterized as follows:
xi∼h(x),the design density
yi=f(xi)+εi, fis the regression function
εi∼(0,σ2) (mean zero, variance σ2)
We construct an estimator for flinearin theyi,
ˆf(x0) =N/summationdisplay
i=1ℓi(x0;X)yi,
where the weights ℓi(x0;X) do not depend on the yi, but do depend on the
entire training sequence of xi, denoted here by X.
(a)Showthatlinearregressionand k-nearest-neighborregressionaremem-
bersofthisclassofestimators.Describeexplicitlythewe ightsℓi(x0;X)
in each of these cases.
(b) Decompose the conditional mean-squared error
EY|X(f(x0)−ˆf(x0))2
intoaconditionalsquaredbiasandaconditionalvariancec omponent.
LikeX,Yrepresents the entire training sequence of yi.
(c) Decompose the (unconditional) mean-squared error
EY,X(f(x0)−ˆf(x0))2
into a squared bias and a variance component.
(d) Establish a relationship between the squared biases and variances in
the above two cases.
Ex. 2.8Compare the classiﬁcation performance of linear regressio n andk–
nearest neighbor classiﬁcation on the zipcodedata. In particular, consider
only the 2’s and3’s, andk= 1,3,5,7 and 15. Show both the training and
test error for each choice. The zipcodedata are available from the book
websitewww-stat.stanford.edu/ElemStatLearn .
Ex. 2.9Consider a linear regression model with pparameters, ﬁt by least
squares to a set of training data ( x1,y1),...,(xN,yN) drawn at random
from a population. Let ˆβbe the least squares estimate. Suppose we have
some test data (˜ x1,˜y1),...,(˜xM,˜yM) drawn at random from the same pop-
ulation as the training data. If Rtr(β) =1
N/summationtextN
1(yi−βTxi)2andRte(β) =
1
M/summationtextM
1(˜yi−βT˜xi)2, prove that
E[Rtr(ˆβ)]≤E[Rte(ˆβ)],

Exercises 41
where the expectations are over all that is random in each exp ression. [This
exercisewasbroughttoourattentionbyRyanTibshirani,fr omahomework
assignment given by Andrew Ng.]

42 2. Overview of Supervised Learning

This is page 43
Printer: Opaque this
3
Linear Methods for Regression
3.1 Introduction
A linear regression model assumes that the regression funct ion E(Y|X) is
linear in the inputs X1,...,X p. Linear models were largely developed in
the precomputer age of statistics, but even in today’s compu ter era there
are still good reasons to study and use them. They are simple a nd often
provide an adequate and interpretable description of how th e inputs aﬀect
the output. For prediction purposes they can sometimes outp erform fancier
nonlinear models, especially in situations with small numb ers of training
cases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe
appliedtotransformationsoftheinputsandthisconsidera blyexpandstheir
scope. These generalizations are sometimes called basis-f unction methods,
and are discussed in Chapter 5.
In this chapter we describe linear methods for regression, w hile in the
nextchapter wediscusslinear methods for classiﬁcation. O nsometopics we
go into considerable detail, as it is our ﬁrm belief that an un derstanding
of linear methods is essential for understanding nonlinear ones. In fact,
many nonlinear techniques are direct generalizations of th e linear methods
discussed here.

44 3. Linear Methods for Regression
3.2 Linear Regression Models and Least Squares
AsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),
and want to predict a real-valued output Y. The linear regression model
has the form
f(X) =β0+p/summationdisplay
j=1Xjβj. (3.1)
The linear model either assumes that the regression functio n E(Y|X) is
linear, or that the linear model is a reasonable approximati on. Here the
βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come
from diﬀerent sources:
•quantitative inputs;
•transformations of quantitative inputs, such as log, squar e-root or
square;
•basisexpansions,suchas X2=X2
1,X3=X3
1,leadingtoapolynomial
representation;
•numeric or “dummy” coding of the levels of qualitative input s. For
example, if Gis a ﬁve-level factor input, we might create Xj, j=
1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-
sents the eﬀect of Gby a set of level-dependent constants, since in/summationtext5
j=1Xjβj, one of the Xjs is one, and the others are zero.
•interactions between variables, for example, X3=X1·X2.
No matter the source of the Xj, the model is linear in the parameters.
Typically we have a set of training data ( x1,y1)...(xN,yN) from which
to estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector
of feature measurements for the ith case. The most popular estimation
methodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T
to minimize the residual sum of squares
RSS(β) =N/summationdisplay
i=1(yi−f(xi))2
=N/summationdisplay
i=1/parenleftig
yi−β0−p/summationdisplay
j=1xijβj/parenrightig2
. (3.2)
From a statistical point of view, this criterion is reasonab le if the training
observations ( xi,yi) represent independent random draws from their popu-
lation. Even if the xi’s were not drawn randomly, the criterion is still valid
if theyi’s are conditionally independent given the inputs xi. Figure 3.1
illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional

3.2 Linear Regression Models and Least Squares 45
•• •
••••
••••
•••
••
•••
•
••
••
•••
••••••
••••
•••
••
••
•
••
••
••••
••
••
•
•••
•
••••
••••
••
••
X1X2Y
FIGURE 3.1. Linear least squares ﬁtting with X∈IR2. We seek the linear
function of Xthat minimizes the sum of squared residuals from Y.
space occupied by the pairs ( X,Y). Note that (3.2) makes no assumptions
about the validity of model (3.1); it simply ﬁnds the best lin ear ﬁt to the
data. Least squares ﬁtting is intuitively satisfying no mat ter how the data
arise; the criterion measures the average lack of ﬁt.
How do we minimize (3.2)? Denote by XtheN×(p+ 1) matrix with
each row an input vector (with a 1 in the ﬁrst position), and si milarly let
ybe theN-vector of outputs in the training set. Then we can write the
residual sum-of-squares as
RSS(β) = (y−Xβ)T(y−Xβ). (3.3)
This is a quadratic function in the p+1 parameters. Diﬀerentiating with
respect toβwe obtain
∂RSS
∂β=−2XT(y−Xβ)
∂2RSS
∂β∂βT= 2XTX.(3.4)
Assuming (for the moment) that Xhas full column rank, and hence XTX
is positive deﬁnite, we set the ﬁrst derivative to zero
XT(y−Xβ) = 0 (3.5)
to obtain the unique solution
ˆβ= (XTX)−1XTy. (3.6)

46 3. Linear Methods for Regression
x1x2y
ˆ y
FIGURE 3.2. TheN-dimensional geometry of least squares regression with two
predictors. The outcome vector yis orthogonally projected onto the hyperplane
spanned by the input vectors x1andx2. The projection ˆyrepresents the vector
of the least squares predictions
The predicted values at an input vector x0are given by ˆf(x0) = (1 :x0)Tˆβ;
the ﬁtted values at the training inputs are
ˆy=Xˆβ=X(XTX)−1XTy, (3.7)
where ˆyi=ˆf(xi). The matrix H=X(XTX)−1XTappearing in equation
(3.7) is sometimes called the “hat” matrix because it puts th e hat on y.
Figure3.2showsadiﬀerentgeometricalrepresentationoft heleastsquares
estimate,thistimeinIRN.Wedenotethecolumnvectorsof Xbyx0,x1,...,xp,
withx0≡1. For much of what follows, this ﬁrst column is treated like a ny
other. These vectors span a subspace of IRN, also referred to as the column
space of X. We minimize RSS( β) =∝⌊a∇⌈⌊ly−Xβ∝⌊a∇⌈⌊l2by choosing ˆβso that the
residual vector y−ˆyis orthogonal to this subspace. This orthogonality is
expressed in (3.5), and the resulting estimate ˆyis hence the orthogonal pro-
jectionofyonto this subspace. The hat matrix Hcomputes the orthogonal
projection, and hence it is also known as a projection matrix .
It might happen that the columns of Xare not linearly independent, so
thatXis not of full rank. This would occur, for example, if two of th e
inputs were perfectly correlated, (e.g., x2= 3x1). ThenXTXis singular
and the least squares coeﬃcients ˆβare not uniquely deﬁned. However,
the ﬁtted values ˆy=Xˆβare still the projection of yonto the column
space of X; there is just more than one way to express that projection
in terms of the column vectors of X. The non-full-rank case occurs most
oftenwhenoneormorequalitative inputsarecodedinaredun dantfashion.
There is usually a natural way to resolve the non-unique repr esentation,
by recoding and/or dropping redundant columns in X. Most regression
software packages detect these redundancies and automatic ally implement

3.2 Linear Regression Models and Least Squares 47
some strategy for removing them. Rank deﬁciencies can also o ccur in signal
and image analysis, where the number of inputs pcan exceed the number
of training cases N. In this case, the features are typically reduced by
ﬁltering or else the ﬁtting is controlled by regularization (Section 5.2.3 and
Chapter 18).
Up to now we have made minimal assumptions about the true dist ribu-
tion of the data. In order to pin down the sampling properties ofˆβ, we now
assume that the observations yiare uncorrelated and have constant vari-
anceσ2, and that the xiare ﬁxed (non random). The variance–covariance
matrix of the least squares parameter estimates is easily de rived from (3.6)
and is given by
Var(ˆβ) = (XTX)−1σ2. (3.8)
Typically one estimates the variance σ2by
ˆσ2=1
N−p−1N/summationdisplay
i=1(yi−ˆyi)2.
TheN−p−1 rather than Nin the denominator makes ˆ σ2an unbiased
estimate of σ2: E(ˆσ2) =σ2.
To draw inferences about the parameters and the model, addit ional as-
sumptions are needed. We now assume that (3.1) is the correct model for
the mean; that is, the conditional expectation of Yis linear inX1,...,X p.
Wealsoassumethatthedeviationsof Yarounditsexpectationareadditive
and Gaussian. Hence
Y= E(Y|X1,...,X p)+ε
=β0+p/summationdisplay
j=1Xjβj+ε, (3.9)
where the error εis a Gaussian random variable with expectation zero and
varianceσ2, writtenε∼N(0,σ2).
Under (3.9), it is easy to show that
ˆβ∼N(β,(XTX)−1σ2). (3.10)
This is a multivariate normal distribution with mean vector and variance–
covariance matrix as shown. Also
(N−p−1)ˆσ2∼σ2χ2
N−p−1, (3.11)
a chi-squared distribution with N−p−1 degrees of freedom. In addition ˆβ
and ˆσ2are statistically independent. We use these distributiona l properties
to form tests of hypothesis and conﬁdence intervals for the p arametersβj.

48 3. Linear Methods for Regression
ZTail Probabilities
2.0 2.2 2.4 2.6 2.8 3.00.01 0.02 0.03 0.04 0.05 0.06t30
t100
normal
FIGURE 3.3. The tail probabilities Pr(|Z|>z)for three distributions, t30,t100
and standard normal. Shown are the appropriate quantiles for t esting signiﬁcance
at thep= 0.05and0.01levels. The diﬀerence between tand the standard normal
becomes negligible for Nbigger than about 100.
To test the hypothesis that a particular coeﬃcient βj= 0, we form the
standardized coeﬃcient or Z-score
zj=ˆβj
ˆσ√vj, (3.12)
wherevjisthejthdiagonalelementof( XTX)−1.Underthenullhypothesis
thatβj= 0,zjis distributed as tN−p−1(atdistribution with N−p−1
degrees of freedom), and hence a large (absolute) value of zjwill lead to
rejection of this null hypothesis. If ˆ σis replaced by a known value σ, then
zjwould have a standard normal distribution. The diﬀerence be tween the
tail quantiles of a t-distribution and a standard normal become negligible
as the sample size increases, and so we typically use the norm al quantiles
(see Figure 3.3).
Often we need to test for the signiﬁcance of groups of coeﬃcie nts simul-
taneously. For example, to test if a categorical variable wi thklevels can
be excluded from a model, we need to test whether the coeﬃcien ts of the
dummy variables used to represent the levels can all be set to zero. Here
we use the Fstatistic,
F=(RSS0−RSS1)/(p1−p0)
RSS1/(N−p1−1), (3.13)
whereRSS 1istheresidualsum-of-squaresfortheleastsquaresﬁtofth ebig-
ger model with p1+1 parameters, and RSS 0the same for the nested smaller
model with p0+1 parameters, having p1−p0parameters constrained to be

3.2 Linear Regression Models and Least Squares 49
zero. TheFstatistic measures the change in residual sum-of-squares p er
additional parameter in the bigger model, and it is normaliz ed by an esti-
mate ofσ2. Under the Gaussian assumptions, and the null hypothesis th at
the smaller model is correct, the Fstatistic will have a Fp1−p0,N−p1−1dis-
tribution. It can be shown (Exercise 3.1) that the zjin (3.12) are equivalent
to theFstatistic for dropping the single coeﬃcient βjfrom the model. For
largeN, the quantiles of Fp1−p0,N−p1−1approach those of χ2
p1−p0/(p1−p0).
Similarly, we can isolate βjin (3.10) to obtain a 1 −2αconﬁdence interval
forβj:
(ˆβj−z(1−α)v1
2
jˆσ,ˆβj+z(1−α)v1
2
jˆσ). (3.14)
Herez(1−α)is the 1−αpercentile of the normal distribution:
z(1−0.025)= 1.96,
z(1−.05)= 1.645,etc.
Hence the standard practice of reporting ˆβ±2·se(ˆβ) amounts to an ap-
proximate 95% conﬁdence interval. Even if the Gaussian erro r assumption
does not hold, this interval will be approximately correct, with its coverage
approaching 1−2αas the sample size N→∞.
In a similar fashion we can obtain an approximate conﬁdence s et for the
entire parameter vector β, namely
Cβ={β|(ˆβ−β)TXTX(ˆβ−β)≤ˆσ2χ2
p+1(1−α)}, (3.15)
whereχ2
ℓ(1−α)is the 1−αpercentile of the chi-squared distribution on ℓ
degrees of freedom: for example, χ2
5(1−0.05)= 11.1,χ2
5(1−0.1)= 9.2. This
conﬁdence set for βgenerates a corresponding conﬁdence set for the true
functionf(x) =xTβ, namely{xTβ|β∈Cβ}(Exercise 3.2; see also Fig-
ure 5.4 in Section 5.2.2 for examples of conﬁdence bands for f unctions).
3.2.1 Example: Prostate Cancer
The data for this example come from a study by Stamey et al. (19 89). They
examined the correlation between the level of prostate-spe ciﬁc antigen and
a number of clinical measures in men who were about to receive a radical
prostatectomy. The variables are log cancer volume ( lcavol), log prostate
weight ( lweight),age, log of the amount of benign prostatic hyperplasia
(lbph), seminal vesicle invasion ( svi), log of capsular penetration ( lcp),
Gleason score ( gleason), and percent of Gleason scores 4 or 5 ( pgg45).
The correlation matrix of the predictors given in Table 3.1 s hows many
strong correlations. Figure 1.1 (page 3) of Chapter 1 is a sca tterplot matrix
showing every pairwise plot between the variables. We see th atsviis a
binary variable, and gleasonis an ordered categorical variable. We see, for

50 3. Linear Methods for Regression
TABLE 3.1. Correlations of predictors in the prostate cancer data.
lcavol lweight age lbph svi lcp gleason
lweight 0.300
age0.286 0.317
lbph0.063 0.437 0.287
svi0.593 0.181 0.129 −0.139
lcp0.692 0.157 0.173 −0.089 0.671
gleason 0.426 0.024 0.366 0.033 0.307 0.476
pgg45 0.483 0.074 0.276 −0.030 0.481 0.663 0.757
TABLE 3.2. Linear model ﬁt to the prostate cancer data. The Z score is the
coeﬃcient divided by its standard error (3.12). Roughly a Zscore larger than two
in absolute value is signiﬁcantly nonzero at the p= 0.05level.
Term Coeﬃcient Std. Error ZScore
Intercept 2.46 0.09 27.60
lcavol 0.68 0.13 5.37
lweight 0.26 0.10 2.75
age−0.14 0.10 −1.40
lbph 0.21 0.10 2.06
svi 0.31 0.12 2.47
lcp−0.29 0.15 −1.87
gleason−0.02 0.15 −0.15
pgg45 0.27 0.15 1.74
example, that both lcavolandlcpshow a strong relationship with the
response lpsa, and with each other. We need to ﬁt the eﬀects jointly to
untangle the relationships between the predictors and the r esponse.
We ﬁt a linear model to the log of prostate-speciﬁc antigen, lpsa, after
ﬁrst standardizing the predictors to have unit variance. We randomly split
the dataset into a training set of size 67 and a test set of size 30. We ap-
plied least squares estimation to the training set, produci ng the estimates,
standard errors and Z-scores shown in Table 3.2. The Z-scores are deﬁned
in (3.12), and measure the eﬀect of dropping that variable fr om the model.
AZ-score greater than 2 in absolute value is approximately sig niﬁcant at
the 5% level. (For our example, we have nine parameters, and t he 0.025 tail
quantiles of the t67−9distribution are±2.002!) The predictor lcavolshows
the strongest eﬀect, with lweightandsvialso strong. Notice that lcpis
not signiﬁcant, once lcavolis in the model (when used in a model without
lcavol,lcpis strongly signiﬁcant). We can also test for the exclusion o f
a number of terms at once, using the F-statistic (3.13). For example, we
consider dropping all the non-signiﬁcant terms in Table 3.2 , namely age,

3.2 Linear Regression Models and Least Squares 51
lcp,gleason, andpgg45. We get
F=(32.81−29.43)/(9−5)
29.43/(67−9)= 1.67, (3.16)
which has a p-value of 0.17 (Pr(F4,58>1.67) = 0.17), and hence is not
signiﬁcant.
The mean prediction error on the test data is 0 .521. In contrast, predic-
tion using the mean training value of lpsahas a test error of 1 .057, which
is called the “base error rate.” Hence the linear model reduc es the base
error rate by about 50%. We will return to this example later t o compare
various selection and shrinkage methods.
3.2.2 The Gauss–Markov Theorem
One of the most famous results in statistics asserts that the least squares
estimates of the parameters βhave the smallest variance among all linear
unbiased estimates. We will make this precise here, and also make clear
that the restriction to unbiased estimates is not necessari ly a wise one. This
observationwillleadustoconsiderbiasedestimatessucha sridgeregression
later in the chapter. We focus on estimation of any linear com bination of
the parameters θ=aTβ; for example, predictions f(x0) =xT
0βare of this
form. The least squares estimate of aTβis
ˆθ=aTˆβ=aT(XTX)−1XTy. (3.17)
Considering Xto be ﬁxed, this is a linear function cT
0yof the response
vectory. If we assume that the linear model is correct, aTˆβis unbiased
since
E(aTˆβ) = E(aT(XTX)−1XTy)
=aT(XTX)−1XTXβ
=aTβ. (3.18)
The Gauss–Markov theorem states that if we have any other lin ear estima-
tor˜θ=cTythat is unbiased for aTβ, that is, E( cTy) =aTβ, then
Var(aTˆβ)≤Var(cTy). (3.19)
The proof (Exercise 3.3) uses the triangle inequality. For s implicity we have
stated the result in terms of estimation of a single paramete raTβ, but with
a few more deﬁnitions one can state it in terms of the entire pa rameter
vectorβ(Exercise 3.3).
Consider the mean squared error of an estimator ˜θin estimating θ:
MSE(˜θ) = E( ˜θ−θ)2
= Var( ˜θ)+[E(˜θ)−θ]2. (3.20)

52 3. Linear Methods for Regression
The ﬁrst term is the variance, while the second term is the squ ared bias.
TheGauss-Markovtheoremimpliesthattheleastsquaresest imatorhasthe
smallest mean squared error of all linear estimators with no bias. However,
there may well exist a biased estimator with smaller mean squ ared error.
Suchanestimatorwouldtradealittlebiasforalargerreduc tioninvariance.
Biased estimates are commonly used. Any method that shrinks or sets to
zero some of the least squares coeﬃcients may result in a bias ed estimate.
We discuss many examples, including variable subset select ion and ridge
regression, later in this chapter. From a more pragmatic poi nt of view, most
models are distortions of the truth, and hence are biased; pi cking the right
model amounts to creating the right balance between bias and variance.
We go into these issues in more detail in Chapter 7.
Mean squared error is intimately related to prediction accu racy, as dis-
cussed in Chapter 2. Consider the prediction of the new respo nse at input
x0,
Y0=f(x0)+ε0. (3.21)
Then the expected prediction error of an estimate ˜f(x0) =xT
0˜βis
E(Y0−˜f(x0))2=σ2+E(xT
0˜β−f(x0))2
=σ2+MSE( ˜f(x0)). (3.22)
Therefore, expected prediction error and mean squared erro r diﬀer only by
the constant σ2, representing the variance of the new observation y0.
3.2.3 Multiple Regression from Simple Univariate Regressi on
The linear model (3.1) with p >1 inputs is called the multiple linear
regression model . The least squares estimates (3.6) for this model are best
understood in terms of the estimates for the univariate (p= 1) linear
model, as we indicate in this section.
Suppose ﬁrst that we have a univariate model with no intercep t, that is,
Y=Xβ+ε. (3.23)
The least squares estimate and residuals are
ˆβ=/summationtextN
1xiyi/summationtextN
1x2
i,
ri=yi−xiˆβ.(3.24)
In convenient vector notation, we let y= (y1,...,y N)T,x= (x1,...,x N)T
and deﬁne
∝an}⌊∇a⌋ketle{tx,y∝an}⌊∇a⌋ket∇i}ht=N/summationdisplay
i=1xiyi,
=xTy, (3.25)

3.2 Linear Regression Models and Least Squares 53
theinner product between xandy1. Then we can write
ˆβ=∝an}⌊∇a⌋ketle{tx,y∝an}⌊∇a⌋ket∇i}ht
∝an}⌊∇a⌋ketle{tx,x∝an}⌊∇a⌋ket∇i}ht,
r=y−xˆβ.(3.26)
As we will see, this simple univariate regression provides t he building block
for multiple linear regression. Suppose next that the input sx1,x2,...,xp
(the columns of the data matrix X) are orthogonal; that is ∝an}⌊∇a⌋ketle{txj,xk∝an}⌊∇a⌋ket∇i}ht= 0
for allj∝ne}ationslash=k. Then it is easy to check that the multiple least squares esti -
matesˆβjare equal to∝an}⌊∇a⌋ketle{txj,y∝an}⌊∇a⌋ket∇i}ht/∝an}⌊∇a⌋ketle{txj,xj∝an}⌊∇a⌋ket∇i}ht—the univariate estimates. In other
words, when the inputs are orthogonal, they have no eﬀect on e ach other’s
parameter estimates in the model.
Orthogonal inputs occur most often with balanced, designed experiments
(where orthogonality is enforced), but almost never with ob servational
data. Hence we will have to orthogonalize them in order to car ry this idea
further. Suppose next that we have an intercept and a single i nputx. Then
the least squares coeﬃcient of xhas the form
ˆβ1=∝an}⌊∇a⌋ketle{tx−¯x1,y∝an}⌊∇a⌋ket∇i}ht
∝an}⌊∇a⌋ketle{tx−¯x1,x−¯x1∝an}⌊∇a⌋ket∇i}ht, (3.27)
where ¯x=/summationtext
ixi/N, and1=x0, the vector of Nones. We can view the
estimate (3.27) as the result of two applications of the simp le regression
(3.26). The steps are:
1. regress xon1to produce the residual z=x−¯x1;
2. regress yon the residual zto give the coeﬃcient ˆβ1.
Inthisprocedure,“regress bona”meansasimpleunivariateregressionof b
onawith no intercept, producing coeﬃcient ˆ γ=∝an}⌊∇a⌋ketle{ta,b∝an}⌊∇a⌋ket∇i}ht/∝an}⌊∇a⌋ketle{ta,a∝an}⌊∇a⌋ket∇i}htand residual
vectorb−ˆγa. We say that bis adjusted for a, or is “orthogonalized” with
respect to a.
Step 1 orthogonalizes xwith respect to x0=1. Step 2 is just a simple
univariate regression, using the orthogonal predictors 1andz. Figure 3.4
shows this process for two general inputs x1andx2. The orthogonalization
does not change the subspace spanned by x1andx2, it simply produces an
orthogonal basis for representing it.
This recipe generalizes to the case of pinputs, as shown in Algorithm 3.1.
Note that the inputs z0,...,zj−1in step 2 are orthogonal, hence the simple
regression coeﬃcients computed there are in fact also the mu ltiple regres-
sion coeﬃcients.
1The inner-product notation is suggestive of generalizations of linea r regression to
diﬀerent metric spaces, as well as to probability spaces.

54 3. Linear Methods for Regression
x1x2y
ˆ yz z z z z
FIGURE 3.4. Least squares regression by orthogonalization of the inputs . The
vectorx2is regressed on the vector x1, leaving the residual vector z. The regres-
sion ofyonzgives the multiple regression coeﬃcient of x2. Adding together the
projections of yon each of x1andzgives the least squares ﬁt ˆy.
Algorithm 3.1 Regression by Successive Orthogonalization.
1. Initialize z0=x0=1.
2. Forj= 1,2,...,p
Regress xjonz0,z1,...,,zj−1to produce coeﬃcients ˆ γℓj=
∝an}⌊∇a⌋ketle{tzℓ,xj∝an}⌊∇a⌋ket∇i}ht/∝an}⌊∇a⌋ketle{tzℓ,zℓ∝an}⌊∇a⌋ket∇i}ht,ℓ= 0,...,j−1 and residual vector zj=
xj−/summationtextj−1
k=0ˆγkjzk.
3. Regress yon the residual zpto give the estimate ˆβp.
The result of this algorithm is
ˆβp=∝an}⌊∇a⌋ketle{tzp,y∝an}⌊∇a⌋ket∇i}ht
∝an}⌊∇a⌋ketle{tzp,zp∝an}⌊∇a⌋ket∇i}ht. (3.28)
Re-arranging the residual in step 2, we can see that each of th exjis a linear
combination of the zk, k≤j. Since the zjare all orthogonal, they form
a basis for the column space of X, and hence the least squares projection
onto this subspace is ˆy. Sincezpalone involves xp(with coeﬃcient 1), we
see that the coeﬃcient (3.28) is indeed the multiple regress ion coeﬃcient of
yonxp. This key result exposes the eﬀect of correlated inputs in mu ltiple
regression. Note also that by rearranging the xj, any one of them could
be in the last position, and a similar results holds. Hence st ated more
generally, we have shown that the jth multiple regression coeﬃcient is the
univariate regression coeﬃcient of yonxj·012...(j−1)(j+1)...,p, the residual
after regressing xjonx0,x1,...,xj−1,xj+1,...,xp:

3.2 Linear Regression Models and Least Squares 55
The multiple regression coeﬃcient ˆβjrepresents the additional
contribution of xjony, afterxjhas been adjusted for x0,x1,...,xj−1,
xj+1,...,xp.
Ifxpis highly correlated with some of the other xk’s, the residual vector
zpwill be close to zero, and from (3.28) the coeﬃcient ˆβpwill be very
unstable. This will be true for all the variables in the corre lated set. In
such situations, we might have all the Z-scores (as in Table 3 .2) be small—
any one of the set can be deleted—yet we cannot delete them all. From
(3.28) we also obtain an alternate formula for the variance e stimates (3.8),
Var(ˆβp) =σ2
∝an}⌊∇a⌋ketle{tzp,zp∝an}⌊∇a⌋ket∇i}ht=σ2
∝⌊a∇⌈⌊lzp∝⌊a∇⌈⌊l2. (3.29)
In other words, the precision with which we can estimate ˆβpdepends on
the length of the residual vector zp; this represents how much of xpis
unexplained by the other xk’s.
Algorithm 3.1 is known as the Gram–Schmidt procedure for multiple
regression, and is also a useful numerical strategy for comp uting the esti-
mates. We can obtain from it not just ˆβp, but also the entire multiple least
squares ﬁt, as shown in Exercise 3.4.
We can represent step 2 of Algorithm 3.1 in matrix form:
X=ZΓ, (3.30)
whereZhas as columns the zj(in order), and Γis the upper triangular ma-
trix with entries ˆ γkj. Introducing the diagonal matrix Dwithjth diagonal
entryDjj=∝⌊a∇⌈⌊lzj∝⌊a∇⌈⌊l, we get
X=ZD−1DΓ
=QR, (3.31)
the so-called QRdecomposition of X. HereQis anN×(p+1) orthogonal
matrix,QTQ=I, andRis a (p+1)×(p+1) upper triangular matrix.
TheQRdecomposition represents a convenient orthogonal basis fo r the
column space of X. It is easy to see, for example, that the least squares
solution is given by
ˆβ=R−1QTy, (3.32)
ˆy=QQTy. (3.33)
Equation (3.32) is easy to solve because Ris upper triangular
(Exercise 3.4).

56 3. Linear Methods for Regression
3.2.4 Multiple Outputs
Suppose we have multiple outputs Y1,Y2,...,Y Kthat we wish to predict
from our inputs X0,X1,X2,...,X p. We assume a linear model for each
output
Yk=β0k+p/summationdisplay
j=1Xjβjk+εk (3.34)
=fk(X)+εk. (3.35)
WithNtraining cases we can write the model in matrix notation
Y=XB+E. (3.36)
HereYis theN×Kresponse matrix, with ikentryyik,Xis theN×(p+1)
input matrix, Bis the (p+ 1)×Kmatrix of parameters and Eis the
N×Kmatrix of errors. A straightforward generalization of the u nivariate
loss function (3.2) is
RSS(B) =K/summationdisplay
k=1N/summationdisplay
i=1(yik−fk(xi))2(3.37)
= tr[(Y−XB)T(Y−XB)]. (3.38)
The least squares estimates have exactly the same form as bef ore
ˆB= (XTX)−1XTY. (3.39)
Hence the coeﬃcients for the kth outcome are just the least squares es-
timates in the regression of ykonx0,x1,...,xp. Multiple outputs do not
aﬀect one another’s least squares estimates.
If the errors ε= (ε1,...,ε K) in (3.34) are correlated, then it might seem
appropriate to modify (3.37) in favor of a multivariate vers ion. Speciﬁcally,
suppose Cov( ε) =Σ, then the multivariate weighted criterion
RSS(B;Σ) =N/summationdisplay
i=1(yi−f(xi))TΣ−1(yi−f(xi)) (3.40)
arises naturally from multivariate Gaussian theory. Here f(x) is the vector
function (f1(x),...,f K(x))T, andyithe vector of Kresponses for obser-
vationi. However, it can be shown that again the solution is given by
(3.39);Kseparate regressions that ignore the correlations (Exerci se 3.11).
If theΣivary among observations, then this is no longer the case, and the
solution for Bno longer decouples.
In Section 3.7 we pursue the multiple outcome problem, and co nsider
situations where it does pay to combine the regressions.

3.3 Subset Selection 57
3.3 Subset Selection
There are two reasons why we are often not satisﬁed with the le ast squares
estimates (3.6).
•The ﬁrst is prediction accuracy : the least squares estimates often have
low bias but large variance. Prediction accuracy can someti mes be
improved by shrinking or setting some coeﬃcients to zero. By doing
sowesacriﬁcealittlebitofbiastoreducethevarianceofth epredicted
values, and hence may improve the overall prediction accura cy.
•The second reason is interpretation . With a large number of predic-
tors, we often would like to determine a smaller subset that e xhibit
the strongest eﬀects. In order to get the “big picture,” we ar e willing
to sacriﬁce some of the small details.
In this section we describe a number of approaches to variabl e subset selec-
tionwithlinearregression.Inlatersectionswediscusssh rinkageandhybrid
approaches for controlling variance, as well as other dimen sion-reduction
strategies. These all fall under the general heading model selection . Model
selection is not restricted to linear models; Chapter 7 cove rs this topic in
some detail.
With subset selection we retain only a subset of the variable s, and elim-
inate the rest from the model. Least squares regression is us ed to estimate
the coeﬃcients of the inputs that are retained. There are a nu mber of dif-
ferent strategies for choosing the subset.
3.3.1 Best-Subset Selection
Best subset regression ﬁnds for each k∈{0,1,2,...,p}the subset of size k
that gives smallest residual sum of squares (3.2). An eﬃcien t algorithm—
theleaps and bounds procedure (Furnival and Wilson, 1974)—makes this
feasible for pas large as 30 or 40. Figure 3.5 shows all the subset models
for the prostate cancer example. The lower boundary represe nts the models
that are eligible for selection by the best-subsets approac h. Note that the
best subset of size 2, for example, need not include the varia ble that was
in the best subset of size 1 (for this example all the subsets a re nested).
The best-subset curve (red lower boundary in Figure 3.5) is n ecessarily
decreasing, so cannot be used to select the subset size k. The question of
how to choose kinvolves the tradeoﬀ between bias and variance, along with
the more subjective desire for parsimony. There are a number of criteria
that one may use; typically we choose the smallest model that minimizes
an estimate of the expected prediction error.
Many of the other approaches that we discuss in this chapter a re similar,
in that they use the training data to produce a sequence of mod els varying
in complexity and indexed by a single parameter. In the next s ection we use

58 3. Linear Methods for Regression
Subset Size kResidual Sum−of−Squares
0 20 40 60 80 100
0 1 2 3 4 5 6 7 8•
••••••••
••••••••••••••••••••••••••
•••••••••••••••••••••••••••••••••••••••••••••••••
••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••
•••••••••••••••••••••••••••••••••••••••••••••
••••••••••••••••••••••••••
••••••••
••
•
•••••• •
FIGURE 3.5. All possible subset models for the prostate cancer example. At
each subset size is shown the residual sum-of-squares for ea ch model of that size.
cross-validation to estimate prediction error and select k; the AIC criterion
is a popular alternative. We defer more detailed discussion of these and
other approaches to Chapter 7.
3.3.2 Forward- and Backward-Stepwise Selection
Rather than search through all possible subsets (which beco mes infeasible
forpmuchlargerthan40),wecanseekagoodpaththroughthem. Forward-
stepwise selection starts with the intercept, and then sequentially adds into
the model the predictor that most improves the ﬁt. With many c andidate
predictors, this might seem like a lot of computation; howev er, clever up-
dating algorithms can exploit the QR decomposition for the c urrent ﬁt to
rapidly establish the next candidate (Exercise 3.9). Like b est-subset re-
gression, forward stepwise produces a sequence of models in dexed byk, the
subset size, which must be determined.
Forward-stepwise selection is a greedy algorithm , producing a nested se-
quence of models. In this sense it might seem sub-optimal com pared to
best-subset selection. However, there are several reasons why it might be
preferred:

3.3 Subset Selection 59
•Computational; for largepwe cannot compute the best subset se-
quence, but we can always compute the forward stepwise seque nce
(even when p≫N).
•Statistical; a price is paid in variance for selecting the best subset
of each size; forward stepwise is a more constrained search, and will
have lower variance, but perhaps more bias.
0 5 10 15 20 25 300.65 0.70 0.75 0.80 0.85 0.90 0.95Best Subset
Forward Stepwise
Backward Stepwise
Forward StagewiseE||ˆβ(k)−β||2
Subset Size k
FIGURE 3.6. Comparison of four subset-selection techniques on a simulate d lin-
ear regression problem Y=XTβ+ε. There are N= 300observations on p= 31
standard Gaussian variables, with pairwise correlations all e qual to0.85. For10of
the variables, the coeﬃcients are drawn at random from a N(0,0.4)distribution;
the rest are zero. The noise ε∼N(0,6.25), resulting in a signal-to-noise ratio of
0.64. Results are averaged over 50simulations. Shown is the mean-squared error
of the estimated coeﬃcient ˆβ(k)at each step from the true β.
Backward-stepwise selection starts with the full model, and sequentially
deletes the predictor that has the least impact on the ﬁt. The candidate for
droppingisthevariablewiththesmallestZ-score(Exercis e3.10).Backward
selection can only be used when N >p, while forward stepwise can always
be used.
Figure 3.6 shows the results of a small simulation study to co mpare
best-subset regression with the simpler alternatives forw ard and backward
selection. Their performance is very similar, as is often th e case. Included in
the ﬁgure is forward stagewise regression (next section), w hich takes longer
to reach minimum error.

60 3. Linear Methods for Regression
On the prostate cancer example, best-subset, forward and ba ckward se-
lection all gave exactly the same sequence of terms.
Some software packages implement hybrid stepwise-selecti on strategies
that consider both forward and backward moves at each step, a nd select
the “best” of the two. For example in the Rpackage the stepfunction uses
the AIC criterion for weighing the choices, which takes prop er account of
the number of parameters ﬁt; at each step an add or drop will be performed
that minimizes the AIC score.
Othermoretraditionalpackagesbasetheselectionon F-statistics,adding
“signiﬁcant” terms, and dropping “non-signiﬁcant” terms. These are out
of fashion, since they do not take proper account of the multi ple testing
issues. It is also tempting after a model search to print out a summary of
the chosen model, such as in Table 3.2; however, the standard errors are
not valid, since they do not account for the search process. T he bootstrap
(Section 8.2) can be useful in such settings.
Finally, we note that often variables come in groups (such as the dummy
variables that code a multi-level categorical predictor). Smart stepwise pro-
cedures (such as stepinR) will add or drop whole groups at a time, taking
proper account of their degrees-of-freedom.
3.3.3 Forward-Stagewise Regression
Forward-stagewise regression (FS) is even more constraine d than forward-
stepwise regression. It starts like forward-stepwise regr ession, with an in-
tercept equal to ¯ y, and centered predictors with coeﬃcients initially all 0.
At each step the algorithm identiﬁes the variable most corre lated with the
current residual. It then computes the simple linear regres sion coeﬃcient
of the residual on this chosen variable, and then adds it to th e current co-
eﬃcient for that variable. This is continued till none of the variables have
correlation with the residuals—i.e. the least-squares ﬁt wh enN >p.
Unlike forward-stepwise regression, none of the other vari ables are ad-
justed when a term is added to the model. As a consequence, for ward
stagewise can take many more than psteps to reach the least squares ﬁt,
and historically has been dismissed as being ineﬃcient. It t urns out that
this “slow ﬁtting” can pay dividends in high-dimensional pr oblems. We
see in Section 3.8.1 that both forward stagewise and a varian t which is
slowed down even further are quite competitive, especially in very high-
dimensional problems.
Forward-stagewise regression is included in Figure 3.6. In this example it
takes over 1000 steps to get all the correlations below 10−4. For subset size
k, we plotted the error for the last step for which there where knonzero
coeﬃcients. Although it catches up with the best ﬁt, it takes longer to
do so.

3.4 Shrinkage Methods 61
3.3.4 Prostate Cancer Data Example (Continued)
Table 3.3 shows the coeﬃcients from a number of diﬀerent sele ction and
shrinkagemethods.Theyare best-subset selection usinganall-subsetssearch,
ridge regression , thelasso,principal components regression andpartial least
squares. Each method has a complexity parameter, and this was chosen to
minimize an estimate of prediction error based on tenfold cr oss-validation;
fulldetailsaregiveninSection7.10.Brieﬂy,cross-valid ationworksbydivid-
ing the training data randomly into ten equal parts. The lear ning method
is ﬁt—for a range of values of the complexity parameter—to nine -tenths of
the data, and the prediction error is computed on the remaini ng one-tenth.
This is done in turn for each one-tenth of the data, and the ten prediction
error estimates are averaged. From this we obtain an estimat ed prediction
error curve as a function of the complexity parameter.
Note that we have already divided these data into a training s et of size
67 and a test set of size 30. Cross-validation is applied to th e training set,
since selecting the shrinkage parameter is part of the train ing process. The
test set is there to judge the performance of the selected mod el.
The estimated prediction error curves are shown in Figure 3. 7. Many of
the curves are very ﬂat over large ranges near their minimum. Included
are estimated standard error bands for each estimated error rate, based on
the ten error estimates computed by cross-validation. We ha ve used the
“one-standard-error” rule—we pick the most parsimonious mo del within
one standard error of the minimum (Section 7.10, page 244). S uch a rule
acknowledges the fact that the tradeoﬀ curve is estimated wi th error, and
hence takes a conservative approach.
Best-subset selection chose to use the two predictors lcvolandlweight.
The last two lines of the table give the average prediction er ror (and its
estimated standard error) over the test set.
3.4 Shrinkage Methods
By retaining a subset of the predictors and discarding the re st, subset selec-
tion produces a model that is interpretable and has possibly lower predic-
tion error than the full model. However, because it is a discr ete process—
variables are either retained or discarded—it often exhibit s high variance,
and so doesn’t reduce the prediction error of the full model. Shrinkage
methods are more continuous, and don’t suﬀer as much from hig h
variability.
3.4.1 Ridge Regression
Ridge regression shrinks the regression coeﬃcients by impo sing a penalty
on their size. The ridge coeﬃcients minimize a penalized res idual sum of

62 3. Linear Methods for Regression
Subset SizeCV Error
0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•
•
•••••••All Subsets
Degrees of FreedomCV Error
0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•
•
•
••••••Ridge Regression
Shrinkage Factor sCV Error
0.0 0.2 0.4 0.6 0.8 1.00.6 0.8 1.0 1.2 1.4 1.6 1.8•
•
•
••••••Lasso
Number of DirectionsCV Error
0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•
••••••••Principal Components Regression
Number of  DirectionsCV Error
0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•
••••••• •Partial Least Squares
FIGURE 3.7. Estimated prediction error curves and their standard error s for
the various selection and shrinkage methods. Each curve is plo tted as a function
of the corresponding complexity parameter for that method. T he horizontal axis
has been chosen so that the model complexity increases as we mo ve from left to
right. The estimates of prediction error and their standard errors were obtained by
tenfold cross-validation; full details are given in Section 7. 10. The least complex
model within one standard error of the best is chosen, indica ted by the purple
vertical broken lines.

3.4 Shrinkage Methods 63
TABLE 3.3. Estimated coeﬃcients and test error results, for diﬀerent su bset
and shrinkage methods applied to the prostate data. The blank e ntries correspond
to variables omitted.
Term LS Best Subset Ridge Lasso PCR PLS
Intercept 2.465 2.477 2.452 2.468 2.497 2.452
lcavol 0.680 0.740 0.420 0.533 0.543 0.419
lweight 0.263 0.316 0.238 0.169 0.289 0.344
age−0.141 −0.046 −0.152−0.026
lbph 0.210 0.162 0.002 0.214 0.220
svi0.305 0.227 0.094 0.315 0.243
lcp−0.288 0.000 −0.051 0.079
gleason −0.021 0.040 0.232 0.011
pgg45 0.267 0.133 −0.056 0.084
Test Error 0.521 0.492 0.492 0.479 0.449 0.528
Std Error 0.179 0.143 0.165 0.164 0.105 0.152
squares,
ˆβridge= argmin
β/braceleftbiggN/summationdisplay
i=1/parenleftbig
yi−β0−p/summationdisplay
j=1xijβj/parenrightbig2+λp/summationdisplay
j=1β2
j/bracerightbigg
.(3.41)
Hereλ≥0 is a complexity parameter that controls the amount of shrin k-
age: the larger the value of λ, the greater the amount of shrinkage. The
coeﬃcients are shrunk toward zero (and each other). The idea of penaliz-
ing by the sum-of-squares of the parameters is also used in ne ural networks,
where it is known as weight decay (Chapter 11).
An equivalent way to write the ridge problem is
ˆβridge= argmin
βN/summationdisplay
i=1/parenleftig
yi−β0−p/summationdisplay
j=1xijβj/parenrightig2
,
subject top/summationdisplay
j=1β2
j≤t,(3.42)
which makes explicit the size constraint on the parameters. There is a one-
to-one correspondence between the parameters λin (3.41) and tin (3.42).
When there are many correlated variables in a linear regress ion model,
their coeﬃcients can become poorly determined and exhibit h igh variance.
A wildly large positive coeﬃcient on one variable can be canc eled by a
similarly large negative coeﬃcient on its correlated cousi n. By imposing a
size constraint on the coeﬃcients, as in (3.42), this proble m is alleviated.
The ridge solutions are not equivariant under scaling of the inputs, and
so one normally standardizes the inputs before solving (3.4 1). In addition,

64 3. Linear Methods for Regression
notice that the intercept β0has been left out of the penalty term. Penal-
ization of the intercept would make the procedure depend on t he origin
chosen forY; that is, adding a constant cto each of the targets yiwould
not simply result in a shift of the predictions by the same amo untc. It
can be shown (Exercise 3.5) that the solution to (3.41) can be separated
into two parts, after reparametrization using centered inputs: each xijgets
replaced by xij−¯xj. We estimate β0by ¯y=1
N/summationtextN
1yi. The remaining co-
eﬃcients get estimated by a ridge regression without interc ept, using the
centeredxij. Henceforth we assume that this centering has been done, so
that the input matrix Xhasp(rather than p+1) columns.
Writing the criterion in (3.41) in matrix form,
RSS(λ) = (y−Xβ)T(y−Xβ)+λβTβ, (3.43)
the ridge regression solutions are easily seen to be
ˆβridge= (XTX+λI)−1XTy, (3.44)
whereIis thep×pidentity matrix. Notice that with the choice of quadratic
penaltyβTβ, the ridge regression solution is again a linear function of
y. The solution adds a positive constant to the diagonal of XTXbefore
inversion. This makes the problem nonsingular, even if XTXis not of full
rank, and was the main motivation for ridge regression when i t was ﬁrst
introducedinstatistics(HoerlandKennard,1970).Tradit ionaldescriptions
of ridge regression start with deﬁnition (3.44). We choose t o motivate it via
(3.41) and (3.42), as these provide insight into how it works .
Figure 3.8 shows the ridge coeﬃcient estimates for the prost ate can-
cer example, plotted as functions of df( λ), theeﬀective degrees of freedom
implied by the penalty λ(deﬁned in (3.50) on page 68). In the case of or-
thonormal inputs, the ridge estimates are just a scaled vers ion of the least
squares estimates, that is, ˆβridge=ˆβ/(1+λ).
Ridge regression can also be derived as the mean or mode of a po ste-
rior distribution, with a suitably chosen prior distributi on. In detail, sup-
poseyi∼N(β0+xT
iβ,σ2), and the parameters βjare each distributed as
N(0,τ2), independently of one another. Then the (negative) log-po sterior
density ofβ, withτ2andσ2assumed known, is equal to the expression
in curly braces in (3.41), with λ=σ2/τ2(Exercise 3.6). Thus the ridge
estimate is the mode of the posterior distribution; since th e distribution is
Gaussian, it is also the posterior mean.
Thesingular value decomposition (SVD) of the centered input matrix X
gives us some additional insight into the nature of ridge reg ression. This de-
composition is extremely useful in the analysis of many stat istical methods.
The SVD of the N×pmatrixXhas the form
X=UDVT. (3.45)

3.4 Shrinkage Methods 65Coefficients
0 2 4 6 8−0.2 0.0 0.2 0.4 0.6•
•••••
•
•
•
•
•
•
•
•
•
•
•
•
••••••
•lcavol
••••••••••••••••••••••••
•lweight
•••••••••••••••••••••••••
age••••••••••••••••••••••••
•lbph••••••••••••••••••••••••
•svi
••••••••••••••••••••••••
•
lcp••••••••••••••••••••••••
•gleason•
•••••••••••••••••••••••
•pgg45
df(λ)
FIGURE 3.8. Proﬁles of ridge coeﬃcients for the prostate cancer example, a s
the tuning parameter λis varied. Coeﬃcients are plotted versus df(λ), the eﬀective
degrees of freedom. A vertical line is drawn at df = 5.0, the value chosen by
cross-validation.

66 3. Linear Methods for Regression
HereUandVareN×pandp×porthogonal matrices, with the columns
ofUspanning the column space of X, and the columns of Vspanning the
row space. Dis ap×pdiagonal matrix, with diagonal entries d1≥d2≥
···≥dp≥0 called the singular values of X. If one or more values dj= 0,
Xis singular.
Using the singular value decomposition we can write the leas t squares
ﬁtted vector as
Xˆβls=X(XTX)−1XTy
=UUTy, (3.46)
after some simpliﬁcation. Note that UTyare the coordinates of ywith
respect to the orthonormal basis U. Note also the similarity with (3.33);
QandUare generally diﬀerent orthogonal bases for the column spac e of
X(Exercise 3.8).
Now the ridge solutions are
Xˆβridge=X(XTX+λI)−1XTy
=U D(D2+λI)−1D UTy
=p/summationdisplay
j=1ujd2
j
d2
j+λuT
jy, (3.47)
where the ujare the columns of U. Note that since λ≥0, we haved2
j/(d2
j+
λ)≤1. Like linear regression, ridge regression computes the co ordinates of
ywithrespecttotheorthonormalbasis U.Itthenshrinksthesecoordinates
by the factors d2
j/(d2
j+λ). This means that a greater amount of shrinkage
is applied to the coordinates of basis vectors with smaller d2
j.
What does a small value of d2
jmean? The SVD of the centered matrix
Xis another way of expressing the principal components of the variables
inX. The sample covariance matrix is given by S=XTX/N, and from
(3.45) we have
XTX=VD2VT, (3.48)
which is the eigen decomposition ofXTX(and ofS, up to a factor N).
The eigenvectors vj(columns of V) are also called the principal compo-
nents(or Karhunen–Loeve) directions of X. The ﬁrst principal component
directionv1has the property that z1=Xv1has the largest sample vari-
ance amongst all normalized linear combinations of the colu mns ofX. This
sample variance is easily seen to be
Var(z1) = Var(Xv1) =d2
1
N, (3.49)
and in fact z1=Xv1=u1d1. The derived variable z1is called the ﬁrst
principal component of X, and hence u1is the normalized ﬁrst principal

3.4 Shrinkage Methods 67
-4 -2 0 2 4-4 -2 0 2 4ooo
ooooooo
o
o
ooo
oo
oo
oo
ooo
o
o
oooo
oo
o
ooo
ooo
ooooo
oo
ooo
ooo
oo
oooo
oo
o
oo
oo
ooo
ooo
ooo
ooo
o
oo
ooooo
o
oooo
ooo
o
oo
o
oo
oo
o
oo
oooooo
ooo
oo
ooo
oo
o
ooo
oooo
o
ooo
ooo
o
ooo
ooo
oo
o
oooo
o
oooo
oo
oo
oo
ooo
o
ooo
oo
oo
o
oo
oo
oooo
o
ooo
ooo
oo
oo
oo
oo
o
oooLargest Principal
Component
Smallest Principal
Component
X1X2
FIGURE 3.9. Principal components of some input data points. The largest p rin-
cipal component is the direction that maximizes the varianc e of the projected data,
and the smallest principal component minimizes that varianc e. Ridge regression
projects yonto these components, and then shrinks the coeﬃcients of th e low–
variance components more than the high-variance component s.
component. Subsequent principal components zjhave maximum variance
d2
j/N, subject to being orthogonal to the earlier ones. Conversel y the last
principal component has minimum variance. Hence the small singular val-
uesdjcorrespond to directions in the column space of Xhaving small
variance, and ridge regression shrinks these directions th e most.
Figure 3.9 illustrates the principal components of some dat a points in
two dimensions. If we consider ﬁtting a linear surface over t his domain
(theY-axis is sticking out of the page), the conﬁguration of the da ta allow
us to determine its gradient more accurately in the long dire ction than
the short. Ridge regression protects against the potential ly high variance
of gradients estimated in the short directions. The implici t assumption is
that the response will tend to vary most in the directions of h igh variance
of the inputs. This is often a reasonable assumption, since p redictors are
often chosen for study because they vary with the response va riable, but
need not hold in general.

68 3. Linear Methods for Regression
In Figure 3.7 we have plotted the estimated prediction error versus the
quantity
df(λ) = tr[ X(XTX+λI)−1XT],
= tr(Hλ)
=p/summationdisplay
j=1d2
j
d2
j+λ. (3.50)
This monotone decreasing function of λis theeﬀective degrees of freedom
of the ridge regression ﬁt. Usually in a linear-regression ﬁ t withpvariables,
the degrees-of-freedom of the ﬁt is p, the number of free parameters. The
idea is that although all pcoeﬃcients in a ridge ﬁt will be non-zero, they
are ﬁt in a restricted fashion controlled by λ. Note that df( λ) =pwhen
λ= 0 (no regularization) and df( λ)→0 asλ→ ∞. Of course there
is always an additional one degree of freedom for the interce pt, which was
removed apriori. This deﬁnition is motivated in more detail in Section 3.4.4
and Sections 7.4–7.6. In Figure 3.7 the minimum occurs at df( λ) = 5.0.
Table 3.3 shows that ridge regression reduces the test error of the full least
squares estimates by a small amount.
3.4.2 The Lasso
The lasso is a shrinkage method like ridge, with subtle but im portant dif-
ferences. The lasso estimate is deﬁned by
ˆβlasso= argmin
βN/summationdisplay
i=1/parenleftig
yi−β0−p/summationdisplay
j=1xijβj/parenrightig2
subject top/summationdisplay
j=1|βj|≤t. (3.51)
Just as in ridge regression, we can re-parametrize the const antβ0by stan-
dardizing the predictors; the solution for ˆβ0is ¯y, and thereafter we ﬁt a
model without an intercept (Exercise 3.5). In the signal pro cessing litera-
ture, the lasso is also known as basis pursuit (Chen et al., 1998).
We can also write the lasso problem in the equivalent Lagrangian form
ˆβlasso= argmin
β/braceleftbigg1
2N/summationdisplay
i=1/parenleftbig
yi−β0−p/summationdisplay
j=1xijβj/parenrightbig2+λp/summationdisplay
j=1|βj|/bracerightbigg
.(3.52)
Notice the similarity to the ridge regression problem (3.42 ) or (3.41): the
L2ridge penalty/summationtextp
1β2
jis replaced by the L1lasso penalty/summationtextp
1|βj|. This
latter constraint makes the solutions nonlinear in the yi, and there is no
closed form expression as in ridge regression. Computing th e lasso solution

3.4 Shrinkage Methods 69
is a quadratic programming problem, although we see in Secti on 3.4.4 that
eﬃcient algorithms are available for computing the entire p ath of solutions
asλis varied, with the same computational cost as for ridge regr ession.
Because of the nature of the constraint, making tsuﬃciently small will
cause some of the coeﬃcients to be exactly zero. Thus the lass o does a kind
of continuous subset selection. If tis chosen larger than t0=/summationtextp
1|ˆβj|(where
ˆβj=ˆβls
j, the least squares estimates), then the lasso estimates are theˆβj’s.
On the other hand, for t=t0/2 say, then the least squares coeﬃcients are
shrunk by about 50% on average. However, the nature of the shr inkage
is not obvious, and we investigate it further in Section 3.4. 4 below. Like
the subset size in variable subset selection, or the penalty parameter in
ridge regression, tshould be adaptively chosen to minimize an estimate of
expected prediction error.
In Figure 3.7, for ease of interpretation, we have plotted th e lasso pre-
diction error estimates versus the standardized parameter s=t//summationtextp
1|ˆβj|.
A value ˆs≈0.36 was chosen by 10-fold cross-validation; this caused four
coeﬃcients to be set to zero (ﬁfth column of Table 3.3). The re sulting
model has the second lowest test error, slightly lower than t he full least
squares model, but the standard errors of the test error esti mates (last line
of Table 3.3) are fairly large.
Figure 3.10 shows the lasso coeﬃcients as the standardized t uning pa-
rameters=t//summationtextp
1|ˆβj|is varied. At s= 1.0 these are the least squares
estimates; they decrease to 0 as s→0. This decrease is not always strictly
monotonic, although it is in this example. A vertical line is drawn at
s= 0.36, the value chosen by cross-validation.
3.4.3 Discussion: Subset Selection, Ridge Regression and t he
Lasso
Inthissectionwediscussandcomparethethreeapproachesd iscussedsofar
for restricting the linear regression model: subset select ion, ridge regression
and the lasso.
In the case of an orthonormal input matrix Xthe three procedures have
explicit solutions. Each method applies a simple transform ation to the least
squares estimate ˆβj, as detailed in Table 3.4.
Ridge regression does a proportional shrinkage. Lasso tran slates each
coeﬃcient by a constant factor λ, truncating at zero. This is called “soft
thresholding,”andisusedinthecontextofwavelet-baseds moothinginSec-
tion 5.9. Best-subset selection drops all variables with co eﬃcients smaller
than theMth largest; this is a form of “hard-thresholding.”
Back to the nonorthogonal case; some pictures help understa nd their re-
lationship. Figure 3.11 depicts the lasso (left) and ridge r egression (right)
when there are only two parameters. The residual sum of squar es has ellip-
tical contours, centered at the full least squares estimate . The constraint

70 3. Linear Methods for Regression
0.0 0.2 0.4 0.6 0.8 1.0−0.2 0.0 0.2 0.4 0.6
Shrinkage Factor sCoefficientslcavol
lweight
agelbphsvi
lcpgleasonpgg45
FIGURE 3.10. Proﬁles of lasso coeﬃcients, as the tuning parameter tis varied.
Coeﬃcients are plotted versus s=t//summationtextp
1|ˆβj|. A vertical line is drawn at s= 0.36,
the value chosen by cross-validation. Compare Figure 3.8 on pa ge 65; the lasso
proﬁles hit zero, while those for ridge do not. The proﬁles are pi ece-wise linear,
and so are computed only at the points displayed; see Section 3. 4.4 for details.

3.4 Shrinkage Methods 71
TABLE 3.4. Estimators of βjin the case of orthonormal columns of X.Mandλ
are constants chosen by the corresponding techniques; signdenotes the sign of its
argument ( ±1), andx+denotes “positive part” of x. Below the table, estimators
are shown by broken red lines. The 45◦line in gray shows the unrestricted estimate
for reference.
Estimator Formula
Best subset (size M)ˆβj·I(|ˆβj|≥|ˆβ(M)|)
Ridge ˆβj/(1+λ)
Lasso sign( ˆβj)(|ˆβj|−λ)+
(0,0) (0,0) (0,0)|ˆβ(M)|λBest Subset Ridge Lasso
β^β^ 2. . β
1β2
β1β
FIGURE 3.11. Estimation picture for the lasso (left) and ridge regression
(right). Shown are contours of the error and constraint func tions. The solid blue
areas are the constraint regions |β1|+|β2| ≤tandβ2
1+β2
2≤t2, respectively,
while the red ellipses are the contours of the least squares erro r function.

72 3. Linear Methods for Regression
region for ridge regression is the disk β2
1+β2
2≤t, while that for lasso is
the diamond|β1|+|β2|≤t. Both methods ﬁnd the ﬁrst point where the
elliptical contours hit the constraint region. Unlike the d isk, the diamond
has corners; if the solution occurs at a corner, then it has on e parameter
βjequal to zero. When p>2, the diamond becomes a rhomboid, and has
many corners, ﬂat edges and faces; there are many more opport unities for
the estimated parameters to be zero.
We can generalize ridge regression and the lasso, and view th em as Bayes
estimates. Consider the criterion
˜β= argmin
β/braceleftiggN/summationdisplay
i=1/parenleftbig
yi−β0−p/summationdisplay
j=1xijβj/parenrightbig2+λp/summationdisplay
j=1|βj|q/bracerightigg
(3.53)
forq≥0. The contours of constant value of/summationtext
j|βj|qare shown in Fig-
ure 3.12, for the case of two inputs.
Thinking of|βj|qas the log-prior density for βj, these are also the equi-
contours of the prior distribution of the parameters. The va lueq= 0 corre-
spondstovariablesubsetselection,asthepenaltysimplyc ountsthenumber
of nonzero parameters; q= 1 corresponds to the lasso, while q= 2 to ridge
regression. Notice that for q≤1, the prior is not uniform in direction, but
concentrates more mass in the coordinate directions. The pr ior correspond-
ing to theq= 1 case is an independent double exponential (or Laplace)
distribution for each input, with density (1 /2τ)exp(−|β|/τ) andτ= 1/λ.
The caseq= 1 (lasso) is the smallest qsuch that the constraint region
is convex; non-convex constraint regions make the optimiza tion problem
more diﬃcult.
In this view, the lasso, ridge regression and best subset sel ection are
Bayes estimates with diﬀerent priors. Note, however, that t hey are derived
as posterior modes, that is, maximizers of the posterior. It is more common
to use the mean of the posterior as the Bayes estimate. Ridge r egression is
also the posterior mean, but the lasso and best subset select ion are not.
Looking again at the criterion (3.53), we might try using oth er values
ofqbesides 0, 1, or 2. Although one might consider estimating qfrom
the data, our experience is that it is not worth the eﬀort for t he extra
variance incurred. Values of q∈(1,2) suggest a compromise between the
lasso and ridge regression. Although this is the case, with q >1,|βj|qis
diﬀerentiable at 0, and so does not share the ability of lasso (q= 1) for
q= 4 q= 2 q= 1 q= 0.5 q= 0.1
FIGURE 3.12. Contours of constant value of/summationtext
j|βj|qfor given values of q.

3.4 Shrinkage Methods 73
q= 1.2 α= 0.2
Lq Elastic Net
FIGURE 3.13. Contours of constant value of/summationtext
j|βj|qforq= 1.2(left plot),
and the elastic-net penalty/summationtext
j(αβ2
j+(1−α)|βj|)forα= 0.2(right plot). Although
visually very similar, the elastic-net has sharp (non-diﬀeren tiable) corners, while
theq= 1.2penalty does not.
setting coeﬃcients exactly to zero. Partly for this reason a s well as for
computational tractability, Zou and Hastie (2005) introdu ced theelastic-
netpenalty
λp/summationdisplay
j=1/parenleftbig
αβ2
j+(1−α)|βj|/parenrightbig
, (3.54)
a diﬀerent compromise between ridge and lasso. Figure 3.13 c ompares the
Lqpenalty with q= 1.2 and the elastic-net penalty with α= 0.2; it is
hard to detect the diﬀerence by eye. The elastic-net selects variables like
the lasso, and shrinks together the coeﬃcients of correlate d predictors like
ridge. It also has considerable computational advantages o ver theLqpenal-
ties. We discuss the elastic-net further in Section 18.4.
3.4.4 Least Angle Regression
Least angle regression (LAR) is a relative newcomer (Efron e t al., 2004),
and can be viewed as a kind of “democratic” version of forward stepwise
regression (Section 3.3.2). As we will see, LAR is intimatel y connected
with the lasso, and in fact provides an extremely eﬃcient alg orithm for
computing the entire lasso path as in Figure 3.10.
Forward stepwise regression builds a model sequentially, a dding one vari-
able at a time. At each step, it identiﬁes the best variable to include in the
active set , and then updates the least squares ﬁt to include all the acti ve
variables.
Least angle regression uses a similar strategy, but only ent ers “as much”
of a predictor as it deserves. At the ﬁrst step it identiﬁes th e variable
most correlated with the response. Rather than ﬁt this varia ble completely,
LAR moves the coeﬃcient of this variable continuously towar d its least-
squares value (causing its correlation with the evolving re sidual to decrease
in absolute value). As soon as another variable “catches up” in terms of
correlation with the residual, the process is paused. The se cond variable
then joins the active set, and their coeﬃcients are moved tog ether in a way
that keeps their correlations tied and decreasing. This pro cess is continued

74 3. Linear Methods for Regression
until all the variables are in the model, and ends at the full l east-squares
ﬁt. Algorithm 3.2 provides the details. The termination con dition in step 5
requires some explanation. If p>N−1, the LAR algorithm reaches a zero
residual solution after N−1 steps (the−1 is because we have centered the
data).
Algorithm 3.2 Least Angle Regression.
1. Standardize the predictors to have mean zero and unit norm . Start
with the residual r=y−¯y,β1,β2,...,β p= 0.
2. Find the predictor xjmost correlated with r.
3. Moveβjfrom 0 towards its least-squares coeﬃcient ∝an}⌊∇a⌋ketle{txj,r∝an}⌊∇a⌋ket∇i}ht, until some
other competitor xkhas as much correlation with the current residual
as doesxj.
4. Moveβjandβkin the direction deﬁned by their joint least squares
coeﬃcient of the current residual on ( xj,xk), until some other com-
petitorxlhas as much correlation with the current residual.
5. Continue in this way until all ppredictors have been entered. After
min(N−1,p) steps, we arrive at the full least-squares solution.
SupposeAkis the active set of variables at the beginning of the kth
step, and let βAkbe the coeﬃcient vector for these variables at this step;
there will be k−1 nonzero values, and the one just entered will be zero. If
rk=y−XAkβAkis the current residual, then the direction for this step is
δk= (XT
AkXAk)−1XT
Akrk. (3.55)
The coeﬃcient proﬁle then evolves as βAk(α) =βAk+α·δk. Exercise 3.23
veriﬁes that the directions chosen in this fashion do what is claimed: keep
the correlations tied and decreasing. If the ﬁt vector at the beginning of
this step is ˆfk, then it evolves as ˆfk(α) =ˆfk+α·uk, whereuk=XAkδk
is the new ﬁt direction. The name “least angle” arises from a g eometrical
interpretation of this process; ukmakes the smallest (and equal) angle
with each of the predictors in Ak(Exercise 3.24). Figure 3.14 shows the
absolute correlations decreasing and joining ranks with ea ch step of the
LAR algorithm, using simulated data.
By construction the coeﬃcients in LAR change in a piecewise l inear fash-
ion. Figure 3.15 [left panel] shows the LAR coeﬃcient proﬁle evolving as a
function of their L1arc length2. Note that we do not need to take small
2TheL1arc-length of a diﬀerentiable curve β(s) fors∈[0,S] is given by TV( β,S) =/integraltextS
0||˙β(s)||1ds,where˙β(s) =∂β(s)/∂s. For the piecewise-linear LAR coeﬃcient proﬁle,
this amounts to summing the L1norms of the changes in coeﬃcients from step to step.

3.4 Shrinkage Methods 75
0 5 10 150.0 0.1 0.2 0.3 0.4v2 v6 v4 v5 v3 v1
L1Arc LengthAbsolute Correlations
FIGURE 3.14. Progression of the absolute correlations during each step of t he
LAR procedure, using a simulated data set with six predictors . The labels at the
top of the plot indicate which variables enter the active set at each step. The step
length are measured in units of L1arc length.
0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Least Angle Regression
0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Lasso
L1Arc Length L1Arc Length
CoeﬃcientsCoeﬃcients
FIGURE 3.15. Left panel shows the LAR coeﬃcient proﬁles on the simulated
data, as a function of the L1arc length. The right panel shows the Lasso proﬁle.
They are identical until the dark-blue coeﬃcient crosses zer o at an arc length of
about18.

76 3. Linear Methods for Regression
steps and recheck the correlations in step 3; using knowledg e of the covari-
ance of the predictors and the piecewise linearity of the alg orithm, we can
workouttheexactsteplengthatthebeginningofeachstep(E xercise3.25).
The right panel of Figure 3.15 shows the lasso coeﬃcient proﬁ les on the
same data. They are almost identical to those in the left pane l, and diﬀer
for the ﬁrst time when the blue coeﬃcient passes back through zero. For the
prostate data, the LAR coeﬃcient proﬁle turns out to be ident ical to the
lasso proﬁle in Figure 3.10, which never crosses zero. These observations
lead to a simple modiﬁcation of the LAR algorithm that gives t he entire
lasso path, which is also piecewise-linear.
Algorithm 3.2a Least Angle Regression: Lasso Modiﬁcation .
4a. If a non-zero coeﬃcient hits zero, drop its variable from the active set
of variables and recompute the current joint least squares d irection.
TheLAR(lasso)algorithmisextremelyeﬃcient,requiringt hesameorder
of computation as that of a single least squares ﬁt using the ppredictors.
Least angle regression always takes psteps to get to the full least squares
estimates. The lasso path can have more than psteps, although the two
are often quite similar. Algorithm 3.2 with the lasso modiﬁc ation 3.2a is
an eﬃcient way of computing the solution to any lasso problem , especially
whenp≫N. Osborne et al. (2000a) also discovered a piecewise-linear path
for computing the lasso, which they called a homotopy algorithm.
Wenowgiveaheuristicargumentforwhytheseproceduresare sosimilar.
Although the LAR algorithm is stated in terms of correlation s, if the input
features are standardized, it is equivalent and easier to wo rk with inner-
products. Suppose Ais the active set of variables at some stage in the
algorithm, tied in their absolute inner-product with the cu rrent residuals
y−Xβ. We can express this as
xT
j(y−Xβ) =γ·sj,∀j∈A (3.56)
wheresj∈{−1,1}indicates the sign of the inner-product, and γis the
common value. Also |xT
k(y−Xβ)|≤γ∀k∝ne}ationslash∈A. Now consider the lasso
criterion (3.52), which we write in vector form
R(β) =1
2||y−Xβ||2
2+λ||β||1. (3.57)
LetBbe the active set of variables in the solution for a given valu e ofλ.
For these variables R(β) is diﬀerentiable, and the stationarity conditions
give
xT
j(y−Xβ) =λ·sign(βj),∀j∈B (3.58)
Comparing (3.58) with (3.56), we see that they are identical only if the
sign ofβjmatches the sign of the inner product. That is why the LAR

3.4 Shrinkage Methods 77
algorithm and lasso start to diﬀer when an active coeﬃcient p asses through
zero; condition (3.58) is violated for that variable, and it is kicked out of the
active setB. Exercise 3.23 shows that these equations imply a piecewise -
linear coeﬃcient proﬁle as λdecreases. The stationarity conditions for the
non-active variables require that
|xT
k(y−Xβ)|≤λ,∀k∝ne}ationslash∈B, (3.59)
which again agrees with the LAR algorithm.
Figure 3.16 compares LAR and lasso to forward stepwise and st agewise
regression. The setup is the same as in Figure 3.6 on page 59, e xcept here
N= 100 here rather than 300, so the problem is more diﬃcult. We s ee
that the more aggressive forward stepwise starts to overﬁt q uite early (well
before the 10 true variables can enter the model), and ultima tely performs
worse than the slower forward stagewise regression. The beh avior of LAR
and lasso is similar to that of forward stagewise regression . Incremental
forward stagewise is similar to LAR and lasso, and is describ ed in Sec-
tion 3.8.1.
Degrees-of-Freedom Formula for LAR and Lasso
Suppose that we ﬁt a linear model via the least angle regressi on procedure,
stoppingatsomenumberofsteps k<p,orequivalentlyusingalassobound
tthat produces a constrained version of the full least square s ﬁt. How many
parameters, or “degrees of freedom” have we used?
Considerﬁrstalinearregressionusingasubsetof kfeatures.Ifthissubset
is prespeciﬁed in advance without reference to the training data, then the
degrees of freedom used in the ﬁtted model is deﬁned to be k. Indeed, in
classical statistics, the number of linearly independent p arameters is what
is meant by “degrees of freedom.” Alternatively, suppose th at we carry out
a best subset selection to determine the “optimal” set of kpredictors. Then
the resulting model has kparameters, but in some sense we have used up
more thankdegrees of freedom.
We need a more general deﬁnition for the eﬀective degrees of f reedom of
an adaptively ﬁtted model. We deﬁne the degrees of freedom of the ﬁtted
vectorˆy= (ˆy1,ˆy2,...,ˆyN) as
df(ˆy) =1
σ2N/summationdisplay
i=1Cov(ˆyi,yi). (3.60)
Here Cov(ˆyi,yi) refers to the sampling covariance between the predicted
value ˆyiand its corresponding outcome value yi. This makes intuitive sense:
the harder that we ﬁt to the data, the larger this covariance a nd hence
df(ˆy). Expression (3.60) is a useful notion of degrees of freedom , one that
can be applied to any model prediction ˆy. This includes models that are

78 3. Linear Methods for Regression
0.0 0.2 0.4 0.6 0.8 1.00.55 0.60 0.65Forward Stepwise
LAR
Lasso
Forward Stagewise
Incremental Forward StagewiseE||ˆβ(k)−β||2
Fraction of L1arc-length
FIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward
stagewise (FS) and incremental forward stagewise (FS 0) regression. The setup
is the same as in Figure 3.6, except N= 100here rather than 300. Here the
slower FS regression ultimately outperforms forward stepwise . LAR and lasso
show similar behavior to FS and FS 0. Since the procedures take diﬀerent numbers
of steps (across simulation replicates and methods), we plot th e MSE as a function
of the fraction of total L1arc-length toward the least-squares ﬁt.
adaptively ﬁtted to the training data. This deﬁnition is mot ivated and
discussed further in Sections 7.4–7.6.
Now for a linear regression with kﬁxed predictors, it is easy to show
that df(ˆy) =k. Likewise for ridge regression, this deﬁnition leads to the
closed-form expression (3.50) on page 68: df( ˆy) = tr(Sλ). In both these
cases, (3.60) is simple to evaluate because the ﬁt ˆy=Hλyis linear in y.
If we think about deﬁnition (3.60) in the context of a best sub set selection
of sizek, it seems clear that df( ˆy) will be larger than k, and this can be
veriﬁed by estimating Cov(ˆ yi,yi)/σ2directly by simulation. However there
is no closed form method for estimating df( ˆy) for best subset selection.
For LAR and lasso, something magical happens. These techniq ues are
adaptiveinasmootherwaythanbestsubsetselection,andhe nceestimation
of degrees of freedom is more tractable. Speciﬁcally it can b e shown that
after thekth step of the LAR procedure, the eﬀective degrees of freedom of
the ﬁt vector is exactly k. Now for the lasso, the (modiﬁed) LAR procedure

3.5 Methods Using Derived Input Directions 79
often takes more than psteps, since predictors can drop out. Hence the
deﬁnitionisalittlediﬀerent;forthelasso,atanystagedf (ˆy)approximately
equals the number of predictors in the model. While this appr oximation
works reasonably well anywhere in the lasso path, for each kit works best
at thelastmodel in the sequence that contains kpredictors. A detailed
study of the degrees of freedom for the lasso may be found in Zo u et al.
(2007).
3.5 Methods Using Derived Input Directions
In many situations we have a large number of inputs, often ver y correlated.
The methods in this section produce a small number of linear c ombinations
Zm, m= 1,...,Mof the original inputs Xj, and theZmare then used in
place of the Xjas inputs in the regression. The methods diﬀer in how the
linear combinations are constructed.
3.5.1 Principal Components Regression
In this approach the linear combinations Zmused are the principal com-
ponents as deﬁned in Section 3.4.1 above.
Principal component regression forms the derived input col umnszm=
Xvm, and then regresses yonz1,z2,...,zMfor someM≤p. Since the zm
are orthogonal, this regression is just a sum of univariate r egressions:
ˆypcr
(M)= ¯y1+M/summationdisplay
m=1ˆθmzm, (3.61)
whereˆθm=∝an}⌊∇a⌋ketle{tzm,y∝an}⌊∇a⌋ket∇i}ht/∝an}⌊∇a⌋ketle{tzm,zm∝an}⌊∇a⌋ket∇i}ht. Since the zmare each linear combinations
of the original xj, we can express the solution (3.61) in terms of coeﬃcients
of thexj(Exercise 3.13):
ˆβpcr(M) =M/summationdisplay
m=1ˆθmvm. (3.62)
As with ridge regression, principal components depend on th e scaling of
the inputs, so typically we ﬁrst standardize them. Note that ifM=p, we
would just get back the usual least squares estimates, since the columns of
Z=UDspan the column space of X. ForM <pwe get a reduced regres-
sion. We see that principal components regression is very si milar to ridge
regression: both operate via the principal components of th e input ma-
trix. Ridge regression shrinks the coeﬃcients of the princi pal components
(Figure 3.17), shrinking more depending on the size of the co rresponding
eigenvalue; principal components regression discards the p−Msmallest
eigenvalue components. Figure 3.17 illustrates this.

80 3. Linear Methods for Regression
IndexShrinkage Factor
2 4 6 80.0 0.2 0.4 0.6 0.8 1.0•
••
••••
•• • • • • • •
• •ridge
pcr
FIGURE 3.17. Ridge regression shrinks the regression coeﬃcients of the p rin-
cipal components, using shrinkage factors d2
j/(d2
j+λ)as in (3.47). Principal
component regression truncates them. Shown are the shrinka ge and truncation
patterns corresponding to Figure 3.7, as a function of the pr incipal component
index.
In Figure 3.7 we see that cross-validation suggests seven te rms; the re-
sulting model has the lowest test error in Table 3.3.
3.5.2 Partial Least Squares
This technique also constructs a set of linear combinations of the inputs
for regression, but unlike principal components regressio n it uses y(in ad-
dition to X) for this construction. Like principal component regressi on,
partial least squares (PLS) is not scale invariant, so we ass ume that each
xjis standardized to have mean 0 and variance 1. PLS begins by co m-
puting ˆϕ1j=∝an}⌊∇a⌋ketle{txj,y∝an}⌊∇a⌋ket∇i}htfor eachj. From this we construct the derived input
z1=/summationtext
jˆϕ1jxj, which is the ﬁrst partial least squares direction. Hence
in the construction of each zm, the inputs are weighted by the strength
of their univariate eﬀect on y3. The outcome yis regressed on z1giving
coeﬃcient ˆθ1, and then we orthogonalize x1,...,xpwith respect to z1. We
continue this process, until M≤pdirections have been obtained. In this
manner, partial least squares produces a sequence of derive d, orthogonal
inputs or directions z1,z2,...,zM. As with principal-component regres-
sion, if we were to construct all M=pdirections, we would get back a
solution equivalent to the usual least squares estimates; u singM < pdi-
rections produces a reduced regression. The procedure is de scribed fully in
Algorithm 3.3.
3Since the xjare standardized, the ﬁrst directions ˆ ϕ1jare the univariate regression
coeﬃcients (up to an irrelevant constant); this is not the case for subsequen t directions.

3.5 Methods Using Derived Input Directions 81
Algorithm 3.3 Partial Least Squares.
1. Standardize each xjto have mean zero and variance one. Set ˆy(0)=
¯y1, andx(0)
j=xj, j= 1,...,p.
2. Form= 1,2,...,p
(a)zm=/summationtextp
j=1ˆϕmjx(m−1)
j, where ˆϕmj=∝an}⌊∇a⌋ketle{tx(m−1)
j,y∝an}⌊∇a⌋ket∇i}ht.
(b)ˆθm=∝an}⌊∇a⌋ketle{tzm,y∝an}⌊∇a⌋ket∇i}ht/∝an}⌊∇a⌋ketle{tzm,zm∝an}⌊∇a⌋ket∇i}ht.
(c)ˆy(m)=ˆy(m−1)+ˆθmzm.
(d) Orthogonalize each x(m−1)
jwith respect to zm:x(m)
j=x(m−1)
j−
[∝an}⌊∇a⌋ketle{tzm,x(m−1)
j∝an}⌊∇a⌋ket∇i}ht/∝an}⌊∇a⌋ketle{tzm,zm∝an}⌊∇a⌋ket∇i}ht]zm,j= 1,2,...,p.
3. Output the sequence of ﬁtted vectors {ˆy(m)}p
1. Since the{zℓ}m
1are
linear in the original xj, so isˆy(m)=Xˆβpls(m). These linear coeﬃ-
cients can be recovered from the sequence of PLS transformat ions.
In the prostate cancer example, cross-validation chose M= 2 PLS direc-
tions in Figure 3.7. This produced the model given in the righ tmost column
of Table 3.3.
What optimization problem is partial least squares solving ? Since it uses
the response yto construct its directions, its solution path is a nonlinea r
function of y. It can be shown (Exercise 3.15) that partial least squares
seeks directions that have high variance andhave high correlation with the
response, in contrast to principal components regression w hich keys only
on high variance (Stone and Brooks, 1990; Frank and Friedman , 1993). In
particular, the mth principal component direction vmsolves:
maxαVar(Xα) (3.63)
subject to||α||= 1, αTSvℓ= 0, ℓ= 1,...,m−1,
whereSis the sample covariance matrix of the xj. The conditions αTSvℓ=
0 ensures that zm=Xαis uncorrelated with all the previous linear com-
binations zℓ=Xvℓ. Themth PLS direction ˆ ϕmsolves:
maxαCorr2(y,Xα)Var(Xα) (3.64)
subject to||α||= 1, αTSˆϕℓ= 0, ℓ= 1,...,m−1.
Further analysis reveals that the variance aspect tends to d ominate, and
so partial least squares behaves much like ridge regression and principal
components regression. We discuss this further in the next s ection.
If the input matrix Xis orthogonal, then partial least squares ﬁnds the
least squares estimates after m= 1 steps. Subsequent steps have no eﬀect

82 3. Linear Methods for Regression
since the ˆϕmjare zero for m>1 (Exercise 3.14). It can also be shown that
the sequence of PLS coeﬃcients for m= 1,2,...,prepresents the conjugate
gradient sequence for computing the least squares solution s (Exercise 3.18).
3.6 Discussion: A Comparison of the Selection and
Shrinkage Methods
There are some simple settings where we can understand bette r the rela-
tionship between the diﬀerent methods described above. Con sider an exam-
ple with two correlated inputs X1andX2, with correlation ρ. We assume
that the true regression coeﬃcients are β1= 4 andβ2= 2. Figure 3.18
shows the coeﬃcient proﬁles for the diﬀerent methods, as the ir tuning pa-
rameters are varied. The top panel has ρ= 0.5, the bottom panel ρ=−0.5.
The tuning parameters for ridge and lasso vary over a continu ous range,
while best subset, PLS and PCR take just two discrete steps to the least
squares solution. In the top panel, starting at the origin, r idge regression
shrinks the coeﬃcients together until it ﬁnally converges t o least squares.
PLS and PCR show similar behavior to ridge, although are disc rete and
more extreme. Best subset overshoots the solution and then b acktracks.
The behavior of the lasso is intermediate to the other method s. When the
correlation is negative (lower panel), again PLS and PCR rou ghly track
the ridge path, while all of the methods are more similar to on e another.
It is interesting to compare the shrinkage behavior of these diﬀerent
methods. Recall that ridge regression shrinks all directio ns, but shrinks
low-variance directions more. Principal components regre ssion leaves M
high-variance directions alone, and discards the rest. Int erestingly, it can
be shown that partial least squares also tends to shrink the l ow-variance
directions, but can actually inﬂate some of the higher varia nce directions.
This can make PLS a little unstable, and cause it to have sligh tly higher
prediction error compared to ridge regression. A full study is given in Frank
and Friedman (1993). These authors conclude that for minimi zing predic-
tion error, ridge regression is generally preferable to var iable subset selec-
tion, principal components regression and partial least sq uares. However
the improvement over the latter two methods was only slight.
To summarize, PLS, PCR and ridge regression tend to behave si milarly.
Ridge regression may be preferred because it shrinks smooth ly, rather than
in discrete steps. Lasso falls somewhere between ridge regr ession and best
subset regression, and enjoys some of the properties of each .

3.6 Discussion: A Comparison of the Selection and Shrinkage Methods 83
0 1 2 3 4 5 6-1 0 1 2 3Least Squares
0Ridge
Lasso
Best SubsetPLS PCR
•
0 1 2 3 4 5 6-1 0 1 2 3Least Squares
Ridge
Best Subset
PLS
PCRLasso•
0ρ= 0.5
ρ=−0.5
β1β1β2 β2
FIGURE 3.18. Coeﬃcient proﬁles from diﬀerent methods for a simple problem:
two inputs with correlation ±0.5, and the true regression coeﬃcients β= (4,2).

84 3. Linear Methods for Regression
3.7 Multiple Outcome Shrinkage and Selection
As noted in Section 3.2.4, the least squares estimates in a mu ltiple-output
linear model are simply the individual least squares estima tes for each of
the outputs.
To apply selection and shrinkage methods in the multiple out put case,
one could apply a univariate technique individually to each outcome or si-
multaneously to all outcomes. With ridge regression, for ex ample, we could
apply formula (3.44) to each of the Kcolumns of the outcome matrix Y,
using possibly diﬀerent parameters λ, or apply it to all columns using the
same value of λ. The former strategy would allow diﬀerent amounts of
regularization to be applied to diﬀerent outcomes but requi re estimation
ofkseparate regularization parameters λ1,...,λ k, while the latter would
permit allkoutputs to be used in estimating the sole regularization pa-
rameterλ.
Other more sophisticated shrinkage and selection strategi es that exploit
correlations in the diﬀerent responses can be helpful in the multiple output
case. Suppose for example that among the outputs we have
Yk=f(X)+εk (3.65)
Yℓ=f(X)+εℓ; (3.66)
i.e., (3.65) and (3.66) share the same structural part f(X) in their models.
It is clear in this case that we should pool our observations o nYkandYl
to estimate the common f.
Combining responses is at the heart of canonical correlation analysis
(CCA), a data reduction technique developed for the multipl e output case.
Similar to PCA, CCA ﬁnds a sequence of uncorrelated linear co mbina-
tionsXvm, m= 1,...,M of thexj, and a corresponding sequence of
uncorrelated linear combinations Yumof the responses yk, such that the
correlations
Corr2(Yum,Xvm) (3.67)
are successively maximized. Note that at most M= min(K,p) directions
can be found. The leading canonical response variates are th ose linear com-
binations (derived responses) best predicted by the xj; in contrast, the
trailing canonical variates can be poorly predicted by the xj, and are can-
didates for being dropped. The CCA solution is computed usin g a general-
ized SVD of the sample cross-covariance matrix YTX/N(assuming Yand
Xare centered; Exercise 3.20).
Reduced-rank regression (Izenman,1975;vanderMerweandZidek,1980)
formalizes this approach in terms of a regression model that explicitly pools
information. Given an error covariance Cov( ε) =Σ, we solve the following

3.7 Multiple Outcome Shrinkage and Selection 85
restricted multivariate regression problem:
ˆBrr(m) = argmin
rank(B)=mN/summationdisplay
i=1(yi−BTxi)TΣ−1(yi−BTxi).(3.68)
WithΣreplaced by the estimate YTY/N, one can show (Exercise 3.21)
that the solution is given by a CCA of YandX:
ˆBrr(m) =ˆBUmU−
m, (3.69)
whereUmis theK×msub-matrix of Uconsisting of the ﬁrst mcolumns,
andUis theK×Mmatrix of leftcanonical vectors u1,u2,...,u M.U−
m
is its generalized inverse. Writing the solution as
ˆBrr(M) = (XTX)−1XT(YUm)U−
m, (3.70)
we see that reduced-rank regression performs a linear regre ssion on the
pooled response matrix YUm, and then maps the coeﬃcients (and hence
the ﬁts as well) back to the original response space. The redu ced-rank ﬁts
are given by
ˆYrr(m) =X(XTX)−1XTYUmU−
m
=HYPm,(3.71)
whereHis the usual linear regression projection operator, and Pmis the
rank-mCCA response projection operator. Although a better estima te of
Σwould be( Y−XˆB)T(Y−XˆB)/(N−pK), onecanshowthat thesolution
remains the same (Exercise 3.22).
Reduced-rank regression borrows strength among responses by truncat-
ing the CCA. Breiman and Friedman (1997) explored with some s uccess
shrinkage of the canonical variates between XandY, a smooth version of
reduced rank regression. Their proposal has the form (compare (3.69))
ˆBc+w=ˆBUΛU−1, (3.72)
whereΛis a diagonal shrinkage matrix (the “c+w” stands for “Curds
and Whey,” the name they gave to their procedure). Based on op timal
prediction in the population setting, they show that Λhas diagonal entries
λm=c2
m
c2m+p
N(1−c2m), m= 1,...,M, (3.73)
wherecmis themth canonical correlation coeﬃcient. Note that as the ratio
of the number of input variables to sample size p/Ngets small, the shrink-
age factors approach 1. Breiman and Friedman (1997) propose d modiﬁed
versions of Λbased on training data and cross-validation, but the genera l
form is the same. Here the ﬁtted response has the form
ˆYc+w=HYSc+w, (3.74)

86 3. Linear Methods for Regression
whereSc+w=UΛU−1is the response shrinkage operator.
Breiman and Friedman (1997) also suggested shrinking in bot h theY
space andXspace. This leads to hybrid shrinkage models of the form
ˆYridge,c+w=AλYSc+w, (3.75)
whereAλ=X(XTX+λI)−1XTis the ridge regression shrinkage operator,
as in (3.46) on page 66. Their paper and the discussions there of contain
many more details.
3.8 More on the Lasso and Related Path
Algorithms
Since the publication of the LAR algorithm (Efron et al., 200 4) there has
been a lot of activity in developing algorithms for ﬁtting re gularization
paths for a variety of diﬀerent problems. In addition, L1regularization has
taken on a life of its own, leading to the development of the ﬁe ldcompressed
sensingin the signal-processing literature. (Donoho, 2006a; Cand es, 2006).
Inthissectionwediscusssomerelatedproposalsandotherp athalgorithms,
starting oﬀ with a precursor to the LAR algorithm.
3.8.1 Incremental Forward Stagewise Regression
Here we present another LAR-like algorithm, this time focus ed on forward
stagewiseregression.Interestingly,eﬀortstounderstan daﬂexiblenonlinear
regression procedure (boosting) led to a new algorithm for l inear models
(LAR). In reading the ﬁrst edition of this book and the forwar d stagewise
Algorithm 3.4 Incremental Forward Stagewise Regression—FS ǫ.
1. Start with the residual requal to yandβ1,β2,...,β p= 0. All the
predictors are standardized to have mean zero and unit norm.
2. Find the predictor xjmost correlated with r
3. Updateβj←βj+δj, whereδj=ǫ·sign[∝an}⌊∇a⌋ketle{txj,r∝an}⌊∇a⌋ket∇i}ht] andǫ>0 is a small
step size, and set r←r−δjxj.
4. Repeat steps 2 and 3 many times, until the residuals are unc orrelated
with all the predictors.
Algorithm 16.1 of Chapter 164, our colleague Brad Efron realized that with
4In the ﬁrst edition, this was Algorithm 10.4 in Chapter 10.

3.8 More on the Lasso and Related Path Algorithms 87−0.2 0.0 0.2 0.4 0.6lcavol
lweight
agelbphsvi
lcpgleasonpgg45
0 50 100 150 200
−0.2 0.0 0.2 0.4 0.6lcavol
lweight
agelbphsvi
lcpgleasonpgg45
0.0 0.5 1.0 1.5 2.0FSǫ FS0
Iteration
CoeﬃcientsCoeﬃcients
L1Arc-length of Coeﬃcients
FIGURE 3.19. Coeﬃcient proﬁles for the prostate data. The left panel shows
incremental forward stagewise regression with step size ǫ= 0.01. The right panel
shows the inﬁnitesimal version FS 0obtained letting ǫ→0. This proﬁle was ﬁt by
the modiﬁcation 3.2b to the LAR Algorithm 3.2. In this example t he FS 0proﬁles
are monotone, and hence identical to those of lasso and LAR.
linearmodels,onecouldexplicitlyconstructthepiecewis e-linearlassopaths
of Figure 3.10. This led him to propose the LAR procedure of Se ction 3.4.4,
as well as the incremental version of forward-stagewise reg ression presented
here.
Consider the linear-regression version of the forward-sta gewise boosting
algorithm16.1proposedinSection16.1(page608).Itgener atesacoeﬃcient
proﬁle by repeatedly updating (by a small amount ǫ) the coeﬃcient of the
variable most correlated with the current residuals. Algor ithm 3.4 gives
the details. Figure 3.19 (left panel) shows the progress of t he algorithm on
the prostate data with step size ǫ= 0.01. Ifδj=∝an}⌊∇a⌋ketle{txj,r∝an}⌊∇a⌋ket∇i}ht(the least-squares
coeﬃcient of the residual on jth predictor), then this is exactly the usual
forward stagewise procedure (FS) outlined in Section 3.3.3 .
Here we are mainly interested in small values of ǫ. Lettingǫ→0 gives
the right panel of Figure 3.19, which in this case is identica l to the lasso
path in Figure 3.10. We call this limiting procedure inﬁnitesimal forward
stagewise regression or FS0. This procedure plays an important role in
non-linear, adaptive methods like boosting (Chapters 10 an d 16) and is the
version of incremental forward stagewise regression that i s most amenable
to theoretical analysis. B¨ uhlmann and Hothorn (2007) refe r to the same
procedure as “L2boost”, because of its connections to boost ing.

88 3. Linear Methods for Regression
Efron originally thought that the LAR Algorithm 3.2 was an im plemen-
tation of FS 0, allowing each tied predictor a chance to update their coeﬃ-
cients in a balanced way, while remaining tied in correlatio n. However, he
then realized that the LAR least-squares ﬁt amongst the tied predictors
can result in coeﬃcients moving in the opposite direction to their correla-
tion, which cannot happen in Algorithm 3.4. The following mo diﬁcation of
the LAR algorithm implements FS 0:
Algorithm 3.2b Least Angle Regression: FS 0Modiﬁcation .
4. Find the new direction by solving the constrained least sq uares prob-
lem
min
b||r−XAb||2
2subject tobjsj≥0, j∈A,
wheresjis the sign of∝an}⌊∇a⌋ketle{txj,r∝an}⌊∇a⌋ket∇i}ht.
The modiﬁcation amounts to a non-negative least squares ﬁt, keeping the
signs of the coeﬃcients the same as those of the correlations . One can show
that this achieves the optimal balancing of inﬁnitesimal “u pdate turns”
for the variables tied for maximal correlation (Hastie et al ., 2007). Like
lasso, the entire FS 0path can be computed very eﬃciently via the LAR
algorithm.
As a consequence of these results, if the LAR proﬁles are mono tone non-
increasing or non-decreasing, as they are in Figure 3.19, th en all three
methods—LAR, lasso, and FS 0—give identical proﬁles. If the proﬁles are
not monotone but do not cross the zero axis, then LAR and lasso are
identical.
Since FS 0is diﬀerent from the lasso, it is natural to ask if it optimize s
a criterion. The answer is more complex than for lasso; the FS 0coeﬃcient
proﬁle is the solution to a diﬀerential equation. While the l asso makes op-
timal progress in terms of reducing the residual sum-of-squ ares per unit
increase in L1-norm of the coeﬃcient vector β, FS0is optimal per unit
increase in L1arc-length traveled along the coeﬃcient path. Hence its co-
eﬃcient path is discouraged from changing directions too of ten.
FS0is more constrained than lasso, and in fact can be viewed as a m ono-
tone version of the lasso; see Figure 16.3 on page 614 for a dra matic exam-
ple. FS 0may be useful in p≫Nsituations, where its coeﬃcient proﬁles
are much smoother and hence have less variance than those of l asso. More
details on FS 0are given in Section 16.2.3 and Hastie et al. (2007). Fig-
ure 3.16 includes FS 0where its performance is very similar to that of the
lasso.

3.8 More on the Lasso and Related Path Algorithms 89
3.8.2 Piecewise-Linear Path Algorithms
The least angle regression procedure exploits the piecewis e linear nature of
the lasso solution paths. It has led to similar “path algorit hms” for other
regularized problems. Suppose we solve
ˆβ(λ) = argminβ[R(β)+λJ(β)], (3.76)
with
R(β) =N/summationdisplay
i=1L(yi,β0+p/summationdisplay
j=1xijβj), (3.77)
where both the loss function Land the penalty function Jare convex.
Then the following are suﬃcient conditions for the solution pathˆβ(λ) to
be piecewise linear (Rosset and Zhu, 2007):
1.Ris quadratic or piecewise-quadratic as a function of β, and
2.Jis piecewise linear in β.
This also implies (in principle) that the solution path can b e eﬃciently
computed. Examples include squared- and absolute-error lo ss, “Huberized”
losses, and the L1,L∞penalties on β. Another example is the “hinge loss”
function used in the support vector machine. There the loss i s piecewise
linear, and the penalty is quadratic. Interestingly, this l eads to a piecewise-
linear path algorithm in the dual space ; more details are given in Sec-
tion 12.3.5.
3.8.3 The Dantzig Selector
Candes and Tao (2007) proposed the following criterion:
minβ||β||1subject to||XT(y−Xβ)||∞≤s. (3.78)
They call the solution the Dantzig selector (DS). It can be written equiva-
lently as
minβ||XT(y−Xβ)||∞subject to||β||1≤t. (3.79)
Here||·||∞denotes the L∞norm, the maximum absolute value of the
components of the vector. In this form it resembles the lasso , replacing
squared error loss by the maximum absolute value of its gradi ent. Note
that astgets large, both procedures yield the least squares solutio n if
N <p. Ifp≥N, they both yield the least squares solution with minimum
L1norm. However for smaller values of t, the DS procedure produces a
diﬀerent path of solutions than the lasso.
Candes and Tao (2007) show that the solution to DS is a linear p ro-
gramming problem; hence the name Dantzig selector, in honor of the late

90 3. Linear Methods for Regression
George Dantzig, the inventor of the simplex method for linea r program-
ming. They also prove a number of interesting mathematical p roperties for
the method, related to its ability to recover an underlying s parse coeﬃ-
cient vector. These same properties also hold for the lasso, as shown later
by Bickel et al. (2008).
Unfortunately the operating properties of the DS method are somewhat
unsatisfactory. The method seems similar in spirit to the la sso, especially
when we look at the lasso’s stationary conditions (3.58). Li ke the LAR al-
gorithm, the lasso maintains the same inner product (and cor relation) with
the current residual for all variables in the active set, and moves their co-
eﬃcients to optimally decrease the residual sum of squares. In the process,
this common correlation is decreased monotonically (Exerc ise 3.23), and at
all times this correlation is larger than that for non-activ e variables. The
Dantzig selector instead tries to minimize the maximum inne r product of
the current residual with all the predictors. Hence it can ac hieve a smaller
maximum than the lasso, but in the process a curious phenomen on can
occur. If the size of the active set is m, there will be mvariables tied with
maximum correlation. However, these need not coincide with the active set!
Hence it can include a variable in the model that has smaller c orrelation
with the current residual than some of the excluded variable s (Efron et
al., 2007). This seems unreasonable and may be responsible f or its some-
times inferior prediction accuracy. Efron et al. (2007) als o show that DS
canyieldextremelyerraticcoeﬃcientpathsastheregulari zation parameter
sis varied.
3.8.4 The Grouped Lasso
In some problems, the predictors belong to pre-deﬁned group s; for example
genes that belong to the same biological pathway, or collect ions of indicator
(dummy) variables for representing the levels of a categori cal predictor. In
this situation it may be desirable to shrink and select the me mbers of a
group together. The grouped lasso is one way to achieve this. Suppose that
theppredictors are divided into Lgroups, with pℓthe number in group
ℓ. For ease of notation, we use a matrix Xℓto represent the predictors
corresponding to the ℓth group, with corresponding coeﬃcient vector βℓ.
The grouped-lasso minimizes the convex criterion
min
β∈IRp/parenleftigg
||y−β01−L/summationdisplay
ℓ=1Xℓβℓ||2
2+λL/summationdisplay
ℓ=1√pℓ||βℓ||2/parenrightigg
, (3.80)
where the√pℓterms accounts for the varying group sizes, and ||·||2is
the Euclidean norm (not squared). Since the Euclidean norm o f a vector
βℓis zero only if all of its components are zero, this procedure encourages
sparsity at both the group and individual levels. That is, fo r some values of
λ, an entire group of predictors may drop out of the model. This procedure

3.8 More on the Lasso and Related Path Algorithms 91
was proposed by Bakin (1999) and Lin and Zhang (2006), and stu died and
generalized by Yuan and Lin (2007). Generalizations includ e more general
L2norms||η||K= (ηTKη)1/2, as well as allowing overlapping groups of
predictors (Zhao et al., 2008). There are also connections t o methods for
ﬁtting sparse additive models (Lin and Zhang, 2006; Ravikum ar et al.,
2008).
3.8.5 Further Properties of the Lasso
A number of authors have studied the ability of the lasso and r elated pro-
cedures to recover the correct model, as Nandpgrow. Examples of this
work include Knight and Fu (2000), Greenshtein and Ritov (20 04), Tropp
(2004), Donoho (2006b), Meinshausen (2007), Meinshausen a nd B¨ uhlmann
(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006) , and Bunea
et al. (2007). For example Donoho (2006b) focuses on the p>Ncase and
considers the lasso solution as the bound tgets large. In the limit this gives
the solution with minimum L1norm among all models with zero training
error. He shows that under certain assumptions on the model m atrixX, if
the true model is sparse, this solution identiﬁes the correc t predictors with
high probability.
Many of the results in this area assume a condition on the mode l matrix
of the form
max
j∈Sc||xT
jXS(XSTXS)−1||1≤(1−ǫ) for someǫ∈(0,1].(3.81)
HereSindexes the subset of features with non-zero coeﬃcients in t he true
underlying model, and XSare the columns of Xcorresponding to those
features. Similarly Scare the features with true coeﬃcients equal to zero,
andXScthe corresponding columns. This says that the least squares coef-
ﬁcients for the columns of XSconXSare not too large, that is, the “good”
variablesSare not too highly correlated with the nuisance variables Sc.
Regarding the coeﬃcients themselves, the lasso shrinkage c auses the esti-
mates of the non-zero coeﬃcients to be biased towards zero, a nd in general
they are not consistent5. One approach for reducing this bias is to run
the lasso to identify the set of non-zero coeﬃcients, and the n ﬁt an un-
restricted linear model to the selected set of features. Thi s is not always
feasible, if the selected set is large. Alternatively, one c an use the lasso to
select the set of non-zero predictors, and then apply the las so again, but
using only the selected predictors from the ﬁrst step. This i s known as the
relaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to
estimate the initial penalty parameter for the lasso, and th en again for a
second penalty parameter applied to the selected set of pred ictors. Since
5Statistical consistency means as the sample size grows, the estimates converge to
the true values.

92 3. Linear Methods for Regression
the variables in the second step have less “competition” fro m noise vari-
ables, cross-validation will tend to pick a smaller value fo rλ, and hence
their coeﬃcients will be shrunken less than those in the init ial estimate.
Alternatively,onecanmodifythelassopenaltyfunctionso thatlargerco-
eﬃcients are shrunken less severely; the smoothly clipped absolute deviation
(SCAD) penalty of Fan and Li (2005) replaces λ|β|byJa(β,λ), where
dJa(β,λ)
dβ=λ·sign(β)/bracketleftig
I(|β|≤λ)+(aλ−|β|)+
(a−1)λI(|β|>λ)/bracketrightig
(3.82)
for somea≥2. The second term in square-braces reduces the amount of
shrinkage in the lasso for larger values of β, with ultimately no shrinkage
asa→∞. Figure 3.20 shows the SCAD penalty, along with the lasso and
−4 −2 0 2 40 1 2 3 4 5
−4 −2 0 2 40.0 0.5 1.0 1.5 2.0 2.5
−4 −2 0 2 40.5 1.0 1.5 2.0|β| SCAD |β|1−ν
β β β
FIGURE 3.20. The lasso and two alternative non-convex penalties designed to
penalize large coeﬃcients less. For SCAD we use λ= 1anda= 4, andν=1
2in
the last panel.
|β|1−ν. However this criterion is non-convex, which is a drawback s ince it
makes the computation much more diﬃcult. The adaptive lasso (Zou, 2006)
uses a weighted penalty of the form/summationtextp
j=1wj|βj|wherewj= 1/|ˆβj|ν,ˆβjis
the ordinary least squares estimate and ν >0. This is a practical approxi-
mation to the|β|qpenalties (q= 1−νhere) discussed in Section 3.4.3. The
adaptive lasso yields consistent estimates of the paramete rs while retaining
the attractive convexity property of the lasso.
3.8.6 Pathwise Coordinate Optimization
An alternate approach to the LARS algorithm for computing th e lasso
solution is simple coordinate descent. This idea was propos ed by Fu (1998)
andDaubechiesetal.(2004),andlaterstudiedandgenerali zedbyFriedman
etal.(2007),WuandLange(2008)andothers.Theideaistoﬁx thepenalty
parameterλin the Lagrangian form (3.52) and optimize successively ove r
each parameter, holding the other parameters ﬁxed at their c urrent values.
Suppose the predictors are all standardized to have mean zer o and unit
norm. Denote by ˜βk(λ) the current estimate for βkat penalty parameter

3.9 Computational Considerations 93
λ. We can rearrange (3.52) to isolate βj,
R(˜β(λ),βj) =1
2N/summationdisplay
i=1/parenleftigg
yi−/summationdisplay
k/ne}ationslash=jxik˜βk(λ)−xijβj/parenrightigg2
+λ/summationdisplay
k/ne}ationslash=j|˜βk(λ)|+λ|βj|,
(3.83)
where we have suppressed the intercept and introduced a fact or1
2for con-
venience. This can be viewed as a univariate lasso problem wi th response
variable the partial residual yi−˜y(j)
i=yi−/summationtext
k/ne}ationslash=jxik˜βk(λ). This has an
explicit solution, resulting in the update
˜βj(λ)←S/parenleftiggN/summationdisplay
i=1xij(yi−˜y(j)
i),λ/parenrightigg
. (3.84)
HereS(t,λ) = sign(t)(|t|−λ)+isthesoft-thresholdingoperatorinTable3.4
on page 71. The ﬁrst argument to S(·) is the simple least-squares coeﬃcient
of the partial residual on the standardized variable xij. Repeated iteration
of (3.84)—cycling through each variable in turn until conver gence—yields
the lasso estimate ˆβ(λ).
We can also use this simple algorithm to eﬃciently compute th e lasso
solutions at a grid of values of λ. We start with the smallest value λmax
for which ˆβ(λmax) = 0, decrease it a little and cycle through the variables
until convergence. Then λis decreased again and the process is repeated,
using the previous solution as a “warm start” for the new valu e ofλ. This
can be faster than the LARS algorithm, especially in large pr oblems. A
key to its speed is the fact that the quantities in (3.84) can b e updated
quickly asjvaries, and often the update is to leave ˜βj= 0. On the other
hand, it delivers solutions over a grid of λvalues, rather than the entire
solution path. The same kind of algorithm can be applied to th e elastic
net, the grouped lasso and many other models in which the pena lty is a
sum of functions of the individual parameters (Friedman et a l., 2010). It
can also be applied, with some substantial modiﬁcations, to the fused lasso
(Section 18.4.2); details are in Friedman et al. (2007).
3.9 Computational Considerations
Least squares ﬁtting is usually done via the Cholesky decomp osition of
the matrix XTXor a QR decomposition of X. WithNobservations and p
features, the Cholesky decomposition requires p3+Np2/2 operations, while
the QR decomposition requires Np2operations. Depending on the relative
size ofNandp, the Cholesky can sometimes be faster; on the other hand,
it can be less numerically stable (Lawson and Hansen, 1974). Computation
of the lasso via the LAR algorithm has the same order of comput ation as
a least squares ﬁt.

94 3. Linear Methods for Regression
Bibliographic Notes
Linear regression is discussed in many statistics books, fo r example, Seber
(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was
introduced by Hoerl and Kennard (1970), while the lasso was p roposed by
Tibshirani (1996). Around the same time, lasso-type penalt ies were pro-
posed in the basis pursuit method for signal processing (Chen et al., 1998).
The least angle regression procedure was proposed in Efron e t al. (2004);
related to this is the earlier homotopy procedure of Osborne et al. (2000a)
and Osborne et al. (2000b). Their algorithm also exploits th e piecewise
linearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The
criterion for the forward stagewise criterion is discussed in Hastie et al.
(2007). Park and Hastie (2007) develop a path algorithm simi lar to least
angle regression for generalized regression models. Parti al least squares
was introduced by Wold (1975). Comparisons of shrinkage met hods may
be found in Copas (1983) and Frank and Friedman (1993).
Exercises
Ex. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient
from a model is equal to the square of the corresponding z-score (3.12).
Ex. 3.2Given data on two variables XandY, consider ﬁtting a cubic
polynomial regression model f(X) =/summationtext3
j=0βjXj. In addition to plotting
the ﬁtted curve, you would like a 95% conﬁdence band about the curve.
Consider the following two approaches:
1. At each point x0, form a 95% conﬁdence interval for the linear func-
tionaTβ=/summationtext3
j=0βjxj
0.
2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates
conﬁdence intervals for f(x0).
How do these approaches diﬀer? Which band is likely to be wide r? Conduct
a small simulation experiment to compare the two methods.
Ex. 3.3Gauss–Markov theorem:
(a) Prove the Gauss–Markov theorem: the least squares estim ate of a
parameteraTβhas variance no bigger than that of any other linear
unbiased estimate of aTβ(Section 3.2.2).
(b) The matrix inequality B∝√∇e⌋e⌈esequalAholds ifA−Bis positive semideﬁnite.
Show that if ˆVis the variance-covariance matrix of the least squares
estimate of βand˜Vis the variance-covariance matrix of any other
linear unbiased estimate, then ˆV∝√∇e⌋e⌈esequal˜V.

Exercises 95
Ex. 3.4Show how the vector of least squares coeﬃcients can be obtain ed
from a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Rep-
resent your solution in terms of the QR decomposition of X.
Ex. 3.5Consider the ridge regression problem (3.41). Show that thi s prob-
lem is equivalent to the problem
ˆβc= argmin
βc/braceleftiggN/summationdisplay
i=1/bracketleftbig
yi−βc
0−p/summationdisplay
j=1(xij−¯xj)βc
j/bracketrightbig2+λp/summationdisplay
j=1βc
j2/bracerightigg
.(3.85)
Give the correspondence between βcand the original βin (3.41). Char-
acterize the solution to this modiﬁed criterion. Show that a similar result
holds for the lasso.
Ex. 3.6Show that the ridge regression estimate is the mean (and mode )
of the posterior distribution, under a Gaussian prior β∼N(0,τI), and
Gaussian sampling model y∼N(Xβ,σ2I). Find the relationship between
the regularization parameter λin the ridge formula, and the variances τ
andσ2.
Ex. 3.7Assumeyi∼N(β0+xT
iβ,σ2),i= 1,2,...,N, and the parameters
βj, j= 1,...,pare each distributed as N(0,τ2), independently of one
another. Assuming σ2andτ2are known, and β0is not governed by a
prior (or has a ﬂat improper prior), show that the (minus) log -posterior
density ofβis proportional to/summationtextN
i=1(yi−β0−/summationtext
jxijβj)2+λ/summationtextp
j=1β2
j
whereλ=σ2/τ2.
Ex. 3.8Consider the QR decomposition of the uncentered N×(p+ 1)
matrixX(whose ﬁrst column is all ones), and the SVD of the N×p
centered matrix ˜X. Show that Q2andUspan the same subspace, where
Q2is the sub-matrix of Qwith the ﬁrst column removed. Under what
circumstances will they be the same, up to sign ﬂips?
Ex. 3.9Forward stepwise regression. Suppose we have the QR decomposi-
tion for the N×qmatrixX1in a multiple regression problem with response
y, and we have an additional p−qpredictors in the matrix X2. Denote the
current residual by r. We wish to establish which one of these additional
variables will reduce the residual-sum-of squares the most when included
with those in X1. Describe an eﬃcient procedure for doing this.
Ex. 3.10 Backward stepwise regression. Suppose we have the multiple re-
gression ﬁt of yonX, along with the standard errors and Z-scores as in
Table 3.2. We wish to establish which variable, when dropped , will increase
the residual sum-of-squares the least. How would you do this ?
Ex. 3.11 Show that the solution to the multivariate linear regressio n prob-
lem (3.40) is given by (3.39). What happens if the covariance matrices Σi
are diﬀerent for each observation?

96 3. Linear Methods for Regression
Ex. 3.12 Show that the ridge regression estimates can be obtained by
ordinary least squares regression on an augmented data set. We augment
the centered matrix Xwithpadditional rows√
λI, and augment ywithp
zeros. By introducing artiﬁcial data having response value zero, the ﬁtting
procedure is forced to shrink the coeﬃcients toward zero. Th is is related to
the idea of hintsdue to Abu-Mostafa (1995), where model constraints are
implemented by adding artiﬁcial data examples that satisfy them.
Ex. 3.13 Derive the expression (3.62), and show that ˆβpcr(p) =ˆβls.
Ex. 3.14 Show that in the orthogonal case, PLS stops after m= 1 steps,
because subsequent ˆ ϕmjin step 2 in Algorithm 3.3 are zero.
Ex. 3.15 Verify expression (3.64), and hence show that the partial le ast
squares directions are a compromise between the ordinary re gression coef-
ﬁcient and the principal component directions.
Ex. 3.16 Derive the entries in Table 3.4, the explicit forms for estim ators
in the orthogonal case.
Ex. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in
Chapter 1.
Ex.3.18 Readaboutconjugategradientalgorithms(Murrayetal.,19 81,for
example), and establish a connection between these algorit hms and partial
least squares.
Ex. 3.19 Show that∝⌊a∇⌈⌊lˆβridge∝⌊a∇⌈⌊lincreases as its tuning parameter λ→0. Does
the same property hold for the lasso and partial least square s estimates?
For the latter, consider the “tuning parameter” to be the suc cessive steps
in the algorithm.
Ex. 3.20 Consider the canonical-correlation problem (3.67). Show t hat the
leading pair of canonical variates u1andv1solve the problem
max
uT(YTY)u=1
vT(XTX)v=1uT(YTX)v, (3.86)
a generalized SVD problem. Show that the solution is given by u1=
(YTY)−1
2u∗
1, andv1= (XTX)−1
2v∗
1, whereu∗
1andv∗
1are the leading left
and right singular vectors in
(YTY)−1
2(YTX)(XTX)−1
2=U∗D∗V∗T. (3.87)
Show that the entire sequence um, vm, m= 1,...,min(K,p) is also given
by (3.87).
Ex. 3.21 Show that the solution to the reduced-rank regression probl em
(3.68), with Σestimated by YTY/N, is given by (3.69). Hint:Transform

Exercises 97
YtoY∗=YΣ−1
2, and solved in terms of the canonical vectors u∗
m. Show
thatUm=Σ−1
2U∗
m, and a generalized inverse is U−
m=U∗
mTΣ1
2.
Ex. 3.22 Show that the solution in Exercise 3.21 does not change if Σis
estimated by the more natural quantity ( Y−XˆB)T(Y−XˆB)/(N−pK).
Ex. 3.23 Consider a regression problem with all variables and respon se hav-
ing mean zero and standard deviation one. Suppose also that e ach variable
has identical absolute correlation with the response:
1
N|∝an}⌊∇a⌋ketle{txj,y∝an}⌊∇a⌋ket∇i}ht|=λ, j= 1,...,p.
Letˆβbe the least-squares coeﬃcient of yonX, and let u(α) =αXˆβfor
α∈[0,1] be the vector that moves a fraction αtoward the least squares ﬁt
u. LetRSSbe the residual sum-of-squares from the full least squares ﬁ t.
(a) Show that
1
N|∝an}⌊∇a⌋ketle{txj,y−u(α)∝an}⌊∇a⌋ket∇i}ht|= (1−α)λ, j= 1,...,p,
and hence the correlations of each xjwith the residuals remain equal
in magnitude as we progress toward u.
(b) Show that these correlations are all equal to
λ(α) =(1−α)/radicalig
(1−α)2+α(2−α)
N·RSS·λ,
and hence they decrease monotonically to zero.
(c) Use these results to show that the LAR algorithm in Sectio n 3.4.4
keeps the correlations tied and monotonically decreasing, as claimed
in (3.55).
Ex. 3.24 LAR directions. Using the notation around equation (3.55) on
page 74, show that the LAR direction makes an equal angle with each of
the predictors in Ak.
Ex. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2). Starting at the be-
ginning of the kth step of the LAR algorithm, derive expressions to identify
the next variable to enter the active set at step k+1, and the value of αat
which this occurs (using the notation around equation (3.55 ) on page 74).
Ex. 3.26 Forward stepwise regression enters the variable at each ste p that
most reduces the residual sum-of-squares. LAR adjusts vari ables that have
the most (absolute) correlation with the current residuals . Show that these
two entry criteria are not necessarily the same. [Hint: let xj.Abe thejth

98 3. Linear Methods for Regression
variable, linearly adjusted for all the variables currentl y in the model. Show
that the ﬁrst criterion amounts to identifying the jfor which Cor( xj.A,r)
is largest in magnitude.
Ex. 3.27 Lasso and LAR : Consider thelassoprobleminLagrange multiplier
form: with L(β) =1
2/summationtext
i(yi−/summationtext
jxijβj)2, we minimize
L(β)+λ/summationdisplay
j|βj| (3.88)
for ﬁxedλ>0.
(a) Setting βj=β+
j−β−
jwithβ+
j,β−
j≥0, expression (3.88) becomes
L(β)+λ/summationtext
j(β+
j+β−
j). Show that the Lagrange dual function is
L(β)+λ/summationdisplay
j(β+
j+β−
j)−/summationdisplay
jλ+
jβ+
j−/summationdisplay
jλ−
jβ−
j(3.89)
and the Karush–Kuhn–Tucker optimality conditions are
∇L(β)j+λ−λ+
j= 0
−∇L(β)j+λ−λ−
j= 0
λ+
jβ+
j= 0
λ−
jβ−
j= 0,
along with the non-negativity constraints on the parameter s and all
the Lagrange multipliers.
(b) Show that|∇L(β)j|≤λ∀j,and that the KKT conditions imply one
of the following three scenarios:
λ= 0⇒ ∇L(β)j= 0∀j
β+
j>0, λ>0⇒λ+
j= 0,∇L(β)j=−λ<0, β−
j= 0
β−
j>0, λ>0⇒λ−
j= 0,∇L(β)j=λ>0, β+
j= 0.
Hence show that for any “active” predictor having βj∝ne}ationslash= 0, we must
have∇L(β)j=−λifβj>0, and∇L(β)j=λifβj<0. Assuming
the predictors are standardized, relate λto the correlation between
thejth predictor and the current residuals.
(c) Suppose that the set of active predictors is unchanged fo rλ0≥λ≥λ1.
Show that there is a vector γ0such that
ˆβ(λ) =ˆβ(λ0)−(λ−λ0)γ0 (3.90)
Thus the lasso solution path is linear as λranges from λ0toλ1(Efron
et al., 2004; Rosset and Zhu, 2007).

Exercises 99
Ex. 3.28 Suppose for a given tin (3.51), the ﬁtted lasso coeﬃcient for
variableXjisˆβj=a. Suppose we augment our set of variables with an
identical copy X∗
j=Xj. Characterize the eﬀect of this exact collinearity
by describing the set of solutions for ˆβjandˆβ∗
j, using the same value of t.
Ex. 3.29 Suppose we run a ridge regression with parameter λon a single
variableX, and get coeﬃcient a. We now include an exact copy X∗=X,
and reﬁt our ridge regression. Show that both coeﬃcients are identical, and
derive their value. Show in general that if mcopies of a variable Xjare
included in a ridge regression, their coeﬃcients are all the same.
Ex. 3.30 Consider the elastic-net optimization problem:
min
β||y−Xβ||2+λ/bracketleftbig
α||β||2
2+(1−α)||β||1/bracketrightbig
. (3.91)
Show how one can turn this into a lasso problem, using an augme nted
version of Xandy.

100 3. Linear Methods for Regression

This is page 101
Printer: Opaque this
4
Linear Methods for Classiﬁcation
4.1 Introduction
In this chapter we revisit the classiﬁcation problem and foc us on linear
methods for classiﬁcation. Since our predictor G(x) takes values in a dis-
crete setG, we can always divide the input space into a collection of reg ions
labeledaccordingtotheclassiﬁcation.WesawinChapter2t hatthebound-
aries of these regions can be rough or smooth, depending on th e prediction
function. For an important class of procedures, these decision boundaries
are linear; this is what we will mean by linear methods for cla ssiﬁcation.
There are several diﬀerent ways in which linear decision bou ndaries can
be found. In Chapter 2 we ﬁt linear regression models to the cl ass indicator
variables, and classify to the largest ﬁt. Suppose there are Kclasses, for
convenience labeled 1 ,2,...,K, and the ﬁtted linear model for the kth
indicator response variable is ˆfk(x) =ˆβk0+ˆβT
kx. The decision boundary
between class kandℓis that set of points for which ˆfk(x) =ˆfℓ(x), that is,
the set{x: (ˆβk0−ˆβℓ0)+(ˆβk−ˆβℓ)Tx= 0}, an aﬃne set or hyperplane.1
Since the same is true for any pair of classes, the input space is divided
into regions of constant classiﬁcation, with piecewise hyp erplanar decision
boundaries. This regression approach is a member of a class o f methods
that model discriminant functions δk(x) for each class, and then classify x
to the class with the largest value for its discriminant func tion. Methods
1Strictly speaking, a hyperplane passes through the origin, while an aﬃne set need
not. We sometimes ignore the distinction and refer in general to hyperpl anes.

102 4. Linear Methods for Classiﬁcation
that model the posterior probabilities Pr( G=k|X=x) are also in this
class. Clearly, if either the δk(x) or Pr(G=k|X=x) are linear in x, then
the decision boundaries will be linear.
Actually, all we require is that some monotone transformati on ofδkor
Pr(G=k|X=x) be linear for the decision boundaries to be linear. For
example, if there are two classes, a popular model for the pos terior proba-
bilities is
Pr(G= 1|X=x) =exp(β0+βTx)
1+exp(β0+βTx),
Pr(G= 2|X=x) =1
1+exp(β0+βTx).(4.1)
Herethemonotonetransformationisthe logittransformation:log[ p/(1−p)],
and in fact we see that
logPr(G= 1|X=x)
Pr(G= 2|X=x)=β0+βTx. (4.2)
The decision boundary is the set of points for which the log-odds are zero,
and this is a hyperplane deﬁned by/braceleftbig
x|β0+βTx= 0/bracerightbig
. We discuss two very
popular but diﬀerent methods that result in linear log-odds or logits: linear
discriminant analysis and linear logistic regression. Alt hough they diﬀer in
their derivation, the essential diﬀerence between them is i n the way the
linear function is ﬁt to the training data.
A more direct approach is to explicitly model the boundaries between
the classes as linear. For a two-class problem in a p-dimensional input
space, this amounts to modeling the decision boundary as a hy perplane—in
other words, a normal vector and a cut-point. We will look at t wo methods
that explicitly look for “separating hyperplanes.” The ﬁrs t is the well-
knownperceptron model of Rosenblatt (1958), with an algorithm that ﬁnds
a separating hyperplane in the training data, if one exists. The second
method, due to Vapnik (1996), ﬁnds an optimally separating hyperplane if
one exists, else ﬁnds a hyperplane that minimizes some measu re of overlap
in the training data. We treat the separable case here, and de fer treatment
of the nonseparable case to Chapter 12.
Whilethisentirechapterisdevotedtolineardecisionboun daries,thereis
considerable scope for generalization. For example, we can expand our vari-
ablesetX1,...,X pbyincludingtheirsquaresandcross-products X2
1,X2
2,...,
X1X2,..., thereby adding p(p+1)/2 additional variables. Linear functions
in the augmented space map down to quadratic functions in the original
space—hence linear decision boundaries to quadratic decisi on boundaries.
Figure 4.1 illustrates the idea. The data are the same: the le ft plot uses
linear decision boundaries in the two-dimensional space sh own, while the
rightplotuseslineardecisionboundariesintheaugmented ﬁve-dimensional
space described above. This approach can be used with any bas is transfor-

4.2 Linear Regression of an Indicator Matrix 103
1
11
11
111
1
111
1
11
11
1111 1
111
1
11
111
1111
11
1
11
111
11
1111
1 1
111
11
1
1
1 1
11
1111
11111
1
11
111
111
1111
1
111
11
11
11
1
11
11
111
11
1
1
11
111
111
11
11
11
11 1
11
11 111
1 11
1
11
1
11
1
11
1
11 1
1
11
11
1111
1111
1
111
11
111
1
111
111
1111
11
1 11
111
11
1
11
11
11
111
12
22
22
222
2
2
22
2 2
2
22
22
22
2222
2
22
222
2
22
2 2
22
222
2
222
22222
22
2
222
22
2
22
22
22
22
22
2222
22
22
222
222
22222
22
222
22
22
22
2
22
2222
22
2
22
2
222
2
222
22
222
222
22
2
22
22
2222 2
222
22
2
22
22
222
222
2
22
22
22222
22
222
22
2
222
22
22
222
222
22
2
22
22
22222
22
3
33
33
33
3
3333
33
3
33
3
333
33
33
333
333
3
3
33
333
333
333
333
3333
333
3333
3
33333
33
3
33
3
33
3333
333
3
333
333
33
333
3
3333
33
333
33
3
3333
333
33333
3
3
3
333
33
3
33
3
33
33
333
33
3
3333
33
333
3
333
33333
33
333
33
33
3333
333
33
33
3
33
33
3
33
33
3
333
3
333
33
33
33
1
11
11
111
1
111
1
11
11
1111 1
111
1
11
111
1111
11
1
11
111
11
1111
1 1
111
11
1
1
1 1
11
1111
11111
1
11
111
111
1111
1
111
11
11
11
1
11
11
111
11
1
1
11
111
111
11
11
11
11 1
11
11 111
1 11
1
11
1
11
1
11
1
11 1
1
11
11
1111
1111
1
111
11
111
1
111
111
1111
11
1 11
111
11
1
11
11
11
111
12
22
22
222
2
2
22
2 2
2
22
22
22
2222
2
22
222
2
22
2 2
22
222
2
222
22222
22
2
222
22
2
22
22
22
22
22
2222
22
22
222
222
22222
22
222
22
22
22
2
22
2222
22
2
22
2
222
2
222
22
222
222
22
2
22
22
2222 2
222
22
2
22
22
222
222
2
22
22
22222
22
222
22
2
222
22
22
222
222
22
2
22
22
22222
22
3
33
33
33
3
3333
33
3
33
3
333
33
33
333
333
3
3
33
333
333
333
333
3333
333
3333
3
33333
33
3
33
3
33
3333
333
3
333
333
33
333
3
3333
33
333
33
3
3333
333
33333
3
3
3
333
33
3
33
3
33
33
333
33
3
3333
33
333
3
333
33333
33
333
33
33
3333
333
33
33
3
33
33
3
33
33
3
333
3
333
33
33
33
FIGURE 4.1. The left plot shows some data from three classes, with linear
decision boundaries found by linear discriminant analysis. T he right plot shows
quadratic decision boundaries. These were obtained by ﬁndi ng linear boundaries
in the ﬁve-dimensional space X1,X2,X1X2,X2
1,X2
2. Linear inequalities in this
space are quadratic inequalities in the original space.
mationh(X) whereh: IRp∝ma√sto→IRqwithq>p, and will be explored in later
chapters.
4.2 Linear Regression of an Indicator Matrix
Here each of the response categories are coded via an indicat or variable.
Thus ifGhasKclasses, there will be Ksuch indicators Yk, k= 1,...,K,
withYk= 1 ifG=kelse 0. These are collected together in a vector
Y= (Y1,...,Y K), and theNtraining instances of these form an N×K
indicator response matrix Y.Yis a matrix of 0’s and 1’s, with each row
having a single 1. We ﬁt a linear regression model to each of th e columns
ofYsimultaneously, and the ﬁt is given by
ˆY=X(XTX)−1XTY. (4.3)
Chapter 3 has more details on linear regression. Note that we have a coeﬃ-
cient vector for each response column yk, and hence a ( p+1)×Kcoeﬃcient
matrixˆB= (XTX)−1XTY. HereXis the model matrix with p+1 columns
corresponding to the pinputs, and a leading column of 1’s for the intercept.
A new observation with input xis classiﬁed as follows:
•compute the ﬁtted output ˆf(x)T= (1,xT)ˆB, aKvector;
•identify the largest component and classify accordingly:
ˆG(x) = argmaxk∈Gˆfk(x). (4.4)

104 4. Linear Methods for Classiﬁcation
What is the rationale for this approach? One rather formal ju stiﬁcation
is to view the regression as an estimate of conditional expec tation. For the
random variable Yk,E(Yk|X=x) = Pr(G=k|X=x), so conditional
expectation of each of the Ykseems a sensible goal. The real issue is: how
good an approximation to conditional expectation is the rat her rigid linear
regression model? Alternatively, are the ˆfk(x) reasonable estimates of the
posterior probabilities Pr( G=k|X=x), and more importantly, does this
matter?
It is quite straightforward to verify that/summationtext
k∈Gˆfk(x) = 1 for any x, as
long as there is an intercept in the model (column of 1’s in X). However,
theˆfk(x) can be negative or greater than 1, and typically some are. Th is
is a consequence of the rigid nature of linear regression, es pecially if we
make predictions outside the hull of the training data. Thes e violations in
themselves do not guarantee that this approach will not work , and in fact
on many problems it gives similar results to more standard li near meth-
ods for classiﬁcation. If we allow linear regression onto ba sis expansions
h(X) of the inputs, this approach can lead to consistent estimat es of the
probabilities. As the size of the training set Ngrows bigger, we adaptively
include more basis elements so that linear regression onto t hese basis func-
tions approaches conditional expectation. We discuss such approaches in
Chapter 5.
A more simplistic viewpoint is to construct targetstkfor each class,
wheretkis thekth column of the K×Kidentity matrix. Our prediction
problem is to try and reproduce the appropriate target for an observation.
With the same coding as before, the response vector yi(ith row of Y) for
observation ihas the value yi=tkifgi=k. We might then ﬁt the linear
model by least squares:
min
BN/summationdisplay
i=1||yi−[(1,xT
i)B]T||2. (4.5)
The criterion is a sum-of-squared Euclidean distances of th e ﬁtted vectors
from their targets. A new observation is classiﬁed by comput ing its ﬁtted
vectorˆf(x) and classifying to the closest target:
ˆG(x) = argmin
k||ˆf(x)−tk||2. (4.6)
This is exactly the same as the previous approach:
•The sum-of-squared-norm criterion is exactly the criterio n for multi-
ple response linear regression, just viewed slightly diﬀer ently. Since
a squared norm is itself a sum of squares, the components deco uple
and can be rearranged as a separate linear model for each elem ent.
Note that this is only possible because there is nothing in th e model
that binds the diﬀerent responses together.

4.2 Linear Regression of an Indicator Matrix 105
Linear Regression
1
11
1
1
11111
11
1
11111
111
11 111
111
1111
111
1
11111
111
11
1111
11
11
11111
11
11
11
1
11
11
1
11
11
1
11
1
1
11
111
111111
11
111
11 1
111
1
1
11
1
11
11
11
111
111
11
11
1 1
1
11
1111
11
1
111
11111
1
1111
11
111
111
111
1
11
1111
11
1
111 11
11
11
11
11
1
1
11
11
1
111
1
11
11
1
111
111 111
11
11
11
111111
1
11
11
11
1111
1111
111
1
11 1
111
1111
11
111
111
11
1
11
111
111
111
11
11
11
111
11 1
111
11
11
11
1
11
11
1
11
11
1
111
1
11111
11111
1
11
111
1
11
111 1
11
1
11 11
1
111
111
11
1
111
1
1
1111
1111
11
11
1111
11
1
11
11
11
1 1
111
1 11
1
11
11
11 1
11
1
1111111
1
11
11
1
11
111
11
111
1111
11
11
11
1 1
11
11111
1
11
11
111
11
11
11
1
1
11
11
111
111
11
111
11
111
1
11
1
11
11
1
111
1111
11
1
11
1
11
1 12
22
2
222
22
222
2
22
222
222
222
2222
2
222
22
2
2
222
2
222
2
222
2222
2
22
22
22
22222
2222
2
222
222
22
2
22
22
222
2
2222222
22
22
222
222
222
22
2
222
22
2
22
22
2 2
22
2222
2
2
2
22
2222
22
222
2
22
222
222
22
222
2
2
222
222
2
22
2222
222
2222
22
222
2
2222
222
22
2
222
2222
222
22
2
222
22
2
22222
2222
22
222
22
22
22
2222
2222
2
22
22
22
222
22222
22
22
2
22
2
2
22
2
22
22
2
22
22
22
22
2
222
22
2222
2222
222
222
222
2222
2
222
2
22
222
222
22
222 22
222
222
22222
22 222
222
2
22
22 2
2
2222
2
22
22
2
22
22
22
2
22 22
2
222
2222
222
2
2 2222 2
22
22
2222
22
222
2
22
22
2
22
22
222
22
22
2222
22
2222
2222
2
222
22 2
22
2
22
22222
22
22
22
222
2
222
22
2
2 22 222
22
22
22 2
22 22
222
22
22
22 22
22
223
33
33
33
3
3
333
3
33
3
333
33
3
33
333
33
333
3
333
3333
3
33
33
333
33
33
33
3
33
33
33333
3
33
33
3
33
33
3
33
3
33
3
333
33
3333
33
33
3
33
33
3333
33
3333
3
33
3
33
33
3
33
33
3
333
333
33
33
3333
3
33
33
333
3
3
33
33
3
33333
33
333
333
3333
33
33
333
3
33
33
33
333
3
3
3333
333
333
3
333
3
3
33333333333
33
333
3
33
333
33
33
3
3
333
33
3
33
3
333
3
3333
333
33
33
33
333
33
3
33
33
33
33
33333
33
33
33
33
3 33
333
3
33 33
3
33
333
333333
3
3333
33
33
3333
33
33
33
333
333
3
3333
3
33
3
3
33333
333
3333
3
33
333
3
333
3
333
33
3
33
33
3333
333
33
33
33 3
3
3333
333
3
3
3
333
33
3
333
33
3
3333
33
3
33
3333
333
33
333
33
3
3
33
333
3
3
3333333
3333
33
3
3
33
3
3333
3
33
33 3
33
3
3333
333
33
3
3
333
33
3
3333
3
33Linear Discriminant Analysis
1
11
1
1
11111
11
1
11111
111
11 111
111
1111
111
1
11111
111
11
1111
11
11
11111
11
11
11
1
11
11
1
11
11
1
11
1
1
11
111
111111
11
111
11 1
111
1
1
11
1
11
11
11
111
111
11
11
1 1
1
11
1111
11
1
111
11111
1
1111
11
111
111
111
1
11
1111
11
1
111 11
11
11
11
11
1
1
11
11
1
111
1
11
11
1
111
111 111
11
11
11
111111
1
11
11
11
1111
1111
111
1
11 1
111
1111
11
111
111
11
1
11
111
111
111
11
11
11
111
11 1
111
11
11
11
1
11
11
1
11
11
1
111
1
11111
11111
1
11
111
1
11
111 1
11
1
11 11
1
111
111
11
1
111
1
1
1111
1111
11
11
1111
11
1
11
11
11
1 1
111
1 11
1
11
11
11 1
11
1
1111111
1
11
11
1
11
111
11
111
1111
11
11
11
1 1
11
11111
1
11
11
111
11
11
11
1
1
11
11
111
111
11
111
11
111
1
11
1
11
11
1
111
1111
11
1
11
1
11
1 12
22
2
222
22
222
2
22
222
222
222
2222
2
222
22
2
2
222
2
222
2
222
2222
2
22
22
22
22222
2222
2
222
222
22
2
22
22
222
2
2222222
22
22
222
222
222
22
2
222
22
2
22
22
2 2
22
2222
2
2
2
22
2222
22
222
2
22
222
222
22
222
2
2
222
222
2
22
2222
222
2222
22
222
2
2222
222
22
2
222
2222
222
22
2
222
22
2
22222
2222
22
222
22
22
22
2222
2222
2
22
22
22
222
22222
22
22
2
22
2
2
22
2
22
22
2
22
22
22
22
2
222
22
2222
2222
222
222
222
2222
2
222
2
22
222
222
22
222 22
222
222
22222
22 222
222
2
22
22 2
2
2222
2
22
22
2
22
22
22
2
22 22
2
222
2222
222
2
2 2222 2
22
22
2222
22
222
2
22
22
2
22
22
222
22
22
2222
22
2222
2222
2
222
22 2
22
2
22
22222
22
22
22
222
2
222
22
2
2 22 222
22
22
22 2
22 22
222
22
22
22 22
22
223
33
33
33
3
3
333
3
33
3
333
33
3
33
333
33
333
3
333
3333
3
33
33
333
33
33
33
3
33
33
33333
3
33
33
3
33
33
3
33
3
33
3
333
33
3333
33
33
3
33
33
3333
33
3333
3
33
3
33
33
3
33
33
3
333
333
33
33
3333
3
33
33
333
3
3
33
33
3
33333
33
333
333
3333
33
33
333
3
33
33
33
333
3
3
3333
333
333
3
333
3
3
33333333333
33
333
3
33
333
33
33
3
3
333
33
3
33
3
333
3
3333
333
33
33
33
333
33
3
33
33
33
33
33333
33
33
33
33
3 33
333
3
33 33
3
33
333
333333
3
3333
33
33
3333
33
33
33
333
333
3
3333
3
33
3
3
33333
333
3333
3
33
333
3
333
3
333
33
3
33
33
3333
333
33
33
33 3
3
3333
333
3
3
3
333
33
3
333
33
3
3333
33
3
33
3333
333
33
333
33
3
3
33
333
3
3
3333333
3333
33
3
3
33
3
3333
3
33
33 3
33
3
3333
333
33
3
3
333
33
3
3333
3
33
X1 X1
X2X2
FIGURE 4.2. The data come from three classes in IR2and are easily separated
by linear decision boundaries. The right plot shows the bounda ries found by linear
discriminant analysis. The left plot shows the boundaries foun d by linear regres-
sion of the indicator response variables. The middle class is co mpletely masked
(never dominates).
•The closest target classiﬁcation rule (4.6) is easily seen t o be exactly
the same as the maximum ﬁtted component criterion (4.4).
Thereisaseriousproblemwiththeregressionapproachwhen thenumber
of classesK≥3, especially prevalent when Kis large. Because of the rigid
nature of the regression model, classes can be maskedby others. Figure 4.2
illustrates anextremesituation when K= 3.Thethreeclasses areperfectly
separated by linear decision boundaries, yet linear regres sion misses the
middle class completely.
In Figure 4.3 we have projected the data onto the line joining the three
centroids (there is no information in the orthogonal direct ion in this case),
and we have included and coded the three response variables Y1,Y2and
Y3. The three regression lines (left panel) are included, and w e see that
the line corresponding to the middle class is horizontal and its ﬁtted values
are never dominant! Thus, observations from class 2 are clas siﬁed either
as class 1 or class 3. The right panel uses quadratic regressi on rather than
linear regression. For this simple example a quadratic rath er than linear
ﬁt (for the middle class at least) would solve the problem. Ho wever, it
can be seen that if there were four rather than three classes l ined up like
this, a quadratic would not come down fast enough, and a cubic would
be needed as well. A loose but general rule is that if K≥3 classes are
lined up, polynomial terms up to degree K−1 might be needed to resolve
them. Note also that these are polynomials along the derived direction
passing through the centroids, which can have arbitrary ori entation. So in

106 4. Linear Methods for Classiﬁcation
111
111
1
1
1111
11
11
1
11
11111
1
11111
11
1
11
11
11
11111
111
111
1
1111
111
111
111
111111
11
1
11
111
1111
11
111
111
111
111
111
1
11
11
111
111
11
11
1111
11
1
111
1
11
1
11
1
11
11
111
1
1111
111
111
12222 2222222 2 2 222 2 2 22 2222 222222 2 2 2222 22 22222 2222222222222 22 22222 222 2222222 2 22 222 2 22222 22 22 222 222222 22222 222 22 2 222222222 22222222 222 222 2 22222222222222 22
3
3
33
3
3333
33
33
33
333
333
3
3
33333
3
33
333
33
333333
333333
3
3333
3
33
3
33
3
33
333
33
33
33333
33
3
3333
33
333
33
3333
33333
3333
33333
33
3
33
33
33
33
33
333
3
333
333
333
3333
3
3333
333
3
33333
0.00.51.0
0.0 0.2 0.4 0.6 0.8 1.0111
111
1
1
1111
11
11
1
11
11111
1
11
111
11
1
11
11
11
11111
11
1
111
1
1111
111
111
111
111111
111
11111
111111
111
111
111
111
111
1
1111111 111
111111111111
1 111 1111111 11111111 111111112
2
22
2
2222
22
22
22
222
222
2
2
22222
2
22
222
22
222222
222222
2
22222
2
22 2
222222222 222222
22
22222222 222222222 22 22
22 22
22
22
222
222
22
22
2222
22
2
222
2
22
2
22
2
22
22
222
2
222
222
222
233 3333
333333
3333
33333333
333333
33
33333
3 333333333333 3333
33
3
33
3
33
3333 333
33333
33
3
333333333
33
3333
33333
3333
33333
33
3
33
33
33
33
33
333
3
333
333
333
3333
3
333
3
33
3
33333
0.00.51.0
0.0 0.2 0.4 0.6 0.8 1.0Degree = 1; Error = 0.33 Degree = 2; Error = 0.04
FIGURE 4.3. The eﬀects of masking on linear regression in IRfor a three-class
problem. The rug plot at the base indicates the positions and class membership of
each observation. The three curves in each panel are the ﬁtte d regressions to the
three-class indicator variables; for example, for the blue class ,yblueis1for the
blue observations, and 0for the green and orange. The ﬁts are linear and quadratic
polynomials. Above each plot is the training error rate. The Bay es error rate is
0.025for this problem, as is the LDA error rate.
p-dimensional input space, one would need general polynomia l terms and
cross-products of total degree K−1,O(pK−1) terms in all, to resolve such
worst-case scenarios.
The example is extreme, but for large Kand smallpsuch maskings
naturally occur. As a more realistic illustration, Figure 4 .4 is a projection
of the training data for a vowel recognition problem onto an i nformative
two-dimensional subspace. There are K= 11 classes in p= 10 dimensions.
This is a diﬃcult classiﬁcation problem, and the best method s achieve
around 40% errors on the test data. The main point here is summ arized in
Table 4.1; linear regression has an error rate of 67%, while a close relative,
lineardiscriminantanalysis,hasanerrorrateof56%.Itse emsthatmasking
has hurt in this case. While all the other methods in this chap ter are based
on linear functions of xas well, they use them in such a way that avoids
this masking problem.
4.3 Linear Discriminant Analysis
Decision theory for classiﬁcation (Section 2.4) tells us th at we need to know
the class posteriors Pr( G|X) for optimal classiﬁcation. Suppose fk(x) is
the class-conditional density of Xin classG=k, and letπkbe the prior
probability of class k, with/summationtextK
k=1πk= 1. A simple application of Bayes

4.3 Linear Discriminant Analysis 107
Coordinate 1 for Training DataCoordinate 2 for Training Data
-4 -2 0 2 4-6 -4 -2 0 2 4oooo oo
ooooooo
o
oooo
oo
o
o
o
o
oooooooooooo
o
o
o
o
o ooooooo
ooooooo
o
oo
o
ooooo
o
ooooooo
oo
oooo
o
o
ooooooooooooo
o
oooooo
o
ooo
oooo
oooo
o
ooooo
oooooo o
o
o
oooooooooo
o
oo
o
oo
ooooooooooooo
oooooooooooooooooo
oooooooooooo
o
o
ooooooo
o
o
ooooooo
ooooo
ooooooo
o
o
oooooooooooooooo
o
o
o
o
oooooooo
oooo
oo
oooooooooo
o
ooo
o
o
o
oooo
o
o
ooooo
o
o o
o
o
o
o
ooooooooo oooo
o
o
oooooo
o
oooooooo
o
oo
o
oooooooo
o
o
o
o
ooo
ooooooooooooooo
ooo
oooooooo
o
ooo
ooooooooooooo
o
oooo
ooooo
ooooo
o
oo
o
o
o
o
ooo
ooo
o
ooooo
o
oooo
o
ooooooooooooo
o ooooooooooooooooo
o
o
ooooo
ooooooooooo
o
o
ooo
ooo
oo oo
oooooo
oooooo
ooooooooo
o
o
ooooooo
oooooooooooo
oo
o
o
oo
••••••••••••••
••
••
••••Linear Discriminant Analysis
FIGURE 4.4. A two-dimensional plot of the vowel training data. There are
eleven classes with X∈IR10, and this is the best view in terms of a LDA model
(Section 4.3.3). The heavy circles are the projected mean vec tors for each class.
The class overlap is considerable.
TABLE 4.1. Training and test error rates using a variety of linear techni ques
on the vowel data. There are eleven classes in ten dimensions, o f which three
account for 90%of the variance (via a principal components analysis). We see
that linear regression is hurt by masking, increasing the tes t and training error
by over10%.
Technique Error Rates
Training Test
Linear regression 0.48 0.67
Linear discriminant analysis 0.32 0.56
Quadratic discriminant analysis 0.01 0.53
Logistic regression 0.22 0.51

108 4. Linear Methods for Classiﬁcation
theorem gives us
Pr(G=k|X=x) =fk(x)πk/summationtextK
ℓ=1fℓ(x)πℓ. (4.7)
We see that in terms of ability to classify, having the fk(x) is almost equiv-
alent to having the quantity Pr( G=k|X=x).
Many techniques are based on models for the class densities:
•linear and quadratic discriminant analysis use Gaussian de nsities;
•moreﬂexiblemixturesofGaussiansallowfornonlineardeci sionbound-
aries (Section 6.8);
•general nonparametric density estimates for each class den sity allow
the most ﬂexibility (Section 6.6.2);
•Naive Bayes models are a variant of the previous case, and assume
that each of the class densities are products of marginal den sities;
that is, they assume that the inputs are conditionally indep endent in
each class (Section 6.6.3).
Suppose that we model each class density as multivariate Gau ssian
fk(x) =1
(2π)p/2|Σk|1/2e−1
2(x−µk)TΣ−1
k(x−µk). (4.8)
Linear discriminant analysis (LDA) arises in the special ca se when we
assume that the classes have a common covariance matrix Σk=Σ∀k. In
comparing two classes kandℓ, it is suﬃcient to look at the log-ratio, and
we see that
logPr(G=k|X=x)
Pr(G=ℓ|X=x)= logfk(x)
fℓ(x)+logπk
πℓ
= logπk
πℓ−1
2(µk+µℓ)TΣ−1(µk−µℓ)
+xTΣ−1(µk−µℓ),(4.9)
an equation linear in x. The equal covariance matrices cause the normal-
ization factors to cancel, as well as the quadratic part in th e exponents.
This linear log-odds function implies that the decision bou ndary between
classeskandℓ—the set where Pr( G=k|X=x) = Pr(G=ℓ|X=x)—is
linear inx; inpdimensions a hyperplane. This is of course true for any pair
of classes, so all the decision boundaries are linear. If we d ivide IRpinto
regions that are classiﬁed as class 1, class 2, etc., these re gions will be sep-
arated by hyperplanes. Figure 4.5 (left panel) shows an idea lized example
with three classes and p= 2. Here the data do arise from three Gaus-
sian distributions with a common covariance matrix. We have included in

4.3 Linear Discriminant Analysis 109
+++
3
21
11
233
3
123
32
11211
33
12 1
23
23
3
12
211
1
13
222
21 3
2 23
13
13
32
13
3
23
133
2133
22
3
22
21
11
11
2
133
1
13
32
222 3
12
FIGURE 4.5. The left panel shows three Gaussian distributions, with the sa me
covariance and diﬀerent means. Included are the contours of c onstant density
enclosing 95% of the probability in each case. The Bayes decision boundari es
between each pair of classes are shown (broken straight lines) , and the Bayes
decision boundaries separating all three classes are the thi cker solid lines (a subset
of the former). On the right we see a sample of 30drawn from each Gaussian
distribution, and the ﬁtted LDA decision boundaries.
the ﬁgure the contours corresponding to 95% highest probabi lity density,
as well as the class centroids. Notice that the decision boun daries are not
the perpendicular bisectors of the line segments joining th e centroids. This
would be the case if the covariance Σwere spherical σ2I, and the class
priors were equal. From (4.9) we see that the linear discriminant functions
δk(x) =xTΣ−1µk−1
2µT
kΣ−1µk+logπk (4.10)
areanequivalentdescriptionofthedecisionrule,with G(x) = argmaxkδk(x).
In practice we do not know the parameters of the Gaussian dist ributions,
and will need to estimate them using our training data:
•ˆπk=Nk/N, whereNkis the number of class- kobservations;
•ˆµk=/summationtext
gi=kxi/Nk;
•ˆΣ=/summationtextK
k=1/summationtext
gi=k(xi−ˆµk)(xi−ˆµk)T/(N−K).
Figure 4.5 (right panel) shows the estimated decision bound aries based on
a sample of size 30 each from three Gaussian distributions. F igure 4.1 on
page 103 is another example, but here the classes are not Gaus sian.
With two classes there is a simple correspondence between li near dis-
criminant analysis and classiﬁcation by linear regression , as in (4.5). The
LDA rule classiﬁes to class 2 if
xTˆΣ−1(ˆµ2−ˆµ1)>1
2(ˆµ2+ ˆµ1)TˆΣ−1(ˆµ2−ˆµ1)−log(N2/N1),(4.11)

110 4. Linear Methods for Classiﬁcation
and class 1 otherwise. Suppose we code the targets in the two c lasses as +1
and−1, respectively. It is easy to show that the coeﬃcient vector from least
squares is proportional to the LDA direction given in (4.11) (Exercise 4.2).
[In fact, this correspondence occurs for any (distinct) cod ing of the targets;
see Exercise 4.2]. However unless N1=N2the intercepts are diﬀerent and
hence the resulting decision rules are diﬀerent.
SincethisderivationoftheLDAdirectionvialeastsquares doesnotusea
Gaussian assumption for the features, its applicability ex tends beyond the
realm of Gaussian data. However the derivation of the partic ular intercept
or cut-point given in (4.11) doesrequire Gaussian data. Thus it makes
sense to instead choose the cut-point that empirically mini mizes training
error for a given dataset. This is something we have found to w ork well in
practice, but have not seen it mentioned in the literature.
With more than two classes, LDA is not the same as linear regre ssion of
the class indicator matrix, and it avoids the masking proble ms associated
with that approach (Hastie et al., 1994). A correspondence b etween regres-
sion and LDA can be established through the notion of optimal scoring ,
discussed in Section 12.5.
Getting back to the general discriminant problem (4.8), if t heΣkare
not assumed to be equal, then the convenient cancellations i n (4.9) do not
occur; in particular the pieces quadratic in xremain. We then get quadratic
discriminant functions (QDA),
δk(x) =−1
2log|Σk|−1
2(x−µk)TΣ−1
k(x−µk)+logπk.(4.12)
The decision boundary between each pair of classes kandℓis described by
a quadratic equation {x:δk(x) =δℓ(x)}.
Figure 4.6 shows an example (from Figure 4.1 on page 103) wher e the
three classes are Gaussian mixtures (Section 6.8) and the de cision bound-
aries are approximated by quadratic equations in x. Here we illustrate
two popular ways of ﬁtting these quadratic boundaries. The r ight plot
uses QDA as described here, while the left plot uses LDA in the enlarged
ﬁve-dimensional quadratic polynomial space. The diﬀerenc es are generally
small; QDA is the preferred approach, with the LDA method a co nvenient
substitute2.
TheestimatesforQDAaresimilartothoseforLDA,excepttha tseparate
covariance matrices must be estimated for each class. When pis large this
can mean a dramatic increase in parameters. Since the decisi on boundaries
are functions of the parameters of the densities, counting t he number of
parameters must be done with care. For LDA, it seems there are (K−
1)×(p+1) parameters, since we only need the diﬀerences δk(x)−δK(x)
2For this ﬁgure and many similar ﬁgures in the book we compute the d ecision bound-
aries by an exhaustive contouring method. We compute the decision rule on a ﬁne lattice
of points, and then use contouring algorithms to compute the bound aries.

4.3 Linear Discriminant Analysis 111
1
11
11
111
1
111
1
11
11
1111 1
111
1
11
111
1111
11
1
11
111
11
1111
1 1
111
11
1
1
1 1
11
1111
11111
1
11
111
111
1111
1
111
11
11
11
1
11
11
111
11
1
1
11
111
111
11
11
11
11 1
11
11 111
1 11
1
11
1
11
1
11
1
11 1
1
11
11
1111
1111
1
111
11
111
1
111
111
1111
11
1 11
111
11
1
11
11
11
111
12
22
22
222
2
2
22
2 2
2
22
22
22
2222
2
22
222
2
22
2 2
22
222
2
222
22222
22
2
222
22
2
22
22
22
22
22
2222
22
22
222
222
22222
22
222
22
22
22
2
22
2222
22
2
22
2
222
2
222
22
222
222
22
2
22
22
2222 2
222
22
2
22
22
222
222
2
22
22
22222
22
222
22
2
222
22
22
222
222
22
2
22
22
22222
22
3
33
33
33
3
3333
33
3
33
3
333
33
33
333
333
3
3
33
333
333
333
333
3333
333
3333
3
33333
33
3
33
3
33
3333
333
3
333
333
33
333
3
3333
33
333
33
3
3333
333
33333
3
3
3
333
33
3
33
3
33
33
333
33
3
3333
33
333
3
333
33333
33
333
33
33
3333
333
33
33
3
33
33
3
33
33
3
333
3
333
33
33
33
1
11
11
111
1
111
1
11
11
1111 1
111
1
11
111
1111
11
1
11
111
11
1111
1 1
111
11
1
1
1 1
11
1111
11111
1
11
111
111
1111
1
111
11
11
11
1
11
11
111
11
1
1
11
111
111
11
11
11
11 1
11
11 111
1 11
1
11
1
11
1
11
1
11 1
1
11
11
1111
1111
1
111
11
111
1
111
111
1111
11
1 11
111
11
1
11
11
11
111
12
22
22
222
2
2
22
2 2
2
22
22
22
2222
2
22
222
2
22
2 2
22
222
2
222
22222
22
2
222
22
2
22
22
22
22
22
2222
22
22
222
222
22222
22
222
22
22
22
2
22
2222
22
2
22
2
222
2
222
22
222
222
22
2
22
22
2222 2
222
22
2
22
22
222
222
2
22
22
22222
22
222
22
2
222
22
22
222
222
22
2
22
22
22222
22
3
33
33
33
3
3333
33
3
33
3
333
33
33
333
333
3
3
33
333
333
333
333
3333
333
3333
3
33333
33
3
33
3
33
3333
333
3
333
333
33
333
3
3333
33
333
33
3
3333
333
33333
3
3
3
333
33
3
33
3
33
33
333
33
3
3333
33
333
3
333
33333
33
333
33
33
3333
333
33
33
3
33
33
3
33
33
3
333
3
333
33
33
33
FIGURE 4.6. Two methods for ﬁtting quadratic boundaries. The left plot sho ws
the quadratic decision boundaries for the data in Figure 4.1 (obtained using LDA
in the ﬁve-dimensional space X1,X2,X1X2,X2
1,X2
2). The right plot shows the
quadratic decision boundaries found by QDA. The diﬀerences are small, as is
usually the case.
between the discriminant functions where Kis some pre-chosen class (here
we have chosen the last), and each diﬀerence requires p+ 1 parameters3.
Likewise for QDA there will be ( K−1)×{p(p+ 3)/2 + 1}parameters.
Both LDA and QDA perform well on an amazingly large and divers e set
of classiﬁcation tasks. For example, in the STATLOG project (Michie et
al., 1994) LDA was among the top three classiﬁers for 7 of the 2 2 datasets,
QDA among the top three for four datasets, and one of the pair w ere in the
top threefor 10datasets. Both techniques arewidely used,a ndentirebooks
are devoted to LDA. It seems that whatever exotic tools are th e rage of the
day, we should always have available these two simple tools. The question
arises why LDA and QDA have such a good track record. The reaso n is not
likely to be that the data are approximately Gaussian, and in addition for
LDA that the covariances are approximately equal. More like ly a reason is
that the data can only support simple decision boundaries su ch as linear or
quadratic, and the estimates provided via the Gaussian mode ls are stable.
This is a bias variance tradeoﬀ—we can put up with the bias of a l inear
decision boundary because it can be estimated with much lowe r variance
than more exotic alternatives. This argument is less believ able for QDA,
since it can have many parameters itself, although perhaps f ewer than the
non-parametric alternatives.
3Althoughweﬁtthecovariancematrix ˆΣtocomputetheLDAdiscriminantfunctions,
a much reduced function of it is all that is required to estimate the O(p) parameters
needed to compute the decision boundaries.

112 4. Linear Methods for Classiﬁcation
Misclassification Rate
0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Regularized Discriminant Analysis on the Vowel Data
Test Data
Train Data
α
FIGURE 4.7. Test and training errors for the vowel data, using regularize d
discriminant analysis with a series of values of α∈[0,1]. The optimum for the
test data occurs around α= 0.9, close to quadratic discriminant analysis.
4.3.1 Regularized Discriminant Analysis
Friedman (1989) proposed a compromise between LDA and QDA, w hich
allows one to shrink the separate covariances of QDA toward a common
covariance as in LDA. These methods are very similar in ﬂavor to ridge
regression. The regularized covariance matrices have the f orm
ˆΣk(α) =αˆΣk+(1−α)ˆΣ, (4.13)
whereˆΣis the pooled covariance matrix as used in LDA. Here α∈[0,1]
allows a continuum of models between LDA and QDA, and needs to be
speciﬁed. In practice αcan be chosen based on the performance of the
model on validation data, or by cross-validation.
Figure 4.7 shows the results of RDA applied to the vowel data. Both
the training and test error improve with increasing α, although the test
error increases sharply after α= 0.9. The large discrepancy between the
training and test error is partly due to the fact that there ar e many repeat
measurements on a small number of individuals, diﬀerent in t he training
and test set.
Similar modiﬁcations allow ˆΣitself to be shrunk toward the scalar
covariance,
ˆΣ(γ) =γˆΣ+(1−γ)ˆσ2I (4.14)
forγ∈[0,1]. Replacing ˆΣin (4.13) by ˆΣ(γ) leads to a more general family
of covariances ˆΣ(α,γ) indexed by a pair of parameters.
In Chapter 12, we discuss other regularized versions of LDA, which are
more suitable when the data arise from digitized analog sign als and images.

4.3 Linear Discriminant Analysis 113
Inthesesituationsthefeaturesarehigh-dimensionalandc orrelated,andthe
LDA coeﬃcients can be regularized to be smooth or sparse in th e original
domain of the signal. This leads to better generalization an d allows for
easier interpretation of the coeﬃcients. In Chapter 18 we al so deal with
very high-dimensional problems, where for example the feat ures are gene-
expression measurements in microarray studies. There the m ethods focus
on the case γ= 0 in (4.14), and other severely regularized versions of LDA .
4.3.2 Computations for LDA
As a lead-in to the next topic, we brieﬂy digress on the comput ations
required for LDA and especially QDA. Their computations are simpliﬁed
by diagonalizing ˆΣorˆΣk. For the latter, suppose we compute the eigen-
decomposition for each ˆΣk=UkDkUT
k, whereUkisp×porthonormal,
andDka diagonal matrix of positive eigenvalues dkℓ. Then the ingredients
forδk(x) (4.12) are
•(x−ˆµk)TˆΣ−1
k(x−ˆµk) = [UT
k(x−ˆµk)]TD−1
k[UT
k(x−ˆµk)];
•log|ˆΣk|=/summationtext
ℓlogdkℓ.
In light of the computational steps outlined above, the LDA c lassiﬁer
can be implemented by the following pair of steps:
•Spherethe data with respect to the common covariance estimate ˆΣ:
X∗←D−1
2UTX, whereˆΣ=UDUT. The common covariance esti-
mate ofX∗will now be the identity.
•Classifytotheclosestclass centroidinthetransformedsp ace,modulo
the eﬀect of the class prior probabilities πk.
4.3.3 Reduced-Rank Linear Discriminant Analysis
So far we have discussed LDA as a restricted Gaussian classiﬁ er. Part of
its popularity is due to an additional restriction that allo ws us to view
informative low-dimensional projections of the data.
TheKcentroids in p-dimensional input space lie in an aﬃne subspace
of dimension≤K−1, and ifpis much larger than K, this will be a con-
siderable drop in dimension. Moreover, in locating the clos est centroid, we
can ignore distances orthogonal to this subspace, since the y will contribute
equally to each class. Thus we might just as well project the X∗onto this
centroid-spanning subspace HK−1, and make distance comparisons there.
Thus there is a fundamental dimension reduction in LDA, name ly, that we
need only consider the data in a subspace of dimension at most K−1.

114 4. Linear Methods for Classiﬁcation
IfK= 3, for instance, this could allow us to view the data in a two-
dimensional plot, color-coding the classes. In doing so we w ould not have
relinquished any of the information needed for LDA classiﬁc ation.
What ifK >3? We might then ask for a L<K−1 dimensional subspace
HL⊆HK−1optimal for LDA in some sense. Fisher deﬁned optimal to
mean that the projected centroids were spread out as much as p ossible in
terms of variance. This amounts to ﬁnding principal compone nt subspaces
of the centroids themselves (principal components are desc ribed brieﬂy in
Section3.5.1,andinmoredetailinSection14.5.1).Figure 4.4showssuchan
optimal two-dimensional subspace for the vowel data. Here t here are eleven
classes, each a diﬀerent vowel sound, in a ten-dimensional i nput space. The
centroids require the full space in this case, since K−1 =p, but we have
shown an optimal two-dimensional subspace. The dimensions are ordered,
sowecancomputeadditionaldimensionsinsequence.Figure 4.8showsfour
additional pairs of coordinates, also known as canonical ordiscriminant
variables. In summary then, ﬁnding the sequences of optimal subspaces
for LDA involves the following steps:
•compute the K×pmatrix of class centroids Mand the common
covariance matrix W(forwithin-class covariance);
•compute M∗=MW−1
2using the eigen-decomposition of W;
•compute B∗,thecovariancematrixof M∗(Bforbetween-class covari-
ance), and its eigen-decomposition B∗=V∗DBV∗T. The columns
v∗
ℓofV∗in sequence from ﬁrst to last deﬁne the coordinates of the
optimal subspaces.
Combining all these operations the ℓthdiscriminant variable is given by
Zℓ=vT
ℓXwithvℓ=W−1
2v∗
ℓ.
Fisher arrived at this decomposition via a diﬀerent route, w ithout refer-
ring to Gaussian distributions at all. He posed the problem:
Find the linear combination Z=aTXsuch that the between-
class variance is maximized relative to the within-class va riance.
Again, the between class variance is the variance of the clas s means of
Z, and the within class variance is the pooled variance about t he means.
Figure 4.9 shows why this criterion makes sense. Although th e direction
joining the centroids separates the means as much as possibl e (i.e., max-
imizes the between-class variance), there is considerable overlap between
the projected classes due to the nature of the covariances. B y taking the
covariance into account as well, a direction with minimum ov erlap can be
found.
The between-class variance of ZisaTBaand the within-class variance
aTWa, whereWis deﬁned earlier, and Bis the covariance matrix of the
class centroid matrix M. Note that B+W=T, where Tis thetotal
covariance matrix of X, ignoring class information.

4.3 Linear Discriminant Analysis 115
Coordinate 1 Coordinate 3 
-4 -2 0 2 4-2 0 2o
ooooo
o
o
ooo
ooo
ooooooo
o
oo
ooo
ooo
oo
ooooooooo
ooo
o
ooo
ooooo
oooooo ooooooo
o
o
ooo oo
o
o
ooo
o
o
oooooooooooo
oooo
oooooo
oo
ooo
oooooooo
ooo
oooo
oo ooo
oooo
oooo
oo
oooo
o
oo
oooooooo
oooo
oooooooooo
oo
ooo
o
ooo
oooooo
ooo
oooooo
o
o
oooooo
oooooooooooo
o
ooo
oo
ooo
ooooo
oo
ooo
ooooo
o
ooooo
o
ooooooo
oooo
o
oooo
oo
ooooo
oooo
o
oo
ooooo
o
o
o
ooo
o
ooooo
ooooooo
o
oooo
oooooo
oooo
o
o
oo ooo
oo
ooo
o
oooo
o
ooooooo
o
oo
oo
o
ooo
ooooo
o
ooooo
o
oo
o
oo
o
oooooo
o
oo
ooooooooooo
oo
oooooo
oo
o
o
ooo o
o
ooooo
o
o
o
ooo
oooooooooo
oooooo
oooooo
oooooo
o
o
ooo
ooooooooooo
oo oo
ooo
oooooo
o
o
oooooooo
ooo
oooooo
o
ooooo
ooo
o
ooo
o
oooooooooo oooo ooooooo
ooooooo
••••••••
••••••••••••••
Coordinate 2 Coordinate 3 
-6 -4 -2 0 2 4-2 0 2o
ooooo
o
o
ooo
ooo
ooooooo
o
oo
ooo
ooo
oo
ooooooooo
ooo
o
ooo
ooooo
oooooo ooooooo
o
o
ooo oo
o
o
ooo
o
o
oooooooooooo
oooo
oooooo
oo
ooo
oooooooo
ooo
oooo
ooooo
oooo
oooo
oo
oooo
o
oo
oooooooo
oooo
oooooooooo
oo
ooo
o
ooo
oooooo
ooo
oooooo
o
o
oooooo
oooooooooooo
o
ooo
oo
ooo
ooooo
oo
ooo
ooooo
o
ooooo
o
ooooooo
oooo
o
oooo
oo
ooooo
oooo
o
oo
ooooo
o
o
o
ooo
o
ooooo
ooooooo
o
oooo
oooooo
oooo
o
o
oo ooo
oo
ooo
o
oooo
o
ooooooo
o
oo
oo
o
ooo
ooooo
o
ooooo
o
oo
o
oo
o
oooooo
o
oo
ooooooooooo
oo
oooooo
oo
o
o
ooo o
o
ooooo
o
o
o
ooo
oooooooooo
oooooo
oooooo
oooooo
o
o
ooo
ooooooooooo
oo oo
ooo
oooooo
o
o
oooooooo
ooo
oooooo
o
ooooo
ooo
o
ooo
o
oooooooooo ooooooooooo
ooooooo
••••••••
••••••••••••••
Coordinate 1 Coordinate 7 
-4 -2 0 2 4-3 -2 -1 0 1 2 3ooo ooo
o
ooooooo
oooo
ooooooooooooooo
ooo
ooo
o
o
ooooooooooooo
oooo
o
ooooo
o
ooooooo
o
o
ooooo
ooooooooooo
ooo
o
oooo
oooo
oooo
oo
oo
o
ooo
oooooooooo
ooo
ooooo
oooooooooooo
oo oooo
oooooo
oooooo
oooooooo
o
ooooooooo
o
ooooooooooo
o
ooooo
oooooo
oo
oooo
ooooooooooo
ooo o
ooo
ooooooooo
ooo
oooooo
oooo
oooooooo
o
oooooo
ooo
ooo
o
oooo
ooooooooooo
o
oooooo
ooooo
ooooooooooooo
o
ooooooo
oooo
oooooooo
o ooo
oooooo
o
o
oooo
ooooo
o
oooooo
oooooooooooo
oooo o
ooooooooooooo
ooo
o
ooo
oo
o
oooooooo
ooooooo
ooo
oo
oooooooooooo
ooooooooo
ooo
oo
ooooo
o
ooo
o
o
ooo
o
oo
o
oooooo
ooooo
oo
oo
o
oooooo
ooo
oo
ooooooooooo
o
oooooo
o
ooooooooooo
oooo
o
oo
••••••••••••••••••••••
Coordinate 9 Coordinate 10 
-2 -1 0 1 2 3-2 -1 0 1 2oo
o
o
oooooooo
ooooo
o
ooooooo
ooooo
ooooo
o
ooooooo
o
o
o
oo
oo
o
ooo
oooooooooo
o
o
oooooooooooo o
oo
ooo
ooooooo
ooooo
oo
oooo
ooooooo
oo
o
ooo
oooo
ooooooo
oooooo
oo
o
oooooo
o
oo
o
oooooooooooooooooo
ooo
ooo
ooo
oo
ooooooo
o
oooo
ooooo
o
ooooooooooo
ooooo
ooooooooooooo
o
oooo
ooooo
o
o
o
o
o
oooo
ooooo
o
ooo
o
oooo
oo
o
oo
oo
oooo
oooo
o
ooooooooooo
o
ooo
oo
o
ooo
o
o oo
oooooooo
ooo
ooooo
ooooo
o
o
ooo
oooooooo
ooo
oo
oo
oooooooooooo
o
o
o
oo
ooooooooo
oooo
o
oooooooooo
ooo
o
o
o
o
o
oo
ooooooo
o
o
oo
o
o
oooooooooo
oooo
ooo
oooooooo
ooo
oooo
ooo
o
ooo
ooooo
ooooooooooooo
ooooo
o
o
ooo
ooooooo
ooo
o
oo
oooo
ooooo
oo
o
o
o
oooooooooooooooo
ooo
o
o ooooo
oo
oo
o
o
o
o••••••••••••••••••••••Linear Discriminant Analysis
FIGURE 4.8. Four projections onto pairs of canonical variates. Notice t hat as
the rank of the canonical variates increases, the centroids become less spread out.
In the lower right panel they appear to be superimposed, and th e classes most
confused.

116 4. Linear Methods for Classiﬁcation
++
++
FIGURE 4.9. Although the line joining the centroids deﬁnes the direction o f
greatest centroid spread, the projected data overlap becaus e of the covariance
(left panel). The discriminant direction minimizes this over lap for Gaussian data
(right panel).
Fisher’s problem therefore amounts to maximizing the Rayleigh quotient ,
max
aaTBa
aTWa, (4.15)
or equivalently
max
aaTBasubject toaTWa= 1. (4.16)
This is a generalized eigenvalue problem, with agiven by the largest
eigenvalue of W−1B. It is not hard to show (Exercise 4.1) that the optimal
a1is identical to v1deﬁned above. Similarly one can ﬁnd the next direction
a2, orthogonal in Wtoa1, such that aT
2Ba2/aT
2Wa2is maximized; the
solution is a2=v2, and so on. The aℓare referred to as discriminant
coordinates , not to be confused with discriminant functions. They are al so
referred to as canonical variates , since an alternative derivation of these
results is through a canonical correlation analysis of the i ndicator response
matrixYon the predictor matrix X. This line is pursued in Section 12.5.
To summarize the developments so far:
•Gaussian classiﬁcation with common covariances leads to li near deci-
sion boundaries. Classiﬁcation can be achieved by sphering the data
with respect to W, and classifying to the closest centroid (modulo
logπk) in the sphered space.
•Since only the relative distances to the centroids count, on e can con-
ﬁne the data to the subspace spanned by the centroids in the sp hered
space.
•This subspace can be further decomposed into successively o ptimal
subspaces in term of centroid separation. This decompositi on is iden-
tical to the decomposition due to Fisher.

4.3 Linear Discriminant Analysis 117
DimensionMisclassification Rate
2 4 6 8 100.3 0.4 0.5 0.6 0.7LDA and Dimension Reduction on the Vowel Data
•
••••• • ••••
•
• •
•
•••••Test Data
Train Data
FIGURE 4.10. Training and test error rates for the vowel data, as a functio n
of the dimension of the discriminant subspace. In this case t he best error rate is
for dimension 2. Figure 4.11 shows the decision boundaries in this space.
The reduced subspaces have been motivated as a data reductio n (for
viewing) tool. Can they also be used for classiﬁcation, and w hat is the
rationale? Clearly they can, as in our original derivation; we simply limit
the distance-to-centroid calculations to the chosen subsp ace. One can show
that this is a Gaussian classiﬁcation rule with the addition al restriction
that the centroids of the Gaussians lie in a L-dimensional subspace of IRp.
Fitting such a model by maximum likelihood, and then constru cting the
posterior probabilities using Bayes’ theorem amounts to th e classiﬁcation
rule described above (Exercise 4.8).
Gaussian classiﬁcation dictates the log πkcorrection factor in the dis-
tance calculation. The reason for this correction can be see n in Figure 4.9.
The misclassiﬁcation rate is based on the area of overlap bet ween the two
densities. If the πkare equal (implicit in that ﬁgure), then the optimal
cut-point is midway between the projected means. If the πkare not equal,
moving the cut-point toward the smallerclass will improve the error rate.
As mentioned earlier for two classes, one can derive the line ar rule using
LDA (or any other method), and then choose the cut-point to mi nimize
misclassiﬁcation error over the training data.
As an example of the beneﬁt of the reduced-rank restriction, we return
to the vowel data. There are 11 classes and 10 variables, and h ence 10
possible dimensions for the classiﬁer. We can compute the tr aining and
test error in each of these hierarchical subspaces; Figure 4 .10 shows the
results. Figure 4.11 shows the decision boundaries for the c lassiﬁer based
on the two-dimensional LDA solution.
There is a close connection between Fisher’s reduced rank di scriminant
analysis and regression of an indicator response matrix. It turns out that

118 4. Linear Methods for Classiﬁcation
oooo
oo o
oo
oo
o
oo o
oo
oooo
o
oo
ooooo
oo
ooo
o
ooo
oooo
oo
oooo
o
ooo
o
oo
ooo
o
o
o
ooo
o
oo
oo oo
oo
ooo
o
oo
oo
oo
o
oo
o
oooo
o
oo
oo
oo
oooo
ooo
o
o
o
o
oo
o
oo
oo
o
o
ooo
o
ooo
ooo
oo
o
ooo
oo
oooo
oo
oo
o
ooo
o
o
ooo
o
ooo
oo
oo
oooo
oo
oo
o
oo
o
ooo
o
oooo
oooo
oo
o o
ooo
o
o
o
oo
oooo
oo
oo
o
oo
ooo
o
oo
oo
oo
o oo
ooo
o
oo
oo
oooo
oo
oo
oo
o
oo
o
oo
oooo
o
ooo
oooooo
ooo
oo
oo
ooooo
ooo
o
ooo
o oo
oo
ooo
o
oooo
oo
o
ooo
o
ooo
ooooo
o
oo
oo
o oo
oo
oo
oo
oo
o
ooo
oo
o
o
oo
o
oo
oooo
o
o
oooo
o
oo
oo
o
oo
oo
o
oo
ooo
oooo
oo
oo
ooo
oo
oo
o
oo
oo
ooo
o
oo
oo
o
oooo
oooo
o
oo
oo
o
ooo
ooooo
ooo
oo
oo
oo
oo
o
o
o
o
o
oo
oo
o
oo
ooo
o
o o
oo
o o
oo
o
oo
o
ooo
oo
ooo
o
oo
o
oo o
ooo
oo
oo
o
oo
oo
oo
oo
ooo
ooo
o
oo
o
oo
oo
oo
oo
o
oo
oo
oo
oo
ooo
o
oo
ooo
oo
oo
o
oo
o
o
o
Canonical Coordinate 1Canonical Coordinate 2Classification in Reduced Subspace
••••••••••••••
••
••
••••
FIGURE 4.11. Decision boundaries for the vowel training data, in the two- di-
mensional subspace spanned by the ﬁrst two canonical variat es. Note that in
any higher-dimensional subspace, the decision boundaries are higher-dimensional
aﬃne planes, and could not be represented as lines.

4.4 Logistic Regression 119
LDA amounts to the regression followed by an eigen-decompos ition of
ˆYTY. In the case of two classes, there is a single discriminant va riable
that is identical up to a scalar multiplication to either of t he columns of ˆY.
These connections are developed in Chapter 12. A related fac t is that if one
transforms the original predictors XtoˆY, then LDA using ˆYis identical
to LDA in the original space (Exercise 4.3).
4.4 Logistic Regression
The logistic regression model arises from the desire to mode l the posterior
probabilities of the Kclasses via linear functions in x, while at the same
time ensuring that they sum to one and remain in [0 ,1]. The model has
the form
logPr(G= 1|X=x)
Pr(G=K|X=x)=β10+βT
1x
logPr(G= 2|X=x)
Pr(G=K|X=x)=β20+βT
2x
...
logPr(G=K−1|X=x)
Pr(G=K|X=x)=β(K−1)0+βT
K−1x.(4.17)
The model is speciﬁed in terms of K−1 log-odds or logit transformations
(reﬂecting the constraint that the probabilities sum to one ). Although the
model uses the last class as the denominator in the odds-rati os, the choice
of denominator is arbitrary in that the estimates are equiva riant under this
choice. A simple calculation shows that
Pr(G=k|X=x) =exp(βk0+βT
kx)
1+/summationtextK−1
ℓ=1exp(βℓ0+βT
ℓx), k= 1,...,K−1,
Pr(G=K|X=x) =1
1+/summationtextK−1
ℓ=1exp(βℓ0+βT
ℓx), (4.18)
and they clearly sum toone. Toemphasize the dependenceon th eentire pa-
rameter set θ={β10,βT
1,...,β (K−1)0,βT
K−1}, we denote the probabilities
Pr(G=k|X=x) =pk(x;θ).
WhenK= 2, this model is especially simple, since there is only a sin gle
linear function. It is widely used in biostatistical applic ations where binary
responses(twoclasses)occurquitefrequently.Forexampl e,patientssurvive
or die, have heart disease or not, or a condition is present or absent.

120 4. Linear Methods for Classiﬁcation
4.4.1 Fitting Logistic Regression Models
Logistic regression models are usually ﬁt by maximum likeli hood, using the
conditional likelihood of GgivenX. Since Pr(G|X) completely speciﬁes the
conditional distribution, the multinomial distribution is appropriate. The
log-likelihood for Nobservations is
ℓ(θ) =N/summationdisplay
i=1logpgi(xi;θ), (4.19)
wherepk(xi;θ) = Pr(G=k|X=xi;θ).
We discuss in detail the two-class case, since the algorithm s simplify
considerably. It is convenient to code the two-class givia a 0/1 responseyi,
whereyi= 1 whengi= 1, andyi= 0 whengi= 2. Letp1(x;θ) =p(x;θ),
andp2(x;θ) = 1−p(x;θ). The log-likelihood can be written
ℓ(β) =N/summationdisplay
i=1/braceleftig
yilogp(xi;β)+(1−yi)log(1−p(xi;β))/bracerightig
=N/summationdisplay
i=1/braceleftig
yiβTxi−log(1+eβTxi)/bracerightig
. (4.20)
Hereβ={β10,β1}, and we assume that the vector of inputs xiincludes
the constant term 1 to accommodate the intercept.
To maximize the log-likelihood, we set its derivatives to ze ro. These score
equations are
∂ℓ(β)
∂β=N/summationdisplay
i=1xi(yi−p(xi;β)) = 0, (4.21)
which arep+1 equations nonlinear inβ. Notice that since the ﬁrst compo-
nentofxiis1,theﬁrstscoreequationspeciﬁesthat/summationtextN
i=1yi=/summationtextN
i=1p(xi;β);
theexpected number of class ones matches the observed number (and hence
also class twos.)
To solve the score equations (4.21), we use the Newton–Raphs on algo-
rithm, which requires the second-derivative or Hessian mat rix
∂2ℓ(β)
∂β∂βT=−N/summationdisplay
i=1xixiTp(xi;β)(1−p(xi;β)). (4.22)
Starting with βold, a single Newton update is
βnew=βold−/parenleftbigg∂2ℓ(β)
∂β∂βT/parenrightbigg−1∂ℓ(β)
∂β, (4.23)
where the derivatives are evaluated at βold.

4.4 Logistic Regression 121
It is convenient to write the score and Hessian in matrix nota tion. Let
ydenote the vector of yivalues,XtheN×(p+ 1) matrix of xivalues,
pthe vector of ﬁtted probabilities with ith element p(xi;βold) andWa
N×Ndiagonal matrix of weights with ith diagonal element p(xi;βold)(1−
p(xi;βold)). Then we have
∂ℓ(β)
∂β=XT(y−p) (4.24)
∂2ℓ(β)
∂β∂βT=−XTWX (4.25)
The Newton step is thus
βnew=βold+(XTWX)−1XT(y−p)
= (XTWX)−1XTW/parenleftbig
Xβold+W−1(y−p)/parenrightbig
= (XTWX)−1XTWz. (4.26)
In the second and third line we have re-expressed the Newton s tep as a
weighted least squares step, with the response
z=Xβold+W−1(y−p), (4.27)
sometimes known as the adjusted response . These equations get solved re-
peatedly, since at each iteration pchanges, and hence so does Wandz.
This algorithm is referred to as iteratively reweighted least squares or IRLS,
since each iteration solves the weighted least squares prob lem:
βnew←argmin
β(z−Xβ)TW(z−Xβ). (4.28)
It seems that β= 0 is a good starting value for the iterative procedure,
although convergence is never guaranteed. Typically the al gorithm does
converge, since the log-likelihood is concave, but oversho oting can occur.
In the rare cases that the log-likelihood decreases, step si ze halving will
guarantee convergence.
For the multiclass case ( K≥3) the Newton algorithm can also be ex-
pressed as an iteratively reweighted least squares algorit hm, but with a
vectorofK−1 responses and a nondiagonal weight matrix per observation .
The latter precludes any simpliﬁed algorithms, and in this c ase it is numer-
ically more convenient to work with the expanded vector θdirectly (Ex-
ercise 4.4). Alternatively coordinate-descent methods (S ection 3.8.6) can
be used to maximize the log-likelihood eﬃciently. The Rpackageglmnet
(Friedman et al., 2010) can ﬁt very large logistic regressio n problems ef-
ﬁciently, both in Nandp. Although designed to ﬁt regularized models,
options allow for unregularized ﬁts.
Logistic regression models are used mostly as a data analysi s and infer-
ence tool, where the goal is to understand the role of the inpu t variables

122 4. Linear Methods for Classiﬁcation
TABLE 4.2. Results from a logistic regression ﬁt to the South African hear t
disease data.
Coeﬃcient Std. Error ZScore
(Intercept) −4.130 0 .964−4.285
sbp 0.006 0 .006 1.023
tobacco 0.080 0 .026 3.034
ldl 0.185 0 .057 3.219
famhist 0.939 0 .225 4.178
obesity -0.035 0 .029−1.187
alcohol 0.001 0 .004 0.136
age 0.043 0 .010 4.184
inexplaining the outcome. Typically many models are ﬁt in a search for a
parsimonious model involving a subset of the variables, pos sibly with some
interactions terms. The following example illustrates som e of the issues
involved.
4.4.2 Example: South African Heart Disease
Here we present an analysis of binary data to illustrate the t raditional
statistical use of the logistic regression model. The data i n Figure 4.12 are a
subset of the Coronary Risk-Factor Study (CORIS) baseline s urvey, carried
out in three rural areas of the Western Cape, South Africa (Ro usseauw et
al., 1983). The aim of the study was to establish the intensit y of ischemic
heart disease risk factors in that high-incidence region. T he data represent
white males between 15 and 64, and the response variable is th e presence or
absence of myocardial infarction (MI) at the time of the surv ey (the overall
prevalence of MI was 5.1% in this region). There are 160 cases in our data
set, and a sample of 302 controls. These data are described in more detail
in Hastie and Tibshirani (1987).
We ﬁt a logistic-regression model by maximum likelihood, gi ving the
results shown in Table 4.2. This summary includes Zscores for each of the
coeﬃcients in the model (coeﬃcients divided by their standa rd errors); a
nonsigniﬁcant Zscoresuggestsacoeﬃcientcanbedroppedfromthemodel.
Each of these correspond formally to a test of the null hypoth esis that the
coeﬃcient in question is zero, while all the others are not (a lso known as
the Wald test). A Zscore greater than approximately 2 in absolute value
is signiﬁcant at the 5% level.
There are some surprises in this table of coeﬃcients, which m ust be in-
terpreted with caution. Systolic blood pressure ( sbp) is not signiﬁcant! Nor
isobesity, and its sign is negative. This confusion is a result of the co rre-
lation between the set of predictors. On their own, both sbpandobesity
are signiﬁcant, and with positive sign. However, in the pres ence of many

4.4 Logistic Regression 123
sbp0 10 20 30
o
o
oo
ooo
oooo
o
oo
ooooo
o
ooo
oo
ooooooo
ooo
oo
ooo
oo
oo
ooo
oooooo
ooo
oo
ooooooooo
oooooooooooo
ooooo
ooooooooo
oo
ooooo
oo
oooo
oo
oooo
oo
ooooo
oooooo
oooooo
oooo
oooooo
ooo
oo
ooo
oo
oo
ooooo
oo
oo
oo
oo
oo
ooo
ooo
ooooo
oo
oo
o
ooo
oooo
o
oooooooo
ooooooo
oooooooooo
ooo
oo
oooo
oooo
oooooooo
ooooo
o
ooooo
oo
oo
ooo
oooo
ooooo
ooo
oooooo
ooo
ooo
ooo
oo
ooo
oooooo
oooooo
ooo
oooo
oooo
oooo
o
ooo
oooo
ooo
oo
ooo
ooooo
ooo
oooo
o oo
o
ooo
ooo
ooooooo
ooooo
oo
oooo
ooooooooo
oooooo
oooo
oooo
oooo
oooooo
o
ooo
oooooo
o
ooo
oooo
oo
o
ooo
oooooooooooo
ooo
oooo
oooo
oo
oo
ooo
o
oooo
o
oo
ooo
oooo
o
oo
oo ooo
o
ooooo
ooooooo
ooo
oo
ooo
oo
oo
ooo
oooooo
ooo
oo
ooooo
ooo
ooooooooooooo
ooooo
ooooooooo
oo
ooooo
oo
oooo
oo
oooo
oo
ooooo
oooooo
oooooo
oooooooooo
ooo
oo
ooo
oo
oo
ooooo
oo
oo
oo
oo
oo
ooo
ooo
ooooo
oo
oo
o
ooo
ooooo
oooooooo
ooooooo
oooooooooo
ooo
oo
oooo
oooo
oooooooo
ooooo
o
ooooo
oo
oo
ooo
oooo
oooooo oo
oooooo
ooo
oooooo
oo
oooo
ooooo
oooooo
ooo
oooo
oooo
oooo
o
ooo
oooo
ooo
oo
ooo
ooooo
ooo
oooo
ooo
o
ooo
ooo
oooo ooo
ooooo
oo
oooo
oooo
ooooo
oooooo
oooo
oooo
oooo
oooooo
o
ooo
oooo oo
oooo
oooo
oo
oooo
ooooo
ooooooo
ooo
oooooooo
oo
oo
ooo
o
ooo0.0 0.4 0.8
o
o
oo
ooo
oooo
o
oo
oo ooo
o
ooo
oo
ooo o ooo
ooo
oo
ooo
oo
oo
ooo
ooo ooo
ooo
oo
ooo oo
oooo
oooooo o ooo oo
oooo o
o ooo
o oooo
oo
o oo
oo
oo
oooo
oo
oooo
oo
ooooo
oooooo
oo
o ooo
oooooooo oo
o oo
oo
oo ooo
oo
ooooo
oo
oo
oo
oo
oo
ooo
ooo
ooooo
oo
ooo
ooo
oooo
o
ooo o
oo oo
oo ooooo
oo o ooooooo
ooo
oo
oo oo
oo oo
oooooooo
ooooo
o
o ooo o
oo
oo
ooo
oo oo
oooooooo
oooo oo
ooo
ooo
ooo
oo
ooooooooo
oooooo
o oo
oooo
oo
oo
oooo
o
ooo
oooo
o oo
oo
ooo
ooooo
ooo
oo oo
ooo
o
o oo
ooo
ooo oo oo
o oooo
oo
ooo o
o o oo
o
o o oo
ooo
ooo
o ooo
oooo
oooo
o oo ooo
o
ooo
oooooo
oooo
o ooo
oo
oooo
ooooo
ooo o ooo
ooo
oo o ooooo
oo
oo
ooo
o
oooo
o
oo
ooo
oooo
o
oo
oo ooo
o
ooooo
ooooooo
ooo
oo
ooo
oo
oo
ooo
oooooo
ooo
oo
ooooooooooooooooooooo
ooooo
ooooooooo
oo
ooo
oo
oo
oooo
oo
oooo
oo
ooooo
oooooo
oo
oooo
oooo
oooooo
ooo
oo
ooooo
ooooooo
oo
oo
oo
oo
oo
ooo
ooo
ooooo
oo
oo
o
ooo
oooo
o
oooooooo
ooooooo
oooo
oooooo
ooo
oo
oooo
oooo
oooooooo
ooooo
o
ooooo
oo
oo
ooo
oooo
oooooooo
oooooo
ooo
oooooo
oo
ooo
oooooo
oooooo
ooo
oooo
oooo
oooo
o
ooo
oooo
ooo
oo
ooo
ooooo
ooo
oooo
ooo
o
ooo
ooo
ooooooo
ooooo
oo
oooo
oooo
ooooo
oooooo
oooo
oooo
oooo
oooooo
o
ooo
oooooo
oooo
oooo
oo
o
ooo
ooooo
ooooooo
ooo
oooooooo
oo
oo
ooo
o
ooo0 50 100
o
o
oo
ooo
oooo
o
oo
ooooo
o
ooooo
ooooooo
ooo
oo
ooo
oo
oo
ooo
oooooo
ooo
oo
oooooooo
ooooooooooooo
ooooo
oooo
ooooo
oo
ooooo
oo
oooo
oo
oo
oo
oo
ooooo
ooo ooo
oo
oooo
oooooooooo
ooo
oo
ooo
oo
ooooooo
oo
oo
oo
oo
oo
ooo
ooo
ooooo
oooo
o
ooo
ooooo
oooooooo
ooooooo
oooooooooo
ooo
oo
oooo
oooo
oooooooo
ooo
oo
o
ooooo
oo
oo
ooo
oooo
oooooooo
oooooo
ooo
oooooo
oo
oooo
ooooo
oooooo
ooo
oooo
oo
oo
oooo
o
ooo
oooo
ooo
oo
ooo
ooooo
ooo
oooo
ooo
o
ooo
ooo
ooooooo
ooooo
oo
oooooooo
ooooo
oooooo
oooo
oooo
oooo
oooooo
o
ooo
oooooo
oooo
oooo
oo
oooo
ooooo
ooooooo
ooo
oooooooo
oo
oo
ooo
o
ooo
100 160 220o
o
oo
ooo
o ooo
o
oo
ooooo
o
ooooo
ooooooo
ooo
oo
ooo
oo
oo
ooo
oooooo
ooo
oo
ooooooo o
oooooooooo ooo
ooooo
oooo
ooooo
oo
ooo
oo
oo
oooo
oo
oooo
oo
ooooo
o ooooo
oooooo
oooo
oooo oo
ooo
oo
ooooo
ooooooo
oo
oo
oo
oo
oo
ooo
ooo
ooooo
oo
oo
o
ooo
ooooo
ooooooo
o
ooo oooo
oo ooooo ooo
ooo
oo
oooo
oooo
oooooooo
ooo
oo
o
ooooo
oo
oo
ooo
oooo
oo ooo
ooo
oooooo
ooo
oooooo
oo
oooooo
ooo
oooo oo
ooo
oooo
oo
oo
oooo
o
ooo
oooo
ooo
o o
ooo
ooooo
ooo
oooo
ooo
o
ooo
ooo
ooo oooo
ooooo
oooooo
oo oo
ooooo
ooo
ooo
oooo
oooo
oooo
oooooo
o
ooo
ooo
ooo
oooo
oooo
oo
oooo
ooooo
ooooooo
ooo
oooooooo
oo
oo
ooo
o
ooo0 10 20 30o
oooo
ooooooo
ooo
ooo
oo
oo
ooo
ooo
o
ooooo
oooo
oo
ooooooo
ooo
oooo
oo
oo
ooooo
oooo
oo
oooo
oooooo
ooo
oo
ooo
ooooooooooooo
o
ooooooo
oooo
ooo
oo
ooooooooo
ooooo
oooooooo oo
ooooooooo
o o
oooo
oo
oo
oo
ooooo
ooooooooooooo
ooooooo
oo
o oooooooooooooooo
oooo
oooooo
oooo
oooooooo
oooo
oooo
oo
ooooo
oo
oooooo
ooooo
ooo
oooooo
ooo
oo
oooo
oo
oo
ooooooo
oooooooooooooo
o
ooooooo
o
oooo
ooo
oooo
o
oooo
ooo oo
ooooo
oooo
ooooo
ooooo
ooo
ooo
ooooooo
oo
oooooo
ooo
oo
oooooooo
ooo
o
oooo
ooooo
oooo
oo
oo o
ooo
o
o
oo
oo
oo
oo
oooo
oo
o
ooooo
ooo oooooooo
ooo
oooooooooooooooo oo
otobaccoo
oooo
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oooooo
oooo
oo
ooooooo
ooo
oooo
oo
oo
ooooo
oooo
oo
oooo
oooooo
ooooo
ooo
ooo oooooooooo
o
ooooooo
oooo
ooo
oo
oo
ooooooo
ooooo
oooooooo oo
ooo
oooooo
oo
oooo
oo
oo
oo
ooooo
ooooooooooooo
ooooooo
oo
ooooo oooooooooooo
oooo
oooooo
oooo
oooooooo
oooo
oooo
oo
ooooo
oo
ooo
ooo
ooooo
ooo
oooooo
ooo
oo
oooooo
oo
oooo
ooo
ooooooo ooooooo
o
ooooooo
o
oooo
ooo
oooo
o
oooo
ooooo
ooooo
oooo
ooooo
oo ooo
ooo
ooo
ooooooo
oo
ooooooooo
oo
oooo oo
oo
ooo
o
oo
oo
ooooo
oooo
oo
ooo
ooo
o
o
oo
oo
oo
oo
oooo
oo
o
ooooo
ooooooooooo
o oo
oooooooooo
oooooooo
oo
o ooo
oo o
oooo
ooo
ooo
oo
oo
o oo
ooo
o
ooooooooo
oo
o o ooooo
ooo
oooo
oo
oo
o o o oo
oooo
oo
oooo
o ooo oo
ooooo
o oo
oo ooooooo oooo
o
o o ooooo
oooo
ooo
oooo
oooooo o
o oooo
ooooooo o oo
o oooo oooo
oo
oooo
oo
oo
oo
ooooo
oooooo o oooooo
o oooo
oo
oo
o o oo oo o oo oooo ooo ooooo
oooooo
oooo
o ooo oo oo
oooo
oooo
o o
ooo oooo
o ooooo
ooooo
ooo
oo oooo
ooo
oo
oo oo
oo
oo
ooooooo
oooooooooooooo
o
ooo o o oo
o
oooo
ooo
oooo
o
oo oo
oo o oo
ooooo
ooo o
ooooo
oo oo o
ooo
ooo
ooooooo
oooo oo oo
ooo
oo
o ooo oooo
o oo
o
oo
oo
ooooo
oooo
oo
ooo
ooo
o
ooo
oo
oo
oo
oooo
oo
oooo oo
ooooo o o o ooo
o oo
oo oo oooooo
o ooo oooo
oo
oooo
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
o
ooooooooo
oo
ooooooo
ooo
oooo
oo
oo
ooooo
oooo
oo
oooo
oooooo
ooo
oo
ooo
ooooo
oooooooo
o
ooooooo
oooo
ooo
oo
ooooooooo
ooooo
oooooooooo
ooooooooo
oo
oooo
oo
oo
oo
ooooo
ooooooooooooo
ooooo
oo
oo
ooooooooooooooooo
oooo
oooooo
oooo
oooooooo
oooo
oooo
oo
ooooooo
oooooo
ooooo
ooo
ooo ooo
ooo
oo
oooooo
oo
ooooooo
oooooooooooooo
o
ooooooo
o
oooo
ooooooo
o
oooo
ooo oo
ooooo
oooo
ooooo
ooooo
ooo
ooo
ooooooo
oooooooo
ooo
oo
oooooooo
ooo
o
oo
oo
ooooo
oooo
oo
ooo
ooo
o
o
oo
oo
oo
oo
oooo
oo
o
ooooo
ooooooo oooo
ooo
oooooooooo
oooo oooo
oo
oooo
ooo
oooo
o oo
ooo
oo
oo
ooo
ooo
o
ooooooooo
oo
ooooooo
ooooooo
oo
oo
ooooo
oooo
oo
oooo
ooo ooo
ooooo
ooo
ooooooooooooo
o
ooooooo
oooo
ooo
oo
ooooooooo
ooooo
ooooooo ooo
ooo
oooooo
oo
oooo
oo
oo
oo
o oooo
ooooooooooooo
ooooo
oo
oo
oooooooooooooooo o
oooo
oooooo
oooo
oooooooo
oooo
oo oo
oo
ooooooo
ooooo o
ooooo
ooo
oooooo
ooo
oo
oooo
oo
oo
oooo
ooo
oooooooooooooo
oooooooo
o
oooo
ooo
oooo
o
oooo
ooooo
ooooo
oooo
ooooo
ooooo
ooo
ooo
ooooooo
ooooooooooo
oo
o ooo oo
oo
ooo
o
oo
oo
ooooo
oooo
oo
ooo
ooo
o
o
oo
oo
oo
oo
oooo
oo
o
ooooo
o ooooooo ooo
ooo
oooooooooooooooooo
oo
oooo
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
o
ooooo
oooo
oo
ooooooo
ooo
oooo
oo
oo
o o ooo
oooo
oo
oooo
oo oooo
ooooo
o oo
ooo oooooo oooo
o
ooooooo
oooo
ooo
oo
oooo ooooo
ooooo
o ooooooo oo
ooo
oooooo
oo
oooo
oo
oo
oo
ooooo
ooooooooooooo
ooooo
oo
oo
ooooooooooooooooooooo
oooo oo
oooo
o ooooooo
oooo
oooo
oo
ooooo
oo
oooooo
ooooo
ooo
ooo ooo
ooo
oo
oooo
oo
oo
oooo
ooo
ooooooooo
ooooo
ooooo ooo
o
oooo
ooo
oooo
ooooo
ooo oo
o oooo
oooo
ooooo
ooooo
ooo
ooo
ooooooo
ooooooooooo
oo
oooooooo
ooo
ooo
oo
ooooo
oooo
oo
ooo
ooo
o
o
oo
oo
oo
oo
oooo
oo
o
ooooo
oooooooooooooo
oooooooooo
ooo o oooo
o
oooo
oo
oooo
oo
ooooo
ooo
ooo
ooo
o
ooo
ooooo
oo
o
ooo
o
oooooo
ooo
oo
ooo
oooo
oooo
ooooo
oooooo
oooooo
oo
o
ooooo
ooo
ooo
oooo
oo
oooo
oooo
ooo
oo
oo
ooo
oooo
ooo
ooo
oo
ooo
oo
ooooooooo
oooooo
oooo
ooo
ooo
ooooooo
ooo
oo
ooo
ooo
oooo
oo
ooooo
oooo
ooooo
o
ooooooooo
oo
o
o
ooo
oooo
oooo
ooo
ooo
oo
ooooooo
ooooo
o
ooooooo
oo
ooo
ooo
ooooo
ooooo
oooo
oooo
oooooooooooo
oo
oo
oooooooo
oo
ooooo oo
oooo
oooooooooooo
ooo
ooo
oo
ooo
o
oo
oo
ooo
o
oooooo
oo
oooooo
oooooooooo
ooo
oooooo
ooo
ooo
oo
ooooooooo
oo
oooooo
o
ooooo
oo
oo
oo
oooooooooo
oooo
ooooo ooooo
oo
oooo
ooo
oooo
o
oooo
oo
oo
ooo
oo
oooo
oo
ooooo
ooo
ooo
o oo
o
ooo
ooooo
oo
o
ooo
o
oooooo
ooo
oo
ooo
oooo
oooo
ooooo
oooooo
oooooo
oo
oooooo
ooo
ooo
oooo
oo
oooo
oooo
ooo
oo
oo
ooo
oooo
ooo
ooo
ooooo
oo
ooooooooo
oooooo
oooo
ooo
ooo
o oooooo
ooo
oo
ooo
ooo
ooo
o
oo
ooooo
oo
oo
ooooo
o
ooo
oooooo
oo
o
o
ooo
oooo
oooo
ooo
ooo
ooo
oooooo
ooooo
o
oooo
oo
o
oo
ooo
ooo
ooooo
ooooo
oooo
oooo
oooooooo oooo
oo
oo
oooooooo
oo
ooooooo
oooo
oooooooooooo
ooo
ooo
oo
ooo
o
oo
oo
ooo
o
ooo
ooo
oo
oooooo
oooooo
oooo
ooo
oooooo
ooo
ooo
oo
o
oooooooo
oo
oooooo
o
ooooo
oo
oo
oo
oooo oooooo
oooo
oooo
oooooo
oo
o
ooo
ooo
oooo
o
ooo
o
oo
oldl
oo oo
oo
oooo
oo
oo o oo
ooo
ooo
ooo
o
o oo
oo ooo
oo
oooo
o
oo oooo
ooo
oo
o oo
oooo
ooo o
o oo
oo
oooooo
oo oo oo
oo
oooo oo
ooo
ooo
oooo
oo
oo oo
ooo o
oo o
oo
oo
ooo
oooo
ooo
ooo
oo
ooo
oo
o o ooooooo
ooo ooo
ooooo ooooo
ooooooo
ooo
oo
o oo
o oo
ooo
o
oo
ooooo
oooo
ooooo
o
ooo
o oo ooo
oo
o
o
ooo
oooo
oooo
ooo
ooo
oo
o
oooooo
oo ooo
o
oooooo
o
oo
ooo
ooo
o oooo
ooooo
o ooo
oooo
oooooooooooo
oo
oo
ooooo o oo
oo
o oo
oo oo
o ooo
ooooo ooooooo
ooo
o oo
ooooo
o
oo
oo
ooo
o
ooo
ooo
oo
oooo
oo
oooooo
oooo
o o o
ooo ooo
ooo
ooo
oo
o
oooooooo
oo
ooo ooo
o
ooooo
oo
oo
oo
oo oooooooo
oo oo
oooo
o ooooo
oo
oooo
ooo
oo oo
o
ooo
o
oo
ooooo
oo
oooo
oo
ooooo
ooo
ooo
ooo
o
ooo
ooooo
oo
o
ooo
o
oo o ooo
ooo
oo
ooo
oooo
oooo
ooooo
oooooo
oooooo
oo
ooooooooo
ooo
oooo
oo
oooo
oooo
ooo
oo
oo
ooo
oooo
ooo
ooo
oo
ooo
oo
oo ooooooo
oooooo
oo
oo
ooo
ooo
ooooooo
ooo
oo
ooo
ooo
oooo
oo
ooooo
oooo
ooo oo
o
ooooooooooo
o
o
ooo
oooo
oooo
ooo
ooo
ooooooooo
ooooo
o
oooooo
o
oo
ooo
ooo
ooooo
ooooo
oooo
oooo
oooooooooooo
oo
oo
oooooooo
oo
ooooooo
oooo
oooooooooooo
ooo
ooo
oo
ooo
o
oo
oo
ooo
o
oooooo
oo
oooooo
oooooooooo
ooo
oooooo
ooo
ooo
oo
o
oooooooo
oo
oooooo
o
ooooo
oo
oo
oo
oooooooooo
ooo o
oooooooooo
oo
oooo
ooo
oooo
o
ooo
o
oo
ooooo
oo
oooo
oo
ooo oo
ooo
ooo
ooo
o
ooo
ooooo
oo
oooo
o
oooooo
ooo
oo
ooo
oooo
oooo
ooooo
oooooo
oooooo
oo
ooooooo
oo
ooo
oooo
oo
oooo
oooo
ooo
oo
oo
ooo
oooo
ooo
ooo
oo
ooo
oo
ooooooooo
oooooo
oooo
o oo
ooo
ooooooo
ooo
oo
ooo
ooo
ooo
o
oo
ooooo
oooo
ooooo
o
ooo
oooooo
oo
o
o
ooo
oo
oo
oooo
ooo
ooo
oo
oo ooooo
ooooo
o
oooooo
o
oo
ooo
ooo
ooooo
ooooo
oooo
oooo
oooooooooooo
oo
oo
oooooooo
oo
ooooooo
oooo
oooooooooooo
o oo
ooo
oo
ooo
o
ooo
o
ooo
o
ooo
ooo
oo
oooo
oo
oooooo
ooooo oo
oooooo
ooo
ooo
oo
o
oooooooo
oo
oooooo
o
ooooo
oo
oo
oo
oooo oooooo
oooo
oooo
oooooo
oo
oooo
ooooooo
o
oooo
oo
o
2 6 10 14oooo
oo
oooo
oo
ooooo
ooo
ooo
ooo
o
ooo
ooooo
oo
o
ooo
o
oo o ooo
ooo
oo
ooo
oooo
oooo
o oooo
oooooo
oooooo
oo
oooooo
oooooo
o ooo
oo
oooo
oooo
ooo
oo
oo
ooo
oo oo
ooo
ooo
oo
o oo
oo
ooo ooooo o
oooooo
o oooo oo
ooo
o ooo ooo
ooo
oo
o oo
ooo
oooo
oo
ooooo
oo
oo
ooooo
o
ooooooooo
oo
o
o
ooo
oooo
oooo
ooo
ooo
oo
ooooooo
o oooo
o
oooooo
o
oo
ooo
ooo
oo ooo
ooooo
oooo
oooo
oooooooooooo
oo
oo
oooooooo
oo
ooo
oooo
oooo
ooo ooooooooo
ooo
ooo
ooooo
o
oo
oo
ooo
o
ooo
o oo
oo
oooo
oo
oooooooooo
oo o
oooooo
ooo
ooo
oo
o
oooooooo
oo
oooooo
o
ooooo
oo
oo
oo
oooooooooo
oooo
oooo
oooooo
oo
oooo
ooooooo
o
oooo
oo
o0.0 0.4 0.8o
ooooo
oooo
oo
oooo
oooo
ooo
o oo
ooo
oooo
oooooooo
ooo
oooo
ooo
ooo
oooooo
ooo
oooo
oo
ooooo
oooo
ooo
oo
oo
oooo
ooooo ooooo
oo
oo
ooooo
oooo
oo
oo
ooo
ooooooo
oo
oo
oooooo
oo
oo
oo
oooooo
oooooooo
ooooo o ooooooooo
oooooo
ooooooooo
oooo
ooo
ooo
ooo
ooo
ooo
ooo
oo
oo
ooooooo o oo
oo
ooo
ooo o
ooooooo
oooo
ooo
ooo
oo
oooooooo
oo
ooooo
ooooo o
oo
oooo
oooooo oo
ooooooooo
ooooooo
ooo
ooo
oooo
oo
oooo
oo
ooo
oo
ooo
oooooo
oooo
oo
oooo
ooo o
oo
ooooooo
ooo
oo
oooo
oooo
ooo
ooo
ooo
ooooo
oooo
ooooo
ooooo
oo
ooooo
oooo
oooooo oo
ooo
o oooo
ooo
oo
oooooo
oo
o oooo
oooo
oooo
oooo
ooo
oo ooo o
ooooo
oooo
oo
oooo
ooo o
oo o
o oo
ooo
oooo
oooooooo
ooo
oooo
ooo
ooo
oooooo
ooo
oooo
oo
ooooo
oooo
ooo
oo
oo
oooo
oooooooooo
oo
oo
o oooo
oooo
oo
oo
ooo
oo ooooo
oo
oo
oooooo
oo
oo
oo
oo oooo
oooooooo
ooooo o ooooooooo
oo oooo
ooooooooo
o ooo
ooo
ooo
ooo
ooo
ooo
ooo
oo
oo
oooooooooo
oo
ooo
oooo
oooo ooo
oooo
ooo
ooo
oo
oooooooo
oo
ooooo
ooooo o
oo
oooo
oooooooo
ooooooooo
ooooooo
ooo
ooo
oooo
oo
oooo
oo
ooo
oo
ooo
oooooo
o ooo
oo
o ooo
oooo
oo
ooooooo
ooo
oo
oooo
oooo
ooo
ooo
ooo
ooooo
oooo
ooooo
ooo oo
oo
ooooo
oo oo
oo o ooooo
ooo
ooooo
ooo
oo
oooooo
oo
ooooo
o ooo
oooo
oooo
ooo
ooooo o
ooooo
oooo
oo
oooo
oooo
ooo
ooo
ooo
oooo
oooooooo
ooo
oooo
ooo
ooo
oooooo
ooo
oooo
oo
ooooo
oooo
ooo
oo
oo
oooo
oooooo oooo
oo
oo
ooooo
oooo
oo
oo
ooo
ooooooo
oo
oo
oooooo
oo
oo
oo
oooooo
ooooooo o
ooooooooooooooo
oooooo
ooooooooo
oooo
ooo
ooo
ooo
ooo
ooo
ooo
oo
oo
o ooo oooooo
oo
ooo
oooo
ooo oooo
oooo
ooo
ooo
oo
oooooooo
oo
ooooo
oooooo
oo
oooo
oooooooo
ooooooo oo
ooooooo
ooo
ooo
oooo
oo
oooo
oo
ooo
oo
ooo
oooooo
oooo
oo
oooo
oooo
oo
oo ooooo
ooo
oo
oooo
oooo
ooo
ooo
ooo
ooooo
oooo
ooooo
ooooo
oo
ooooo
oooo
oooo oooo
ooo
ooooo
ooo
oo
oooooo
oo
ooooo
oooo
oooo
oooo
ooo
ooo oo
famhisto
ooooo
oooo
oo
oooo
oooo
ooo
ooo
ooo
oooo
oooooooo
ooo
o ooo
ooo
ooo
oo oooo
oo o
oooo
oo
oo ooo
oooo
ooo
oo
oo
oooo
oooooooooo
oo
oo
ooooo
oooo
oo
oo
ooo
ooo oooo
oo
oo
oooooo
oo
oo
oo
oooooo
oooooooo
oo ooooooooooooo
oooooo
ooooooooo
oooo
ooo
o oo
ooo
ooo
ooo
ooo
oo
oo
oooooooooo
oo
ooo
ooo o
ooooooo
oooo
ooo
ooo
oo
oooooooo
oo
ooooo
oooooo
oo
oooo
oooooooo
oooooo ooo
ooooooo
ooo
ooo
oooo
oo
oooo
oo
oo o
oo
ooo
oooooo
oooo
oo
oooo
oooo
oo
ooooooo
ooo
oo
oooo
oooo
ooo
ooo
ooo
ooooo
oooo
ooooo
ooooo
oo
ooooo
oooo
oooooooo
oo o
ooooo
ooo
oo
oooooo
oo
ooooo
oooo
oooo
oooo
o oo
ooooo o
ooooo
oooo
oo
oooo
oooo
ooo
ooo
ooo
oooo
ooooooo o
ooo
oooo
ooo
ooo
oo oooo
ooo
oooo
oo
ooooo
oooo
ooo
oo
oo
oooo
oooooooooo
oo
oo
oo o oo
oooo
oo
oo
ooo
ooooooo
oo
oo
oooooo
oo
oo
oo
oooooo
oooooooo
o ooooooo ooooooo
oooooo
ooooooooo
oooo
ooo
ooo
ooo
ooo
ooo
ooo
oo
oo
oooooooooo
oo
ooo
ooo o
ooo o ooo
oooo
ooo
ooo
oo
o ooooooo
oo
ooooo
o ooooo
oo
oooo
oooooooo
ooooooooo
ooooooo
ooo
ooo
oooo
oo
oooo
oo
ooo
oo
ooo
oooooo
oooo
oo
ooo o
oooo
oo
ooooooo
ooo
oo
oooo
oooo
oo o
oo o
ooo
ooooo
oooo
ooooo
ooooo
oo
ooooo
oooo
o ooooooo
ooo
oo ooo
ooo
oo
oooooo
oo
ooooo
oooo
ooo o
oooo
ooo
ooooo o
ooooo
oo oo
oo
o ooo
oooo
ooo
ooo
ooo
oooo
oooooo oo
ooo
o ooo
o oo
ooo
oooooo
ooo
oooo
oo
oo ooo
oooo
ooo
oo
oo
oooo
o o o ooooooo
oo
oo
oo ooo
oooo
oo
oo
ooo
o o ooooo
oo
oo
oooooo
oo
oo
oo
oooooo
oooooooo
o o ooo o ooo oooooo
oo oooo
oo o oooooo
oooo
ooo
ooo
ooo
o oo
o oo
oo o
oo
oo
o oo oooo o oo
oo
ooo
oooo
oo ooooo
oo oo
ooo
ooo
oo
o o oooooo
oo
oo ooo
oo oooo
oo
oooo
oooooooo
ooooooo oo
oooo ooo
ooo
oo o
oooo
oo
o ooo
oo
ooo
oo
ooo
ooo ooo
oooo
oo
oooo
o oo o
oo
ooooooo
ooo
oo
oooo
oooo
ooo
oo o
ooo
ooooo
oooo
ooooo
ooooo
oo
ooooo
oooo
oo oooooo
ooo
oo ooo
ooo
oo
oooooo
oo
ooooo
oooo
oooo
o ooo
ooo
ooooo
oooo
oo
ooooo
ooooooo
oo
ooooo
oo
ooo
ooooo
ooo
ooo
oooo
ooo
ooooo
ooooooooooo
ooo
oo
oooo
oo
ooooooo
oo
oo
oo
oo
ooo
oo
ooooooooooo
oo
oooo
oo
oo
oooo
oooooo
oo
oo
oo
ooooooooo
ooo
ooooo
ooooooooo
oooo
oooooooo
o
ooo
ooo
ooooo
o
oo
oo
ooooo
oo
ooooo
o ooo
ooooo
oo
oo
oo
oo
oooo
oo
oo
ooooooooo
ooooo
oooooo
o
oooooo
oooooo
ooo
oooo
oooo
oooo
oooooo
ooooooo
ooooo
ooooo
ooooo
ooo
o
ooo
ooo
oooo
o
ooooooooo
ooo
oooooo
ooooooo
oooooo
o
oooooo
ooooo
ooooooooo
o
ooooo
ooooooo
ooo
ooo
oooooo
ooo
oo
ooooo
ooo
ooo
oooo
o
oo
ooo
ooo oo
ooooo
oo
oooo
ooooooo
oooo
oo
ooo
ooooo
oo
oo
oo
ooooo
oo
oooo
o
ooooooooo
ooooooo
ooo
ooooo
ooo
ooo
oooo
ooo
ooooo
ooooooooooo
ooo
oo
oooo
oo
ooooooo
oo
oo
oo
oo
ooo
oo
oooo
ooooooo
oo
oooo
oo
oo
oooo
oooooo
oo
oo
oo
ooooooooo
ooo
ooooo
ooooooooo
oooo
oooooooo
o
ooo
ooo
ooooo
o
oo
ooooooo
oo
ooooooooo
ooooooooo
oo
oo
oooo
oo
oo
oooooo
ooo
ooooo
oooooo
o
oooooo
oooooo
ooo
oooo
oo oo
oooo
oooooo
ooooooo
ooooo
ooooo
ooooo
ooo
o
ooo
ooo
oooo
o
ooooooooo
ooo
oooooo
ooooooo
ooooooo
oooooo
ooooo
ooooooooo
o
oooooooo
oooo
ooo
ooo
oooooo
ooo
oo
ooooo
ooo
ooo
oooo
o
oo
ooo
ooooo
ooooo
oo
ooooooooooo
oooo
oo
ooo
ooooo
oo
oo
oo
ooooo
oo
oooooooooo oo
oo
ooooooo
ooo
ooooo
ooo
ooo
oooo
ooo
ooooo
ooooooooooo
ooo
oo
oooo
oo
ooooo
oo
oo
oo
oo
oo
ooo
oo
ooooo oooooo
oo
oooo
oo
oo
oooo
oooooo
oo
oo
oo
oo
oo
ooooo
ooo
ooooo
ooooooooo
oooo
oooooooo
o
ooo
ooo
ooooo
o
oo
oo
ooooo
oo
oooo
ooooo
ooooooo
oo
oooo
oooo
oo
oo
ooooooooo
ooooo
oooooo
o
oooooo
oooooo
ooo
oooo
oooo
oooo
oooooo
ooooooo
ooooo
ooooo
ooooo
ooo
o
ooo
ooo
oooo
o
ooooooooo
ooo
ooo
ooo
ooooooo
ooooooo
oooooo
ooooo
oooo
ooooo
o
oooooooo
oooo
ooo
ooo
oooooo
ooo
oo
ooooo
ooo
ooo
oooo
o
oo
ooo
ooooo
ooooo
oo
ooo oooooooo
oooo
oo
ooo
ooooo
oo
oo
oo
ooo oo
oo
ooooo
ooo oo oooo
ooooooo
oo o
ooooo
ooo
ooo
oooo
ooo
oo ooo
oooooo o ooo o
ooo
oo
oooo
oo
ooooo
oo
o o
oo
oo
oo
ooo
oo
oooooooo ooo
oo
ooo o
oo
oo
oooo
oooooo
oo
oo
oo
ooooo o ooo
ooo
oo oo o
ooooooo oo
oooo
oooooooo
o
ooo
ooo
ooooo
o
oo
ooo oooo
oo
o ooo
ooo oo
o oooooo
oo
oo
oo
oooo
oo
oo
ooo o ooooo
ooooo
oooo oo
o
o oo oo o
oooooo
o oo
oooo
o
ooo
oooo
o o
oooo
ooooooo
ooooo
ooooo
o
oooo
ooo
o
ooo
ooo
oooo
o
o oooo oo oo
ooo
ooo
ooo
oo ooooo
ooooooo
oooooo
ooooo
o oooooo oo
o
oo
oooooooooo
ooo
ooo
oooooo
ooo
oo
oo ooo
ooo
ooo
oooo
o
oo
o oo
ooooo
ooooo
oo
ooo oooooooo
oooo
oo
ooo
ooooo
oo
oo
oo
oobesity
oooo
oo
oooo
oooooooooo
ooooo
oo
ooo
ooooo
ooo
ooo
oooo
ooo
ooooo
oooooooooo o
ooo
oo
oooo
oo
ooooooo
oo
oo
oo
oo
ooooo
oooo
ooooooooo
oooo
oo
oo
oooo
oooooo
oo
oo
oo
oo
ooooooo
ooo
ooooo
ooooooooo
oooo
oooooooo
o
ooo
ooo
ooooo
o
oo
ooooooo
oo
oooo
ooooo
ooooooo
oo
oo
oo
oooo
oo
oo
ooooooooo
ooooo
oooooo
o
oooooo
oooooo
ooo
oooo
o
ooo
oooo
ooooo
o
ooooooo
ooooo
ooooo
ooooo
ooo
o
ooo
ooo
oooo
o
ooooooooo
ooo
ooo
ooo
ooooooo
ooooooo
oooooo
ooooo
ooooooooo
o
oo
ooooo
ooooo
ooo
ooo
oooooo
ooo
oo
ooooo
oooooo
oooo
o
oo
ooo
ooo oo
ooooo
oo
ooooooooo oo
oooo
oo
ooo
oooo
o
oo
oo
oo
o
15 25 35 45oooo
oo
ooooo
ooo oooooo
ooooooo
ooo
ooooo
ooo
ooo
oooo
ooo
ooooo
oooooo ooooo
ooo
oo
oooo
oo
ooooooo
oo
oo
oo
oo
ooo
oo
ooooooooooo
oo
oooo
oo
oo
oooo
oooooo
oo
oo
oo
oooo
ooooo
ooo
ooooo
oooo
ooo oo
oooo
ooo ooooo
o
ooo
ooo
ooooo
o
oo
oo
ooooo
oo
ooooo
oooo
ooooooo
oo
oo
oo
oooo
oo
oo
ooooooooo
ooooo
oooooo
o
oooooo
oooooo
ooo
oooo
o
ooo
oooo
oo
ooo
o
ooooooo
ooooo
ooooo
o
oooo
ooo
o
ooo
ooo
oooo
o
ooooooooo
ooo
ooo
ooo
ooooooo
ooooooo
oooooo
ooooo
ooooooooo
o
oo
oooooo
oooo
ooo
ooo
oooooo
ooo
oo
ooooo
ooo
ooo
oooo
o
oo
ooo
ooooo
ooooo
oo
oooo
ooooooo
oooo
oo
ooo
ooooo
oo
oo
oo
o0 50 100o
oooo
oooooo
ooo ooo
oo
ooo
oooooo
ooooooooo
ooo
ooooooooo
ooo
oooo
oooooooooo
ooooooo
ooooooo
oooooo
ooooooooooo
ooooooooo
o
oooooooo
oo
oo
oooooo
oooo
ooo
ooo
ooo
oooo
oo
ooooooooooo
o
oo
oo
oo
ooooooooo oooooooooooooo
ooooooooo
ooooooo
o
ooo
ooooo
oooooooooooooooo
oo
ooo
oooo
oooooo
oo
oooooooo
oooooo
ooo
oooo
oooooo
oooooooooooo
ooo
ooooo
oooo
oo
ooo
ooooooo
ooo
o
oo
ooo
o
oo
ooo
oo
oooo oooooooooo
ooo
o
oo
ooo
oooooo
oo
ooo
ooooooo
oo
oooooo
o
oo
oo
oo
ooo
ooo
ooooo
oooo
oooo
o
ooooooo
oo
ooooo
o ooo
oooo
oo
oooo
o
oooooo ooooooooo
oo
ooooo
o
oo
ooo
ooooo
oo
oooo
oooooo
oo oooo
oo
ooo
o ooooo
ooooooooo
ooo
oooo
ooooo
ooo
oooo
oooooooooo
ooooooo
ooooooo
oooooo
ooooooooooo
ooo
oooooo
ooooooooo
oo
oo
oooooo
oooo
ooo
ooo
ooo
oooo
oo
ooooooooooo
o
oo
oo
oo
ooooooooooooooooooooooo
oooooooooooooooo
o
ooo
o oo
oo
oo
oooooooooooooooo
ooo
oooo
oooooo
oo
oooooooo
oooooo
ooo
oooo
oooooo
oooooooooooo
ooo
ooooo
oooo
oo
ooo
oooooooooo
o
oo
ooo
o
oo
ooo
oo
ooooooooo
ooooo
ooo
o
oo
ooo
oooooooo
ooo
oo
ooooo
oo
oooooo
o
oo
oo
oo
ooo
ooo
ooooo
oooo
oooo
o
ooooooooo
oo ooo
oooo
oooo
oo
oooo
o
oo ooooo oooooooo
oo
ooooo
o
oo
ooo
ooooo
oo
oooo
oooooo
oooooo
oo
ooo
oooooo
ooooooooo
ooo
ooooo oooo
ooo
oo
oo
oooooooooo
ooooooo
ooooooo
oooooo
ooo oooooooo
ooo
oooooo
ooooooooo
oo
oo
oooooo
oooo
ooo
ooo
ooo
oooooo
ooooo
oooooo
o
oo
oo
oo
ooooooooooooooooooooooo
oooooooooooooooo
o
ooo
ooo
oo
oo
oooooooooooooo
oo
ooo
oooo
oooooo
oo
o ooooooo
oooooo
ooo
oooo
oooooo
oooooooooooo
ooo
ooooo
oooo
oo
ooo
oooooooooo
o
oo
ooo
o
oo
ooo
oo
oooooooooooooo
ooo
o
oo
ooo
oooooo
oo
ooo
ooooooo
oo
oooooo
o
oo
oo
oo
ooo
ooo
ooooo
oooo
oooo
o
ooooooooo
ooooooooo
oooo
oo
oooo
o
oooooooooo ooo oo
oo
ooooo
o
oo
ooo
oooo o
oo
o ooo
oo oooo
o oo oo o
oo
ooo
o o o o oo
o oooooooo
ooo
o ooooo ooo
o oo
oooo
ooo o o ooooo
ooooooo
o ooo ooo
o o oooo
o oo oooooooo
oooo o oooo
oo ooo o o oo
oo
oo
oooooo
o ooo
ooo
ooo
o oo
oooo
oo
ooooo
oooooo
o
oo
oo
oo
ooooooooo ooo ooooooooooo
o oooo o ooooooo ooo
o
ooo
o oo
oo
ooooooooooo ooo oo
oo
ooo
oooo
oooooo
oo
o oo oo ooo
oo oooo
ooo
oooo
o ooooo
o o ooo ooooooo
ooo
oo ooo
oooo
oo
ooo
oo oo o oo
oooo
oo
ooo
o
oo
ooo
oo
o oo o oooooooooo
ooo
o
oo
o oo
oo o ooooo
ooo
oo
oo ooo
oo
ooo o oo
o
oo
oo
oo
ooo
o oo
oo ooo
oooo
oo oo
o
ooooo o ooo
o oooooooo
oooo
oo
o ooo
o
o ooooo o oooooo oo
oo
o oooo
o
oo
ooo
ooooo
oo
oooo
oooooo
oooooo
oo
ooo
ooooooooooooooo
ooo
ooooo oooo
ooo
oooo
ooooooo
ooo
ooooooo
ooooooo
oooooo
ooooooooooo
ooooooooo
o
oooooooo
oo
oo
oooooo
oooo
ooo
ooo
ooo
oooooo
ooooo
oo oooo
o
oo
oo
oo
ooooooooo oooooooooooooo
oooooo oooooooooo
o
ooo
ooo
oo
ooooooooooo ooooo
oo
ooo
oooo
oooooo
oo
oooooooo
oooooo
ooo
oooo
oooooo
oooooooooooo
ooo
ooooo
oooo
oo
ooo
ooooooo
ooo
o
oo
ooo
o
oo
ooo
oo
oooooooooooooo
ooo
o
oo
ooo
oooooooo
ooo
oo
ooooo
oo
oooooo
o
oo
oo
oo
ooo
ooo
ooooo
oooo
oooo
o
ooooooooo
ooooooooo
oooo
oo
oooo
o
ooooooooooooooo
oo
ooooo
o
oo
ooo
ooooo
oalcoholo
oooo
oooooo
ooo ooo
oo
ooo
oo oooo
ooooooooo
ooo
ooooo oooo
ooo
oooo
ooo o ooo
ooo
ooooooo
ooooooo
oooooo
oooo o oooooo
ooooooooo
ooooo oooo
oo
oo
oo oooo
o ooo
ooo
ooo
ooo
oooo
oo
ooooo
oooooo
o
oo
oo
oo
ooooooooooooooooo o ooooo
ooooooooo
ooooooo
o
ooo
ooooo
oooo o oooooo ooooo
oo
ooo
oo oo
ooo o
oo
oo
oooooooo
o oooo
o
ooo
oooo
o oooo o
oooooooooooo
ooo
ooooo
oooo
oo
ooo
ooooo oo
oooo
oo
ooo
o
oo
ooo
oo
oooo ooooo
ooooo
ooo
ooo
ooo
oooooooo
ooo
oooo ooo
oo
oooooo
o
oo
oo
oo
ooo
ooo
ooooo
oooo
oooo
o
ooooooooo
oo ooo
oooo
oooo
oo
oooo
o
o ooooo ooooooo oo
oo
ooooo
o
oo
ooo
o ooo o
o
100 160 220oo
oo
oo
oo
ooo
o
ooo
ooooo
oooo
o
ooo
oo
ooo
oo
ooo
oo
o
o
oo
ooo
o
oooo
o
ooo
oo
oo
ooo
o
oo
oooo
ooo
oooooo
ooooo
ooo
ooo
oo
oooo
ooo
oo
oo
oooo
o
o
ooo
ooo
o
ooo
oo
ooo
oo
oooooo
oo
oo
oo
ooo
ooo
o o
oo
oo
oo
oooo
oo
oo
oo
ooooo
o
oo
o
ooooo
o
oooooo
oo
oo
ooo
oo
o
oooooo
o
oooo
o
o
ooooo
ooo
oo
o
ooo
ooo
oo
o
ooo
oooo
ooooo
o
oooooooooo
o
o
oo
ooo
ooo
o
ooo
oo
ooo
ooo
o
ooooo
o
ooooo
oo
ooooo
oooo
oo
oooo
o
ooo
oooo
ooo
oo
ooo
o
oo
oo
ooo
o
oo
ooo
ooooo
ooo
ooooo
oo
ooo
oo
o
ooo
oo
ooo
oo
ooo
o
oooooo
o
oo
oo
oo
oo
o
o
o
oooo
o
oo
oooo
o
oo
oo
oooooo
ooo
o
ooo
oo
o
oo
oo
ooo
oo
o
ooo
oo
oooooooooo
o
oo
o
ooo
ooo
oooo
o
ooooo
oooo
oo
oo
oo
ooo
o
ooo
ooooo
oooo
o
ooo
oo
ooo
oo
ooo
oo
o
o
oo
ooo
o
ooooo
ooo
oo
oo
ooo
o
oo
oooo
ooo
oooooo
ooooo
ooo
ooo
oo
oooo
ooo
oo
oo
oooo
o
o
ooo
oo o
o
ooo
oo
ooo
oo
ooooo
o
oo
oo
oo
ooo
ooo
oo
oo
oo
oo
o
ooo
oo
oo
oo
oo
ooo
o
oo
o
ooooo
o
oooo
oo
oo
oo
ooo
oo
o
o
ooooo
ooooo
o
o
ooooo
ooo
oo
o
ooo
ooo
oo
o
ooo
oooo
ooooo
o
oooooooooo
o
o
oo
ooo
ooo
o
ooo
oo
ooo
ooo
o
ooooo
o
ooooo
oo
ooooo
oooo
oo
oooo
o
ooo
oooo
ooo
oo
ooo
o
oo
oo
ooo
o
oo
ooo
oooo o
ooo
ooooo
oo
ooo
oo
o
ooo
oo
ooo
oo
ooo
o
oooooo
o
oo
oo
oo
oo
o
o
o
oooo
o
oo
oooo
o
oo
oo
oooooo
ooo
o
ooooo
o
oo
oo
ooo
oo
o
ooo
oo
oooooooooo
o
oo
o
ooo
ooo
oooo
o
ooo
oo
oo
2 6 10 14oo
oo
oo
oo
ooo
o
ooo
ooooo
oooo
o
ooooo
ooo
oo
ooo
oo
o
o
oo
ooo
o
oooo
o
oo
o
oo
oo
ooo
o
oo
oooo
ooo
oooooo
ooooo
ooo
ooo
oo
oooo
ooo
oo
oo
oooo
o
o
ooo
ooo
o
ooo
oo
ooo
oo
oooooo
oo
oo
oo
ooo
ooo
oo
oo
oo
oo
oooo
oo
oo
oo
oo
ooo
o
oo
o
ooooo
o
oooooo
oo
oo
ooo
oo
o
oooooo
ooooo
o
o
oooo o
ooo
oo
o
ooo
ooo
oo
o
ooo
ooo
o
ooooo
o
ooo
oooooooo
o
oo
ooo
ooo
o
ooo
oo
ooo
ooo
o
ooooo
o
ooooo
oo
ooooo
oooo
oo
oooo
o
ooo
oo
oo
ooo
oo
ooo
o
oo
oo
ooo
o
oo
ooo
ooooo
ooo
ooooo
oo
ooo
oo
o
ooo
oo
ooo
oo
ooo
o
oooooo
o
oo
oo
oo
oo
o
o
o
oooo
o
oo
oooo
o
oo
oo
oooooo
ooo
o
ooooo
o
oo
oo
ooo
oo
o
ooo
oo
oooooooooo
o
oo
o
ooo
ooo
o
ooo
o
ooo
oo
oooo
oo
oo
oo
ooo
o
ooo
ooooo
oooo
o
ooooo
ooo
oo
ooo
oo
o
o
oo
ooo
o
oooo
o
ooo
oo
oo
ooo
o
oo
o oo o
ooo
oooo o o
oooo o
ooo
ooo
oo
oooo
ooo
oo
oo
oooo
o
o
ooo
oo o
o
ooo
oo
ooo
oo
oooo oo
oo
oo
oo
oo o
ooo
oo
oo
oo
oo
oooo
oo
oo
oo
oo
ooo
o
oo
o
ooooo
o
oooooo
oo
oo
ooo
oo
o
oooooo
ooo oo
o
o
oo o o o
ooo
oo
o
ooo
ooo
oo
o
ooo
oooo
ooooo
o
ooo
oooo oooo
o
oo
ooo
ooo
o
ooo
oo
ooo
ooo
o
ooooo
o
ooooo
oo
ooooo
oooo
oo
oooo
o
ooo
oooo
ooo
oo
ooo
o
oo
oo
ooo
o
oo
ooo
ooooo
oo o
oooo o
oo
ooo
oo
o
ooo
oo
oo o
oo
ooo
o
oooooo
o
oo
oo
oo
oo
o
oo
oooo
o
oo
oooo
o
oo
oo
ooooo o
oo o
o
ooo
oo
o
oo
oo
ooo
oo
o
o oo
o o
ooooo ooooo
o
oo
o
ooo
ooo
o
ooo
o
oo o
oo
oo
15 25 35 45oo
oo
o
o
oo
ooo
o
ooo
ooooo
oooo
o
ooo
oo
ooo
oo
ooo
oo
o
o
oo
ooo
o
oooo
o
ooo
oo
oo
ooo
o
oo
oooo
ooo
oooooo
ooo oo
ooo
ooo
oo
oooo
ooo
oo
oo
oooo
o
o
ooo
ooo
o
ooo
oo
ooo
oo
oooooo
oo
oo
oo
ooo
ooo
oo
oo
oooo
oooo
oo
oo
oo
ooooo
o
oo
o
ooooo
o
oooooo
oo
oo
ooo
oo
o
oooooo
oooooo
o
ooooo
ooo
oo
o
ooo
ooo
oo
o
ooo
oooo
ooooo
o
ooo
ooooooo
o
o
oo
ooo
ooo
o
ooo
oo
ooo
ooo
o
ooooo
o
ooooo
oo
ooooo
oooo
oo
oooo
o
ooo
oooo
ooo
oo
ooo
o
oo
oo
ooo
o
oo
ooo
ooooo
ooo
ooooo
oo
ooo
oo
o
ooo
oo
ooo
oo
ooo
o
oooooo
o
oo
oo
oo
oo
o
oo
oooo
o
oo
oooo
o
oo
oo
ooooooooo
o
ooooo
o
oo
oo
ooo
oo
o
ooo
oo
oooooooooo
o
oo
o
ooo
ooo
oooo
o
oo ooo
oooo
oo
oo
oo
ooo
o
ooo
ooooo
oooo
o
ooooo
ooo
oo
ooo
oo
o
o
oo
ooo
o
oooo
o
oo
o
oo
oo
ooo
o
oo
oooo
ooo
oooooo
ooooo
ooo
ooo
oo
oooo
ooo
oo
oo
oooo
o
o
ooo
ooo
o
ooo
oo
ooo
oo
oooooo
oo
oo
oo
ooo
ooo
oo
oo
oo
oo
oooo
oo
oo
oo
oo
ooo
o
oo
o
ooooo
o
oooooo
oo
oo
ooo
oo
o
oooooo
ooooo
o
o
ooooo
ooo
oo
o
ooo
ooo
oo
o
ooo
oooo
o oooo
o
ooo
ooooooo
o
o
oo
ooo
ooo
o
ooo
oo
ooo
ooo
o
ooooo
o
ooooo
oo
ooooo
oooo
oo
oooo
o
ooo
oooo
ooo
oo
ooo
o
oo
oo
ooo
o
oo
ooo
ooooo
ooo
ooooo
oo
ooo
oo
o
ooo
oo
ooo
oo
ooo
o
oooooo
o
oo
oo
oo
oo
o
oo
oooo
o
oo
oooo
o
oo
oo
oooooo
ooo
o
ooooo
o
oo
oo
ooo
oo
o
ooo
oo
oooooooooo
o
oo
o
ooo
ooo
o
ooo
o
ooo
oo
oo
20 40 60
20 40 60age
FIGURE 4.12. A scatterplot matrix of the South African heart disease data.
Each plot shows a pair of risk factors, and the cases and contro ls are color coded
(red is a case). The variable family history of heart disease ( famhist)is binary
(yes or no).

124 4. Linear Methods for Classiﬁcation
TABLE 4.3. Results from stepwise logistic regression ﬁt to South African heart
disease data.
Coeﬃcient Std. Error Zscore
(Intercept) −4.204 0 .498−8.45
tobacco 0.081 0 .026 3.16
ldl 0.168 0 .054 3.09
famhist 0.924 0 .223 4.14
age 0.044 0 .010 4.52
other correlated variables, they are no longer needed (and c an even get a
negative sign).
At this stage the analyst might do some model selection; ﬁnd a subset
of the variables that are suﬃcient for explaining their join t eﬀect on the
prevalence of chd. One way to proceed by is to drop the least signiﬁcant co-
eﬃcient, and reﬁt the model. This is done repeatedly until no further terms
can be dropped from the model. This gave the model shown in Tab le 4.3.
A better but more time-consuming strategy is to reﬁt each of t he models
with one variable removed, and then perform an analysis of deviance to
decide which variable to exclude. The residual deviance of a ﬁtted model
is minus twice its log-likelihood, and the deviance between two models is
the diﬀerence of their individual residual deviances (in an alogy to sums-of-
squares). This strategy gave the same ﬁnal model as above.
How does one interpret a coeﬃcient of 0 .081 (Std. Error = 0 .026) for
tobacco, for example? Tobacco is measured in total lifetime usage in kilo-
grams, with a median of 1 .0kg for the controls and 4 .1kg for the cases. Thus
an increase of 1kg in lifetime tobacco usage accounts for an i ncrease in the
odds of coronary heart disease of exp(0 .081) = 1.084 or 8.4%. Incorporat-
ing the standard error we get an approximate 95% conﬁdence in terval of
exp(0.081±2×0.026) = (1.03,1.14).
We return to these data in Chapter 5, where we see that some of t he
variables have nonlinear eﬀects, and when modeled appropri ately, are not
excluded from the model.
4.4.3 Quadratic Approximations and Inference
The maximum-likelihood parameter estimates ˆβsatisfy a self-consistency
relationship: they are the coeﬃcients of a weighted least sq uares ﬁt, where
the responses are
zi=xT
iˆβ+(yi−ˆpi)
ˆpi(1−ˆpi), (4.29)

4.4 Logistic Regression 125
and the weights are wi= ˆpi(1−ˆpi), both depending on ˆβitself. Apart from
providing a convenient algorithm, this connection with lea st squares has
more to oﬀer:
•The weighted residual sum-of-squares is the familiar Pears on chi-
square statistic
N/summationdisplay
i=1(yi−ˆpi)2
ˆpi(1−ˆpi), (4.30)
a quadratic approximation to the deviance.
•Asymptotic likelihood theory says that if the model is corre ct, then
ˆβis consistent (i.e., converges to the trueβ).
•A central limit theorem then shows that the distribution of ˆβcon-
verges toN(β,(XTWX)−1). This and other asymptotics can be de-
riveddirectlyfromtheweightedleastsquaresﬁtbymimicki ngnormal
theory inference.
•Model building can be costly for logistic regression models , because
each model ﬁtted requires iteration. Popular shortcuts are theRao
score test which tests for inclusion of a term, and the Wald test which
can be used to test for exclusion of a term. Neither of these re quire
iterative ﬁtting, and are based on the maximum-likelihood ﬁ t of the
current model. It turns out that both of these amount to addin g
or dropping a term from the weighted least squares ﬁt, using t he
sameweights. Such computations can be done eﬃciently, without
recomputing the entire weighted least squares ﬁt.
Software implementations can take advantage of these conne ctions. For
example, the generalized linear modeling software in R (whi ch includes lo-
gistic regression as part of the binomial family of models) e xploits them
fully.GLM(generalizedlinearmodel)objectscanbetreate daslinearmodel
objects, and all the tools available for linear models can be applied auto-
matically.
4.4.4L1Regularized Logistic Regression
TheL1penalty used in the lasso (Section 3.4.2) can be used for vari able
selection and shrinkage with any linear regression model. F or logistic re-
gression, we would maximize a penalized version of (4.20):
max
β0,β

N/summationdisplay
i=1/bracketleftig
yi(β0+βTxi)−log(1+eβ0+βTxi)/bracketrightig
−λp/summationdisplay
j=1|βj|

.(4.31)
As with the lasso, we typically do not penalize the intercept term, and stan-
dardize the predictors for the penalty to be meaningful. Cri terion (4.31) is

126 4. Linear Methods for Classiﬁcation
concave, and a solution can be found using nonlinear program ming meth-
ods (Koh et al., 2007, for example). Alternatively, using th e same quadratic
approximations that were used in the Newton algorithm in Sec tion 4.4.1,
we can solve (4.31) by repeated application of a weighted las so algorithm.
Interestingly, the score equations [see (4.24)] for the var iables with non-zero
coeﬃcients have the form
xT
j(y−p) =λ·sign(βj), (4.32)
which generalizes (3.58) in Section 3.4.4; the active varia bles are tied in
theirgeneralized correlation with the residuals.
Path algorithms such as LAR for lasso are more diﬃcult, becau se the
coeﬃcient proﬁles are piecewise smooth rather than linear. Nevertheless,
progress can be made using quadratic approximations.
******************************* ************** ****** *************************************************** ****************************************** **************************************************************************** *****************
0.0 0.5 1.0 1.5 2.00.0 0.2 0.4 0.6******************************* * ******************************************************************************************************************************************************************************************** *****************
******************************* ************** * ****************************************************************************************************************************************************************************** *****************
****************************** ********************************************************************************************************************************************************************************************** *****************
******************************* ************** ****** *************************************************** *************************************************** ************************ ******************************************* ************************************************ ************** ****** *************************************************** *************************************************** ************************ *************************** *************** * ********************************************************************************************************************************************************************************************************************************************* *****************
obesityalcoholsbptobaccoldlfamhistage1 2 4 5 6 7Coeﬃcients βj(λ)
||β(λ)||1
FIGURE 4.13. L1regularized logistic regression coeﬃcients for the South
African heart disease data, plotted as a function of the L1norm. The variables
were all standardized to have unit variance. The proﬁles are c omputed exactly at
each of the plotted points.
Figure 4.13 shows the L1regularization path for the South African
heart disease data of Section 4.4.2. This was produced using theRpackage
glmpath (Park and Hastie, 2007), which uses predictor–corrector methods
of convex optimization to identify the exact values of λat which the active
set of non-zero coeﬃcients changes (vertical lines in the ﬁg ure). Here the
proﬁles look almost linear; in other examples the curvature will be more
visible.
Coordinate descent methods (Section 3.8.6) are very eﬃcien t for comput-
ing the coeﬃcient proﬁles on a grid of values for λ. TheRpackageglmnet

4.4 Logistic Regression 127
(Friedman et al., 2010) can ﬁt coeﬃcient paths for very large logistic re-
gression problems eﬃciently (large in Norp). Their algorithms can exploit
sparsity in the predictor matrix X, which allows for even larger problems.
See Section 18.4 for more details, and a discussion of L1-regularized multi-
nomial models.
4.4.5 Logistic Regression or LDA?
In Section 4.3 we ﬁnd that the log-posterior odds between cla sskandK
are linear functions of x(4.9):
logPr(G=k|X=x)
Pr(G=K|X=x)= logπk
πK−1
2(µk+µK)TΣ−1(µk−µK)
+xTΣ−1(µk−µK)
=αk0+αT
kx. (4.33)
This linearity is a consequence of the Gaussian assumption f or the class
densities, as well as the assumption of a common covariance m atrix. The
linear logistic model (4.17) by construction has linear log its:
logPr(G=k|X=x)
Pr(G=K|X=x)=βk0+βT
kx. (4.34)
Itseemsthatthemodelsarethesame.Althoughtheyhaveexac tlythesame
form, the diﬀerence lies in the way the linear coeﬃcients are estimated. The
logistic regression model is more general, in that it makes l ess assumptions.
We can write the joint density ofXandGas
Pr(X,G=k) = Pr(X)Pr(G=k|X), (4.35)
where Pr(X) denotes the marginal density of the inputs X. For both LDA
and logistic regression, the second term on the right has the logit-linear
form
Pr(G=k|X=x) =eβk0+βT
kx
1+/summationtextK−1
ℓ=1eβℓ0+βT
ℓx, (4.36)
where we have again arbitrarily chosen the last class as the r eference.
The logistic regression model leaves the marginal density o fXas an arbi-
trary density function Pr( X), and ﬁts the parameters of Pr( G|X) by max-
imizing the conditional likelihood —the multinomial likelihood with proba-
bilities the Pr( G=k|X). Although Pr( X) is totally ignored, we can think
of this marginal density as being estimated in a fully nonpar ametric and
unrestricted fashion, using the empirical distribution fu nction which places
mass 1/Nat each observation.
With LDA we ﬁt the parameters by maximizing the full log-like lihood,
based on the joint density
Pr(X,G=k) =φ(X;µk,Σ)πk, (4.37)

128 4. Linear Methods for Classiﬁcation
whereφis the Gaussian density function. Standard normal theory le ads
easily to the estimates ˆ µk,ˆΣ, and ˆπkgiven in Section 4.3. Since the linear
parameters of the logistic form (4.33) are functions of the G aussian param-
eters, we get their maximum-likelihood estimates by pluggi ng in the corre-
sponding estimates. However, unlike in the conditional cas e, the marginal
density Pr( X) does play a role here. It is a mixture density
Pr(X) =K/summationdisplay
k=1πkφ(X;µk,Σ), (4.38)
which also involves the parameters.
What role can this additional component/restriction play? By relying
on the additional model assumptions, we have more informati on about the
parameters, and hence can estimate them more eﬃciently (low er variance).
If in fact the true fk(x) are Gaussian, then in the worst case ignoring this
marginal part of the likelihood constitutes a loss of eﬃcien cy of about 30%
asymptotically in the error rate (Efron, 1975). Paraphrasi ng: with 30%
more data, the conditional likelihood will do as well.
For example, observations far from the decision boundary (w hich are
down-weighted by logistic regression) play a role in estima ting the common
covariance matrix. This is not all good news, because it also means that
LDA is not robust to gross outliers.
From the mixture formulation, it is clear that even observat ions without
class labels have information about the parameters. Often i t is expensive
to generate class labels, but unclassiﬁed observations com e cheaply. By
relying on strong model assumptions, such as here, we can use both types
of information.
The marginal likelihood can be thought of as a regularizer, r equiring
in some sense that class densities be visiblefrom this marginal view. For
example, if the data in a two-class logistic regression mode l can be per-
fectly separated by a hyperplane, the maximum likelihood es timates of the
parameters are undeﬁned (i.e., inﬁnite; see Exercise 4.5). The LDA coeﬃ-
cients for the same data will be well deﬁned, since the margin al likelihood
will not permit these degeneracies.
In practice these assumptions are never correct, and often s ome of the
components of Xare qualitative variables. It is generally felt that logist ic
regression is a safer, more robust bet than the LDA model, rel ying on fewer
assumptions. It is our experience that the models give very s imilar results,
even when LDA is used inappropriately, such as with qualitat ive predictors.

4.5 Separating Hyperplanes 129
FIGURE 4.14. A toy example with two classes separable by a hyperplane. The
orange line is the least squares solution, which misclassiﬁes on e of the training
points. Also shown are two blue separating hyperplanes found by theperceptron
learning algorithm with diﬀerent random starts.
4.5 Separating Hyperplanes
We have seen that linear discriminant analysis and logistic regression both
estimate linear decision boundaries in similar but slightl y diﬀerent ways.
For the rest of this chapter we describe separating hyperpla ne classiﬁers.
These procedures construct linear decision boundaries tha t explicitly try
to separate the data into diﬀerent classes as well as possibl e. They provide
the basis for support vector classiﬁers, discussed in Chapt er 12. The math-
ematical level of this section is somewhat higher than that o f the previous
sections.
Figure 4.14 shows 20 data points in two classes in IR2. These data can be
separated by a linear boundary. Included in the ﬁgure (blue l ines) are two
of the inﬁnitely many possible separating hyperplanes . The orange line is
the least squares solution to the problem, obtained by regre ssing the−1/1
responseYonX(with intercept); the line is given by
{x:ˆβ0+ˆβ1x1+ˆβ2x2= 0}. (4.39)
This least squares solution does not do a perfect job in separ ating the
points, and makes one error. This is the same boundary found b y LDA,
in light of its equivalence with linear regression in the two -class case (Sec-
tion 4.3 and Exercise 4.2).
Classiﬁers such as (4.39), that compute a linear combinatio n of the input
featuresandreturnthesign,werecalled perceptrons intheengineeringliter-

130 4. Linear Methods for Classiﬁcation
x0x
β∗β0+βTx= 0
FIGURE 4.15. The linear algebra of a hyperplane (aﬃne set).
ature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations
for the neural network models of the 1980s and 1990s.
Beforewecontinue,letusdigressslightlyandreviewsomev ectoralgebra.
Figure 4.15 depicts a hyperplane or aﬃne setLdeﬁned by the equation
f(x) =β0+βTx= 0; since we are in IR2this is a line.
Here we list some properties:
1. For any two points x1andx2lying inL,βT(x1−x2) = 0, and hence
β∗=β/||β||is the vector normal to the surface of L.
2. For any point x0inL,βTx0=−β0.
3. The signed distance of any point xtoLis given by
β∗T(x−x0) =1
∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l(βTx+β0)
=1
||f′(x)||f(x). (4.40)
Hencef(x) is proportional to the signed distance from xto the hyperplane
deﬁned byf(x) = 0.
4.5.1 Rosenblatt’s Perceptron Learning Algorithm
Theperceptron learning algorithm tries to ﬁnd a separating hyperplane by
minimizing the distance of misclassiﬁed points to the decis ion boundary. If

4.5 Separating Hyperplanes 131
a responseyi= 1 is misclassiﬁed, then xT
iβ+β0<0, and the opposite for
a misclassiﬁed response with yi=−1. The goal is to minimize
D(β,β0) =−/summationdisplay
i∈Myi(xT
iβ+β0), (4.41)
whereMindexes the set of misclassiﬁed points. The quantity is non-
negative and proportional to the distance of the misclassiﬁ ed points to
the decision boundary deﬁned by βTx+β0= 0. The gradient (assuming
Mis ﬁxed) is given by
∂D(β,β0)
∂β=−/summationdisplay
i∈Myixi, (4.42)
∂D(β,β0)
∂β0=−/summationdisplay
i∈Myi. (4.43)
The algorithm in fact uses stochastic gradient descent to minimize this
piecewise linear criterion. This means that rather than com puting the sum
of the gradient contributions of each observation followed by a step in the
negative gradient direction, a step is taken after each obse rvation is visited.
Hence the misclassiﬁed observations are visited in some seq uence, and the
parameters βare updated via
/parenleftbigg
β
β0/parenrightbigg
←/parenleftbigg
β
β0/parenrightbigg
+ρ/parenleftbigg
yixi
yi/parenrightbigg
. (4.44)
Hereρis the learning rate, which in this case can be taken to be 1 wit hout
loss in generality. If the classes are linearly separable, i t can be shown that
the algorithm converges to a separating hyperplane in a ﬁnit e number of
steps (Exercise 4.6). Figure 4.14 shows two solutions to a to y problem, each
started at a diﬀerent random guess.
There are a number of problems with this algorithm, summariz ed in
Ripley (1996):
•When the data are separable, there are many solutions, and wh ich
one is found depends on the starting values.
•The “ﬁnite” number of steps can be very large. The smaller the gap,
the longer the time to ﬁnd it.
•When the data are not separable, the algorithm will not conve rge,
and cycles develop. The cycles can be long and therefore hard to
detect.
The second problem can often be eliminated by seeking a hyper plane not
in the original space, but in a much enlarged space obtained b y creating

132 4. Linear Methods for Classiﬁcation
many basis-function transformations of the original varia bles. This is anal-
ogous to driving the residuals in a polynomial regression pr oblem down
to zero by making the degree suﬃciently large. Perfect separ ation cannot
always be achieved: for example, if observations from two di ﬀerent classes
share the same input. It may not be desirable either, since th e resulting
model is likely to be overﬁt and will not generalize well. We r eturn to this
point at the end of the next section.
A rather elegant solution to the ﬁrst problem is to add additi onal con-
straints to the separating hyperplane.
4.5.2 Optimal Separating Hyperplanes
Theoptimal separating hyperplane separates the two classes and maximizes
the distance to the closest point from either class (Vapnik, 1996). Not only
does this provide a unique solution to the separating hyperp lane problem,
butbymaximizingthemarginbetweenthetwoclassesonthetr ainingdata,
this leads to better classiﬁcation performance on test data .
Weneedtogeneralizecriterion(4.41).Considertheoptimi zationproblem
max
β,β0,||β||=1M
subject toyi(xT
iβ+β0)≥M, i= 1,...,N.(4.45)
The set of conditions ensure that all the points are at least a signed
distanceMfrom the decision boundary deﬁned by βandβ0, and we seek
the largest such Mand associated parameters. We can get rid of the ||β||=
1 constraint by replacing the conditions with
1
||β||yi(xT
iβ+β0)≥M, (4.46)
(which redeﬁnes β0) or equivalently
yi(xT
iβ+β0)≥M||β||. (4.47)
Since for any βandβ0satisfying these inequalities, any positively scaled
multiple satisﬁes them too, we can arbitrarily set ||β||= 1/M. Thus (4.45)
is equivalent to
min
β,β01
2||β||2
subject toyi(xT
iβ+β0)≥1, i= 1,...,N.(4.48)
In light of (4.40), the constraints deﬁne an empty slab or mar gin around the
linear decision boundary of thickness 1 /||β||. Hence we choose βandβ0to
maximize its thickness. This is a convex optimization probl em (quadratic

4.5 Separating Hyperplanes 133
criterion with linear inequality constraints). The Lagran ge (primal) func-
tion, to be minimized w.r.t. βandβ0, is
LP=1
2||β||2−N/summationdisplay
i=1αi[yi(xT
iβ+β0)−1]. (4.49)
Setting the derivatives to zero, we obtain:
β=N/summationdisplay
i=1αiyixi, (4.50)
0 =N/summationdisplay
i=1αiyi, (4.51)
and substituting these in (4.49) we obtain the so-called Wol fe dual
LD=N/summationdisplay
i=1αi−1
2N/summationdisplay
i=1N/summationdisplay
k=1αiαkyiykxT
ixk
subject toαi≥0 andN/summationdisplay
i=1αiyi= 0. (4.52)
The solution is obtained by maximizing LDin the positive orthant, a sim-
pler convex optimization problem, for which standard softw are can be used.
In addition the solution must satisfy the Karush–Kuhn–Tuck er conditions,
which include (4.50), (4.51), (4.52) and
αi[yi(xT
iβ+β0)−1] = 0∀i. (4.53)
From these we can see that
•ifαi>0, thenyi(xT
iβ+β0) = 1, or in other words, xiis on the
boundary of the slab;
•ifyi(xT
iβ+β0)>1,xiis not on the boundary of the slab, and αi= 0.
From (4.50) we see that the solution vector βis deﬁned in terms of a linear
combination of the support points xi—those points deﬁned to be on the
boundary of the slab via αi>0. Figure 4.16 shows the optimal separating
hyperplane for our toy example; there are three support poin ts. Likewise,
β0is obtained by solving (4.53) for any of the support points.
The optimal separating hyperplane produces a function ˆf(x) =xTˆβ+ˆβ0
for classifying new observations:
ˆG(x) = signˆf(x). (4.54)
Although none of the training observations fall in the margi n (by con-
struction), this will not necessarily be the case for test ob servations. The

134 4. Linear Methods for Classiﬁcation
FIGURE 4.16. The same data as in Figure 4.14. The shaded region delineates
the maximum margin separating the two classes. There are thre e support points
indicated, which lie on the boundary of the margin, and the opt imal separating
hyperplane (blue line) bisects the slab. Included in the ﬁgure is t he boundary found
using logistic regression (red line), which is very close to the optimal separating
hyperplane (see Section 12.3.3).
intuition is that a large margin on the training data will lea d to good
separation on the test data.
The description of the solution in terms of support points se ems to sug-
gest that the optimal hyperplane focuses more on the points t hat count,
and is more robust to model misspeciﬁcation. The LDA solutio n, on the
other hand, depends on all of the data, even points far away fr om the de-
cision boundary. Note, however, that the identiﬁcation of t hese support
points required the use of all the data. Of course, if the clas ses are really
Gaussian, then LDA is optimal, and separating hyperplanes w ill pay a price
for focusing on the (noisier) data at the boundaries of the cl asses.
Included in Figure 4.16 is the logistic regression solution to this prob-
lem, ﬁt by maximum likelihood. Both solutions are similar in this case.
When a separating hyperplane exists, logistic regression w ill always ﬁnd
it, since the log-likelihood can be driven to 0 in this case (E xercise 4.5).
The logistic regression solution shares some other qualita tive features with
the separating hyperplane solution. The coeﬃcient vector i s deﬁned by a
weighted least squares ﬁt of a zero-mean linearized respons e on the input
features, and the weights are larger for points near the deci sion boundary
than for those further away.
When the data are not separable, there will be no feasible sol ution to
this problem, and an alternative formulation is needed. Aga in one can en-
large the space using basis transformations, but this can le ad to artiﬁcial

Exercises 135
separation through over-ﬁtting. In Chapter 12 we discuss a m ore attractive
alternative known as the support vector machine , which allows for overlap,
but minimizes a measure of the extent of this overlap.
Bibliographic Notes
Good general texts on classiﬁcation include Duda et al. (200 0), Hand
(1981), McLachlan (1992) and Ripley (1996). Mardia et al. (1 979) have
a concise discussion of linear discriminant analysis. Mich ie et al. (1994)
compare a large number of popular classiﬁers on benchmark da tasets. Lin-
ear separating hyperplanes are discussed in Vapnik (1996). Our account of
the perceptron learning algorithm follows Ripley (1996).
Exercises
Ex. 4.1Show how to solve the generalized eigenvalue problem max aTBa
subject toaTWa= 1 by transforming to a standard eigenvalue problem.
Ex. 4.2Suppose we have features x∈IRp, a two-class response, with class
sizesN1,N2, and the target coded as −N/N1,N/N2.
(a) Show that the LDA rule classiﬁes to class 2 if
xTˆΣ−1(ˆµ2−ˆµ1)>1
2(ˆµ2+ ˆµ1)TˆΣ−1(ˆµ2−ˆµ1)−log(N2/N1),
and class 1 otherwise.
(b) Consider minimization of the least squares criterion
N/summationdisplay
i=1(yi−β0−xT
iβ)2. (4.55)
Show that the solution ˆβsatisﬁes
/bracketleftig
(N−2)ˆΣ+NˆΣB/bracketrightig
β=N(ˆµ2−ˆµ1) (4.56)
(after simpliﬁcation), where ˆΣB=N1N2
N2(ˆµ2−ˆµ1)(ˆµ2−ˆµ1)T.
(c) Hence show that ˆΣBβis in the direction (ˆ µ2−ˆµ1) and thus
ˆβ∝ˆΣ−1(ˆµ2−ˆµ1). (4.57)
Therefore the least-squares regression coeﬃcient is ident ical to the
LDA coeﬃcient, up to a scalar multiple.

136 4. Linear Methods for Classiﬁcation
(d) Show that this result holds for any (distinct) coding of t he two classes.
(e) Find the solution ˆβ0(up to the same scalar multiple as in (c), and
hence the predicted value ˆf(x) =ˆβ0+xTˆβ. Consider the following
rule: classify to class 2 if ˆf(x)>0 and class 1 otherwise. Show this is
not the same as the LDA rule unless the classes have equal numb ers
of observations.
(Fisher, 1936; Ripley, 1996)
Ex. 4.3Suppose we transform the original predictors XtoˆYvia linear
regression. In detail, let ˆY=X(XTX)−1XTY=XˆB, where Yis the
indicator response matrix. Similarly for any input x∈IRp, we get a trans-
formed vector ˆ y=ˆBTx∈IRK. Show that LDA using ˆYis identical to
LDA in the original space.
Ex. 4.4Consider the multilogit model with Kclasses (4.17). Let βbe the
(p+ 1)(K−1)-vector consisting of all the coeﬃcients. Deﬁne a suitabl y
enlarged version of the input vector xto accommodate this vectorized co-
eﬃcient matrix. Derive the Newton-Raphson algorithm for ma ximizing the
multinomial log-likelihood, and describe how you would imp lement this
algorithm.
Ex. 4.5Consider a two-class logistic regression problem with x∈IR. Char-
acterize the maximum-likelihood estimates of the slope and intercept pa-
rameterifthesample xiforthetwoclassesareseparatedbyapoint x0∈IR.
Generalize this result to (a) x∈IRp(see Figure 4.16), and (b) more than
two classes.
Ex. 4.6Suppose we have Npointsxiin IRpin general position, with class
labelsyi∈{−1,1}. Prove that the perceptron learning algorithm converges
to a separating hyperplane in a ﬁnite number of steps:
(a) Denote a hyperplane by f(x) =βT
1x+β0= 0, or in more compact
notationβTx∗= 0, where x∗= (x,1) andβ= (β1,β0). Letzi=
x∗
i/||x∗
i||. Show that separability implies the existence of a βsepsuch
thatyiβT
sepzi≥1∀i
(b)Givenacurrent βold,theperceptronalgorithmidentiﬁesapoint zithat
is misclassiﬁed, and produces the update βnew←βold+yizi. Show
that||βnew−βsep||2≤||βold−βsep||2−1, and hence that the algorithm
converges to a separating hyperplane in no more than ||βstart−βsep||2
steps (Ripley, 1996).
Ex. 4.7Consider the criterion
D∗(β,β0) =−N/summationdisplay
i=1yi(xT
iβ+β0), (4.58)

Exercises 137
a generalization of (4.41) where we sum over all the observat ions. Consider
minimizing D∗subject to||β||= 1. Describe this criterion in words. Does
it solve the optimal separating hyperplane problem?
Ex. 4.8Consider the multivariate Gaussian model X|G=k∼N(µk,Σ),
with the additional restriction that rank {µk}K
1=L <max(K−1,p).
Derive the constrained MLEs for the µkandΣ. Show that the Bayes clas-
siﬁcation rule is equivalent to classifying in the reduced s ubspace computed
by LDA (Hastie and Tibshirani, 1996b).
Ex. 4.9Write a computer program to perform a quadratic discriminan t
analysis by ﬁtting a separate Gaussian model per class. Try i t out on the
vowel data, and compute the misclassiﬁcation error for the t est data. The
datacanbefoundinthebookwebsite www-stat.stanford.edu/ElemStatLearn .

138 4. Linear Methods for Classiﬁcation

This is page 139
Printer: Opaque this
5
Basis Expansions and Regularization
5.1 Introduction
We have already made use of models linear in the input feature s, both for
regression and classiﬁcation. Linear regression, linear d iscriminant analysis,
logistic regression and separating hyperplanes all rely on a linear model.
It is extremely unlikely that the true function f(X) is actually linear in
X. In regression problems, f(X) = E(Y|X) will typically be nonlinear and
nonadditive in X, and representing f(X) by a linear model is usually a con-
venient, and sometimes a necessary, approximation. Conven ient because a
linear model is easy to interpret, and is the ﬁrst-order Tayl or approxima-
tion tof(X). Sometimes necessary, because with Nsmall and/or plarge,
a linear model might be all we are able to ﬁt to the data without overﬁt-
ting. Likewise in classiﬁcation, a linear, Bayes-optimal d ecision boundary
implies that some monotone transformation of Pr( Y= 1|X) is linear in X.
This is inevitably an approximation.
In this chapter and the next we discuss popular methods for mo ving
beyond linearity. The core idea in this chapter is to augment /replace the
vector of inputs Xwith additional variables, which are transformations of
X, and then use linear models in this new space of derived input features.
Denote by hm(X) : IRp∝ma√sto→IR themth transformation of X,m=
1,...,M. We then model
f(X) =M/summationdisplay
m=1βmhm(X), (5.1)

140 5. Basis Expansions and Regularization
alinear basis expansion inX. The beauty of this approach is that once the
basis functions hmhave been determined, the models are linear in these
new variables, and the ﬁtting proceeds as before.
Some simple and widely used examples of the hmare the following:
•hm(X) =Xm, m= 1,...,precovers the original linear model.
•hm(X) =X2
jorhm(X) =XjXkallowsustoaugmenttheinputswith
polynomial terms to achieve higher-order Taylor expansion s. Note,
however, that the number of variables grows exponentially i n the de-
gree of the polynomial. A full quadratic model in pvariables requires
O(p2) square and cross-product terms, or more generally O(pd) for a
degree-dpolynomial.
•hm(X) = log(Xj),/radicalbig
Xj,...permits other nonlinear transformations
of single inputs. More generally one can use similar functio ns involv-
ing several inputs, such as hm(X) =||X||.
•hm(X) =I(Lm≤Xk<Um), an indicator for a region of Xk. By
breaking the range of Xkup intoMksuch nonoverlapping regions
results in a model with a piecewise constant contribution fo rXk.
Sometimestheproblemathandwillcallforparticularbasis functionshm,
suchaslogarithmsorpowerfunctions.Moreoften,however, weusethebasis
expansions as a device to achieve more ﬂexible representati ons forf(X).
Polynomials are an example of the latter, although they are l imited by
their global nature—tweaking the coeﬃcients to achieve a fun ctional form
in one region can cause the function to ﬂap about madly in remo te regions.
In this chapter we consider more useful families of piecewise-polynomials
andsplinesthat allow for local polynomial representations. We also di scuss
thewaveletbases, especially useful for modeling signals and images. T hese
methods produce a dictionaryDconsisting of typically a very large number
|D|of basis functions, far more than we can aﬀord to ﬁt to our data . Along
with the dictionary we require a method for controlling the c omplexity
of our model, using basis functions from the dictionary. The re are three
common approaches:
•Restriction methods, where we decide before-hand to limit t he class
of functions. Additivity is an example, where we assume that our
model has the form
f(X) =p/summationdisplay
j=1fj(Xj)
=p/summationdisplay
j=1Mj/summationdisplay
m=1βjmhjm(Xj). (5.2)

5.2 Piecewise Polynomials and Splines 141
The size of the model is limited by the number of basis functio nsMj
used for each component function fj.
•Selection methods, which adaptively scan the dictionary an d include
only those basis functions hmthat contribute signiﬁcantly to the ﬁt of
the model. Here the variable selection techniques discusse d in Chap-
ter 3 are useful. The stagewise greedy approaches such as CAR T,
MARS and boosting fall into this category as well.
•Regularization methods where we use the entire dictionary b ut re-
strict the coeﬃcients. Ridge regression is a simple example of a regu-
larization approach, while the lasso is both a regularizati on and selec-
tion method. Here we discuss these and more sophisticated me thods
for regularization.
5.2 Piecewise Polynomials and Splines
We assume until Section 5.7 that Xis one-dimensional. A piecewise poly-
nomial function f(X) is obtained by dividing the domain of Xinto contigu-
ous intervals, and representing fby a separate polynomial in each interval.
Figure 5.1 shows two simple piecewise polynomials. The ﬁrst is piecewise
constant, with three basis functions:
h1(X) =I(X <ξ 1), h2(X) =I(ξ1≤X <ξ 2), h3(X) =I(ξ2≤X).
Since these are positive over disjoint regions, the least sq uares estimate of
the modelf(X) =/summationtext3
m=1βmhm(X) amounts to ˆβm=¯Ym, the mean of Y
in themth region.
The top right panel shows a piecewise linear ﬁt. Three additi onal basis
functions are needed: hm+3=hm(X)X, m= 1,...,3. Except in special
cases, we would typically prefer the third panel, which is al so piecewise
linear, but restricted to be continuous at the two knots. The se continu-
ity restrictions lead to linear constraints on the paramete rs; for example,
f(ξ−
1) =f(ξ+
1) implies that β1+ξ1β4=β2+ξ1β5. In this case, since there
are two restrictions, we expect to get backtwo parameters, leaving four free
parameters.
A more direct way to proceed in this case is to use a basis that i ncorpo-
rates the constraints:
h1(X) = 1, h2(X) =X, h 3(X) = (X−ξ1)+, h4(X) = (X−ξ2)+,
wheret+denotes the positive part. The function h3is shown in the lower
right panel of Figure 5.1. We often prefer smoother function s, and these
can be achieved by increasing the order of the local polynomi al. Figure 5.2
shows a series of piecewise-cubic polynomials ﬁt to the same data, with

142 5. Basis Expansions and Regularization
OO
OOO
O
OO
O
OO
O
OO
OO
OO
O
OOO
O
O
OOO
O
OOO
OOO
O
OOO
O
OOOO
O
O
OO
O
OOPiecewise Constant
OO
OOO
O
OO
O
OO
O
OO
OO
OO
O
OOO
O
O
OOO
O
OOO
OOO
O
OOO
O
OOOO
O
O
OO
O
OOPiecewise Linear
OO
OOO
O
OO
O
OO
O
OO
OO
OO
O
OOO
O
O
OOO
O
OOO
OOO
O
OOO
O
OOOO
O
O
OO
O
OOContinuous Piecewise Linear Piecewise-linear Basis Function
•
•••
••••
••
•• • ••
••••
••
•••
•
•••
•
•••
••
•••
• ••••
• ••
•
••
••
ξ1 ξ1ξ1 ξ1
ξ2 ξ2ξ2 ξ2
(X−ξ1)+
FIGURE 5.1. The top left panel shows a piecewise constant function ﬁt to so me
artiﬁcial data. The broken vertical lines indicate the posit ions of the two knots
ξ1andξ2. The blue curve represents the true function, from which the d ata were
generated with Gaussian noise. The remaining two panels show p iecewise lin-
ear functions ﬁt to the same data—the top right unrestricted , and the lower left
restricted to be continuous at the knots. The lower right pane l shows a piecewise–
linear basis function, h3(X) = (X−ξ1)+, continuous at ξ1. The black points
indicate the sample evaluations h3(xi), i= 1,...,N.

5.2 Piecewise Polynomials and Splines 143
OO
OOO
OOO
O
OOO
OO
OO
OO
O
OOO
O
O
OOO
O
OOO
OOO
O
OOO
O
OOOOO
O
OO
OOODiscontinuous
OO
OOO
OOO
O
OOO
OO
OO
OO
O
OOO
O
O
OOO
O
OOO
OOO
O
OOO
O
OOOOO
O
OO
OOOContinuous
OO
OOO
OOO
O
OOO
OO
OO
OO
O
OOO
O
O
OOO
O
OOO
OOO
O
OOO
O
OOOOO
O
OO
OOOContinuous First Derivative
OO
OOO
OOO
O
OOO
OO
OO
OO
O
OOO
O
O
OOO
O
OOO
OOO
O
OOO
O
OOOOO
O
OO
OOOContinuous Second DerivativePiecewise Cubic Polynomials
ξ1 ξ1ξ1 ξ1
ξ2 ξ2ξ2 ξ2
FIGURE 5.2. A series of piecewise-cubic polynomials, with increasing ord ers of
continuity.
increasing orders of continuity at the knots. The function i n the lower
right panel is continuous, and has continuous ﬁrst and secon d derivatives
at the knots. It is known as a cubic spline . Enforcing one more order of
continuity would lead to a global cubic polynomial. It is not hard to show
(Exercise 5.1) that the following basis represents a cubic s pline with knots
atξ1andξ2:
h1(X) = 1, h3(X) =X2, h5(X) = (X−ξ1)3
+,
h2(X) =X, h 4(X) =X3, h6(X) = (X−ξ2)3
+.(5.3)
Therearesixbasisfunctionscorrespondingtoasix-dimens ionallinearspace
of functions. A quick check conﬁrms the parameter count: (3 r egions)×(4
parameters per region) −(2 knots)×(3 constraints per knot)= 6.

144 5. Basis Expansions and Regularization
More generally, an order- Mspline with knots ξj, j= 1,...,Kis a
piecewise-polynomial of order M, and has continuous derivatives up to
orderM−2. A cubic spline has M= 4. In fact the piecewise-constant
function in Figure 5.1 is an order-1 spline, while the contin uous piece-
wise linear function is an order-2 spline. Likewise the gene ral form for the
truncated-power basis set would be
hj(X) =Xj−1, j= 1,...,M,
hM+ℓ(X) = (X−ξℓ)M−1
+, ℓ= 1,...,K.
It is claimed that cubic splines are the lowest-order spline for which the
knot-discontinuity is not visible to the human eye. There is seldom any
good reason to go beyond cubic-splines, unless one is intere sted in smooth
derivatives. In practice the most widely used orders are M= 1,2 and 4.
These ﬁxed-knot splines are also known as regression splines . One needs
to select the order of the spline, the number of knots and thei r placement.
One simple approach is to parameterize a family of splines by the number
of basis functions or degrees of freedom, and have the observ ationsxide-
termine the positions of the knots. For example, the express ionbs(x,df=7)
inRgenerates a basis matrix of cubic-spline functions evaluat ed at theN
observations in x, with the 7−3 = 41interior knots at the appropriate per-
centilesof x(20,40,60and80th.)Onecanbemoreexplicit,however; bs(x,
degree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,
with three interior knots, and returns an N×4 matrix.
Sincethespaceofsplinefunctionsofaparticularorderand knotsequence
is a vector space, there are many equivalent bases for repres enting them
(just as there are for ordinary polynomials.) While the trun cated power
basis is conceptually simple, it is not too attractive numer ically: powers of
large numbers can lead to severe rounding problems. The B-spline basis,
described in the Appendix to this chapter, allows for eﬃcien t computations
even when the number of knots Kis large.
5.2.1 Natural Cubic Splines
We know that the behavior of polynomials ﬁt to data tends to be erratic
near the boundaries, and extrapolation can be dangerous. Th ese problems
are exacerbated with splines. The polynomials ﬁt beyond the boundary
knots behave even more wildly than the corresponding global polynomials
in that region. This can be conveniently summarized in terms of the point-
wise variance of spline functions ﬁt by least squares (see th e example in the
next section for details on these variance calculations). F igure 5.3 compares
1A cubic spline with four knots is eight-dimensional. The bs()function omits by
default the constant term in the basis, since terms like this are typical ly included with
other terms in the model.

5.2 Piecewise Polynomials and Splines 145
XPointwise Variances
0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5 0.6•
••
•
••
•••••••••••••• •••• ••••••••••••••••••••••••••
•••••
•••••••••••••••••••••••••••••••••••••••••••
•
••
•
••••••••••••••••••••••••••
•
••••••••••••••••••
•••••••••••••••••••• ••••••••••••••••••••••••• ••••Global Linear
Global Cubic Polynomial
Cubic Spline - 2 knots
Natural Cubic Spline - 6 knots
FIGURE 5.3. Pointwise variance curves for four diﬀerent models, with Xcon-
sisting of 50points drawn at random from U[0,1], and an assumed error model
with constant variance. The linear and cubic polynomial ﬁts ha ve two and four
degrees of freedom, respectively, while the cubic spline and na tural cubic spline
each have six degrees of freedom. The cubic spline has two knot s at0.33and0.66,
while the natural spline has boundary knots at 0.1and0.9, and four interior knots
uniformly spaced between them.
the pointwise variances for a variety of diﬀerent models. Th e explosion of
the variance near the boundaries is clear, and inevitably is worst for cubic
splines.
Anatural cubic spline adds additional constraints, namely that the func-
tion is linear beyond the boundary knots. This frees up four d egrees of
freedom (two constraints each in both boundary regions), wh ich can be
spent more proﬁtably by sprinkling more knots in the interio r region. This
tradeoﬀ is illustrated in terms of variance in Figure 5.3. Th ere will be a
price paid in bias near the boundaries, but assuming the func tion is lin-
ear near the boundaries (where we have less information anyw ay) is often
considered reasonable.
A natural cubic spline with Kknots is represented by Kbasis functions.
One can start from a basis for cubic splines, and derive the re duced ba-
sis by imposing the boundary constraints. For example, star ting from the
truncated power series basis described in Section 5.2, we ar rive at (Exer-
cise 5.4):
N1(X) = 1, N2(X) =X, N k+2(X) =dk(X)−dK−1(X),(5.4)

146 5. Basis Expansions and Regularization
where
dk(X) =(X−ξk)3
+−(X−ξK)3
+
ξK−ξk. (5.5)
Each of these basis functions can be seen to have zero second a nd third
derivative for X≥ξK.
5.2.2 Example: South African Heart Disease (Continued)
In Section 4.4.2 we ﬁt linear logistic regression models to t he South African
heart disease data. Here we explore nonlinearities in the fu nctions using
natural splines. The functional form of the model is
logit[Pr(chd|X)] =θ0+h1(X1)Tθ1+h2(X2)Tθ2+···+hp(Xp)Tθp,(5.6)
where each of the θjare vectors of coeﬃcients multiplying their associated
vector of natural spline basis functions hj.
We use four natural spline bases for each term in the model. Fo r example,
withX1representing sbp,h1(X1) is a basis consisting of four basis func-
tions. This actually implies three rather than two interior knots (chosen at
uniform quantiles of sbp), plus two boundary knots at the extremes of the
data, since we exclude the constant term from each of the hj.
Sincefamhist is a two-level factor, it is coded by a simple binary or
dummy variable, and is associated with a single coeﬃcient in the ﬁt of the
model.
More compactly we can combine all pvectors of basis functions (and
the constant term) into one big vector h(X), and then the model is simply
h(X)Tθ, with total number of parameters df = 1 +/summationtextp
j=1dfj, the sum of
the parameters in each component term. Each basis function i s evaluated
at each of the Nsamples, resulting in a N×df basis matrix H. At this
point the model is like any other linear logistic model, and t he algorithms
described in Section 4.4.1 apply.
We carried out a backward stepwise deletion process, droppi ng terms
from this model while preserving the group structure of each term, rather
than dropping one coeﬃcient at a time. The AIC statistic (Sec tion 7.5) was
used to drop terms, and all the terms remaining in the ﬁnal mod el would
cause AIC to increase if deleted from the model (see Table 5.1 ). Figure 5.4
shows a plot of the ﬁnal model selected by the stepwise regres sion. The
functions displayed are ˆfj(Xj) =hj(Xj)Tˆθjfor each variable Xj. The
covariance matrix Cov( ˆθ) =Σis estimated by ˆΣ= (HTWH)−1, whereW
is the diagonal weight matrix from the logistic regression. Hencevj(Xj) =
Var[ˆfj(Xj)] =hj(Xj)TˆΣjjhj(Xj) is the pointwise variance function of ˆfj,
whereCov( ˆθj) =ˆΣjjistheappropriatesub-matrixof ˆΣ.Theshadedregion
in each panel is deﬁned by ˆfj(Xj)±2/radicalbig
vj(Xj).
The AIC statistic is slightly more generous than the likelih ood-ratio test
(deviance test). Both sbpandobesityare included in this model, while

5.2 Piecewise Polynomials and Splines 147
100 120 140 160 180 200 220-2 0 2 4
0 5 10 15 20 25 300 2 4 6 8
2 4 6 8 10 12 14-4 -2 0 2 4
-4 -2 0 2 4
Absent Present
15 20 25 30 35 40 45-2 0 2 4 6
20 30 40 50 60-6 -4 -2 0 2ˆf(sbp)
sbp
ˆf(tobacco)
tobaccoˆf(ldl)
ldlˆf(obesity)
obesity
ˆf(age)
ageˆf(famhist)
famhist
FIGURE 5.4. Fitted natural-spline functions for each of the terms in the ﬁn al
model selected by the stepwise procedure. Included are pointw ise standard-error
bands. The rug plot at the base of each ﬁgure indicates the location of each of the
sample values for that variable (jittered to break ties).

148 5. Basis Expansions and Regularization
TABLE 5.1. Final logistic regression model, after stepwise deletion of na tural
splines terms. The column labeled “LRT” is the likelihood-ratio te st statistic when
that term is deleted from the model, and is the change in devianc e from the full
model (labeled “none”).
Terms Df Deviance AIC LRT P-value
none 458.09 502.09
sbp4 467.16 503.16 9.076 0.059
tobacco 4 470.48 506.48 12.387 0.015
ldl4 472.39 508.39 14.307 0.006
famhist 1 479.44 521.44 21.356 0.000
obesity 4 466.24 502.24 8.147 0.086
age4 481.86 517.86 23.768 0.000
they were not in the linear model. The ﬁgure explains why, sin ce their
contributions are inherently nonlinear. These eﬀects at ﬁr st may come as
a surprise, but an explanation lies in the nature of the retro spective data.
These measurements were made sometime after the patients su ﬀered a
heart attack, and in many cases they had already beneﬁted fro m a healthier
diet and lifestyle, hence the apparent increase in risk at low values for
obesityandsbp. Table 5.1 shows a summary of the selected model.
5.2.3 Example: Phoneme Recognition
In this example we use splines to reduce ﬂexibility rather th an increase it;
the application comes under the general heading of functional modeling. In
the top panel of Figure 5.5 are displayed a sample of 15 log-pe riodograms
for each of the two phonemes “aa” and “ao” measured at 256 freq uencies.
The goal is to use such data to classify a spoken phoneme. Thes e two
phonemes were chosen because they are diﬃcult to separate.
The input feature is a vector xof length 256, which we can think of as
a vector of evaluations of a function X(f) over a grid of frequencies f. In
reality there is a continuous analog signal which is a functi on of frequency,
and we have a sampled version of it.
The gray lines in the lower panel of Figure 5.5 show the coeﬃci ents of
a linear logistic regression model ﬁt by maximum likelihood to a training
sample of 1000 drawn from the total of 695 “aa”s and 1022 “ao”s . The
coeﬃcients are also plotted as a function of frequency, and i n fact we can
think of the model in terms of its continuous counterpart
logPr(aa|X)
Pr(ao|X)=/integraldisplay
X(f)β(f)df, (5.7)

5.2 Piecewise Polynomials and Splines 149
FrequencyLog-periodogram
0 50 100 150 200 2500 5 10 15 20 25Phoneme Examples
aa
ao
FrequencyLogistic Regression Coefficients
0 50 100 150 200 250-0.4 -0.2 0.0 0.2 0.4Phoneme Classification: Raw and Restricted Logistic Regression
FIGURE 5.5. The top panel displays the log-periodogram as a function of fre -
quency for 15examples each of the phonemes “aa” and “ao” sampled from a total
of695“aa”s and 1022“ao”s. Each log-periodogram is measured at 256uniformly
spaced frequencies. The lower panel shows the coeﬃcients (as a function of fre-
quency) of a logistic regression ﬁt to the data by maximum likeli hood, using the
256log-periodogram values as inputs. The coeﬃcients are restric ted to be smooth
in the red curve, and are unrestricted in the jagged gray curv e.

150 5. Basis Expansions and Regularization
which we approximate by
256/summationdisplay
j=1X(fj)β(fj) =256/summationdisplay
j=1xjβj. (5.8)
The coeﬃcients compute a contrast functional, and will have appreciable
values in regions of frequency where the log-periodograms d iﬀer between
the two classes.
The gray curves are very rough. Since the input signals have f airly strong
positive autocorrelation, this results in negative autoco rrelation in the co-
eﬃcients. In addition the sample size eﬀectively provides o nly four obser-
vations per coeﬃcient.
Applications such as this permit a natural regularization. We force the
coeﬃcientstovarysmoothlyasafunctionoffrequency.Ther edcurveinthe
lower panel of Figure 5.5 shows such a smooth coeﬃcient curve ﬁt to these
data.Weseethatthelowerfrequenciesoﬀerthemostdiscrim inatorypower.
Not only does the smoothing allow easier interpretation of t he contrast, it
also produces a more accurate classiﬁer:
RawRegularized
Training error 0.080 0.185
Test error 0.255 0.158
The smooth red curve was obtained through a very simple use of natural
cubic splines. We can represent the coeﬃcient function as an expansion of
splinesβ(f) =/summationtextM
m=1hm(f)θm. In practice this means that β=Hθwhere,
His ap×Mbasis matrix of natural cubic splines, deﬁned on the set of
frequencies. Here we used M= 12 basis functions, with knots uniformly
placed over the integers 1 ,2,...,256 representing the frequencies. Since
xTβ=xTHθ, we can simply replace the input features xby their ﬁltered
versionsx∗=HTx, and ﬁtθby linear logistic regression on the x∗. The
red curve is thus ˆβ(f) =h(f)Tˆθ.
5.3 Filtering and Feature Extraction
In the previous example, we constructed a p×Mbasis matrix H, and then
transformed our features xinto new features x∗=HTx. These ﬁltered
versions of the features were then used as inputs into a learn ing procedure:
in the previous example, this was linear logistic regressio n.
Preprocessing of high-dimensional features is a very gener al and pow-
erful method for improving the performance of a learning alg orithm. The
preprocessing need not be linear as it was above, but can be a g eneral

5.4 Smoothing Splines 151
(nonlinear) function of the form x∗=g(x). The derived features x∗can
then be used as inputs into any (linear or nonlinear) learnin g procedure.
Forexample,forsignalorimagerecognitionapopularappro achistoﬁrst
transform the raw features via a wavelet transform x∗=HTx(Section 5.9)
and then use the features x∗as inputs into a neural network (Chapter 11).
Wavelets are eﬀective in capturing discrete jumps or edges, and the neural
network is a powerful tool for constructing nonlinear funct ions of these
features for predicting the target variable. By using domai n knowledge
to construct appropriate features, one can often improve up on a learning
method that has only the raw features xat its disposal.
5.4 Smoothing Splines
Here we discuss a spline basis method that avoids the knot sel ection prob-
lem completely by using a maximal set of knots. The complexit y of the ﬁt
is controlled by regularization. Consider the following pr oblem: among all
functionsf(x) with two continuous derivatives, ﬁnd one that minimizes th e
penalized residual sum of squares
RSS(f,λ) =N/summationdisplay
i=1{yi−f(xi)}2+λ/integraldisplay
{f′′(t)}2dt, (5.9)
whereλis a ﬁxed smoothing parameter . The ﬁrst term measures closeness
to the data, while the second term penalizes curvature in the function, and
λestablishes a tradeoﬀ between the two. Two special cases are :
λ= 0 :fcan be any function that interpolates the data.
λ=∞:the simple least squares line ﬁt, since no second derivative can
be tolerated.
Thesevaryfromveryroughtoverysmooth,andthehopeisthat λ∈(0,∞)
indexes an interesting class of functions in between.
The criterion (5.9) is deﬁned on an inﬁnite-dimensional fun ction space—
in fact, a Sobolev space of functions for which the second ter m is deﬁned.
Remarkably, it can be shown that (5.9) has an explicit, ﬁnite -dimensional,
unique minimizer which is a natural cubic spline with knots a t the unique
values of the xi, i= 1,...,N(Exercise 5.7). At face value it seems that
the family is still over-parametrized, since there are as ma ny asNknots,
which implies Ndegrees of freedom. However, the penalty term translates
to a penalty on the spline coeﬃcients, which are shrunk some o f the way
toward the linear ﬁt.
Since the solution is a natural spline, we can write it as
f(x) =N/summationdisplay
j=1Nj(x)θj, (5.10)

152 5. Basis Expansions and Regularization
AgeRelative Change in Spinal BMD
10 15 20 25-0.05 0.0 0.05 0.10 0.15 0.20••
••
••
••
•
••
••
••
••
•
•
•
••••
••
•
••
•
•
••
•••
••
•••
•••
•
•
•
••
•
••••
•••••
••
•••••
•
•
••
•••
••
•
••
•••
•
••
••••
•
•••
••
•••••
•
••
••
•
•
••••
• ••• ••••
••
••
•••
•
••
•••••
•
•••••
••
•••
•
• •••
•••
••••
••
••••
• •
••••
••• •
••••
•
•••
•
••••••
••
•
••
•
•
••
••
•
••
•
• •••
•
•
••••••
•
••
••
••••
•
••
••
•
••••
••
•
•••
••
•
•••
•
• •••
• ••
•••••
•
••
•••
•
••••
••
•
••
•••
•
•••
•
••
•••••
••
•
••••••
•
•
•••
• ••
•••••••
•
•••
••
•
•••
•
•••
••
••
••
••
•• ••
•
••••
•
•
•••
••
• •
•••
•
•••
• •
••
•
•••
••••
•
••••
••
•
••
••••
••••
•
••
•
•
••••
•
••
•
••••
• ••
•
••••
•
••
•••
•
••
•••
•
•
••
••
•
•
••
••
••
•••
•••
•
••
•
•••
•
•
•••
••
••
••
•
•••
•••
• ••••
••
••
••
•
•Male
Female
FIGURE 5.6. The response is the relative change in bone mineral density me a-
sured at the spine in adolescents, as a function of age. A separ ate smoothing spline
was ﬁt to the males and females, with λ≈0.00022. This choice corresponds to
about12degrees of freedom.
where the Nj(x) are anN-dimensional set of basis functions for repre-
senting this family of natural splines (Section 5.2.1 and Ex ercise 5.4). The
criterion thus reduces to
RSS(θ,λ) = (y−Nθ)T(y−Nθ)+λθTΩNθ, (5.11)
where{N}ij=Nj(xi) and{ΩN}jk=/integraltext
N′′
j(t)N′′
k(t)dt. The solution is
easily seen to be
ˆθ= (NTN+λΩN)−1NTy, (5.12)
a generalized ridge regression. The ﬁtted smoothing spline is given by
ˆf(x) =N/summationdisplay
j=1Nj(x)ˆθj. (5.13)
Eﬃcient computational techniques for smoothing splines ar e discussed in
the Appendix to this chapter.
Figure 5.6 shows a smoothing spline ﬁt to some data on bone min eral
density (BMD) in adolescents. The response is relative chan ge in spinal
BMD over two consecutive visits, typically about one year ap art. The data
are color coded by gender, and two separate curves were ﬁt. Th is simple

5.4 Smoothing Splines 153
summary reinforces the evidence in the data that the growth s purt for
females precedes that for males by about two years. In both ca ses the
smoothing parameter λwas approximately 0 .00022; this choice is discussed
in the next section.
5.4.1 Degrees of Freedom and Smoother Matrices
We have not yet indicated how λis chosen for the smoothing spline. Later
in this chapter we describe automatic methods using techniq ues such as
cross-validation. In this section we discuss intuitive way s of prespecifying
the amount of smoothing.
A smoothing spline with prechosen λis an example of a linear smoother
(as in linear operator). This is because the estimated param eters in (5.12)
are a linear combination of the yi. Denote by ˆftheN-vector of ﬁtted values
ˆf(xi) at the training predictors xi. Then
ˆf=N(NTN+λΩN)−1NTy
=Sλy. (5.14)
Again the ﬁt is linear in y, and the ﬁnite linear operator Sλis known as
thesmoother matrix . One consequence of this linearity is that the recipe
for producing ˆffromydoes not depend on yitself;Sλdepends only on
thexiandλ.
Linear operators are familiar in more traditional least squ ares ﬁtting as
well. Suppose Bξis aN×Mmatrix ofMcubic-spline basis functions
evaluated at the Ntraining points xi, with knot sequence ξ, andM≪N.
Then the vector of ﬁtted spline values is given by
ˆf=Bξ(BT
ξBξ)−1BT
ξy
=Hξy. (5.15)
Here the linear operator Hξis a projection operator, also known as the hat
matrix in statistics. There are some important similaritie s and diﬀerences
between HξandSλ:
•Both are symmetric, positive semideﬁnite matrices.
•HξHξ=Hξ(idempotent), while SλSλ∝√∇e⌋e⌈esequalSλ, meaning that the right-
hand side exceeds the left-hand side by a positive semideﬁni te matrix.
This is a consequence of the shrinking nature of Sλ, which we discuss
further below.
•Hξhas rankM, whileSλhas rankN.
The expression M= trace(Hξ) gives the dimension of the projection space,
which is also the number of basis functions, and hence the num ber of pa-
rameters involved in the ﬁt. By analogy we deﬁne the eﬀective degrees of

154 5. Basis Expansions and Regularization
freedomof a smoothing spline to be
dfλ= trace(Sλ), (5.16)
the sum of the diagonal elements of Sλ. This very useful deﬁnition allows
us a more intuitive way to parameterize the smoothing spline , and indeed
many other smoothers as well, in a consistent fashion. For ex ample, in Fig-
ure 5.6 we speciﬁed df λ= 12 for each of the curves, and the corresponding
λ≈0.00022 was derived numerically by solving trace( Sλ) = 12. There are
many arguments supporting this deﬁnition of degrees of free dom, and we
cover some of them here.
SinceSλis symmetric (and positive semideﬁnite), it has a real eigen -
decomposition. Before we proceed, it is convenient to rewri teSλin the
Reinschform
Sλ= (I+λK)−1, (5.17)
whereKdoes not depend on λ(Exercise 5.9). Since ˆf=Sλysolves
min
f(y−f)T(y−f)+λfTKf, (5.18)
Kis known as the penalty matrix , and indeed a quadratic form in Khas
a representation in terms of a weighted sum of squared (divid ed) second
diﬀerences. The eigen-decomposition of Sλis
Sλ=N/summationdisplay
k=1ρk(λ)ukuT
k (5.19)
with
ρk(λ) =1
1+λdk, (5.20)
anddkthe corresponding eigenvalue of K. Figure 5.7 (top) shows the re-
sults of applying a cubic smoothing spline to some air pollut ion data (128
observations). Two ﬁts are given: a smoother ﬁt corresponding to a larger
penaltyλand arougherﬁt for a smaller penalty. The lower panels repre-
sent the eigenvalues (lower left) and some eigenvectors (lo wer right) of the
corresponding smoother matrices. Some of the highlights of the eigenrep-
resentation are the following:
•Theeigenvectorsarenotaﬀectedbychangesin λ,andhencethewhole
family of smoothing splines (for a particular sequence x) indexed by
λhave the same eigenvectors.
•Sλy=/summationtextN
k=1ukρk(λ)∝an}⌊∇a⌋ketle{tuk,y∝an}⌊∇a⌋ket∇i}ht, and hence the smoothing spline oper-
ates by decomposing yw.r.t. the (complete) basis {uk}, and diﬀer-
entially shrinking the contributions using ρk(λ). This is to be con-
trasted with a basis-regression method, where the componen ts are

5.4 Smoothing Splines 155
Daggot Pressure GradientOzone Concentration
-50 0 50 1000 10 20 30•••
••••••
••
••••••
•
•••
•
•••
••
•••
•
••
•
••
•••
•
••
••
••
••
••••
•••••
•
•••
•••
••
•
••
•••
••
••
•••
••••
••
••
•••
•
••••
••
••
••
•••
•••
•••
••
•
••
••••
••
•••
•
•
•
•
OrderEigenvalues
5 10 15 20 25-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2•••
•
•
•
••••••••••• •• • • •• • ••••••••
•
•
•
•
•
•••••••••••••df=5
df=11
-50 0 50 100 -50 0 50 100
FIGURE 5.7. (Top:) Smoothing spline ﬁt of ozone concentration versus Dag got
pressure gradient. The two ﬁts correspond to diﬀerent values of the smoothing
parameter, chosen to achieve ﬁve and eleven eﬀective degrees of freedom, deﬁned
by dfλ=trace(Sλ). (Lower left:) First 25eigenvalues for the two smoothing-spline
matrices. The ﬁrst two are exactly 1, and all are ≥0. (Lower right:) Third to
sixth eigenvectors of the spline smoother matrices. In each c ase,ukis plotted
againstx, and as such is viewed as a function of x. Therugat the base of the
plots indicate the occurrence of data points. The damped func tions represent the
smoothed versions of these functions (using the 5df smoother).

156 5. Basis Expansions and Regularization
either left alone, or shrunk to zero—that is, a projection mat rix such
asHξabove hasMeigenvalues equal to 1, and the rest are 0. For
this reason smoothing splines are referred to as shrinking smoothers,
while regression splines are projection smoothers (see Figure 3.17 on
page 80).
•The sequence of uk, ordered by decreasing ρk(λ), appear to increase
incomplexity. Indeed,theyhave thezero-crossingbehavio r ofpolyno-
mials of increasing degree. Since Sλuk=ρk(λ)uk, we see how each of
the eigenvectors themselves are shrunk by the smoothing spl ine: the
higher the complexity, the more they are shrunk. If the domai n ofX
is periodic, then the ukare sines and cosines at diﬀerent frequencies.
•The ﬁrst two eigenvalues are alwaysone, and they correspond to the
two-dimensional eigenspace of functions linear in x(Exercise 5.11),
which are never shrunk.
•The eigenvalues ρk(λ) = 1/(1 +λdk) are an inverse function of the
eigenvalues dkof the penalty matrix K, moderated by λ;λcontrols
the rate at which the ρk(λ) decrease to zero. d1=d2= 0 and again
linear functions are not penalized.
•One can reparametrize the smoothing spline using the basis v ectors
uk(theDemmler–Reinsch basis). In this case the smoothing spline
solves
min
θ∝⌊a∇⌈⌊ly−Uθ∝⌊a∇⌈⌊l2+λθTDθ, (5.21)
whereUhas columns ukandDis a diagonal matrix with elements
dk.
•dfλ= trace( Sλ) =/summationtextN
k=1ρk(λ). For projection smoothers, all the
eigenvalues are 1, each one corresponding to a dimension of t he pro-
jection subspace.
Figure 5.8 depicts a smoothing spline matrix, with the rows o rdered with
x. The banded nature of this representation suggests that a sm oothing
spline is a local ﬁtting method, much like the locally weight ed regression
procedures in Chapter 6. The right panel shows in detail sele cted rows of
S, which we call the equivalent kernels . Asλ→0, dfλ→N, andSλ→I,
theN-dimensional identity matrix. As λ→∞, dfλ→2, andSλ→H, the
hat matrix for linear regression on x.
5.5 Automatic Selection of the Smoothing
Parameters
The smoothing parameters for regression splines encompass the degree of
the splines, and the number and placement of the knots. For sm oothing

5.5 Automatic Selection of the Smoothing Parameters 157
11510075502512Smoother Matrix
••••• • •••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••
•Row 115••••• ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••
•Row 100••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Row 75•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 50•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 25•••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 12Equivalent Kernels
FIGURE 5.8. The smoother matrix for a smoothing spline is nearly banded,
indicating an equivalent kernel with local support. The left pa nel represents the
elements of Sas an image. The right panel shows the equivalent kernel or wei ght-
ing function in detail for the indicated rows.

158 5. Basis Expansions and Regularization
splines, we have only the penalty parameter λto select, since the knots are
at all the unique training X’s, and cubic degree is almost always used in
practice.
Selectingtheplacementandnumberofknotsforregressions plinescanbe
a combinatorially complex task, unless some simpliﬁcation s are enforced.
The MARS procedure in Chapter 9 uses a greedy algorithm with s ome
additional approximations to achieve a practical compromi se. We will not
discuss this further here.
5.5.1 Fixing the Degrees of Freedom
Since df λ= trace(Sλ) is monotone in λfor smoothing splines, we can in-
vert the relationship and specify λby ﬁxing df. In practice this can be
achieved by simple numerical methods. So, for example, in Rone can use
smooth.spline(x,y,df=6) to specify the amount of smoothing. This encour-
ages a more traditional mode of model selection, where we mig ht try a cou-
ple of diﬀerent values of df, and select one based on approxim ateF-tests,
residual plots and other more subjective criteria. Using df in this way pro-
vides a uniform approach to compare many diﬀerent smoothing methods.
It is particularly useful in generalized additive models (Chapter 9), where
several smoothing methods can be simultaneously used in one model.
5.5.2 The Bias–Variance Tradeoﬀ
Figure 5.9 shows the eﬀect of the choice of df λwhen using a smoothing
spline on a simple example:
Y=f(X)+ε,
f(X) =sin(12(X+0.2))
X+0.2,(5.22)
withX∼U[0,1] andε∼N(0,1). Our training sample consists of N= 100
pairsxi,yidrawn independently from this model.
The ﬁtted splines for three diﬀerent values of df λare shown. The yellow
shaded region in the ﬁgure represents the pointwise standar d error of ˆfλ,
that is, we have shaded the region between ˆfλ(x)±2·se(ˆfλ(x)). Since
ˆf=Sλy,
Cov(ˆf) =SλCov(y)ST
λ
=SλST
λ. (5.23)
The diagonal contains the pointwise variances at the traini ngxi. The bias
is given by
Bias(ˆf) =f−E(ˆf)
=f−Sλf, (5.24)

5.5 Automatic Selection of the Smoothing Parameters 159
6 8 10 12 141.0 1.1 1.2 1.3 1.4 1.5
0.0 0.2 0.4 0.6 0.8 1.0−4 −2 0 2yO
O
O
OO
OO
O
OOO
OO
O
OO
OOOO
OO
OOOOO
OOO
O
OO
OO
OO
OO
O
O
OO
OOO
OOO
O
OOO
OO
OO
OO
O
OOO
OOOO
OO
O
OO
OOOOOO
O
OOO
OOO
OO
OO
OOOOO
OOOO
OO
0.0 0.2 0.4 0.6 0.8 1.0−4 −2 0 2yO
O
O
OO
OO
O
OOO
OO
O
OO
OOOO
OO
OOOOO
OOO
O
OO
OO
OO
OO
O
O
OO
OOO
OOO
O
OOO
OO
OO
OO
O
OOO
OOOO
OO
O
OO
OOOOOO
O
OOO
OOO
OO
OO
OOOOO
OOOO
OO
0.0 0.2 0.4 0.6 0.8 1.0−4 −2 0 2yO
O
O
OO
OO
O
OOO
OO
O
OO
OOOO
OO
OOOOO
OOO
O
OO
OO
OO
OO
O
O
OO
OOO
OOO
O
OOO
OO
OO
OO
O
OOO
OOOO
OO
O
OO
OOOOOO
O
OOO
OOO
OO
OO
OOOOO
OOOO
OOEPECV
X XXdfλ= 5
dfλ= 9 dfλ= 15dfλCross-ValidationEPE(λ) and CV( λ)
FIGURE 5.9. The top left panel shows the EPE(λ)andCV(λ)curves for a
realization from a nonlinear additive error model (5.22). The remaining panels
show the data, the true functions (in purple), and the ﬁtted cu rves (in green) with
yellow shaded ±2×standard error bands, for three diﬀerent values of dfλ.

160 5. Basis Expansions and Regularization
wherefis the (unknown) vector of evaluations of the true fat the training
X’s. The expectations and variances are with respect to repea ted draws
of samples of size N= 100 from the model (5.22). In a similar fashion
Var(ˆfλ(x0)) and Bias( ˆfλ(x0)) can be computed at any point x0(Exer-
cise 5.10). The three ﬁts displayed in the ﬁgure give a visual demonstration
of the bias-variance tradeoﬀ associated with selecting the smoothing
parameter.
dfλ= 5:The spline under ﬁts, and clearly trims down the hills and ﬁlls in
the valleys . This leads to a bias that is most dramatic in regions of
high curvature. The standard error band is very narrow, so we esti-
mate a badly biased version of the true function with great re liability!
dfλ= 9:Here the ﬁtted function is close to the true function, althou gh a
slight amount of bias seems evident. The variance has not inc reased
appreciably.
dfλ= 15:The ﬁtted function is somewhat wiggly, but close to the true
function. The wiggliness also accounts for the increased wi dth of the
standard error bands—the curve is starting to follow some ind ividual
points too closely.
Note that in these ﬁgures we are seeing a single realization o f data and
hence ﬁtted spline ˆfin each case, while the bias involves an expectation
E(ˆf). We leave it as an exercise (5.10) to compute similar ﬁgures where the
bias is shown as well. The middle curve seems “just right,” in that it has
achieved a good compromise between bias and variance.
The integrated squared prediction error (EPE) combines bot h bias and
variance in a single summary:
EPE(ˆfλ) = E(Y−ˆfλ(X))2
= Var(Y)+E/bracketleftig
Bias2(ˆfλ(X))+Var( ˆfλ(X))/bracketrightig
=σ2+MSE( ˆfλ). (5.25)
Note that this is averaged both over the training sample (giv ing rise to ˆfλ),
and the values of the (independently chosen) prediction poi nts (X,Y). EPE
is a natural quantity of interest, and does create a tradeoﬀ b etween bias
and variance. The blue points in the top left panel of Figure 5 .9 suggest
that df λ= 9 is spot on!
Sincewedon’tknowthetruefunction,wedonothaveaccessto EPE,and
need an estimate. This topic is discussed in some detail in Ch apter 7, and
techniques such as K-fold cross-validation, GCV and Cpare all in common
use. In Figure 5.9 we include the N-fold (leave-one-out) cross-validation
curve:

5.6 Nonparametric Logistic Regression 161
CV(ˆfλ) =1
NN/summationdisplay
i=1(yi−ˆf(−i)
λ(xi))2(5.26)
=1
NN/summationdisplay
i=1/parenleftigg
yi−ˆfλ(xi)
1−Sλ(i,i)/parenrightigg2
, (5.27)
which can (remarkably) be computed for each value of λfrom the original
ﬁtted values and the diagonal elements Sλ(i,i) ofSλ(Exercise 5.13).
The EPE and CV curves have a similar shape, but the entire CV cu rve
is above the EPE curve. For some realizations this is reverse d, and overall
the CV curve is approximately unbiased as an estimate of the E PE curve.
5.6 Nonparametric Logistic Regression
The smoothing spline problem (5.9) in Section 5.4 is posed in a regression
setting. It is typically straightforward to transfer this t echnology to other
domains. Here we consider logistic regression with a single quantitative
inputX. The model is
logPr(Y= 1|X=x)
Pr(Y= 0|X=x)=f(x), (5.28)
which implies
Pr(Y= 1|X=x) =ef(x)
1+ef(x). (5.29)
Fittingf(x) in a smooth fashion leads to a smooth estimate of the condi-
tional probability Pr( Y= 1|x), which can be used for classiﬁcation or risk
scoring.
We construct the penalized log-likelihood criterion
ℓ(f;λ) =N/summationdisplay
i=1[yilogp(xi)+(1−yi)log(1−p(xi))]−1
2λ/integraldisplay
{f′′(t)}2dt
=N/summationdisplay
i=1/bracketleftig
yif(xi)−log(1+ef(xi))/bracketrightig
−1
2λ/integraldisplay
{f′′(t)}2dt,(5.30)
where we have abbreviated p(x) = Pr(Y= 1|x). The ﬁrst term in this ex-
pression is the log-likelihood based on the binomial distri bution (c.f. Chap-
ter 4, page 120). Arguments similar to those used in Section 5 .4 show that
the optimal fis aﬁnite-dimensional natural spline with knots at the uniq ue

162 5. Basis Expansions and Regularization
values ofx. This means that we can represent f(x) =/summationtextN
j=1Nj(x)θj. We
compute the ﬁrst and second derivatives
∂ℓ(θ)
∂θ=NT(y−p)−λΩθ, (5.31)
∂2ℓ(θ)
∂θ∂θT=−NTWN−λΩ, (5.32)
wherepis theN-vector with elements p(xi), andWis a diagonal matrix
of weightsp(xi)(1−p(xi)). The ﬁrst derivative (5.31) is nonlinear in θ, so
we need to use an iterative algorithm as in Section 4.4.1. Usi ng Newton–
Raphson as in (4.23) and (4.26) for linear logistic regressi on, the update
equation can be written
θnew= (NTWN+λΩ)−1NTW/parenleftbig
Nθold+W−1(y−p)/parenrightbig
= (NTWN+λΩ)−1NTWz. (5.33)
We can also express this update in terms of the ﬁtted values
fnew=N(NTWN+λΩ)−1NTW/parenleftbig
fold+W−1(y−p)/parenrightbig
=Sλ,wz. (5.34)
Referring back to (5.12) and (5.14), we see that the update ﬁt s a weighted
smoothing spline to the working response z(Exercise 5.12).
The form of (5.34) is suggestive. It is tempting to replace Sλ,wby any
nonparametric (weighted) regression operator, and obtain general fami-
lies of nonparametric logistic regression models. Althoug h herexis one-
dimensional, this procedure generalizes naturally to high er-dimensional x.
These extensions are at the heart of generalized additive models , which we
pursue in Chapter 9.
5.7 Multidimensional Splines
So far we have focused on one-dimensional spline models. Eac h of the ap-
proaches have multidimensional analogs. Suppose X∈IR2, and we have
a basis of functions h1k(X1), k= 1,...,M 1for representing functions of
coordinate X1, and likewise a set of M2functionsh2k(X2) for coordinate
X2. Then the M1×M2dimensional tensor product basis deﬁned by
gjk(X) =h1j(X1)h2k(X2), j= 1,...,M 1, k= 1,...,M 2(5.35)
can be used for representing a two-dimensional function:
g(X) =M1/summationdisplay
j=1M2/summationdisplay
k=1θjkgjk(X). (5.36)

5.7 Multidimensional Splines 163
FIGURE 5.10. A tensor product basis of B-splines, showing some selected pai rs.
Each two-dimensional function is the tensor product of the c orresponding one
dimensional marginals.
Figure 5.10 illustrates a tensor product basis using B-spli nes. The coeﬃ-
cients can be ﬁt by least squares, as before. This can be gener alized tod
dimensions, but note that the dimension of the basis grows ex ponentially
fast—yet another manifestation of the curse of dimensionali ty. The MARS
procedure discussed in Chapter 9 is a greedy forward algorit hm for includ-
ing only those tensor products that are deemed necessary by l east squares.
Figure5.11 illustrates thediﬀerencebetween additive and tensor product
(natural) splines on the simulated classiﬁcation example f rom Chapter 2.
A logistic regression model logit[Pr( T|x)] =h(x)Tθis ﬁt to the binary re-
sponse, and the estimated decision boundary is the contour h(x)Tˆθ= 0.
The tensor product basis can achieve more ﬂexibility at the d ecision bound-
ary, but introduces some spurious structure along the way.

164 5. Basis Expansions and Regularization
Additive Natural Cubic Splines - 4 df each
.. .. . .. . . .. . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . . .. . . .. . . .. . . .. . .. . .. . .. .. .. ...
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.23
Test Error:       0.28
Bayes Error:    0.21
Natural Cubic Splines - Tensor Product - 4 df each
. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . ... . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.230
Test Error:       0.282
Bayes Error:    0.210
FIGURE 5.11. The simulation example of Figure 2.1. The upper panel shows the
decision boundary of an additive logistic regression model, u sing natural splines
in each of the two coordinates (total df = 1+(4 −1)+(4−1) = 7). The lower
panel shows the results of using a tensor product of natural sp line bases in each
coordinate (total df = 4×4 = 16). The broken purple boundary is the Bayes
decision boundary for this problem.

5.7 Multidimensional Splines 165
One-dimensionalsmoothingsplines(viaregularization)g eneralizetohigh-
er dimensions as well. Suppose we have pairs yi,xiwithxi∈IRd, and we
seek ad-dimensional regression function f(x). The idea is to set up the
problem
min
fN/summationdisplay
i=1{yi−f(xi)}2+λJ[f], (5.37)
whereJis an appropriate penalty functional for stabilizing a func tionfin
IRd.Forexample,anaturalgeneralizationoftheone-dimensio nalroughness
penalty (5.9) for functions on IR2is
J[f] =/integraldisplay /integraldisplay
IR2/bracketleftig/parenleftbigg∂2f(x)
∂x2
1/parenrightbigg2
+2/parenleftbigg∂2f(x)
∂x1∂x2/parenrightbigg2
+/parenleftbigg∂2f(x)
∂x2
2/parenrightbigg2/bracketrightig
dx1dx2.(5.38)
Optimizing (5.37) with this penalty leads to a smooth two-di mensional
surface, known as a thin-plate spline. It shares many proper ties with the
one-dimensional cubic smoothing spline:
•asλ→0, the solution approaches an interpolating function [the o ne
with smallest penalty (5.38)];
•asλ→∞, the solution approaches the least squares plane;
•for intermediate values of λ, the solution can be represented as a
linear expansion of basis functions, whose coeﬃcients are o btained
by a form of generalized ridge regression.
The solution has the form
f(x) =β0+βTx+N/summationdisplay
j=1αjhj(x), (5.39)
wherehj(x) =||x−xj||2log||x−xj||. Thesehjare examples of radial
basis functions , which are discussed in more detail in the next section. The
coeﬃcients are found by plugging (5.39) into (5.37), which r educes to a
ﬁnite-dimensional penalized least squares problem. For th e penalty to be
ﬁnite, the coeﬃcients αjhave to satisfy a set of linear constraints; see
Exercise 5.14.
Thin-plate splines are deﬁned more generally for arbitrary dimensiond,
for which an appropriately more general Jis used.
There are a number of hybrid approaches that are popular in pr actice,
both for computational and conceptual simplicity. Unlike o ne-dimensional
smoothing splines, the computational complexity for thin- plate splines is
O(N3), since there is not in general any sparse structure that can be ex-
ploited. However, as with univariate smoothing splines, we can get away
with substantially less than the Nknots prescribed by the solution (5.39).

166 5. Basis Expansions and Regularization
125
130135140145150155
15202530354045
20 30 40 50 60
AgeObesitySystolic Blood Pressure
120125130135140145150155160
•• ••
••
••••
•
•
•• ••••
••
••
••
•
••
••
•
•••••
•••
•••
•
••
•••
••
•
••
•••
••
••••••
•••
••
•
•
••
••
••
•
••
••
••
••
••
••
••
•
••
••••
••••
•
•
•
••
•
•••
••
••
•
•••
•••
•••
••
••
••
••
•
•
•••••
•••
•
• •••
••••
•••
••
••••
•
••••
• ••
•
•••
•••
•••••
•
••
••
•••
••
••
••••
•
••••
•••
••
••
••
••
••
•
•••
••
•
•
••••
••
•••
••••
•
•••
•••
•
•••••
•
•
•••
••
•••
••
•
••
•
•••
••••
• •
•••
•
•••
••
••
••
•
••
•
••••
•
••••
•••
•
•••
•••
••
••
•
••••
••
• ••
•••
•••
•••
••• •••
•
•
•
••
••
•
••
••
••
•••••
•••
•
••
•••
•
••
•
••
••
•
••• •
••
•
•••
••
••
••
•••
••
•••
••
•••
•••
•
•••
•
••
•••
••
•••
••••
•
••
••••
•••
•
•••
••
••
••
•••
••••
•
••
••
••
•• • • • • •• • • • • • •• • • • • • •• • • • • •• • • •• •
• • • • • • • •• •••• •• • • •• • • • • •• • • • • • • •
FIGURE 5.12. A thin-plate spline ﬁt to the heart disease data, displayed as a
contour plot. The response is systolic blood pressure , modeled as a function
ofageandobesity. The data points are indicated, as well as the lattice of point s
used as knots. Care should be taken to use knots from the lattice inside the convex
hull of the data (red), and ignore those outside (green).
In practice, it is usually suﬃcient to work with a lattice of k nots covering
the domain. The penalty is computed for the reduced expansio n just as
before. Using Kknots reduces the computations to O(NK2+K3). Fig-
ure 5.12 shows the result of ﬁtting a thin-plate spline to som e heart disease
risk factors, representing the surface as a contour plot. In dicated are the
location of the input features, as well as the knots used in th e ﬁt. Note that
λwas speciﬁed via df λ= trace(Sλ) = 15.
More generally one can represent f∈IRdas an expansion in any arbi-
trarily large collection of basis functions, and control th e complexity by ap-
plying a regularizer such as (5.38). For example, we could co nstruct a basis
by forming the tensor products of all pairs of univariate smo othing-spline
basis functions as in (5.35), using, for example, the univar iateB-splines
recommended in Section 5.9.2 as ingredients. This leads to a n exponential

5.8 Regularization and Reproducing Kernel Hilbert Spaces 16 7
growth in basis functions as the dimension increases, and ty pically we have
to reduce the number of functions per coordinate accordingl y.
The additive spline models discussed in Chapter 9 are a restr icted class
of multidimensional splines. They can be represented in thi s general formu-
lation as well; that is, there exists a penalty J[f] that guarantees that the
solution has the form f(X) =α+f1(X1)+···+fd(Xd) and that each of
the functions fjare univariate splines. In this case the penalty is somewhat
degenerate, and it is more natural to assumethatfis additive, and then
simply impose an additional penalty on each of the component functions:
J[f] =J(f1+f2+···+fd)
=d/summationdisplay
j=1/integraldisplay
f′′
j(tj)2dtj. (5.40)
These are naturally extended to ANOVA spline decomposition s,
f(X) =α+/summationdisplay
jfj(Xj)+/summationdisplay
j<kfjk(Xj,Xk)+···,(5.41)
where each of the components are splines of the required dime nsion. There
are many choices to be made:
•The maximum order of interaction—we have shown up to order 2
above.
•Which terms to include—not all main eﬀects and interactions a re
necessarily needed.
•What representation to use—some choices are:
–regression splines with a relatively small number of basis f unc-
tions per coordinate, and their tensor products for interac tions;
–a complete basis as in smoothing splines, and include approp ri-
ate regularizers for each term in the expansion.
In many cases when the number of potential dimensions (featu res) is large,
automatic methods are more desirable. The MARS and MART proc edures
(Chapters 9 and 10, respectively), both fall into this categ ory.
5.8 Regularization and Reproducing Kernel
Hilbert Spaces
Inthissectionwecastsplinesintothelargercontextofreg ularizationmeth-
ods and reproducing kernel Hilbert spaces. This section is q uite technical
and can be skipped by the disinterested or intimidated reade r.

168 5. Basis Expansions and Regularization
A general class of regularization problems has the form
min
f∈H/bracketleftiggN/summationdisplay
i=1L(yi,f(xi))+λJ(f)/bracketrightigg
(5.42)
whereL(y,f(x)) is a loss function, J(f) is a penalty functional, and His
a space of functions on which J(f) is deﬁned. Girosi et al. (1995) describe
quite general penalty functionals of the form
J(f) =/integraldisplay
IRd|˜f(s)|2
˜G(s)ds, (5.43)
where˜fdenotes the Fourier transform of f, and˜Gis some positive function
thatfalls oﬀtozeroas ||s||→∞.Theideaisthat1 /˜Gincreasesthepenalty
for high-frequency components of f. Under some additional assumptions
they show that the solutions have the form
f(X) =K/summationdisplay
k=1αkφk(X)+N/summationdisplay
i=1θiG(X−xi), (5.44)
where theφkspan the null space of the penalty functional J, andGis the
inverse Fourier transform of ˜G. Smoothing splines and thin-plate splines
fall into this framework. The remarkable feature of this sol ution is that
while the criterion (5.42) is deﬁned over an inﬁnite-dimens ional space, the
solution is ﬁnite-dimensional. In the next sections we look at some speciﬁc
examples.
5.8.1 Spaces of Functions Generated by Kernels
An important subclass of problems of the form (5.42) are gene rated by
a positive deﬁnite kernel K(x,y), and the corresponding space of func-
tionsHKis called a reproducing kernel Hilbert space (RKHS). The penalty
functionalJis deﬁned in terms of the kernel as well. We give a brief and
simpliﬁed introduction to this class of models, adapted fro m Wahba (1990)
and Girosi et al. (1995), and nicely summarized in Evgeniou e t al. (2000).
Letx,y∈IRp. We consider the space of functions generated by the linear
span of{K(·,y), y∈IRp)}; i.e arbitrary linear combinations of the form
f(x) =/summationtext
mαmK(x,ym), where each kernel term is viewed as a function
of the ﬁrst argument, and indexed by the second. Suppose that Khas an
eigen-expansion
K(x,y) =∞/summationdisplay
i=1γiφi(x)φi(y) (5.45)
withγi≥0,/summationtext∞
i=1γ2
i<∞. Elements ofHKhave an expansion in terms of
these eigen-functions,
f(x) =∞/summationdisplay
i=1ciφi(x), (5.46)

5.8 Regularization and Reproducing Kernel Hilbert Spaces 16 9
with the constraint that
||f||2
HKdef=∞/summationdisplay
i=1c2
i/γi<∞, (5.47)
where||f||HKis the norm induced by K. The penalty functional in (5.42)
for the spaceHKis deﬁned to be the squared norm J(f) =||f||2
HK. The
quantityJ(f) can be interpreted as a generalized ridge penalty, where
functions with large eigenvalues in the expansion (5.45) ge t penalized less,
and vice versa.
Rewriting (5.42) we have
min
f∈HK/bracketleftiggN/summationdisplay
i=1L(yi,f(xi))+λ||f||2
HK/bracketrightigg
(5.48)
or equivalently
min
{cj}∞
1
N/summationdisplay
i=1L(yi,∞/summationdisplay
j=1cjφj(xi))+λ∞/summationdisplay
j=1c2
j/γj
. (5.49)
It can be shown (Wahba, 1990, see also Exercise 5.15) that the solution
to (5.48) is ﬁnite-dimensional, and has the form
f(x) =N/summationdisplay
i=1αiK(x,xi). (5.50)
The basis function hi(x) =K(x,xi) (as a function of the ﬁrst argument) is
known as the representer of evaluation atxiinHK, since forf∈HK, it is
easily seen that∝an}⌊∇a⌋ketle{tK(·,xi),f∝an}⌊∇a⌋ket∇i}htHK=f(xi). Similarly∝an}⌊∇a⌋ketle{tK(·,xi),K(·,xj)∝an}⌊∇a⌋ket∇i}htHK=
K(xi,xj) (thereproducing property ofHK), and hence
J(f) =N/summationdisplay
i=1N/summationdisplay
j=1K(xi,xj)αiαj (5.51)
forf(x) =/summationtextN
i=1αiK(x,xi).
In light of (5.50) and (5.51), (5.48) reduces to a ﬁnite-dime nsional crite-
rion
min
αL(y,Kα)+λαTKα. (5.52)
We are using a vector notation, in which Kis theN×Nmatrix with ijth
entryK(xi,xj) and so on. Simple numerical algorithms can be used to
optimize (5.52). This phenomenon, whereby the inﬁnite-dim ensional prob-
lem (5.48) or (5.49) reduces to a ﬁnite dimensional optimiza tion problem,
has been dubbed the kernel property in the literature on support-vector
machines (see Chapter 12).

170 5. Basis Expansions and Regularization
There is a Bayesian interpretation of this class of models, i n whichf
is interpreted as a realization of a zero-mean stationary Ga ussian process,
with prior covariance function K. The eigen-decomposition produces a se-
ries of orthogonal eigen-functions φj(x) with associated variances γj. The
typical scenario is that “smooth” functions φjhave large prior variance,
while “rough” φjhave small prior variances. The penalty in (5.48) is the
contribution of the prior to the joint likelihood, and penal izes more those
components with smaller prior variance (compare with (5.43 )).
For simplicity we have dealt with the case here where all memb ers ofH
are penalized, as in (5.48). More generally, there may be som e components
inHthat we wish to leave alone, such as the linear functions for c ubic
smoothing splines in Section 5.4. The multidimensional thi n-plate splines
of Section 5.7 and tensor product splines fall into this cate gory as well.
In these cases there is a more convenient representation H=H0⊕H1,
with the null spaceH0consisting of, for example, low degree polynomi-
als inxthat do not get penalized. The penalty becomes J(f) =∝⌊a∇⌈⌊lP1f∝⌊a∇⌈⌊l,
whereP1is the orthogonal projection of fontoH1. The solution has the
formf(x) =/summationtextM
j=1βjhj(x)+/summationtextN
i=1αiK(x,xi), where the ﬁrst term repre-
sents an expansion in H0. From a Bayesian perspective, the coeﬃcients of
components inH0have improper priors, with inﬁnite variance.
5.8.2 Examples of RKHS
The machinery above is driven by the choice of the kernel Kand the loss
functionL. We consider ﬁrst regression using squared-error loss. In t his
case (5.48) specializes to penalized least squares, and the solution can be
characterized in two equivalent ways corresponding to (5.4 9) or (5.52):
min
{cj}∞
1N/summationdisplay
i=1
yi−∞/summationdisplay
j=1cjφj(xi)
2
+λ∞/summationdisplay
j=1c2
j
γj(5.53)
an inﬁnite-dimensional, generalized ridge regression pro blem, or
min
α(y−Kα)T(y−Kα)+λαTKα. (5.54)
The solution for αis obtained simply as
ˆα= (K+λI)−1y, (5.55)
and
ˆf(x) =N/summationdisplay
j=1ˆαjK(x,xj). (5.56)

5.8 Regularization and Reproducing Kernel Hilbert Spaces 17 1
The vector of Nﬁtted values is given by
ˆf=Kˆα
=K(K+λI)−1y (5.57)
= (I+λK−1)−1y. (5.58)
The estimate (5.57) also arises as the krigingestimate of a Gaussian ran-
dom ﬁeld in spatial statistics (Cressie, 1993). Compare als o (5.58) with the
smoothing spline ﬁt (5.17) on page 154.
Penalized Polynomial Regression
The kernel K(x,y) = (∝an}⌊∇a⌋ketle{tx,y∝an}⌊∇a⌋ket∇i}ht+ 1)d(Vapnik, 1996), for x,y∈IRp, has
M=/parenleftbigp+d
d/parenrightbig
eigen-functions that span the space of polynomials in IRpof
total degree d. For example, with p= 2 andd= 2,M= 6 and
K(x,y) = 1+2 x1y1+2x2y2+x2
1y2
1+x2
2y2
2+2x1x2y1y2(5.59)
=M/summationdisplay
m=1hm(x)hm(y) (5.60)
with
h(x)T= (1,√
2x1,√
2x2,x2
1,x2
2,√
2x1x2). (5.61)
One can represent hin terms of the Morthogonal eigen-functions and
eigenvalues of K,
h(x) =VD1
2γφ(x), (5.62)
whereDγ= diag(γ1,γ2,...,γ M), andVisM×Mand orthogonal.
Suppose we wish to solve the penalized polynomial regressio n problem
min
{βm}M
1N/summationdisplay
i=1/parenleftigg
yi−M/summationdisplay
m=1βmhm(xi)/parenrightigg2
+λM/summationdisplay
m=1β2
m. (5.63)
Substituting (5.62) into (5.63), we get an expression of the form (5.53) to
optimize (Exercise 5.16).
The number of basis functions M=/parenleftbigp+d
d/parenrightbig
can be very large, often much
larger than N. Equation (5.55) tells us that if we use the kernel represen-
tation for the solution function, we have only to evaluate th e kernelN2
times, and can compute the solution in O(N3) operations.
This simplicity is not without implications. Each of the pol ynomialshm
in (5.61) inherits a scaling factor from the particular form ofK, which has
a bearing on the impact of the penalty in (5.63). We elaborate on this in
the next section.

172 5. Basis Expansions and Regularization
−2 −1 0 1 2 3 40.0 0.4 0.8
XRadial Kernel in IR1K(·,xm)
FIGURE 5.13. Radial kernels kk(x)for the mixture data, with scale parameter
ν= 1. The kernels are centered at ﬁve points xmchosen at random from the 200.
Gaussian Radial Basis Functions
In the preceding example, the kernel is chosen because it rep resents an
expansion of polynomials and can conveniently compute high -dimensional
innerproducts.Inthisexamplethekernelischosenbecause ofitsfunctional
form in the representation (5.50).
The Gaussian kernel K(x,y) =e−ν||x−y||2along with squared-error loss,
for example, leads to a regression model that is an expansion in Gaussian
radial basis functions,
km(x) =e−ν||x−xm||2, m= 1,...,N, (5.64)
each one centered at one of the training feature vectors xm. The coeﬃcients
are estimated using (5.54).
Figure 5.13 illustrates radial kernels in IR1using the ﬁrst coordinate of
the mixture example from Chapter 2. We show ﬁve of the 200 kern el basis
functionskm(x) =K(x,xm).
Figure 5.14 illustrates the implicit feature space for the r adial kernel
withx∈IR1. We computed the 200 ×200 kernel matrix K, and its eigen-
decomposition ΦDγΦT. We can think of the columns of Φand the corre-
sponding eigenvalues in Dγas empirical estimates of the eigen expansion
(5.45)2. Although the eigenvectors are discrete, we can represent t hem as
functions on IR1(Exercise 5.17). Figure 5.15 shows the largest 50 eigenval-
ues ofK. The leading eigenfunctions are smooth, and they are succes sively
more wiggly as the order increases. This brings to life the pe nalty in (5.49),
where we see the coeﬃcients of higher-order functions get pe nalized more
thanlower-order ones.Theright panelinFigure5.14shows t hecorrespond-
2Theℓth column of Φis an estimate of φℓ, evaluated at each of the Nobservations.
Alternatively, the ith row of Φis the estimated vector of basis functions φ(xi), evaluated
at the point xi. Although in principle, there can be inﬁnitely many elements in φ, our
estimate has at most Nelements.

5.8 Regularization and Reproducing Kernel Hilbert Spaces 17 3
*
******* ** ** ******
******
*******
******
**********
****
** *******
*****
**
***
**********
*****
*****
***
**
***
*****
*************
******
***
**
***
**
***********
**
****
**** *
** ******* * **
**************
********* ***
**
*******
***
**
****
***
***
**
***
*****
*******
**
**
**
**
***
**
*****
**
******
*
***
**
**
*********
**
**
****
**
**
**
****
****
***
**
**
*******
**
***
**
*
****
**
****
**
**
****
****
**
**
*****
****
**
****
*
**
***
**
**
****
****
**
**
*
***
**
*********
******
*
***
** **
****
*
***
***
***
***
*******
**
**
**
*
*******
**
***
***
******
**
***
**
******
**
**
*
**
**
**
*
***
**
**
****
*** **
*********
****
**
****
** **
** ****
****
***
***
**
** ***
**
**
*
***
***
* * * **
****
**
**
****
*****
****
* **** ***
**
*** **** * *
**
* *
**
**
******
***
***
****
***
***
*
**
**
**
**
***
***
***
****
***
********
*******
** ** **
**
***
***
**
*
*********
**********
*
****
***
***
**
**
***
*****
****
**
**
****
***
***
*
**
**
***
**
**
******
*******
***
***
****
****
***
****
* ***
* * ** *
* ***
**
*
**
***
***
**
***
**
*******
**
**
*******
**
*
* *********
***
******
******
** ***
*
**
*
**
**
***
* **
***
*
***
**
***********
**
**
**
***
**
***
**
**
*********
***
***
**
**
*
**
*
**
***
* * **
*****
**
**
*****
**
**
** ******
**
**
***** ****
***
****
* *****
****
****
***
**
**
**
* *******
****
****
***
**
****
*****
******
*****
*
*****
***
*
***
******
*****
***
***********
***
***** **
* *
***
*****
*******
*****
**
****
**
*
**
*
*****
***
*******
******* *
*****
*
**
***
*
*****
**
*****
**
*** *
***
***
******
**
***
********
***
**
**
***
***** ****
***
**
****
********
*******
***
**
***
**
***
**
*********
*
*******
*****
*
**
**
***
***
*
**
*
**
**
******
**
*
*****
*
***
** **
*
***
** ***
****
**
*******
****
**
****
*****
******
**
****
**
**
** ***
*****
*
**
**
***
**
****
**
*****
***
**
****
***
**
**
*
**
***
*
*****
***
******* ***
**
****
* ***
******
*****
*****
***
**
* ** ***
***
***
*
***
***
****
****
***
*****
*****
***
****
**
**
**
*
*****
**
******
********* *
**
*
****
**
*
* *
***
**
****
***
****
*** ****
****
**
*****
***
**
**
***
*****
*
***
*** *
******
*
***********
**
******
**
*******
**
****
*
**********
*
**** *
**
******
*
* **
***
*
**
*****
*
**
*
****
**
*
***
** *
****
******
***
**
*
**
**** ****
**
***
**
***
***
*****
**** * *
**
**
******
*****
*****
***
** *
****
*
****
** *
*****
****
**** *
*****
**
****
*****
** **
***
****
****
**
** *** **
**
*****
***
**
****
*
****
****
**
*****
***
********
*
****
***
**
***
**
*****
**
***
*****
********
****
* *******
******* * *
***
****
**** *
**
**
***
****
**
**
**
******
**
**
**
****
*****
***
****
*****
*
***********
**
**
**
**
**
*******
******
***********
***
***
*****
****
***
* ***
*****
**
*********
*
**
***
** ****
** * *
**
***
***
**
*****
***
*****
****
**
**
***
*****
* * **
**
**
* **
***
**
*****
***
***
***
*******
**
*** *
****
*
* ***
****
***
***
***
***
*****
* ***
****
****
***
****
*****
*******
***
**
***
****
*
***
***
*
**
*
*****
**
***
**
**
***
**
*** *
******
**
* **
****
**
***
**
***
*******
**
***
**
****
***
****
** *
****
* ***
***
**
**
******
***
**
*****
**
**
*
***
***
**
** *
******
*****
***
*
***
****
**
* **
*******
****
*******
**
***
***
*
*****
*
********
***
*
***
***
**
*******
**
*
*****
***
**
***
***
****
* ***
******
***
***
*****
*** **
**
**
***
***
********
**
****
**** *****
****
****
**
***
**
****
** *****
**
**
*****
**
**
**
***
****
*****
*** *
*******
****
********
**
** **
*****
**
***
*
******
**
***
***
*
****
**
***
**
******
***
***
*
**
*******
**
****
****
*
*******
**** ******
*
****
****
***
*
**
****
** **
** *
*** *
**
****
***************
***
***
*****
*****
* *****
***
****
*****
**
***
**
****
**
***
*
*****
***
**
****
***
**
**
*******
*****
***
*
***
*******
*
**
**** ***
****
*
****
****
*****
*****
***
***
*****
**
***
**
*
**
**
*****
**
**
*
**
**
**
****
**** *
*
**
******
****
**
****
***
*
*
****
****
****
***
***
******
***
****
*
***
**
*****
*
* ****
****
**
*
**
*
***
****
**
**
***
****
***
**
*
**
**
********* *
**
******
**
***
**
**
****
***
***
**
**
*******
***
**
**
***
**
*
**
*
**
**Orthonormal Basis Φ
*
*****
** *
* ***
*
****
***
*
**
*
***
***
******
**
**
***
***
****
**
*
**
****
*****
**
***
**
******
**
**
*
**
**
***
***
**
***
*****
*
***
*
***
*****
***
***
***
**
***
**
*
**
********
**
****
*
***
*
** *
*
***
** ***
**
**
**
**
***
***
***
***
***
***
**
***
****
***
**
****
***
***
**
***
****
*
**
**
***
**
**
**
**
**
*
**
****
*
**
******
*
***
**
**
*
**
**
*
*
**
**
**
****
*
*
**
**
**
**
****
***
**
**
***
***
*
**
***
**
*
****
**
*
***
**
**
****
****
**
**
****
*
****
**
****
*
**
***
**
**
***
*
****
**
**
*
***
**
***
**
*
**
*
******
*
**
*
***
*
***
*
*
***
***
***
*
**
****
**
*
**
**
**
*
***
***
*
**
***
**
*
****
**
**
***
**
*****
*
**
**
*
**
**
**
*
***
**
**
***
*
*****
*****
****
****
**
*
***
** **
***
***
***
*
**
*
***
**
** ***
**
**
*
***
***
** ***
****
**
**
***
*
**
***
*
***
*******
*
**
*
***
*** **
**
* *
**
**
***
***
***
***
****
***
***
*
**
**
**
*
*
***
**
*
*
**
**
**
***
***
*****
*
******
** ****
**
***
**
*
**
*
****
*
****
**
********
*
****
**
*
***
**
**
***
*****
****
**
**
****
***
*
**
*
**
**
***
**
**
******
*****
**
***
**
*
****
****
***
*
**
*
* ***
* *** *
* ***
**
*
**
***
***
**
*
**
**
*******
**
**
**
*****
**
*
* ****
***
**
***
*
*****
******
** ***
*
**
*
**
**
*
**
* **
***
*
***
**
*****
***
***
**
**
**
***
**
***
**
**
**
*******
*
**
***
**
**
*
**
*
**
***
* ***
*****
**
**
*****
**
**
** ***
*
**
**
**
***** ****
***
****
******
****
****
***
**
**
**
* *******
**
**
****
*
**
**
****
*****
******
*****
*
*****
***
*
***
******
*****
***
****
*****
**
***
***** **
* *
***
*****
*******
****
*
**
****
**
*
**
*
***
**
***
***
****
******* *
*****
*
**
***
*
*****
**
*****
**
*** *
***
***
******
**
***
********
***
**
**
***
***** ****
***
**
****
********
*******
***
**
***
**
***
**
*********
*
*******
*****
*
**
**
***
***
*
**
*
**
**
******
**
*
*****
*
***
** **
*
***
** ***
****
**
*******
****
**
****
*****
******
**
****
****
** ********
***
*****
**
****
**
**********
****
***
**
***
**
****
*****
***
******* ***
**
****
* ***
******
*****
*****
****** ** ***
***
***
*
***
***
****
*******
*****
*****
***
* *****
**
**
*
*****
**
******
********* ****
****
***
* ****
**
****
***
******* ** ********
********
**
**
********
****
*** *
***** ***************
******
**
*** ****
******************** ** ****** ***
** **
***
*
**********
*****
******** *
****
** * ******
********* * ***
**
********
******* ***** * *
****** ***** *** *********** ************ ** ***** ******* **
**** ************** *** ********* **** * *** **************** ********** ***************** *******
*****
***
******** *************** ******* ** * ****
******* * ******* ** ** * ***** *** ** ************* ******** ** ***** ** ***** **
**** * ************************* ******************************** ** * ** *** *********** * ************** * ****** * *************** ***************
****** * **** * ** ** *** ***** ***** ************* ******** ** ***** ** ************ ************* * ********** * **** ********* ********************* ** * *** *********** ** ********** ** * * ****** * **************** ****** *** ***********
* **** * ** ** ** * **** * *** ** ************* ******** ********* ***** ****** * ************* * ** ******** * **** ********* *********** ** ****** *** ** *** *********** * * ***** * **** ** * * ****** * * ******* ******* * ****** ** * *********** * **** * ** ** ** * **** * *** ** *** ********** ******** ** ***** ** ***** ** **** * **** ********* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** ** ****** * * ************** * ****** ** * **** * ****** * ** ** * ** ** ** * **** * *** ** ************* ******** ** **** *** ***** ** **** * ************* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * *** ** * **** ** * * ****** * * ************** * ****** ** * **** * ****** * **** * ** ** ** * **** * *** ** **** ***** **** ******** ** ***** ** ***** ** **** * ************* * ** ******** * **** ********* *** * ******* ** ****** ** * * * *** *********** ** ***** * **** ** * * * * **** * * ************** * ****** ** * **** * ******Feature Space H
FIGURE 5.14. (Left panel) The ﬁrst 16normalized eigenvectors of K, the
200×200kernel matrix for the ﬁrst coordinate of the mixture data. Th ese are
viewed as estimates ˆφℓof the eigenfunctions in (5.45), and are represented as
functions in IR1with the observed values superimposed in color. They are arran ged
in rows, starting at the top left. (Right panel) Rescaled versio nshℓ=√ˆγℓˆφℓof
the functions in the left panel, for which the kernel computes t he “inner product.”
0 10 20 30 40 501e−15 1e−11 1e−07 1e−03 1e+01Eigenvalue
FIGURE 5.15. The largest 50eigenvalues of K; all those beyond the 30th are
eﬀectively zero.

174 5. Basis Expansions and Regularization
ingfeature space representation of the eigenfunctions
hℓ(x) =/radicalbig
ˆγℓˆφℓ(x), ℓ= 1,...,N. (5.65)
Notethat∝an}⌊∇a⌋ketle{th(xi),h(xi′)∝an}⌊∇a⌋ket∇i}ht=K(xi,xi′).Thescalingbytheeigenvaluesquickly
shrinks most of the functions down to zero, leaving an eﬀecti ve dimension
of about 12 in this case. The corresponding optimization pro blem is a stan-
dard ridge regression, as in (5.63). So although in principl e the implicit
feature space is inﬁnite dimensional, the eﬀective dimensi on is dramat-
ically lower because of the relative amounts of shrinkage ap plied to each
basis function. The kernel scale parameter νplays a role here as well; larger
νimplies more local kmfunctions, and increases the eﬀective dimension of
the feature space. See Hastie and Zhu (2006) for more details .
It is also known (Girosi et al., 1995) that a thin-plate splin e (Section 5.7)
is an expansion in radial basis functions, generated by the k ernel
K(x,y) =∝⌊a∇⌈⌊lx−y∝⌊a∇⌈⌊l2log(∝⌊a∇⌈⌊lx−y∝⌊a∇⌈⌊l). (5.66)
Radial basis functions are discussed in more detail in Secti on 6.7.
Support Vector Classiﬁers
The support vector machines of Chapter 12 for a two-class cla ssiﬁcation
problem have the form f(x) =α0+/summationtextN
i=1αiK(x,xi), where the parameters
are chosen to minimize
min
α0,α/braceleftiggN/summationdisplay
i=1[1−yif(xi)]++λ
2αTKα/bracerightigg
, (5.67)
whereyi∈{−1,1}, and [z]+denotes the positive part of z. This can be
viewed as a quadratic optimization problem with linear cons traints, and
requires a quadratic programming algorithm for its solutio n. The name
support vector arises from the fact that typically many of the ˆ αi= 0 [due
to the piecewise-zero nature of the loss function in (5.67)] , and so ˆfis an
expansion in a subset of the K(·,xi). See Section 12.3.3 for more details.
5.9 Wavelet Smoothing
We have seen two diﬀerent modes of operation with dictionari es of basis
functions. With regression splines, we select a subset of th e bases, using
either subject-matter knowledge, or else automatically. T he more adaptive
procedures such as MARS (Chapter 9) can capture both smooth a nd non-
smooth behavior. With smoothing splines, we use a complete b asis, but
then shrink the coeﬃcients toward smoothness.

5.9 Wavelet Smoothing 175
Time0.0 0.2 0.4 0.6 0.8 1.0Haar Wavelets
Time0.0 0.2 0.4 0.6 0.8 1.0Symmlet-8 Wavelets
ψ1,0ψ2,1ψ2,3ψ3,2ψ3,5ψ4,4ψ4,9ψ5,1ψ5,15ψ6,15ψ6,35
FIGURE 5.16. Some selected wavelets at diﬀerent translations and dilations
for the Haar and symmlet families. The functions have been scale d to suit the
display.
Wavelets typically use a complete orthonormal basis to repr esent func-
tions, but then shrink and select the coeﬃcients toward a sparserepresen-
tation. Just as a smooth function can be represented by a few s pline basis
functions, a mostly ﬂat function with a few isolated bumps ca n be repre-
sented with a few (bumpy) basis functions. Wavelets bases ar e very popular
in signal processing and compression, since they are able to represent both
smooth and/or locally bumpy functions in an eﬃcient way—a phe nomenon
dubbedtime and frequency localization . In contrast, the traditional Fourier
basis allows only frequency localization.
Before we give details, let’s look at the Haar wavelets in the left panel
of Figure 5.16 to get an intuitive idea of how wavelet smoothi ng works.
The vertical axis indicates the scale (frequency) of the wav elets, from low
scale at the bottom to high scale at the top. At each scale the w avelets are
“packedin”side-by-sidetocompletelyﬁllthetimeaxis:we haveonlyshown

176 5. Basis Expansions and Regularization
a selected subset. Wavelet smoothing ﬁts the coeﬃcients for this basis by
least squares, and then thresholds (discards, ﬁlters) the s maller coeﬃcients.
Since there are many basis functions at each scale, it can use bases where
it needs them and discard the ones it does not need, to achieve time and
frequencylocalization.TheHaarwaveletsaresimpletound erstand,butnot
smooth enough for most purposes. The symmlet wavelets in the right panel
of Figure 5.16 have the same orthonormal properties, but are smoother.
Figure 5.17 displays an NMR (nuclear magnetic resonance) si gnal, which
appears to be composed of smooth components and isolated spi kes, plus
some noise. The wavelet transform, using a symmlet basis, is shown in the
lower left panel. The wavelet coeﬃcients are arranged in row s, from lowest
scale at the bottom, to highest scale at the top. The length of each line
segment indicates the size of the coeﬃcient. The bottom righ t panel shows
the wavelet coeﬃcients after they have been thresholded. Th e threshold
procedure, given below in equation (5.69), is the same soft- thresholding
rule that arises in the lasso procedure for linear regressio n (Section 3.4.2).
Notice that many of the smaller coeﬃcients have been set to ze ro. The
green curve in the top panel shows the back-transform of the t hresholded
coeﬃcients: this is the smoothed version of the original sig nal. In the next
section we give the details of this process, including the co nstruction of
wavelets and the thresholding rule.
5.9.1 Wavelet Bases and the Wavelet Transform
In this section we give details on the construction and ﬁlter ing of wavelets.
Wavelet bases are generated by translations and dilations o f a single scal-
ing function φ(x) (also known as the father). The red curves in Figure 5.18
are theHaarandsymmlet-8 scaling functions. The Haar basis is particu-
larly easy to understand, especially for anyone with experi ence in analysis
of variance or trees, since it produces a piecewise-constan t representation.
Thus ifφ(x) =I(x∈[0,1]), thenφ0,k(x) =φ(x−k),kan integer, generates
an orthonormal basis for functions with jumps at the integer s. Call this ref-
erencespaceV0. The dilations φ1,k(x) =√
2φ(2x−k) form an orthonormal
basis for a space V1⊃V0of functions piecewise constant on intervals of
length1
2. In fact, more generally we have ···⊃V1⊃V0⊃V−1⊃···where
eachVjis spanned by φj,k= 2j/2φ(2jx−k).
Now to the deﬁnition of wavelets. In analysis of variance, we often rep-
resent a pair of means µ1andµ2by their grand mean µ=1
2(µ1+µ2), and
then a contrast α=1
2(µ1−µ2). A simpliﬁcation occurs if the contrast αis
very small, because then we can set it to zero. In a similar man ner we might
represent a function in Vj+1by a component in Vjplus the component in
the orthogonal complement WjofVjtoVj+1, written as Vj+1=Vj⊕Wj.
The component in Wjrepresents detail, and we might wish to set some ele-
ments of this component to zero. It is easy to see that the func tionsψ(x−k)

5.9 Wavelet Smoothing 177
NMR Signal
0 200 400 600 800 10000 20 40 60
0 200 400 600 800 1000Wavelet Transform - Original Signal
0 200 400 600 800 1000Wavelet Transform - WaveShrunk Signal
Signal Signal
W9 W9
W8 W8
W7 W7
W6 W6
W5 W5
W4 W4
V4 V4
FIGURE 5.17. The top panel shows an NMR signal, with the wavelet-shrunk
version superimposed in green. The lower left panel represent s the wavelet trans-
form of the original signal, down to V4, using the symmlet-8 basis. Each coeﬃ-
cient is represented by the height (positive or negative) of the vertical bar. The
lower right panel represents the wavelet coeﬃcients after bei ng shrunken using
thewaveshrink function in S-PLUS, which implements the SureShrink method
of wavelet adaptation of Donoho and Johnstone.

178 5. Basis Expansions and Regularization
Haar Basis Symmlet Basis
φ(x) φ(x)
ψ(x) ψ(x)
FIGURE 5.18. TheHaarandsymmlet father (scaling) wavelet φ(x)and mother
waveletψ(x).
generatedbythe mother wavelet ψ(x) =φ(2x)−φ(2x−1)formanorthonor-
mal basis for W0for the Haar family. Likewise ψj,k= 2j/2ψ(2jx−k) form
a basis forWj.
NowVj+1=Vj⊕Wj=Vj−1⊕Wj−1⊕Wj, so besides representing a
function by its level- jdetail and level- jrough components, the latter can
be broken down to level-( j−1) detail and rough, and so on. Finally we get
a representation of the form VJ=V0⊕W0⊕W1···⊕WJ−1. Figure 5.16
on page 175 shows particular wavelets ψj,k(x).
Notice that since these spaces are orthogonal, all the basis functions are
orthonormal. In fact, if the domain is discrete with N= 2J(time) points,
this is as far as we can go. There are 2jbasis elements at level j, and
adding up, we have a total of 2J−1 elements in the Wj, and one in V0.
This structured orthonormal basis allows for a multiresolution analysis ,
which we illustrate in the next section.
While helpful for understanding the construction above, th e Haar basis
is often too coarse for practical purposes. Fortunately, ma ny clever wavelet
bases have been invented. Figures 5.16 and 5.18 include the Daubechies
symmlet-8 basis. This basis has smoother elements than the correspond ing
Haar basis, but there is a tradeoﬀ:
•Each wavelet has a support covering 15 consecutive time inte rvals,
rather than one for the Haar basis. More generally, the symml et-p
family has a support of 2 p−1 consecutive intervals. The wider the
support, the more time the wavelet has to die to zero, and so it can

5.9 Wavelet Smoothing 179
achieve this more smoothly. Note that the eﬀective support s eems to
be much narrower.
•The symmlet- pwaveletψ(x) haspvanishing moments; that is,
/integraldisplay
ψ(x)xjdx= 0, j= 0,...,p−1.
Oneimplicationisthatanyorder- ppolynomialoverthe N= 2Jtimes
points is reproduced exactly in V0(Exercise 5.18). In this sense V0
is equivalent to the null space of the smoothing-spline pena lty. The
Haar wavelets have one vanishing moment, and V0can reproduce any
constant function.
The symmlet- pscaling functions are one of many families of wavelet
generators. The operations are similar to those for the Haar basis:
•IfV0is spanned by φ(x−k), thenV1⊃V0is spanned by φ1,k(x) =√
2φ(2x−k)andφ(x) =/summationtext
k∈Zh(k)φ1,k(x), for some ﬁlter coeﬃcients
h(k).
•W0is spanned by ψ(x) =/summationtext
k∈Zg(k)φ1,k(x), with ﬁlter coeﬃcients
g(k) = (−1)1−kh(1−k).
5.9.2 Adaptive Wavelet Filtering
Wavelets are particularly useful when the data are measured on a uniform
lattice, such as a discretized signal, image, or a time serie s. We will focus on
the one-dimensional case, and having N= 2Jlattice-points is convenient.
Suppose yis the response vector, and Wis theN×Northonormal wavelet
basis matrix evaluated at the Nuniformly spaced observations. Then y∗=
WTyis called the wavelet transform ofy(and is the full least squares
regression coeﬃcient). A popular method for adaptive wavel et ﬁtting is
known as SURE shrinkage (Stein Unbiased Risk Estimation, Donoho and
Johnstone (1994)). We start with the criterion
min
θ||y−Wθ||2
2+2λ||θ||1, (5.68)
which is the same as the lasso criterion in Chapter 3. Because Wis or-
thonormal, this leads to the simple solution:
ˆθj= sign(y∗
j)(|y∗
j|−λ)+. (5.69)
The least squares coeﬃcients are translated toward zero, an d truncated
at zero. The ﬁtted function (vector) is then given by the inverse wavelet
transform ˆf=Wˆθ.

180 5. Basis Expansions and Regularization
A simple choice for λisλ=σ√2logN, whereσis an estimate of the
standarddeviationofthenoise.Wecangivesomemotivation forthischoice.
SinceWis an orthonormal transformation, if the elements of yare white
noise (independent Gaussian variates with mean 0 and varian ceσ2), then
so arey∗. Furthermore if random variables Z1,Z2,...,Z Nare white noise,
the expected maximum of |Zj|,j= 1,...,Nis approximately σ√2logN.
Hence all coeﬃcients below σ√2logNare likely to be noise and are set to
zero.
The space Wcould be any basis of orthonormal functions: polynomials,
naturalsplinesorcosinusoids.Whatmakeswaveletsspecia listheparticular
form of basis functions used, which allows for a representat ionlocalized in
time and in frequency .
Let’s look again at the NMR signal of Figure 5.17. The wavelet transform
was computed using a symmlet−8 basis. Notice that the coeﬃcients do not
descend all the way to V0, but stop at V4which has 16 basis functions.
As we ascend to each level of detail, the coeﬃcients get small er, except in
locationswherespikybehaviorispresent.Thewaveletcoeﬃ cientsrepresent
characteristics of the signal localized in time (the basis f unctions at each
levelaretranslationsofeachother)andlocalizedinfrequ ency.Eachdilation
increases the detail by a factor of two, and in this sense corr esponds to
doubling the frequency in a traditional Fourier representa tion. In fact, a
more mathematical understanding of wavelets reveals that t he wavelets at
a particular scale have a Fourier transform that is restrict ed to a limited
range or octave of frequencies.
Theshrinking/truncationintherightpanelwasachievedus ingtheSURE
approach described in the introduction to this section. The orthonormal
N×Nbasis matrix Whas columns which are the wavelet basis functions
evaluated at the Ntime points. In particular, in this case there will be 16
columns corresponding to the φ4,k(x), and the remainder devoted to the
ψj,k(x), j= 4,...,11. In practice λdepends on the noise variance, and has
to be estimated from the data (such as the variance of the coeﬃ cients at
the highest level).
Notice the similarity between the SURE criterion (5.68) on p age 179,
and the smoothing spline criterion (5.21) on page 156:
•Both are hierarchically structured from coarse to ﬁne detai l, although
wavelets are also localized in time within each resolution l evel.
•The splines build in a bias toward smooth functions by imposi ng
diﬀerential shrinking constants dk. Early versions of SURE shrinkage
treated all scales equally. The S+wavelets function waveshrink() has
many options, some of which allow for diﬀerential shrinkage .
•The spline L2penalty cause pure shrinkage, while the SURE L1
penalty does shrinkage and selection.

Exercises 181
More generally smoothing splines achieve compression of th e original signal
by imposing smoothness, while wavelets impose sparsity. Fi gure 5.19 com-
pares a wavelet ﬁt (using SURE shrinkage) to a smoothing spli ne ﬁt (using
cross-validation) on two examples diﬀerent in nature. For t he NMR data in
the upper panel, the smoothing spline introduces detail eve rywhere in order
to capture the detail in the isolated spikes; the wavelet ﬁt n icely localizes
the spikes. In the lower panel, the true function is smooth, a nd the noise is
relatively high. The wavelet ﬁt has let in some additional an d unnecessary
wiggles—a price it pays in variance for the additional adapti vity.
The wavelet transform is not performed by matrix multiplica tion as in
y∗=WTy. In fact, using clever pyramidal schemes y∗can be obtained
inO(N) computations, which is even faster than the Nlog(N) of the fast
Fourier transform (FFT). While the general construction is beyond the
scope of this book, it is easy to see for the Haar basis (Exerci se 5.19).
Likewise, the inverse wavelet transform Wˆθis alsoO(N).
This has been a very brief glimpse of this vast and growing ﬁel d. There is
a very large mathematical and computational base built on wa velets. Mod-
ern image compression is often performed using two-dimensi onal wavelet
representations.
Bibliographic Notes
Splines and B-splines are discussed in detail in de Boor (1978). Green
and Silverman (1994) and Wahba (1990) give a thorough treatm ent of
smoothing splines and thin-plate splines; the latter also c overs reproducing
kernel Hilbert spaces. See also Girosi et al. (1995) and Evge niou et al.
(2000) for connections between many nonparametric regress ion techniques
using RKHS approaches. Modeling functional data, as in Sect ion 5.2.3, is
covered in detail in Ramsay and Silverman (1997).
Daubechies (1992) is a classic and mathematical treatment o f wavelets.
OtherusefulsourcesareChui(1992)andWickerhauser(1994 ).Donohoand
Johnstone (1994) developed the SURE shrinkage and selectio n technology
from a statistical estimation framework; see also Vidakovi c (1999). Bruce
and Gao (1996) is a useful applied introduction, which also d escribes the
wavelet software in S-PLUS.
Exercises
Ex. 5.1Show that the truncated power basis functions in (5.3) repre sent a
basis for a cubic spline with the two knots as indicated.

182 5. Basis Expansions and Regularization
NMR Signal0 200 400 600 800 10000 20 40 60spline
wavelet
Smooth Function (Simulated)n
0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2 4spline
wavelet
true•
••
•
•••
•
•
•
••••
•••
••••
••
••
••
••••••
•••
•
••
••
•••••
•••••••
••••
•
•••
••
••
••
•••
••••
••••
••
••
•••
•••
•••
••
•
•••
•••
•
••••••
••••
••
•
•••••
••••
••
•••
FIGURE 5.19. Wavelet smoothing compared with smoothing splines on two
examples. Each panel compares the SURE-shrunk wavelet ﬁt to th e cross-validated
smoothing spline ﬁt.

Exercises 183
Ex. 5.2Suppose that Bi,M(x) is an order- M B-spline deﬁned in the Ap-
pendix on page 186 through the sequence (5.77)–(5.78).
(a) Show by induction that Bi,M(x) = 0 forx∝ne}ationslash∈[τi,τi+M]. This shows, for
example, that the support of cubic B-splines is at most 5 knots.
(b) Show by induction that Bi,M(x)>0 forx∈(τi,τi+M). TheB-splines
are positive in the interior of their support.
(c) Show by induction that/summationtextK+M
i=1Bi,M(x) = 1∀x∈[ξ0,ξK+1].
(d) Show that Bi,Mis a piecewise polynomial of order M(degreeM−1)
on [ξ0,ξK+1], with breaks only at the knots ξ1,...,ξ K.
(e) Show that an order- M B-spline basis function is the density function
of a convolution of Muniform random variables.
Ex. 5.3Write a program to reproduce Figure 5.3 on page 145.
Ex.5.4Considerthetruncatedpowerseriesrepresentationforcub icsplines
withKinterior knots. Let
f(X) =3/summationdisplay
j=0βjXj+K/summationdisplay
k=1θk(X−ξk)3
+. (5.70)
Prove that the natural boundary conditions for natural cubi c splines (Sec-
tion 5.2.1) imply the following linear constraints on the co eﬃcients:
β2= 0,/summationtextK
k=1θk= 0,
β3= 0,/summationtextK
k=1ξkθk= 0.(5.71)
Hence derive the basis (5.4) and (5.5).
Ex. 5.5Write a program to classify the phonemedata using a quadratic dis-
criminant analysis (Section 4.3). Since there are many corr elated features,
you should ﬁlter them using a smooth basis of natural cubic sp lines (Sec-
tion 5.2.3). Decide beforehand on a series of ﬁve diﬀerent ch oices for the
number and position of the knots, and use tenfold cross-vali dation to make
the ﬁnal selection. The phonemedata are available from the book website
www-stat.stanford.edu/ElemStatLearn .
Ex. 5.6Suppose you wish to ﬁt a periodic function, with a known perio dT.
Describe how you could modify the truncated power series bas is to achieve
this goal.
Ex. 5.7Derivation of smoothing splines (Green and Silverman, 1994). Sup-
pose thatN≥2, and that gis the natural cubic spline interpolant to the
pairs{xi,zi}N
1, witha < x 1<···< xN< b. This is a natural spline

184 5. Basis Expansions and Regularization
with a knot at every xi; being anN-dimensional space of functions, we can
determine the coeﬃcients such that it interpolates the sequ enceziexactly.
Let ˜gbe any other diﬀerentiable function on [ a,b] that interpolates the N
pairs.
(a) Leth(x) = ˜g(x)−g(x). Use integration by parts and the fact that gis
a natural cubic spline to show that
/integraldisplayb
ag′′(x)h′′(x)dx=−N−1/summationdisplay
j=1g′′′(x+
j){h(xj+1)−h(xj)}(5.72)
= 0.
(b) Hence show that/integraldisplayb
a˜g′′(t)2dt≥/integraldisplayb
ag′′(t)2dt,
and that equality can only hold if his identically zero in [ a,b].
(c) Consider the penalized least squares problem
min
f/bracketleftiggN/summationdisplay
i=1(yi−f(xi))2+λ/integraldisplayb
af′′(t)2dt/bracketrightigg
.
Use (b) to argue that the minimizer must be a cubic spline with knots
at each of the xi.
Ex. 5.8In the appendix to this chapter we show how the smoothing spli ne
computations could be more eﬃciently carried out using a ( N+4) dimen-
sional basis of B-splines. Describe a slightly simpler scheme using a ( N+2)
dimensional B-spline basis deﬁned on the N−2 interior knots.
Ex. 5.9Derive the Reinsch form Sλ= (I+λK)−1for the smoothing spline.
Ex. 5.10 Derive an expression for Var( ˆfλ(x0)) and bias( ˆfλ(x0)). Using the
example (5.22), create a version of Figure 5.9 where the mean and several
(pointwise) quantiles of ˆfλ(x) are shown.
Ex. 5.11 Prove that for a smoothing spline the null space of Kis spanned
by functions linear in X.
Ex. 5.12 Characterize the solution to the following problem,
min
fRSS(f,λ) =N/summationdisplay
i=1wi{yi−f(xi)}2+λ/integraldisplay
{f′′(t)}2dt, (5.73)
where thewi≥0 are observation weights.
Characterize the solution to the smoothing spline problem ( 5.9) when
the training data have ties in X.

Exercises 185
Ex. 5.13 You have ﬁtted a smoothing spline ˆfλto a sample of Npairs
(xi,yi).Supposeyouaugmentyouroriginalsamplewiththepair x0,ˆfλ(x0),
and reﬁt; describe the result. Use this to derive the N-fold cross-validation
formula (5.26).
Ex. 5.14 Derive the constraints on the αjin the thin-plate spline expan-
sion (5.39) to guarantee that the penalty J(f) is ﬁnite. How else could one
ensure that the penalty was ﬁnite?
Ex. 5.15 This exercise derives some of the results quoted in Section 5 .8.1.
SupposeK(x,y) satisfying the conditions (5.45) and let f(x)∈HK. Show
that
(a)∝an}⌊∇a⌋ketle{tK(·,xi),f∝an}⌊∇a⌋ket∇i}htHK=f(xi).
(b)∝an}⌊∇a⌋ketle{tK(·,xi),K(·,xj)∝an}⌊∇a⌋ket∇i}htHK=K(xi,xj).
(c) Ifg(x) =/summationtextN
i=1αiK(x,xi), then
J(g) =N/summationdisplay
i=1N/summationdisplay
j=1K(xi,xj)αiαj.
Suppose that ˜ g(x) =g(x)+ρ(x), withρ(x)∈HK, and orthogonal in HK
to each ofK(x,xi), i= 1,...,N. Show that
(d)
N/summationdisplay
i=1L(yi,˜g(xi))+λJ(˜g)≥N/summationdisplay
i=1L(yi,g(xi))+λJ(g) (5.74)
with equality iﬀ ρ(x) = 0.
Ex. 5.16 Consider the ridge regression problem (5.53), and assume M≥N.
Assume you have a kernel Kthat computes the inner product K(x,y) =/summationtextM
m=1hm(x)hm(y).
(a) Derive (5.62) on page 171 in the text. How would you comput e the
matrices VandDγ, givenK? Hence show that (5.63) is equivalent
to (5.53).
(b) Show that
ˆf=Hˆβ
=K(K+λI)−1y, (5.75)
whereHis theN×Mmatrix of evaluations hm(xi), andK=HHT
theN×Nmatrix of inner-products h(xi)Th(xj).

186 5. Basis Expansions and Regularization
(c) Show that
ˆf(x) =h(x)Tˆβ
=N/summationdisplay
i=1K(x,xi)ˆαi (5.76)
andˆα= (K+λI)−1y.
(d) How would you modify your solution if M <N?
Ex. 5.17 Show how to convert the discrete eigen-decomposition of Kin
Section 5.8.2 to estimates of the eigenfunctions of K.
Ex. 5.18 The wavelet function ψ(x) of the symmlet- pwavelet basis has
vanishing moments up to order p. Show that this implies that polynomials
of orderpare represented exactly in V0, deﬁned on page 176.
Ex. 5.19 Show that the Haar wavelet transform of a signal of length N= 2J
can be computed in O(N) computations.
Appendix: Computations for Splines
In this Appendix, we describe the B-spline basis for representing polyno-
mial splines. We also discuss their use in the computations o f smoothing
splines.
B-splines
Before we can get started, we need to augment the knot sequenc e deﬁned
in Section 5.2. Let ξ0<ξ1andξK<ξK+1be twoboundary knots, which
typically deﬁne the domain over which we wish to evaluate our spline. We
now deﬁne the augmented knot sequence τsuch that
•τ1≤τ2≤···≤τM≤ξ0;
•τj+M=ξj, j= 1,···,K;
•ξK+1≤τK+M+1≤τK+M+2≤···≤τK+2M.
The actual values of these additional knots beyond the bound ary are arbi-
trary, and it is customary to make them all the same and equal t oξ0and
ξK+1, respectively.
Denote by Bi,m(x) theithB-spline basis function of order mfor the
knot-sequence τ,m≤M. They are deﬁned recursively in terms of divided

Appendix: Computations for Splines 187
diﬀerences as follows:
Bi,1(x) =/braceleftbigg
1 ifτi≤x<τi+1
0 otherwise(5.77)
fori= 1,...,K+2M−1. These are also known as Haar basis functions.
Bi,m(x) =x−τi
τi+m−1−τiBi,m−1(x)+τi+m−x
τi+m−τi+1Bi+1,m−1(x)
fori= 1,...,K+2M−m.
(5.78)
Thus with M= 4,Bi,4, i= 1,···,K+ 4 are the K+ 4 cubicB-spline
basis functions for the knot sequence ξ. This recursion can be contin-
ued and will generate the B-spline basis for any order spline. Figure 5.20
shows the sequence of B-splines up to order four with knots at the points
0.0,0.1,...,1.0. Since we have created some duplicate knots, some care
has to be taken to avoid division by zero. If we adopt the conve ntion
thatBi,1= 0 ifτi=τi+1, then by induction Bi,m= 0 ifτi=τi+1=
...=τi+m. Note also that in the construction above, only the subset
Bi,m, i=M−m+ 1,...,M+Kare required for the B-spline basis
of orderm<Mwith knots ξ.
To fully understand the properties of these functions, and t o show that
they do indeed span the space of cubic splines for the knot seq uence, re-
quires additional mathematical machinery, including the p roperties of di-
vided diﬀerences. Exercise 5.2 explores these issues.
The scope of B-splines is in fact bigger than advertised here, and has to
do with knot duplication. If we duplicate an interior knot in the construc-
tion of the τsequence above, and then generate the B-spline sequence as
before, the resulting basis spans the space of piecewise pol ynomials with
one less continuous derivative at the duplicated knot. In ge neral, if in ad-
dition to the repeated boundary knots, we include the interi or knotξj
1≤rj≤Mtimes, then the lowest-order derivative to be discontinuou s
atx=ξjwill be order M−rj. Thus for cubic splines with no repeats,
rj= 1, j= 1,...,K, and at each interior knot the third derivatives (4 −1)
are discontinuous. Repeating the jth knot three times leads to a discontin-
uous 1st derivative; repeating it four times leads to a disco ntinuous zeroth
derivative, i.e., the function is discontinuous at x=ξj. This is exactly what
happens at the boundary knots; we repeat the knots Mtimes, so the spline
becomes discontinuous at the boundary knots (i.e., undeﬁne d beyond the
boundary).
The local support of B-splines has important computational implica-
tions, especially when the number of knots Kis large. Least squares com-
putations with Nobservations and K+Mvariables (basis functions) take
O(N(K+M)2+(K+M)3) ﬂops (ﬂoating point operations.) If Kis some
appreciable fraction of N, this leads to O(N3) algorithms which becomes

188 5. Basis Expansions and Regularization
B-splines of Order 1
0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2
B-splines of Order 2
0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2
B-splines of Order 3
0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2
B-splines of Order 4
0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2
FIGURE 5.20. The sequence of B-splines up to order four with ten knots evenly
spaced from 0to1. TheB-splines have local support ; they are nonzero on an
interval spanned by M+1knots.

Appendix: Computations for Splines 189
unacceptable for large N. IftheNobservations aresorted, the N×(K+M)
regression matrix consisting of the K+M B-spline basis functions evalu-
ated at the Npoints has many zeros, which can be exploited to reduce the
computational complexity back to O(N). We take this up further in the
next section.
Computations for Smoothing Splines
Although natural splines (Section 5.2.1) provide a basis fo r smoothing
splines, it is computationally more convenient to operate i n the larger space
of unconstrained B-splines. We write f(x) =/summationtextN+4
1γjBj(x), whereγjare
coeﬃcients and the Bjare the cubic B-spline basis functions. The solution
looks the same as before,
ˆγ= (BTB+λΩB)−1BTy, (5.79)
except now the N×NmatrixNis replaced by the N×(N+ 4) matrix
B, and similarly the ( N+ 4)×(N+ 4) penalty matrix ΩBreplaces the
N×Ndimensional ΩN. Although at face value it seems that there are
no boundary derivative constraints, it turns out that the pe nalty term
automatically imposes them by giving eﬀectively inﬁnite we ight to any non
zero derivative beyond the boundary. In practice, ˆ γis restricted to a linear
subspace for which the penalty is always ﬁnite.
Since the columns of Bare the evaluated B-splines, in order from left
to right and evaluated at the sortedvalues ofX, and the cubic B-splines
have local support, Bis lower 4-banded. Consequently the matrix M=
(BTB+λΩ) is 4-banded and hence its Cholesky decomposition M=LLT
canbecomputedeasily.Onethensolves LLTγ=BTybyback-substitution
to giveγand hence the solution ˆfinO(N) operations.
In practice, when Nis large, it is unnecessary to use all Ninterior knots,
and any reasonable thinning strategy will save in computations and have
negligible eﬀect on the ﬁt. For example, the smooth.spline function in S-
PLUS uses an approximately logarithmic strategy: if N <50 all knots are
included, but even at N= 5,000 only 204 knots are used.

190 5. Basis Expansions and Regularization

This is page 191
Printer: Opaque this
6
Kernel Smoothing Methods
In this chapter we describe a class of regression techniques that achieve
ﬂexibility in estimating the regression function f(X) over the domain IRp
by ﬁtting a diﬀerent but simple model separately at each quer y pointx0.
This is done by using only those observations close to the tar get pointx0to
ﬁtthesimplemodel,andinsuchawaythattheresultingestim atedfunction
ˆf(X) issmoothin IRp. This localization is achieved via a weighting function
orkernelKλ(x0,xi), which assigns a weight to xibased on its distance from
x0. The kernels Kλare typically indexed by a parameter λthat dictates
the width of the neighborhood. These memory-based methods require in
principle little or no training; all the work gets done at eva luation time.
The only parameter that needs to be determined from the train ing data is
λ. The model, however, is the entire training data set.
We also discuss more general classes of kernel-based techni ques , which
tie in with structured methods in other chapters, and are use ful for density
estimation and classiﬁcation.
The techniques in this chapter should not be confused with th ose asso-
ciated with the more recent usage of the phrase “kernel metho ds”. In this
chapter kernels are mostly used as a device for localization . We discuss ker-
nel methods in Sections 5.8, 14.5.4, 18.5 and Chapter 12; in t hose contexts
the kernel computes an inner product in a high-dimensional ( implicit) fea-
ture space, and is used for regularized nonlinear modeling. We make some
connections to the methodology in this chapter at the end of S ection 6.7.

192 6. Kernel Smoothing Methods
Nearest-Neighbor Kernel
0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO
OOOO
OO
OO
OOOO
OO
OO
OOOOO
OO
OO
O
OO
O
OO
OO
O
O
OO
O
OO
OO
O
OO
O
OOO
OO
OOOO
OO
OO
OO
OO
OO
OO
OOO
O
OO
OOO
OOO
O
OO
O
OO
O
OOO
O
O
OO
OO
O
OOOO
O
OO
OO
O
OO
O
OOO
OO
OOOO
OO
OO
OO
OO
OO
OO
OO•
x0ˆf(x0)Epanechnikov Kernel
0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO
OOOO
OO
OO
OOOO
OO
OO
OOOOO
OO
OO
O
OO
O
OO
OO
O
O
OO
O
OO
OO
O
OO
O
OOO
OO
OOOO
OO
OO
OO
OO
OO
OO
OOO
O
OO
OOO
OOO
O
OO
O
OO
O
OOO
O
O
OO
OO
O
OOO
OO
OO
O
O
OO
O
OO
OO
O
OO
O
OOO
OO
OOOO
OO
OO
OO
OO
OO
OO
OOO
O
OO•
x0ˆf(x0)
FIGURE 6.1. In each panel 100pairsxi, yiare generated at random from the
blue curve with Gaussian errors: Y= sin(4X)+ε,X∼U[0,1],ε∼N(0,1/3). In
the left panel the green curve is the result of a 30-nearest-neighbor running-mean
smoother. The red point is the ﬁtted constant ˆf(x0), and the red circles indicate
those observations contributing to the ﬁt at x0. The solid yellow region indicates
the weights assigned to observations. In the right panel, the green curve is the
kernel-weighted average, using an Epanechnikov kernel with (half) window width
λ= 0.2.
6.1 One-Dimensional Kernel Smoothers
In Chapter 2, we motivated the k–nearest-neighbor average
ˆf(x) = Ave(yi|xi∈Nk(x)) (6.1)
as an estimate of the regression function E( Y|X=x). HereNk(x) is the set
ofkpoints nearest to xin squared distance, and Ave denotes the average
(mean). The idea is to relax the deﬁnition of conditional exp ectation, as
illustrated in the left panel of Figure 6.1, and compute an av erage in a
neighborhood of the target point. In this case we have used th e 30-nearest
neighborhood—the ﬁt at x0is the average of the 30 pairs whose xivalues
are closest to x0. The green curve is traced out as we apply this deﬁnition
at diﬀerent values x0. The green curve is bumpy, since ˆf(x) is discontinuous
inx. As we move x0from left to right, the k-nearest neighborhood remains
constant, until a point xito the right of x0becomes closer than the furthest
pointxi′in the neighborhood to the left of x0, at which time xireplacesxi′.
The average in (6.1) changes in a discrete way, leading to a di scontinuous
ˆf(x).
This discontinuity is ugly and unnecessary. Rather than giv e all the
points in the neighborhood equal weight, we can assign weigh ts that die
oﬀ smoothly with distance from the target point. The right pa nel shows
an example of this, using the so-called Nadaraya–Watson ker nel-weighted

6.1 One-Dimensional Kernel Smoothers 193
average
ˆf(x0) =/summationtextN
i=1Kλ(x0,xi)yi/summationtextN
i=1Kλ(x0,xi), (6.2)
with the Epanechnikov quadratic kernel
Kλ(x0,x) =D/parenleftbigg|x−x0|
λ/parenrightbigg
, (6.3)
with
D(t) =/braceleftbigg3
4(1−t2) if|t|≤1;
0 otherwise .(6.4)
The ﬁtted function is now continuous, and quite smooth in the right panel
of Figure 6.1. As we move the target from left to right, points enter the
neighborhood initially with weight zero, and then their con tribution slowly
increases (see Exercise 6.1).
In the right panel we used a metric window size λ= 0.2 for the kernel
ﬁt, which does not change as we move the target point x0, while the size
of the 30-nearest-neighbor smoothing window adapts to the l ocal density
of thexi. One can, however, also use such adaptive neighborhoods wit h
kernels, but we need to use a more general notation. Let hλ(x0) be a width
function (indexed by λ) that determines the width of the neighborhood at
x0. Then more generally we have
Kλ(x0,x) =D/parenleftbigg|x−x0|
hλ(x0)/parenrightbigg
. (6.5)
In (6.3),hλ(x0) =λis constant. For k-nearest neighborhoods, the neigh-
borhood size kreplacesλ, and we have hk(x0) =|x0−x[k]|wherex[k]is
thekth closestxitox0.
There are a number of details that one has to attend to in pract ice:
•The smoothing parameter λ, which determines the width of the local
neighborhood, has to be determined. Large λimplies lower variance
(averages over more observations) but higher bias (we essen tially as-
sume the true function is constant within the window).
•Metric window widths (constant hλ(x)) tend to keep the bias of the
estimate constant, but the variance is inversely proportio nal to the
local density. Nearest-neighbor window widths exhibit the opposite
behavior; the variance stays constant and the absolute bias varies
inversely with local density.
•Issues arise with nearest-neighbors when there are ties in t hexi. With
most smoothing techniques one can simply reduce the data set by
averaging the yiat tied values of X, and supplementing these new
observations at the unique values of xiwith an additional weight wi
(which multiples the kernel weight).

194 6. Kernel Smoothing Methods
-3 -2 -1 0 1 2 30.0 0.4 0.8Epanechnikov
Tri-cube
GaussianKλ(x0,x)
FIGURE 6.2. A comparison of three popular kernels for local smoothing. Each
has been calibrated to integrate to 1. The tri-cube kernel is compact and has two
continuous derivatives at the boundary of its support, while the Epanechnikov ker-
nel has none. The Gaussian kernel is continuously diﬀerentiab le, but has inﬁnite
support.
•This leaves a more general problem to deal with: observation weights
wi. Operationally we simply multiply them by the kernel weight s be-
fore computing the weighted average. With nearest neighbor hoods, it
is now natural to insist on neighborhoods with a total weight content
k(relative to/summationtextwi). In the event of overﬂow (the last observation
needed in a neighborhood has a weight wjwhich causes the sum of
weights to exceed the budget k), then fractional parts can be used.
•Boundary issues arise. The metric neighborhoods tend to con tain less
points on the boundaries, while the nearest-neighborhoods get wider.
•The Epanechnikov kernel has compact support (needed when us ed
with nearest-neighbor window size). Another popular compa ct kernel
is based on the tri-cube function
D(t) =/braceleftbigg
(1−|t|3)3if|t|≤1;
0 otherwise(6.6)
This is ﬂatter on the top (like the nearest-neighbor box) and is diﬀer-
entiable at the boundary of its support. The Gaussian densit y func-
tionD(t) =φ(t) is a popular noncompact kernel, with the standard-
deviation playing the role of the window size. Figure 6.2 com pares
the three.
6.1.1 Local Linear Regression
We have progressed from the raw moving average to a smoothly v arying
locally weighted average by using kernel weighting. The smo oth kernel ﬁt
still has problems, however, as exhibited in Figure 6.3 (lef t panel). Locally-
weighted averages can be badly biased on the boundaries of th e domain,

6.1 One-Dimensional Kernel Smoothers 195
N-W Kernel at Boundary
0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O
OO
OO
OO
OO
OO
OOOOO
OOO
OOO
OOO
O
OO
OO
O
OO
OOO
OOOO
O
OO
OO
O
OO
OO
OOOO
OO
OOOO
OOO
OO
OO
O
OOO
OOOO
O
O
OO
OOO
OO
O
OOO
OO
O
OO
O
OOO
O
OOO
OO
OO
OO
OO
OO
OOOOO
OOO
OOO
OOO
O
OO
OO
•
x0ˆf(x0)Local Linear Regression at Boundary
0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O
OO
OO
OO
OO
OO
OOOOO
OOO
OOO
OOO
O
OO
OO
O
OO
OOO
OOOO
O
OO
OO
O
OO
OO
OOOO
OO
OOOO
OOO
OO
OO
O
OOO
OOOO
O
O
OO
OOO
OO
O
OOO
OO
O
OO
O
OOO
O
OOO
OO
OO
OO
OO
OO
OOOOO
OOO
OOO
OOO
O
OO
OO
•
x0ˆf(x0)
FIGURE 6.3. The locally weighted average has bias problems at or near the
boundaries of the domain. The true function is approximately linear here, but
most of the observations in the neighborhood have a higher me an than the target
point, so despite weighting, their mean will be biased upwar ds. By ﬁtting a locally
weighted linear regression (right panel), this bias is remove d to ﬁrst order.
because of the asymmetry of the kernel in that region. By ﬁtti ng straight
lines rather than constants locally, we can remove this bias exactly to ﬁrst
order; see Figure 6.3 (right panel). Actually, this bias can be present in the
interior of the domain as well, if the Xvalues are not equally spaced (for
the same reasons, but usually less severe). Again locally we ighted linear
regression will make a ﬁrst-order correction.
Locallyweightedregressionsolvesaseparateweightedlea stsquaresprob-
lem at each target point x0:
min
α(x0),β(x0)N/summationdisplay
i=1Kλ(x0,xi)[yi−α(x0)−β(x0)xi]2. (6.7)
The estimate is then ˆf(x0) = ˆα(x0)+ˆβ(x0)x0. Notice that although we ﬁt
an entire linear model to the data in the region, we only use it to evaluate
the ﬁt at the single point x0.
Deﬁne the vector-valued function b(x)T= (1,x). LetBbe theN×2
regression matrix with ith rowb(xi)T, andW(x0) theN×Ndiagonal
matrix with ith diagonal element Kλ(x0,xi). Then
ˆf(x0) =b(x0)T(BTW(x0)B)−1BTW(x0)y (6.8)
=N/summationdisplay
i=1li(x0)yi. (6.9)
Equation (6.8) gives an explicit expression for the local li near regression
estimate, and (6.9) highlights the fact that the estimate is linearin the

196 6. Kernel Smoothing Methods
Local Linear Equivalent Kernel at Boundary
0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO
OO
OO
OOO
OO
OOO
OOO
O
OOOOOOO
OO
OO
OO
OO
O
OO
OOOOO
OO
OO
OOOO
OO
O
OO
O
OO
OO
OO
O
OO
OOO
OOO
OOOO
OO
O
OOO
OOOOO
O
OO
OO
OOOO
OO
OOO
OOO
OO
OO
OOO
OO
OOO
OOO
O
OOOOOO
•••••••••
•
••
••
•••••••••••••• • ••••• •• • • • ••• • ••• • •• ••• • •• ••••• ••• • ••• •••• •• • •• • • •• • •• •••• ••• • • • ••••••
x0ˆf(x0)Local Linear Equivalent Kernel in Interior
0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO
OO
OO
OOO
OO
OOO
OOO
O
OOOOOOO
OO
OO
OO
OO
O
OO
OOOOO
OO
OO
OOOO
OO
O
OO
O
OO
OO
OO
O
OO
OOO
OOO
OOOO
OO
O
OOO
OOOOO
O
OO
OO
OOOO
OO
OOO
OO
OO
OO
OO
O
OO
OOOOO
OO
OO
OOOO
OO
O
OO
O
OO
OO
OO
O
OO
OOO
OOO•
•• • ••• •••• •••• • •••••••• ••• ••••••••••••••••••••••••••••••••••••••
•••
••
• •• • ••• • •• • •• •••• ••• • • • ••••••
x0ˆf(x0)
FIGURE 6.4. The green points show the equivalent kernel li(x0)for local re-
gression. These are the weights in ˆf(x0) =/summationtextN
i=1li(x0)yi, plotted against their
corresponding xi. For display purposes, these have been rescaled, since in fact
they sum to 1. Since the yellow shaded region is the (rescaled) equivalent ke rnel
for the Nadaraya–Watson local average, we see how local regres sion automati-
cally modiﬁes the weighting kernel to correct for biases due t o asymmetry in the
smoothing window.
yi(theli(x0) do not involve y). These weights li(x0) combine the weight-
ing kernelKλ(x0,·) and the least squares operations, and are sometimes
referred to as the equivalent kernel . Figure 6.4 illustrates the eﬀect of lo-
cal linear regression on the equivalent kernel. Historical ly, the bias in the
Nadaraya–Watson and other local average kernel methods wer e corrected
by modifying the kernel. These modiﬁcations were based on th eoretical
asymptotic mean-square-error considerations, and beside s being tedious to
implement, are only approximate for ﬁnite sample sizes. Loc al linear re-
gression automatically modiﬁes the kernel to correct the bias exactlyto
ﬁrst order, a phenomenon dubbed as automatic kernel carpentry . Consider
the following expansion for E ˆf(x0), using the linearity of local regression
and a series expansion of the true function faroundx0,
Eˆf(x0) =N/summationdisplay
i=1li(x0)f(xi)
=f(x0)N/summationdisplay
i=1li(x0)+f′(x0)N/summationdisplay
i=1(xi−x0)li(x0)
+f′′(x0)
2N/summationdisplay
i=1(xi−x0)2li(x0)+R, (6.10)
where the remainder term Rinvolves third- and higher-order derivatives of
f, and is typically small under suitable smoothness assumpti ons. It can be

6.1 One-Dimensional Kernel Smoothers 197
Local Linear in Interior
0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO
O
OO
OOO
OOO
O
OOO
OOO
O
OOO
O
OOO
OO
O
OO
OOO
OO
OO
O
OOO
OO
OOO
OO
OO
O
OO
O
OOOOO
OOO
O
O
O
OOO
OOOO
OOOO
OOOOO
OO
OO
OO
O
OO
O
O
OO
OO
OOO
OOO
O
OOO
O
OOO
OO
O
OO
OOO
OO
OO
O
OOO
OO
OOO
OO
OO
O
OO
O
OOOOO
O•ˆf(x0)Local Quadratic in Interior
0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO
O
OO
OOO
OOO
O
OOO
OOO
O
OOO
O
OOO
OO
O
OO
OOO
OO
OO
O
OOO
OO
OOO
OO
OO
O
OO
O
OOOOO
OOO
O
O
O
OOO
OOOO
OOOO
OOOOO
OO
OO
OO
O
OO
O
O
OO
OO
OOO
OOO
O
OOO
O
OOO
OO
O
OO
OOO
OO
OO
O
OOO
OO
OOO
OO
OO
O
OO
O
OOOOO
O•ˆf(x0)
FIGURE 6.5. Local linear ﬁts exhibit bias in regions of curvature of the tr ue
function. Local quadratic ﬁts tend to eliminate this bias.
shown (Exercise 6.2) that for local linear regression,/summationtextN
i=1li(x0) = 1 and/summationtextN
i=1(xi−x0)li(x0) = 0. Hence the middle term equals f(x0), and since
the bias is E ˆf(x0)−f(x0), we see that it depends only on quadratic and
higher–order terms in the expansion of f.
6.1.2 Local Polynomial Regression
Why stop at local linear ﬁts? We can ﬁt local polynomial ﬁts of any de-
greed,
min
α(x0),βj(x0), j=1,...,dN/summationdisplay
i=1Kλ(x0,xi)
yi−α(x0)−d/summationdisplay
j=1βj(x0)xj
i
2
(6.11)
with solution ˆf(x0) = ˆα(x0)+/summationtextd
j=1ˆβj(x0)xj
0. In fact, an expansion such as
(6.10)willtellusthatthebiaswillonlyhavecomponentsof degreed+1and
higher(Exercise6.2). Figure6.5illustrates localquadra ticregression.Local
linear ﬁts tend to be biased in regions of curvature of the tru e function, a
phenomenon referred to as trimming the hills andﬁlling the valleys . Local
quadratic regression is generally able to correct this bias .
There is of course a price to be paid for this bias reduction, a nd that is
increased variance. The ﬁt in the right panel of Figure 6.5 is slightly more
wiggly, especially in the tails. Assuming the model yi=f(xi) +εi, with
εiindependent and identically distributed with mean zero and variance
σ2, Var(ˆf(x0)) =σ2||l(x0)||2, wherel(x0) is the vector of equivalent kernel
weights atx0. It can be shown (Exercise 6.3) that ||l(x0)||increases with d,
and so there is a bias–variance tradeoﬀ in selecting the poly nomial degree.
Figure 6.6 illustrates these variance curves for degree zer o, one and two

198 6. Kernel Smoothing Methods
Variance
0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5Constant
Linear
Quadratic
FIGURE 6.6. The variances functions ||l(x)||2for local constant, linear and
quadratic regression, for a metric bandwidth ( λ= 0.2) tri-cube kernel.
local polynomials. To summarize some collected wisdom on th is issue:
•Local linear ﬁts can help bias dramatically at the boundarie s at a
modest cost in variance. Local quadratic ﬁts do little at the bound-
aries for bias, but increase the variance a lot.
•Local quadratic ﬁts tend to be most helpful in reducing bias d ue to
curvature in the interior of the domain.
•Asymptotic analysis suggest that local polynomials of odd d egree
dominate those of even degree. This is largely due to the fact that
asymptotically the MSE is dominated by boundary eﬀects.
While it may be helpful to tinker, and move from local linear ﬁ ts at the
boundary to local quadratic ﬁts in the interior, we do not rec ommend such
strategies. Usually the application will dictate the degre e of the ﬁt. For
example, if we are interested in extrapolation, then the bou ndary is of
more interest, and local linear ﬁts are probably more reliab le.
6.2 Selecting the Width of the Kernel
In each of the kernels Kλ,λis a parameter that controls its width:
•For the Epanechnikov or tri-cube kernel with metric width, λis the
radius of the support region.
•For the Gaussian kernel, λis the standard deviation.
•λis the number kof nearest neighbors in k-nearest neighborhoods,
often expressed as a fraction or spank/Nof the total training sample.

6.2 Selecting the Width of the Kernel 199
•••
••••••
•
••
••
•••••
••••• ••••••• •••• •• ••• ••••• • • ••••• •••• ••• • ••••• •••••••• • • • •• •• •••• • ••• • ••••••••••••••••
••
••
••••••
••
•• ••••••• •••• •• ••• •••••• • ••••• ••••••• • ••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• •• ••••• •• •• • •••• •• •• ••••••••••••••••••••• •• ••••• •••••
•
••••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• ••••••• •• •• • •••••• ••••••••••••••••••••••• • • •••••••••••
•••••••••••••••• •• •• •• •••• •••• • ••••••
FIGURE 6.7. Equivalent kernels for a local linear regression smoother (tri- cube
kernel; orange) and a smoothing spline (blue), with matching de grees of freedom.
The vertical spikes indicates the target points.
There is a natural bias–variance tradeoﬀ as we change the wid th of the
averaging window, which is most explicit for local averages :
•If the window is narrow, ˆf(x0) is an average of a small number of yi
close tox0, and its variance will be relatively large—close to that of
an individual yi. The bias will tend to be small, again because each
of theE(yi) =f(xi) should be close to f(x0).
•If the window is wide, the variance of ˆf(x0) will be small relative to
the variance of any yi, because of the eﬀects of averaging. The bias
will be higher, because we are now using observations xifurther from
x0, and there is no guarantee that f(xi) will be close to f(x0).
Similar arguments apply to local regression estimates, say local linear: as
the width goes to zero, the estimates approach a piecewise-l inear function
that interpolates the training data1; as the width gets inﬁnitely large, the
ﬁt approaches the global linear least-squares ﬁt to the data .
The discussion in Chapter 5 on selecting the regularization parameter for
smoothing splines applies here, and will not be repeated. Lo cal regression
smoothers are linear estimators; the smoother matrix in ˆf=Sλyis built up
fromtheequivalentkernels(6.8),andhas ijthentry{Sλ}ij=li(xj).Leave-
one-out cross-validation is particularly simple (Exercis e 6.7), as is general-
ized cross-validation, Cp(Exercise 6.10), and k-fold cross-validation. The
eﬀective degrees of freedom is again deﬁned as trace( Sλ), and can be used
to calibrate the amount of smoothing. Figure 6.7 compares th e equivalent
kernels for a smoothing spline and local linear regression. The local regres-
sion smoother has a span of 40%, which results in df = trace( Sλ) = 5.86.
The smoothing spline was calibrated to have the same df, and t heir equiv-
alent kernels are qualitatively quite similar.
1With uniformly spaced xi; with irregularly spaced xi, the behavior can deteriorate.

200 6. Kernel Smoothing Methods
6.3 Local Regression in IRp
Kernel smoothing and local regression generalize very natu rally to two or
more dimensions. The Nadaraya–Watson kernel smoother ﬁts a constant
locally with weights supplied by a p-dimensional kernel. Local linear re-
gression will ﬁt a hyperplane locally in X, by weighted least squares, with
weights supplied by a p-dimensional kernel. It is simple to implement and
is generally preferred to the local constant ﬁt for its super ior performance
on the boundaries.
Letb(X) be a vector of polynomial terms in Xof maximum degree d.
For example, with d= 1 andp= 2 we get b(X) = (1,X1,X2); withd= 2
we getb(X) = (1,X1,X2,X2
1,X2
2,X1X2); and trivially with d= 0 we get
b(X) = 1. At each x0∈IRpsolve
min
β(x0)N/summationdisplay
i=1Kλ(x0,xi)(yi−b(xi)Tβ(x0))2(6.12)
to producethe ﬁt ˆf(x0) =b(x0)Tˆβ(x0). Typically the kernel will be aradial
function, such as the radial Epanechnikov or tri-cube kerne l
Kλ(x0,x) =D/parenleftbigg||x−x0||
λ/parenrightbigg
, (6.13)
where||·||is the Euclidean norm. Since the Euclidean norm depends on th e
units in each coordinate, it makes most sense to standardize each predictor,
for example, to unit standard deviation, prior to smoothing .
While boundary eﬀects are a problem in one-dimensional smoo thing,
they are a much bigger problem in two or higher dimensions, si nce the
fraction of points on the boundary is larger. In fact, one of t he manifesta-
tions of the curse of dimensionality is that the fraction of p oints close to the
boundary increases to one as the dimension grows. Directly m odifying the
kernel to accommodate two-dimensional boundaries becomes very messy,
especially for irregular boundaries. Local polynomial reg ression seamlessly
performs boundary correction to the desired order in any dim ensions. Fig-
ure 6.8 illustrates local linear regression on some measure ments from an
astronomical study with an unusual predictor design (star- shaped). Here
the boundary is extremely irregular, and the ﬁtted surface m ust also inter-
polateoverregionsofincreasingdatasparsityasweapproa chtheboundary.
Local regression becomes less useful in dimensions much hig her than two
or three. We have discussed in some detail the problems of dim ensional-
ity, for example, in Chapter 2. It is impossible to simultane ously main-
tain localness (⇒low bias) and a sizable sample in the neighborhood ( ⇒
low variance) as the dimension increases, without the total sample size in-
creasing exponentially in p. Visualization of ˆf(X) also becomes diﬃcult in
higher dimensions, and this is often one of the primary goals of smoothing.

6.4 Structured Local Regression Models in IRp201
East-WestSouth-NorthVelocity
East-WestSouth-NorthVelocity
FIGURE 6.8. The left panel shows three-dimensional data, where the respo nse
is the velocity measurements on a galaxy, and the two predictor s record positions
on the celestial sphere. The unusual “star”-shaped design in dicates the way the
measurements were made, and results in an extremely irregular b oundary. The
right panel shows the results of local linear regression smooth ing inIR2, using a
nearest-neighbor window with 15%of the data.
Although the scatter-cloud and wire-frame pictures in Figu re 6.8 look at-
tractive, it is quite diﬃcult to interpret the results excep t at a gross level.
From a data analysis perspective, conditional plots are far more useful.
Figure 6.9 shows an analysis of some environmental data with three pre-
dictors. The trellisdisplay here shows ozone as a function of radiation,
conditioned on the other two variables, temperature and win d speed. How-
ever, conditioning on the value of a variable really implies local to that
value (as in local regression). Above each of the panels in Fi gure 6.9 is an
indication of the range of values present in that panel for ea ch of the condi-
tioning values. In the panel itself the data subsets are disp layed (response
versus remaining variable), and a one-dimensional local li near regression is
ﬁt to the data. Although this is not quite the same as looking a t slices of
a ﬁtted three-dimensional surface, it is probably more usef ul in terms of
understanding the joint behavior of the data.
6.4 Structured Local Regression Models in IRp
When the dimension to sample-size ratio is unfavorable, loc al regression
does not help us much, unless we are willing to make some struc tural as-
sumptions about the model. Much of this book is about structu red regres-
sion and classiﬁcation models. Here we focus on some approac hes directly
related to kernel methods.

202 6. Kernel Smoothing Methods
12345TempWind
0 50 150 250TempWind
TempWind
0 50 150 250TempWindTempWind
TempWind
TempWind
12345TempWind12345TempWind
TempWind
TempWind
TempWindTempWind
TempWind0 50 150 250
TempWind
12345TempWind0 50 150 250
Solar Radiation (langleys)Cube Root Ozone (cube root ppb)
FIGURE 6.9. Three-dimensional smoothing example. The response is (cube -root
of) ozone concentration, and the three predictors are tempe rature, wind speed and
radiation. The trellisdisplay shows ozone as a function of radiation, conditioned
on intervals of temperature and wind speed (indicated by dark er green or orange
shaded bars). Each panel contains about 40%of the range of each of the condi-
tioned variables. The curve in each panel is a univariate local linear regression,
ﬁt to the data in the panel.

6.4 Structured Local Regression Models in IRp203
6.4.1 Structured Kernels
One line of approach is to modify the kernel. The default sphe rical ker-
nel (6.13) gives equal weight to each coordinate, and so a nat ural default
strategy is to standardize each variable to unit standard de viation. A more
general approach is to use a positive semideﬁnite matrix Ato weigh the
diﬀerent coordinates:
Kλ,A(x0,x) =D/parenleftbigg(x−x0)TA(x−x0)
λ/parenrightbigg
. (6.14)
Entire coordinates or directions can be downgraded or omitt ed by imposing
appropriate restrictions on A. For example, if Ais diagonal, then we can
increase or decrease the inﬂuence of individual predictors Xjby increasing
or decreasing Ajj. Often the predictors are many and highly correlated,
suchasthosearisingfromdigitizedanalogsignalsorimage s.Thecovariance
function of the predictors can be used to tailor a metric Athat focuses less,
say, on high-frequency contrasts (Exercise 6.4). Proposal s have been made
for learning the parameters for multidimensional kernels. For example, the
projection-pursuitregressionmodeldiscussedinChapter 11isofthisﬂavor,
where low-rank versions of Aimply ridge functions for ˆf(X). More general
models for Aare cumbersome, and we favor instead the structured forms
for the regression function discussed next.
6.4.2 Structured Regression Functions
We are trying to ﬁt a regression function E(Y|X) =f(X1,X2,...,X p) in
IRp, in which every level of interaction is potentially present . It is natural
to consider analysis-of-variance (ANOVA) decompositions of the form
f(X1,X2,...,X p) =α+/summationdisplay
jgj(Xj)+/summationdisplay
k<ℓgkℓ(Xk,Xℓ)+···(6.15)
andthenintroducestructurebyeliminatingsomeofthehigh er-orderterms.
Additive models assume only main eﬀect terms: f(X) =α+/summationtextp
j=1gj(Xj);
second-order models will have terms with interactions of or der at most
two, and so on. In Chapter 9, we describe iterative backﬁtting algorithms
for ﬁtting such low-order interaction models. In the additi ve model, for
example, if all but the kth term is assumed known, then we can estimate gk
bylocalregressionof Y−/summationtext
j/ne}ationslash=kgj(Xj)onXk.Thisisdoneforeachfunction
in turn, repeatedly, until convergence. The important deta il is that at any
stage, one-dimensional local regression is all that is need ed. The same ideas
can be used to ﬁt low-dimensional ANOVA decompositions.
An important special case of these structured models are the class of
varying coeﬃcient models . Suppose, for example, that we divide the ppre-
dictors inXinto a set (X1,X2,...,X q) withq <p, and the remainder of

204 6. Kernel Smoothing Methods
1012141618202224DepthFemale
20 30 40 50 60DepthFemale
DepthFemale
20 30 40 50 60DepthFemale
DepthFemale
20 30 40 50 60DepthFemaleDepthMale
DepthMale20 30 40 50 60
DepthMale
DepthMale20 30 40 50 60
DepthMale
1012141618202224DepthMale20 30 40 50 60
AgeDiameterAortic Diameter vs Age
FIGURE 6.10. In each panel the aorta diameter is modeled as a linear func-
tion ofage. The coeﬃcients of this model vary with genderanddepthdown
theaorta(left is near the top, right is low down). There is a clear trend in the
coeﬃcients of the linear model.
the variables we collect in the vector Z. We then assume the conditionally
linear model
f(X) =α(Z)+β1(Z)X1+···+βq(Z)Xq. (6.16)
For givenZ, this is a linear model, but each of the coeﬃcients can vary
withZ. It is natural to ﬁt such a model by locally weighted least squ ares:
min
α(z0),β(z0)N/summationdisplay
i=1Kλ(z0,zi)(yi−α(z0)−x1iβ1(z0)−···−xqiβq(z0))2.
(6.17)
Figure 6.10 illustrates the idea on measurements of the huma n aorta.
A longstanding claim has been that the aorta thickens with age. Here we
model the diameter of the aorta as a linear function of age, but allow the
coeﬃcients to vary with genderanddepthdown the aorta. We used a local
regression model separately for males and females. While th e aorta clearly
does thicken with age at the higher regions of the aorta, the r elationship
fades with distance down the aorta. Figure 6.11 shows the int ercept and
slope as a function of depth.

6.5 Local Likelihood and Other Models 205
MaleAge Intercept
Distance Down AortaAge Slope
0.0 0.2 0.4 0.6 0.8 1.0Female
14 16 18 20
Distance Down Aorta
0.0 0.4 0.8 1.2
0.0 0.2 0.4 0.6 0.8 1.0
FIGURE 6.11. The intercept and slope of ageas a function of distance down
the aorta, separately for males and females. The yellow bands ind icate one stan-
dard error.
6.5 Local Likelihood and Other Models
The concept of local regression and varying coeﬃcient model s is extremely
broad: any parametric model can be made local if the ﬁtting me thod ac-
commodates observation weights. Here are some examples:
•Associated with each observation yiis a parameter θi=θ(xi) =xT
iβ
linear in the covariate(s) xi, and inference for βis based on the log-
likelihoodl(β) =/summationtextN
i=1l(yi,xT
iβ). We can model θ(X) more ﬂexibly
by using the likelihood local to x0for inference of θ(x0) =xT
0β(x0):
l(β(x0)) =N/summationdisplay
i=1Kλ(x0,xi)l(yi,xT
iβ(x0)).
Many likelihood models, in particular the family of general ized linear
models including logistic and log-linear models, involve t he covariates
inalinearfashion.Locallikelihoodallowsarelaxationfr omaglobally
linear model to one that is locally linear.

206 6. Kernel Smoothing Methods
•As above, except diﬀerent variables are associated with θfrom those
used for deﬁning the local likelihood:
l(θ(z0)) =N/summationdisplay
i=1Kλ(z0,zi)l(yi,η(xi,θ(z0))).
For example, η(x,θ) =xTθcould be a linear model in x. This will ﬁt
a varying coeﬃcient model θ(z) by maximizing the local likelihood.
•Autoregressive time series models of order khave the form yt=
β0+β1yt−1+β2yt−2+···+βkyt−k+εt. Denoting the lag setby
zt= (yt−1,yt−2,...,y t−k), the model looks like a standard linear
modelyt=zT
tβ+εt, and is typically ﬁt by least squares. Fitting
by local least squares with a kernel K(z0,zt) allows the model to
vary according to the short-term history of the series. This is to be
distinguished from the more traditional dynamic linear mod els that
vary by windowing time.
As an illustration of local likelihood, we consider the loca l version of the
multiclass linear logistic regression model (4.36) of Chap ter 4. The data
consistoffeatures xiandanassociatedcategoricalresponse gi∈{1,2,...,J},
and the linear model has the form
Pr(G=j|X=x) =eβj0+βT
jx
1+/summationtextJ−1
k=1eβk0+βT
kx. (6.18)
The local log-likelihood for this Jclass model can be written
N/summationdisplay
i=1Kλ(x0,xi)/braceleftigg
βgi0(x0)+βgi(x0)T(xi−x0)
−log/bracketleftigg
1+J−1/summationdisplay
k=1exp/parenleftbig
βk0(x0)+βk(x0)T(xi−x0)/parenrightbig/bracketrightigg/bracerightigg
.
(6.19)
Notice that
•we have used gias a subscript in the ﬁrst line to pick out the appro-
priate numerator;
•βJ0= 0 andβJ= 0 by the deﬁnition of the model;
•we have centered the local regressions at x0, so that the ﬁtted poste-
rior probabilities at x0are simply
ˆPr(G=j|X=x0) =eˆβj0(x0)
1+/summationtextJ−1
k=1eˆβk0(x0). (6.20)

6.5 Local Likelihood and Other Models 207
Systolic Blood PressurePrevalence CHD
100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0
ObesityPrevalence CHD
15 25 35 450.0 0.2 0.4 0.6 0.8 1.0
FIGURE 6.12. Each plot shows the binary response CHD (coronary heart dis-
ease) as a function of a risk factor for the South African hear t disease data.
For each plot we have computed the ﬁtted prevalence of CHD using a local linear
logistic regression model. The unexpected increase in the pre valence of CHD at
the lower ends of the ranges is because these are retrospectiv e data, and some of
the subjects had already undergone treatment to reduce their blood pressure and
weight. The shaded region in the plot indicates an estimated p ointwise standard
error band.
This model can be used for ﬂexible multiclass classiﬁcation in moderately
low dimensions, although successes have been reported with the high-
dimensional ZIP-code classiﬁcation problem. Generalized additive models
(Chapter 9) using kernel smoothing methods are closely rela ted, and avoid
dimensionality problems by assuming an additive structure for the regres-
sion function.
As a simple illustration we ﬁt a two-class local linear logis tic model to
the heart disease data of Chapter 4. Figure 6.12 shows the uni variate local
logistic models ﬁt to two of the risk factors (separately). T his is a useful
screeningdevicefordetectingnonlinearities,whentheda tathemselveshave
little visual information to oﬀer. In this case an unexpecte d anomaly is
uncovered in the data, which may have gone unnoticed with tra ditional
methods.
SinceCHDis a binary indicator, we could estimate the conditional pre va-
lence Pr(G=j|x0) by simply smoothing this binary response directly with-
out resorting to a likelihood formulation. This amounts to ﬁ tting a locally
constant logistic regression model (Exercise 6.5). In orde r to enjoy the bias-
correction of local-linear smoothing, it is more natural to operate on the
unrestricted logit scale.
Typically with logistic regression, we compute parameter e stimates as
well as their standard errors. This can be done locally as wel l, and so

208 6. Kernel Smoothing Methods
Systolic Blood Pressure (for CHD group)Density Estimate
100 120 140 160 180 200 2200.0 0.005 0.010 0.015 0.020
FIGURE 6.13. A kernel density estimate for systolic blood pressure (for the
CHD group). The density estimate at each point is the average contribution from
each of the kernels at that point. We have scaled the kernels down by a factor of
10 to make the graph readable.
we can produce, as shown in the plot, estimated pointwise sta ndard-error
bands about our ﬁtted prevalence.
6.6 Kernel Density Estimation and Classiﬁcation
Kernel density estimation is an unsupervised learning proc edure, which
historically precedes kernel regression. It also leads nat urally to a simple
family of procedures for nonparametric classiﬁcation.
6.6.1 Kernel Density Estimation
Suppose we have a random sample x1,...,x Ndrawn from a probability
densityfX(x), and we wish to estimate fXat a pointx0. For simplicity we
assume for now that X∈IR. Arguing as before, a natural local estimate
has the form
ˆfX(x0) =#xi∈N(x0)
Nλ, (6.21)
whereN(x0) is a small metric neighborhood around x0of widthλ. This
estimate is bumpy, and the smooth Parzenestimate is preferred
ˆfX(x0) =1
NλN/summationdisplay
i=1Kλ(x0,xi), (6.22)

6.6 Kernel Density Estimation and Classiﬁcation 209
Systolic Blood PressureDensity Estimates
100 140 180 2200.0 0.010 0.020CHD
no CHD
Systolic Blood PressurePosterior Estimate
100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0
FIGURE 6.14. The left panel shows the two separate density estimates for
systolic blood pressure in the CHD versus no-CHD groups, using a Gaussian
kernel density estimate in each. The right panel shows the es timated posterior
probabilities for CHD, using (6.25).
because it counts observations close to x0with weights that decrease with
distance from x0.Inthis caseapopular choice for Kλis theGaussiankernel
Kλ(x0,x) =φ(|x−x0|/λ). Figure 6.13 shows a Gaussian kernel density ﬁt
tothesamplevaluesfor systolic blood pressure fortheCHDgroup.Letting
φλdenote the Gaussian density with mean zero and standard-dev iationλ,
then (6.22) has the form
ˆfX(x) =1
NN/summationdisplay
i=1φλ(x−xi)
= (ˆF ⋆φλ)(x), (6.23)
the convolution of the sample empirical distribution ˆFwithφλ. The dis-
tribution ˆF(x) puts mass 1 /Nat each of the observed xi, and is jumpy; in
ˆfX(x) we have smoothed ˆFby adding independent Gaussian noise to each
observation xi.
The Parzen density estimate is the equivalent of the local av erage, and
improvements have been proposed along the lines of local reg ression [on the
log scale for densities; see Loader (1999)]. We will not purs ue these here.
In IRpthe natural generalization of the Gaussian density estimat e amounts
to using the Gaussian product kernel in (6.23),
ˆfX(x0) =1
N(2λ2π)p
2N/summationdisplay
i=1e−1
2(||xi−x0||/λ)2. (6.24)

210 6. Kernel Smoothing Methods
0.00.51.0
FIGURE 6.15. The population class densities may have interesting structur e
(left) that disappears when the posterior probabilities are f ormed (right).
6.6.2 Kernel Density Classiﬁcation
Onecanusenonparametricdensityestimatesforclassiﬁcat ioninastraight-
forward fashion using Bayes’ theorem. Suppose for a Jclass problem we ﬁt
nonparametric density estimates ˆfj(X), j= 1,...,Jseparately in each of
the classes, and we also have estimates of the class priors ˆ πj(usually the
sample proportions). Then
ˆPr(G=j|X=x0) =ˆπjˆfj(x0)/summationtextJ
k=1ˆπkˆfk(x0). (6.25)
Figure 6.14 uses this method to estimate the prevalence of CH D for the
heart risk factor study, and should be compared with the left panel of Fig-
ure 6.12. The main diﬀerence occurs in the region of high SBP i n the right
panel of Figure 6.14. In this region the data are sparse for bo th classes, and
since the Gaussian kernel density estimates use metric kern els, the density
estimates are low and of poor quality (high variance) in thes e regions. The
local logistic regression method (6.20) uses the tri-cube k ernel withk-NN
bandwidth; this eﬀectively widens the kernel in this region , and makes use
of the local linear assumption to smooth out the estimate (on the logit
scale).
If classiﬁcation is the ultimate goal, then learning the sep arate class den-
sities well may be unnecessary, and can in fact be misleading . Figure 6.15
shows an example where the densities are both multimodal, bu t the pos-
terior ratio is quite smooth. In learning the separate densi ties from data,
one might decide to settle for a rougher, high-variance ﬁt to capture these
features, which are irrelevant for the purposes of estimati ng the posterior
probabilities. In fact, if classiﬁcation is the ultimate go al, then we need only
to estimate the posterior well near the decision boundary (f or two classes,
this is the set{x|Pr(G= 1|X=x) =1
2}).
6.6.3 The Naive Bayes Classiﬁer
This is a technique that has remained popular over the years, despite its
name (also known as “Idiot’s Bayes”!) It is especially appro priate when

6.6 Kernel Density Estimation and Classiﬁcation 211
the dimension pof the feature space is high, making density estimation
unattractive. The naive Bayes model assumes that given a cla ssG=j, the
featuresXkare independent:
fj(X) =p/productdisplay
k=1fjk(Xk). (6.26)
While this assumption is generally not true, it does simplif y the estimation
dramatically:
•The individual class-conditional marginal densities fjkcan each be
estimated separately using one-dimensional kernel densit y estimates.
This is in fact a generalization of the original naive Bayes p rocedures,
which used univariate Gaussians to represent these margina ls.
•If a component XjofXis discrete, then an appropriate histogram
estimate can be used. This provides a seamless way of mixing v ariable
types in a feature vector.
Despite these rather optimistic assumptions, naive Bayes c lassiﬁers often
outperform far more sophisticated alternatives. The reaso ns are related to
Figure 6.15: although the individual class density estimat es may be biased,
this bias might not hurt the posterior probabilities as much , especially
near the decision regions. In fact, the problem may be able to withstand
considerable bias for the savings in variance such a “naive” assumption
earns.
Starting from (6.26) we can derive the logit-transform (usi ng classJas
the base):
logPr(G=ℓ|X)
Pr(G=J|X)= logπℓfℓ(X)
πJfJ(X)
= logπℓ/producttextp
k=1fℓk(Xk)
πJ/producttextp
k=1fJk(Xk)
= logπℓ
πJ+p/summationdisplay
k=1logfℓk(Xk)
fJk(Xk)
=αℓ+p/summationdisplay
k=1gℓk(Xk).(6.27)
Thishastheformofa generalized additive model ,whichisdescribedinmore
detail in Chapter 9. The models are ﬁt in quite diﬀerent ways t hough; their
diﬀerences are explored in Exercise 6.9. The relationship b etween naive
Bayes and generalized additive models is analogous to that b etween linear
discriminant analysis and logistic regression (Section 4. 4.5).

212 6. Kernel Smoothing Methods
6.7 Radial Basis Functions and Kernels
In Chapter 5, functions are represented as expansions in bas is functions:
f(x) =/summationtextM
j=1βjhj(x). The art of ﬂexible modeling using basis expansions
consists of picking an appropriate family of basis function s, and then con-
trolling the complexity of the representation by selection , regularization, or
both. Some of the families of basis functions have elements t hat are deﬁned
locally; for example, B-splines are deﬁned locally in IR. If more ﬂexibility
is desired in a particular region, then that region needs to b e represented
by more basis functions (which in the case of B-splines translates to more
knots). Tensor products of IR-local basis functions delive r basis functions
local in IRp. Not all basis functions are local—for example, the truncate d
power bases for splines, or the sigmoidal basis functions σ(α0+αx) used
in neural-networks (see Chapter 11). The composed function f(x) can nev-
ertheless show local behavior, because of the particular si gns and values
of the coeﬃcients causing cancellations of global eﬀects. F or example, the
truncated power basis has an equivalent B-spline basis for the same space
of functions; the cancellation is exact in this case.
Kernel methods achieve ﬂexibility by ﬁtting simple models i n a region
local to the target point x0. Localization is achieved via a weighting kernel
Kλ, and individual observations receive weights Kλ(x0,xi).
Radial basis functions combine these ideas, by treating the kernel func-
tionsKλ(ξ,x) as basis functions. This leads to the model
f(x) =M/summationdisplay
j=1Kλj(ξj,x)βj
=M/summationdisplay
j=1D/parenleftbigg||x−ξj||
λj/parenrightbigg
βj, (6.28)
where each basis element is indexed by a location or prototype parameterξj
and a scale parameter λj. A popular choice for Dis the standard Gaussian
density function. There are several approaches to learning the parameters
{λj,ξj,βj}, j= 1,...,M. For simplicity we will focus on least squares
methods for regression, and use the Gaussian kernel.
•Optimize the sum-of-squares with respect to all the paramet ers:
min
{λj,ξj,βj}M
1N/summationdisplay
i=1
yi−β0−M/summationdisplay
j=1βjexp/braceleftigg
−(xi−ξj)T(xi−ξj)
λ2
j/bracerightigg
2
.
(6.29)
This model is commonly referred to as an RBF network, an alter na-
tive to the sigmoidal neural network discussed in Chapter 11 ; theξj
andλjplaying the role of the weights. This criterion is nonconvex

6.7 Radial Basis Functions and Kernels 2130 2 4 6 8 0.0 0.4 0.8 1.2
FIGURE 6.16. Gaussian radial basis functions in IRwith ﬁxed width can leave
holes (top panel). Renormalized Gaussian radial basis function s avoid this prob-
lem, and produce basis functions similar in some respects to B-splines.
with multiple local minima, and the algorithms for optimiza tion are
similar to those used for neural networks.
•Estimate the{λj,ξj}separately from the βj. Given the former, the
estimation of the latter is a simple least squares problem. O ften the
kernel parameters λjandξjare chosen in an unsupervised way using
theXdistribution alone. One of the methods is to ﬁt a Gaussian
mixture density model to the training xi, which provides both the
centersξjand the scales λj. Other even more adhoc approaches use
clustering methods to locate the prototypes ξj, and treat λj=λ
as a hyper-parameter. The obvious drawback of these approac hes is
that the conditional distribution Pr( Y|X) and in particular E(Y|X)
is having no say in where the action is concentrated. On the po sitive
side, they are much simpler to implement.
While it would seem attractive to reduce the parameter set an d assume
a constant value for λj=λ, this can have an undesirable side eﬀect of
creating holes—regions of IRpwhere none of the kernels has appreciable
support, as illustrated in Figure 6.16 (upper panel). Renormalized radial
basis functions,
hj(x) =D(||x−ξj||/λ)/summationtextM
k=1D(||x−ξk||/λ), (6.30)
avoid this problem (lower panel).
The Nadaraya–Watson kernel regression estimator (6.2) in I Rpcan be
viewed as an expansion in renormalized radial basis functio ns,
ˆf(x0) =/summationtextN
i=1yiKλ(x0,xi)/summationtextN
i=1Kλ(x0,xi)
=/summationtextN
i=1yihi(x0) (6.31)

214 6. Kernel Smoothing Methods
with a basis function hilocated at every observation and coeﬃcients yi;
that is,ξi=xi,ˆβi=yi, i= 1,...,N.
Note the similarity between the expansion (6.31) and the sol ution (5.50)
on page 169 to the regularization problem induced by the kern elK. Radial
basis functions form the bridge between the modern “kernel m ethods” and
local ﬁtting technology.
6.8 Mixture Models for Density Estimation and
Classiﬁcation
Themixturemodelisausefultoolfordensityestimation,an dcanbeviewed
as a kind of kernel method. The Gaussian mixture model has the form
f(x) =M/summationdisplay
m=1αmφ(x;µm,Σm) (6.32)
with mixing proportions αm,/summationtext
mαm= 1, and each Gaussian density has
a meanµmand covariance matrix Σm. In general, mixture models can use
any component densities in place of the Gaussian in (6.32): t he Gaussian
mixture model is by far the most popular.
The parameters are usually ﬁt by maximum likelihood, using t he EM
algorithm as described in Chapter 8. Some special cases aris e:
•If the covariance matrices are constrained to be scalar: Σm=σmI,
then (6.32) has the form of a radial basis expansion.
•If in addition σm=σ >0 is ﬁxed, and M↑N, then the max-
imum likelihood estimate for (6.32) approaches the kernel d ensity
estimate (6.22) where ˆ αm= 1/Nand ˆµm=xm.
Using Bayes’ theorem, separate mixture densities in each cl ass lead to ﬂex-
ible models for Pr( G|X); this is taken up in some detail in Chapter 12.
Figure 6.17 shows an application of mixtures to the heart dis ease risk-
factor study. In the top row are histograms of Agefor theno CHDandCHD
groups separately, and then combined on the right. Using the combined
data, we ﬁt a two-component mixture of the form (6.32) with th e (scalars)
Σ1andΣ2not constrained to be equal. Fitting was done via the EM
algorithm (Chapter 8): note that the procedure does not use k nowledge of
theCHDlabels. The resulting estimates were
ˆµ1= 36.4,ˆΣ1= 157.7,ˆα1= 0.7,
ˆµ2= 58.0,ˆΣ2= 15.6, ˆα2= 0.3.
The component densities φ(ˆµ1,ˆΣ1) andφ(ˆµ2,ˆΣ2) are shown in the lower-
left and middle panels. The lower-right panel shows these co mponent den-
sities (orange and blue) along with the estimated mixture de nsity (green).

6.8 Mixture Models for Density Estimation and Classiﬁcation 215
No CHD
AgeCount
20 30 40 50 600 5 10 15 20CHD
AgeCount
20 30 40 50 600 5 10 15Combined
AgeCount
20 30 40 50 600 5 10 15 20 25 30
20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10
AgeMixture Estimate
20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10
AgeMixture Estimate
20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10
AgeMixture Estimate
FIGURE 6.17. Application of mixtures to the heart disease risk-factor stu dy.
(Top row:) Histograms of Agefor theno CHDandCHDgroups separately, and
combined. (Bottom row:) estimated component densities fro m a Gaussian mix-
ture model, (bottom left, bottom middle); (bottom right:) Esti mated component
densities (blue and orange) along with the estimated mixture d ensity (green). The
orange density has a very large standard deviation, and appro ximates a uniform
density.
The mixture model also provides an estimate of the probabili ty that
observation ibelongs to component m,
ˆrim=ˆαmφ(xi;ˆµm,ˆΣm)/summationtextM
k=1ˆαkφ(xi;ˆµk,ˆΣk), (6.33)
wherexiisAgein our example. Suppose we threshold each value ˆ ri2and
hence deﬁne ˆδi=I(ˆri2>0.5). Then we can compare the classiﬁcation of
each observation by CHDand the mixture model:
Mixture model
ˆδ= 0ˆδ= 1
CHD No 232 70
Yes 76 84
Although the mixture model did not use the CHDlabels, it has done a fair
job in discovering the two CHDsubpopulations. Linear logistic regression,
using the CHDas a response, achieves the same error rate (32%) when ﬁt to
these data using maximum-likelihood (Section 4.4).

216 6. Kernel Smoothing Methods
6.9 Computational Considerations
Kernel and local regression and density estimation are memory-based meth-
ods: the model is the entire training data set, and the ﬁtting is done at
evaluation or prediction time. For many real-time applicat ions, this can
make this class of methods infeasible.
The computational cost to ﬁt at a single observation x0isO(N) ﬂops,
except in oversimpliﬁed cases (such as square kernels). By c omparison,
an expansion in Mbasis functions costs O(M) for one evaluation, and
typicallyM∼O(logN). Basis function methods have an initial cost of at
leastO(NM2+M3).
The smoothing parameter(s) λfor kernel methods are typically deter-
mined oﬀ-line, for example using cross-validation, at a cos t ofO(N2) ﬂops.
Popularimplementationsoflocalregression,suchasthe loessfunctionin
S-PLUS and Rand the locfitprocedure (Loader, 1999), use triangulation
schemes to reduce the computations. They compute the ﬁt exac tly atM
carefully chosen locations ( O(NM)), and then use blending techniques to
interpolate the ﬁt elsewhere ( O(M) per evaluation).
Bibliographic Notes
There is a vast literature on kernel methods which we will not attempt to
summarize. Rather we will point to a few good references that themselves
have extensive bibliographies. Loader (1999) gives excell ent coverage of lo-
cal regression and likelihood, and also describes state-of -the-art software
for ﬁtting these models. Fan and Gijbels (1996) cover these m odels from
a more theoretical aspect. Hastie and Tibshirani (1990) dis cuss local re-
gression in the context of additive modeling. Silverman (19 86) gives a good
overview of density estimation, as does Scott (1992).
Exercises
Ex. 6.1Show that the Nadaraya–Watson kernel smooth with ﬁxed metri c
bandwidth λand a Gaussian kernel is diﬀerentiable. What can be said for
the Epanechnikov kernel? What can be said for the Epanechnik ov kernel
with adaptive nearest-neighbor bandwidth λ(x0)?
Ex. 6.2Show that/summationtextN
i=1(xi−x0)li(x0) = 0for local linear regression.Deﬁne
bj(x0) =/summationtextN
i=1(xi−x0)jli(x0). Show that b0(x0) = 1 for local polynomial
regression of any degree (including local constants). Show thatbj(x0) = 0
for allj∈{1,2,...,k}for local polynomial regression of degree k. What
are the implications of this on the bias?

Exercises 217
Ex. 6.3Show that||l(x)||(Section 6.1.2) increases with the degree of the
local polynomial.
Ex. 6.4Suppose that the ppredictorsXarise from sampling relatively
smooth analog curves at puniformly spaced abscissa values. Denote by
Cov(X|Y) =Σthe conditional covariance matrix of the predictors, and
assume this does not change much with Y. Discuss the nature of Maha-
lanobischoiceA=Σ−1for the metric in (6.14). How does this compare
withA=I? How might you construct a kernel Athat (a) downweights
high-frequency components in the distance metric; (b) igno res them
completely?
Ex. 6.5Show that ﬁtting a locally constant multinomial logit model of
the form (6.19) amounts to smoothing the binary response ind icators for
eachclassseparatelyusingaNadaraya–Watsonkernelsmoot herwithkernel
weightsKλ(x0,xi).
Ex. 6.6Suppose that all you have is software for ﬁtting local regres sion,
but you can specify exactly which monomials are included in t he ﬁt. How
could you use this software to ﬁt a varying-coeﬃcient model i n some of the
variables?
Ex. 6.7Derive an expression for the leave-one-out cross-validate d residual
sum-of-squares for local polynomial regression.
Ex. 6.8Suppose that for continuous response Yand predictor X, we model
the joint density of X,Yusing a multivariate Gaussian kernel estimator.
Note that the kernel in this case would be the product kernel φλ(X)φλ(Y).
Show that the conditional mean E(Y|X) derived from this estimate is a
Nadaraya–Watson estimator. Extend this result to classiﬁc ation by pro-
viding a suitable kernel for the estimation of the joint dist ribution of a
continuous Xand discrete Y.
Ex. 6.9Explore the diﬀerences between the naive Bayes model (6.27) and
a generalized additive logistic regression model, in terms of (a) model as-
sumptions and (b) estimation. If all the variables Xkare discrete, what can
you say about the corresponding GAM?
Ex.6.10Supposewehave Nsamplesgeneratedfromthemodel yi=f(xi)+
εi, withεiindependent and identically distributed with mean zero and
varianceσ2, thexiassumed ﬁxed (non random). We estimate fusing a
linear smoother (local regression, smoothing spline, etc. ) with smoothing
parameterλ. Thus the vector of ﬁtted values is given by ˆf=Sλy. Consider
thein-sample prediction error
PE(λ) = E1
NN/summationdisplay
i=1(y∗
i−ˆfλ(xi))2(6.34)

218 6. Kernel Smoothing Methods
for predicting new responses at the Ninput values. Show that the aver-
age squared residual on the training data, ASR( λ), is a biased estimate
(optimistic) for PE( λ), while
Cλ= ASR(λ)+2σ2
Ntrace(Sλ) (6.35)
is unbiased.
Ex. 6.11 Show that for the Gaussian mixture model (6.32) the likeliho od
is maximized at + ∞, and describe how.
Ex. 6.12 Write a computer program to perform a local linear discrimi-
nant analysis. At each query point x0, the training data receive weights
Kλ(x0,xi) from a weighting kernel, and the ingredients for the linear deci-
sion boundaries (see Section 4.3) are computed by weighted a verages. Try
out your program on the zipcodedata, and show the training and test er-
rorsforaseriesofﬁvepre-chosenvaluesof λ.Thezipcodedataareavailable
from the book website www-stat.stanford.edu/ElemStatLearn .

This is page 219
Printer: Opaque this
7
Model Assessment and Selection
7.1 Introduction
Thegeneralization performance of a learning method relates to its predic-
tion capability on independent test data. Assessment of thi s performance
is extremely important in practice, since it guides the choi ce of learning
method or model, and gives us a measure of the quality of the ul timately
chosen model.
In this chapter we describe and illustrate the key methods fo r perfor-
mance assessment, and show how they are used to select models . We begin
the chapter with a discussion of the interplay between bias, variance and
model complexity.
7.2 Bias, Variance and Model Complexity
Figure 7.1 illustrates the important issue in assessing the ability of a learn-
ingmethodtogeneralize.Considerﬁrstthecaseofaquantit ativeorinterval
scale response. We have a target variable Y, a vector of inputs X, and a
prediction model ˆf(X) that has been estimated from a training set T.
The loss function for measuring errors between Yandˆf(X) is denoted by
L(Y,ˆf(X)). Typical choices are
L(Y,ˆf(X)) =/braceleftigg
(Y−ˆf(X))2squared error
|Y−ˆf(X)|absolute error .(7.1)

220 7. Model Assessment and Selection
0 5 10 15 20 25 30 350.0 0.2 0.4 0.6 0.8 1.0 1.2
Model Complexity (df)Prediction ErrorHigh Bias Low Bias
High Variance Low Variance
FIGURE 7.1. Behavior of test sample and training sample error as the model
complexity is varied. The light blue curves show the training er rorerr, while the
light red curves show the conditional test error ErrTfor100training sets of size
50each, as the model complexity is increased. The solid curves sh ow the expected
test error Errand the expected training error E[err].
Test error , also referred to as generalization error , is the prediction error
over an independent test sample
ErrT= E[L(Y,ˆf(X))|T] (7.2)
where both XandYare drawn randomly from their joint distribution
(population). Here the training set Tis ﬁxed, and test error refers to the
error for this speciﬁc training set. A related quantity is th e expected pre-
diction error (or expected test error)
Err = E[L(Y,ˆf(X))] = E[Err T]. (7.3)
Note that this expectation averages over everything that is random, includ-
ing the randomness in the training set that produced ˆf.
Figure 7.1 shows the prediction error (light red curves) Err Tfor 100
simulated training sets each of size 50. The lasso (Section 3 .4.2) was used
to produce the sequence of ﬁts. The solid red curve is the aver age, and
hence an estimate of Err.
Estimation of Err Twill be our goal, although we will see that Err is
more amenable to statistical analysis, and most methods eﬀe ctively esti-
mate the expected error. It does not seem possible to estimat e conditional

7.2 Bias, Variance and Model Complexity 221
error eﬀectively, given only the information in the same tra ining set. Some
discussion of this point is given in Section 7.12.
Training error is the average loss over the training sample
err =1
NN/summationdisplay
i=1L(yi,ˆf(xi)). (7.4)
We would like to know the expected test error of our estimated model
ˆf. As the model becomes more and more complex, it uses the train ing
data more and is able to adapt to more complicated underlying structures.
Hence there is a decrease in bias but an increase in variance. There is some
intermediate model complexity that gives minimum expected test error.
Unfortunately training error is not a good estimate of the te st error,
as seen in Figure 7.1. Training error consistently decrease s with model
complexity, typically dropping to zero if we increase the mo del complexity
enough. However, a model with zero training error is overﬁt t o the training
data and will typically generalize poorly.
The story is similar for a qualitative or categorical respon seGtaking
one ofKvalues in a setG, labeled for convenience as 1 ,2,...,K. Typically
we model the probabilities pk(X) = Pr(G=k|X) (or some monotone
transformations fk(X)), and then ˆG(X) = argmax kˆpk(X). In some cases,
such as 1-nearest neighbor classiﬁcation (Chapters 2 and 13 ) we produce
ˆG(X) directly. Typical loss functions are
L(G,ˆG(X)) =I(G∝ne}ationslash=ˆG(X)) (0–1 loss) , (7.5)
L(G,ˆp(X)) =−2K/summationdisplay
k=1I(G=k)log ˆpk(X)
=−2log ˆpG(X) (−2×log-likelihood) .(7.6)
The quantity−2×the log-likelihood is sometimes referred to as the
deviance.
Again, test error here is Err T= E[L(G,ˆG(X))|T], the population mis-
classiﬁcation error of the classiﬁer trained on T, and Err is the expected
misclassiﬁcation error.
Training error is the sample analogue, for example,
err =−2
NN/summationdisplay
i=1log ˆpgi(xi), (7.7)
the sample log-likelihood for the model.
The log-likelihood can be used as a loss-function for genera l response
densities, such as the Poisson, gamma, exponential, log-no rmal and others.
If Prθ(X)(Y) is the density of Y, indexed by a parameter θ(X) that depends
on the predictor X, then
L(Y,θ(X)) =−2·logPrθ(X)(Y). (7.8)

222 7. Model Assessment and Selection
The “−2” in the deﬁnition makes the log-likelihood loss for the Gau ssian
distribution match squared-error loss.
Foreaseofexposition,fortheremainderofthischapterwew illuseYand
f(X) to represent all of the above situations, since we focus mai nly on the
quantitative response (squared-error loss) setting. For t he other situations,
the appropriate translations are obvious.
In this chapter we describe a number of methods for estimatin g the
expected test error for a model. Typically our model will hav e a tuning
parameter or parameters αand so we can write our predictions as ˆfα(x).
The tuning parameter varies the complexity of our model, and we wish to
ﬁnd the value of αthat minimizes error, that is, produces the minimum of
the average test error curve in Figure 7.1. Having said this, for brevity we
will often suppress the dependence of ˆf(x) onα.
It is important to note that there are in fact two separate goa ls that we
might have in mind:
Model selection: estimating the performance of diﬀerent models in order
to choose the best one.
Model assessment: having chosen a ﬁnal model, estimating its predic-
tion error (generalization error) on new data.
If we are in a data-rich situation, the best approach for both problems is
to randomly divide the dataset into three parts: a training s et, a validation
set, and a test set. The training set is used to ﬁt the models; t he validation
set is used to estimate prediction error for model selection ; the test set is
used for assessment of the generalization error of the ﬁnal c hosen model.
Ideally, the test set should be kept in a “vault,” and be broug ht out only
at the end of the data analysis. Suppose instead that we use th e test-set
repeatedly, choosing the model with smallest test-set erro r. Then the test
set error of the ﬁnal chosen model will underestimate the tru e test error,
sometimes substantially.
It is diﬃcult to give a general rule on how to choose the number of
observations in each of the three parts, as this depends on th e signal-to-
noise ratio in the data and the training sample size. A typica l split might
be 50% for training, and 25% each for validation and testing:
Test Train Validation TestTrain Validation Test Validation Train Validation TestTrain
The methods in this chapter are designed for situations wher e there is
insuﬃcient data to split it into three parts. Again it is too d iﬃcult to give
a general rule on how much training data is enough; among othe r things,
this depends on the signal-to-noise ratio of the underlying function, and
the complexity of the models being ﬁt to the data.

7.3 The Bias–Variance Decomposition 223
The methods of this chapter approximate the validation step either an-
alytically (AIC, BIC, MDL, SRM) or by eﬃcient sample re-use ( cross-
validation and the bootstrap). Besides their use in model se lection, we also
examine to what extent each method provides a reliable estim ate of test
error of the ﬁnal chosen model.
Before jumping into these topics, we ﬁrst explore in more det ail the
nature of test error and the bias–variance tradeoﬀ.
7.3 The Bias–Variance Decomposition
As in Chapter 2, if we assume that Y=f(X) +εwhere E(ε) = 0 and
Var(ε) =σ2
ε, we can derive an expression for the expected prediction err or
of a regression ﬁt ˆf(X) at an input point X=x0, using squared-error loss:
Err(x0) =E[(Y−ˆf(x0))2|X=x0]
=σ2
ε+[Eˆf(x0)−f(x0)]2+E[ˆf(x0)−Eˆf(x0)]2
=σ2
ε+Bias2(ˆf(x0))+Var( ˆf(x0))
= Irreducible Error+Bias2+Variance. (7.9)
The ﬁrst term is the variance of the target around its true mea nf(x0), and
cannot be avoided no matter how well we estimate f(x0), unlessσ2
ε= 0.
The second term is the squared bias, the amount by which the av erage of
our estimate diﬀers from the true mean; the last term is the va riance; the
expected squared deviation of ˆf(x0) around its mean. Typically the more
complex we make the model ˆf, the lower the (squared) bias but the higher
the variance.
For thek-nearest-neighbor regression ﬁt, these expressions have t he sim-
ple form
Err(x0) =E[(Y−ˆfk(x0))2|X=x0]
=σ2
ε+/bracketleftigg
f(x0)−1
kk/summationdisplay
ℓ=1f(x(ℓ))/bracketrightigg2
+σ2
ε
k.(7.10)
Here we assume for simplicity that training inputs xiare ﬁxed, and the ran-
domness arises from the yi. The number of neighbors kis inversely related
to the model complexity. For small k, the estimate ˆfk(x) can potentially
adapt itself better to the underlying f(x). As we increase k, the bias—the
squared diﬀerence between f(x0) and the average of f(x) at thek-nearest
neighbors—will typically increase, while the variance decr eases.
For a linear model ﬁt ˆfp(x) =xTˆβ, where the parameter vector βwith
pcomponents is ﬁt by least squares, we have
Err(x0) =E[(Y−ˆfp(x0))2|X=x0]

224 7. Model Assessment and Selection
=σ2
ε+[f(x0)−Eˆfp(x0)]2+||h(x0)||2σ2
ε.(7.11)
Hereh(x0) =X(XTX)−1x0, theN-vector of linear weights that produce
the ﬁtˆfp(x0) =x0T(XTX)−1XTy, and hence Var[ ˆfp(x0)] =||h(x0)||2σ2
ε.
While this variance changes with x0, its average (with x0taken to be each
of the sample values xi) is (p/N)σ2
ε, and hence
1
NN/summationdisplay
i=1Err(xi) =σ2
ε+1
NN/summationdisplay
i=1[f(xi)−Eˆf(xi)]2+p
Nσ2
ε,(7.12)
thein-sample error. Here model complexity is directly related to the num-
ber of parameters p.
The test error Err( x0) for a ridge regression ﬁt ˆfα(x0) is identical in
form to (7.11), except the linear weights in the variance ter m are diﬀerent:
h(x0) =X(XTX+αI)−1x0. The bias term will also be diﬀerent.
For a linear model family such as ridge regression, we can bre ak down
the bias more ﬁnely. Let β∗denote the parameters of the best-ﬁtting linear
approximation to f:
β∗= argmin
βE/parenleftbig
f(X)−XTβ/parenrightbig2. (7.13)
Here the expectation is taken with respect to the distributi on of the input
variablesX. Then we can write the average squared bias as
Ex0/bracketleftig
f(x0)−Eˆfα(x0)/bracketrightig2
= Ex0/bracketleftbig
f(x0)−xT
0β∗/bracketrightbig2+Ex0/bracketleftig
xT
0β∗−ExT
0ˆβα/bracketrightig2
= Ave[Model Bias]2+Ave[Estimation Bias]2
(7.14)
The ﬁrst term on the right-hand side is the average squared model bias , the
error between the best-ﬁtting linear approximation and the true function.
The second term is the average squared estimation bias , the error between
the average estimate E( xT
0ˆβ) and the best-ﬁtting linear approximation.
Forlinearmodelsﬁtbyordinaryleastsquares,theestimati onbiasiszero.
For restricted ﬁts, such as ridge regression, it is positive , and we trade it oﬀ
with the beneﬁts of a reduced variance. The model bias can onl y be reduced
by enlarging the class of linear models to a richer collectio n of models, by
including interactions and transformations of the variabl es in the model.
Figure 7.2 shows the bias–variance tradeoﬀ schematically. In the case
of linear models, the model space is the set of all linear pred ictions from
pinputs and the black dot labeled “closest ﬁt” is xTβ∗. The blue-shaded
region indicates the error σεwith which we see the truth in the training
sample.
Also shown is the variance of the least squares ﬁt, indicated by the large
yellow circle centered at the black dot labeled “closest ﬁt i n population,’

7.3 The Bias–Variance Decomposition 225
RealizationClosest fit in population
Estimation BiasSPACE
VarianceEstimationClosest fit
Truth
Model bias
RESTRICTEDShrunken fit
MODELSPACEMODEL
FIGURE 7.2. Schematic of the behavior of bias and variance. The model spa ce
is the set of all possible predictions from the model, with the “ closest ﬁt” labeled
with a black dot. The model bias from the truth is shown, along wi th the variance,
indicated by the large yellow circle centered at the black dot labe led “closest ﬁt
in population.” A shrunken or regularized ﬁt is also shown, havi ng additional
estimation bias, but smaller prediction error due to its decr eased variance.

226 7. Model Assessment and Selection
Now if we were to ﬁt a model with fewer predictors, or regulari ze the coef-
ﬁcients by shrinking them toward zero (say), we would get the “shrunken
ﬁt” shown in the ﬁgure. This ﬁt has an additional estimation b ias, due to
the fact that it is not the closest ﬁt in the model space. On the other hand,
it has smaller variance. If the decrease in variance exceeds the increase in
(squared) bias, then this is worthwhile.
7.3.1 Example: Bias–Variance Tradeoﬀ
Figure 7.3 shows the bias–variance tradeoﬀ for two simulate d examples.
There are 80 observations and 20 predictors, uniformly dist ributed in the
hypercube [0 ,1]20. The situations are as follows:
Left panels: Yis 0 ifX1≤1/2 and 1 ifX1>1/2, and we apply k-nearest
neighbors.
Right panels: Yis 1 if/summationtext10
j=1Xjis greater than 5 and 0 otherwise, and we
use best subset linear regression of size p.
The top row is regression with squared error loss; the bottom row is classi-
ﬁcation with 0–1 loss. The ﬁgures show the prediction error ( red), squared
bias (green) and variance (blue), all computed for a large te st sample.
In the regression problems, bias and variance add to produce the predic-
tion error curves, with minima at about k= 5 fork-nearest neighbors, and
p≥10 for the linear model. For classiﬁcation loss (bottom ﬁgur es), some
interesting phenomena can be seen. The bias and variance cur ves are the
same as in the top ﬁgures, and prediction error now refers to m isclassiﬁ-
cation rate. We see that prediction error is no longer the sum of squared
bias and variance. For the k-nearest neighbor classiﬁer, prediction error
decreases or stays the same as the number of neighbors is incr eased to 20,
despite the fact that the squared bias is rising. For the line ar model classi-
ﬁer the minimum occurs for p≥10 as in regression, but the improvement
over thep= 1 model is more dramatic. We see that bias and variance seem
to interact in determining prediction error.
Why does this happen? There is a simple explanation for the ﬁr st phe-
nomenon. Suppose at a given input point, the true probabilit y of class 1 is
0.9 while the expected value of our estimate is 0 .6. Then the squared bias—
(0.6−0.9)2—is considerable, but the prediction error is zero since we ma ke
the correct decision. In other words, estimation errors tha t leave us on the
right side of the decision boundary don’t hurt. Exercise 7.2 demonstrates
this phenomenon analytically, and also shows the interacti on eﬀect between
bias and variance.
The overall point is that the bias–variance tradeoﬀ behaves diﬀerently
for 0–1 loss than it does for squared error loss. This in turn m eans that
the best choices of tuning parameters may diﬀer substantial ly in the two

7.3 The Bias–Variance Decomposition 2270.0 0.1 0.2 0.3 0.4
Number of Neighbors k50 40 30 20 10 0k−NN − Regression
5 10 15 200.0 0.1 0.2 0.3 0.4
Subset Size pLinear Model − Regression0.0 0.1 0.2 0.3 0.4
Number of Neighbors k50 40 30 20 10 0k−NN − Classification
5 10 15 200.0 0.1 0.2 0.3 0.4
Subset Size pLinear Model − Classification
FIGURE 7.3. Expected prediction error (orange), squared bias (green) a nd vari-
ance (blue) for a simulated example. The top row is regression wi th squared error
loss; the bottom row is classiﬁcation with 0–1loss. The models are k-nearest
neighbors (left) and best subset regression of size p(right). The variance and bias
curves are the same in regression and classiﬁcation, but the p rediction error curve
is diﬀerent.

228 7. Model Assessment and Selection
settings. One should base the choice of tuning parameter on a n estimate of
prediction error, as described in the following sections.
7.4 Optimism of the Training Error Rate
Discussions of error rate estimation can be confusing, beca use we have
to make clear which quantities are ﬁxed and which are random1. Before
we continue, we need a few deﬁnitions, elaborating on the mat erial of Sec-
tion 7.2. Given a training set T={(x1,y1),(x2,y2),...(xN,yN)}the gen-
eralization error of a model ˆfis
ErrT= EX0,Y0[L(Y0,ˆf(X0))|T]; (7.15)
Notethatthetrainingset Tisﬁxedinexpression(7.15).Thepoint( X0,Y0)
is a new test data point, drawn from F, the joint distribution of the data.
Averaging over training sets Tyields the expected error
Err = E TEX0,Y0[L(Y0,ˆf(X0))|T], (7.16)
which is more amenable to statistical analysis. As mentione d earlier, it
turns out that most methods eﬀectively estimate the expecte d error rather
than E T; see Section 7.12 for more on this point.
Now typically, the training error
err =1
NN/summationdisplay
i=1L(yi,ˆf(xi)) (7.17)
will be less than the true error Err T, because the same data is being used
to ﬁt the method and assess its error (see Exercise 2.9). A ﬁtt ing method
typically adapts to the training data, and hence the apparen t or training
errorerr will be an overly optimistic estimate of the generalizat ion error
ErrT.
Part of the discrepancy is due to where the evaluation points occur. The
quantity Err Tcan be thought of as extra-sample error, since the test input
vectors don’t need to coincide with the training input vecto rs. The nature
of the optimism in err is easiest to understand when we focus instead on
thein-sample error
Errin=1
NN/summationdisplay
i=1EY0[L(Y0
i,ˆf(xi))|T] (7.18)
TheY0notation indicates that we observe Nnew response values at
each of the training points xi, i= 1,2,...,N. We deﬁne the optimism as
1Indeed, in the ﬁrst edition of our book, this section wasn’t suﬃciently clea r.

7.4 Optimism of the Training Error Rate 229
the diﬀerence between Err inand the training error err:
op≡Errin−err. (7.19)
Thisistypicallypositivesince errisusuallybiaseddownwardasanestimate
of prediction error. Finally, the average optimism is the ex pectation of the
optimism over training sets
ω≡Ey(op). (7.20)
Here the predictors in the training set are ﬁxed, and the expe ctation is
over the training set outcome values; hence we have used the n otation E y
instead of E T. We can usually estimate only the expected error ωrather
than op, in the same way that we can estimate the expected erro r Err
rather than the conditional error Err T.
For squared error, 0–1, and other loss functions, one can sho w quite
generally that
ω=2
NN/summationdisplay
i=1Cov(ˆyi,yi), (7.21)
where Cov indicates covariance. Thus the amount by which err underesti-
mates the true error depends on how strongly yiaﬀects its own prediction.
The harder we ﬁt the data, the greater Cov(ˆ yi,yi) will be, thereby increas-
ingtheoptimism.Exercise7.4provesthisresultforsquare derrorlosswhere
ˆyiis the ﬁtted value from the regression. For 0–1 loss, ˆ yi∈{0,1}is the
classiﬁcation at xi, and for entropy loss, ˆ yi∈[0,1] is the ﬁtted probability
of class 1 at xi.
In summary, we have the important relation
Ey(Errin) = Ey(err)+2
NN/summationdisplay
i=1Cov(ˆyi,yi). (7.22)
This expression simpliﬁes if ˆ yiis obtained by a linear ﬁt with dinputs
or basis functions. For example,
N/summationdisplay
i=1Cov(ˆyi,yi) =dσ2
ε (7.23)
for the additive error model Y=f(X)+ε, and so
Ey(Errin) = Ey(err)+2·d
Nσ2
ε. (7.24)
Expression (7.23) is the basis for the deﬁnition of the eﬀective number of
parameters discussed in Section 7.6 The optimism increases linearly wi th

230 7. Model Assessment and Selection
the number dof inputs or basis functions we use, but decreases as the
training sample size increases. Versions of (7.24) hold app roximately for
other error models, such as binary data and entropy loss.
An obvious way to estimate prediction error is to estimate th e optimism
and then add it to the training error err. The methods described in the
next section— Cp, AIC, BIC and others—work in this way, for a special
class of estimates that are linear in their parameters.
In contrast, cross-validation and bootstrap methods, desc ribed later in
the chapter, are direct estimates of the extra-sample error Err. These gen-
eral tools can be used with any loss function, and with nonlin ear, adaptive
ﬁtting techniques.
In-sample error is not usually of direct interest since futu re values of the
features are not likely to coincide with their training set v alues. But for
comparison between models, in-sample error is convenient a nd often leads
to eﬀective model selection. The reason is that the relative (rather than
absolute) size of the error is what matters.
7.5 Estimates of In-Sample Prediction Error
The general form of the in-sample estimates is
/hatwidestErrin=err+ ˆω, (7.25)
where ˆωis an estimate of the average optimism.
Using expression (7.24), applicable when dparameters are ﬁt under
squared error loss, leads to a version of the so-called Cpstatistic,
Cp=err+2·d
Nˆσε2. (7.26)
Here ˆσε2is an estimate of the noise variance, obtained from the mean-
squarederrorofalow-biasmodel.Usingthiscriterionwead justthetraining
error by a factor proportional to the number of basis functio ns used.
TheAkaike information criterion is a similar but more generally appli-
cable estimate of Err inwhen a log-likelihood loss function is used. It relies
on a relationship similar to (7.24) that holds asymptotical ly asN→∞:
−2·E[logPr ˆθ(Y)]≈−2
N·E[loglik]+2·d
N. (7.27)
Here Pr θ(Y) is a family of densities for Y(containing the “true” density),
ˆθis the maximum-likelihood estimate of θ, and “loglik” is the maximized
log-likelihood:
loglik =N/summationdisplay
i=1logPrˆθ(yi). (7.28)

7.5 Estimates of In-Sample Prediction Error 231
For example, for the logistic regression model, using the bi nomial log-
likelihood, we have
AIC =−2
N·loglik+2·d
N. (7.29)
For the Gaussian model (with variance σ2
ε= ˆσε2assumed known), the AIC
statistic is equivalent to Cp, and so we refer to them collectively as AIC.
To use AIC for model selection, we simply choose the model giv ing small-
est AIC over the set of models considered. For nonlinear and o ther complex
models, we need to replace dby some measure of model complexity. We
discuss this in Section 7.6.
Given a set of models fα(x) indexed by a tuning parameter α, denote
byerr(α) andd(α) the training error and number of parameters for each
model. Then for this set of models we deﬁne
AIC(α) =err(α)+2·d(α)
Nˆσε2. (7.30)
The function AIC( α) provides an estimate of the test error curve, and we
ﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model
isfˆα(x). Note that if the basis functions are chosen adaptively, (7 .23) no
longer holds. For example, if we have a total of pinputs, and we choose
the best-ﬁtting linear model with d < pinputs, the optimism will exceed
(2d/N)σ2
ε. Put another way, by choosing the best-ﬁtting model with d
inputs, the eﬀective number of parameters ﬁt is more than d.
Figure 7.4 shows AIC in action for the phoneme recognition ex ample
of Section 5.2.3 on page 148. The input vector is the log-peri odogram of
the spoken vowel, quantized to 256 uniformly spaced frequen cies. A lin-
ear logistic regression model is used to predict the phoneme class, with
coeﬃcient function β(f) =/summationtextM
m=1hm(f)θm, an expansion in Mspline ba-
sis functions. For any given M, a basis of natural cubic splines is used
for thehm, with knots chosen uniformly over the range of frequencies ( so
d(α) =d(M) =M). Using AIC to select the number of basis functions will
approximately minimize Err( M) for both entropy and 0–1 loss.
The simple formula
(2/N)N/summationdisplay
i=1Cov(ˆyi,yi) = (2d/N)σ2
ε
holds exactly for linear models with additive errors and squ ared error loss,
and approximately for linear models and log-likelihoods. I n particular, the
formula does not hold in general for 0–1 loss (Efron, 1986), a lthough many
authors nevertheless use it in that context (right panel of F igure 7.4).

232 7. Model Assessment and Selection
Number of Basis FunctionsLog-likelihood
0.5 1.0 1.5 2.0 2.5Log-likelihood Loss
2 4 8 16 32 64 128O
O
O
OOO
O
OO
O
O
OOOOO
O
O
O
OOOOOTrain
Test
AIC
Number of Basis FunctionsMisclassification Error
0.10 0.15 0.20 0.25 0.30 0.350-1 Loss
2 4 8 16 32 64 128O
O
O
OO
O
O
OO
O
O
OOOOOO
O
O
OOOOO
FIGURE 7.4. AIC used for model selection for the phoneme recogni-
tion example of Section 5.2.3. The logistic regression coeﬃci ent function
β(f) =/summationtextM
m=1hm(f)θmis modeled as an expansion in Mspline basis functions.
In the left panel we see the AIC statistic used to estimate Errinusing log-likeli-
hood loss. Included is an estimate of Errbased on an independent test sample. It
does well except for the extremely over-parametrized case ( M= 256parameters
forN= 1000 observations). In the right panel the same is done for 0–1 loss .
Although the AIC formula does not strictly apply here, it does a re asonable job in
this case.
7.6 The Eﬀective Number of Parameters
The concept of “number of parameters” can be generalized, es pecially to
models where regularization is used in the ﬁtting. Suppose w e stack the
outcomesy1,y2,...,y Ninto a vector y, and similarly for the predictions
ˆy. Then a linear ﬁtting method is one for which we can write
ˆy=Sy, (7.31)
whereSis anN×Nmatrix depending on the input vectors xibut not on
theyi. Linear ﬁtting methods include linear regression on the ori ginal fea-
tures or on a derived basis set, and smoothing methods that us e quadratic
shrinkage, such as ridge regression and cubic smoothing spl ines. Then the
eﬀective number of parameters is deﬁned as
df(S) = trace( S), (7.32)
the sum of the diagonal elements of S(also known as the eﬀective degrees-
of-freedom ). Note that if Sis an orthogonal-projection matrix onto a basis

7.7 The Bayesian Approach and BIC 233
setspannedby Mfeatures,thentrace( S) =M.Itturnsoutthattrace( S) is
exactly the correct quantity to replace das the number of parameters in the
Cpstatistic (7.26). If yarises from an additive-error model Y=f(X)+ε
with Var(ε) =σ2
ε, then one can show that/summationtextN
i=1Cov(ˆyi,yi) = trace( S)σ2
ε,
which motivates the more general deﬁnition
df(ˆy) =/summationtextN
i=1Cov(ˆyi,yi)
σ2ε(7.33)
(Exercises 7.4 and 7.5). Section 5.4.1 on page 153 gives some more intuition
for the deﬁnition df = trace( S) in the context of smoothing splines.
For models like neural networks, in which we minimize an erro r function
R(w) with weight decay penalty (regularization) α/summationtext
mw2
m, the eﬀective
number of parameters has the form
df(α) =M/summationdisplay
m=1θm
θm+α, (7.34)
where theθmare the eigenvalues of the Hessian matrix ∂2R(w)/∂w∂wT.
Expression (7.34) follows from (7.32) if we make a quadratic approximation
to the error function at the solution (Bishop, 1995).
7.7 The Bayesian Approach and BIC
TheBayesianinformationcriterion(BIC),likeAIC,isappl icableinsettings
where the ﬁtting is carried out by maximization of a log-like lihood. The
generic form of BIC is
BIC =−2·loglik+(log N)·d. (7.35)
TheBICstatistic(times1/2)isalsoknownastheSchwarzcri terion(Schwarz,
1978).
Under the Gaussian model, assuming the variance σ2
εis known,−2·loglik
equals(uptoaconstant)/summationtext
i(yi−ˆf(xi))2/σ2
ε,whichisN·err/σ2
εforsquared
error loss. Hence we can write
BIC =N
σ2ε/bracketleftig
err+(logN)·d
Nσ2
ε/bracketrightig
. (7.36)
Therefore BIC is proportional to AIC ( Cp), with the factor 2 replaced
by logN. Assuming N >e2≈7.4, BIC tends to penalize complex models
more heavily, giving preference to simpler models in select ion. As with AIC,
σ2
εis typically estimated by the mean squared error of a low-bia s model.
For classiﬁcation problems, use of the multinomial log-lik elihood leads to a
similar relationship with the AIC, using cross-entropy as t he error measure.

234 7. Model Assessment and Selection
Note however that the misclassiﬁcation error measure does n ot arise in the
BIC context, since it does not correspond to the log-likelih ood of the data
under any probability model.
Despite its similarity with AIC, BIC is motivated in quite a d iﬀerent
way. It arises in the Bayesian approach to model selection, w hich we now
describe.
Suppose we have a set of candidate models Mm,m= 1,...,M and
corresponding model parameters θm, and we wish to choose a best model
from among them. Assuming we have a prior distribution Pr( θm|Mm) for
the parameters of each model Mm, the posterior probability of a given
model is
Pr(Mm|Z)∝Pr(Mm)·Pr(Z|Mm) (7.37)
∝Pr(Mm)·/integraldisplay
Pr(Z|θm,Mm)Pr(θm|Mm)dθm,
whereZrepresents the training data {xi,yi}N
1. To compare two models
MmandMℓ, we form the posterior odds
Pr(Mm|Z)
Pr(Mℓ|Z)=Pr(Mm)
Pr(Mℓ)·Pr(Z|Mm)
Pr(Z|Mℓ). (7.38)
If the odds are greater than one we choose model m, otherwise we choose
modelℓ. The rightmost quantity
BF(Z) =Pr(Z|Mm)
Pr(Z|Mℓ)(7.39)
is called the Bayes factor , the contribution of the data toward the posterior
odds.
Typically we assume that the prior over models is uniform, so that
Pr(Mm) is constant. We need some way of approximating Pr( Z|Mm).
A so-called Laplace approximation to the integral followed by some other
simpliﬁcations (Ripley, 1996, page 64) to (7.37) gives
logPr(Z|Mm) = logPr( Z|ˆθm,Mm)−dm
2·logN+O(1).(7.40)
Hereˆθmis a maximum likelihood estimate and dmis the number of free
parameters in model Mm. If we deﬁne our loss function to be
−2logPr(Z|ˆθm,Mm),
this is equivalent to the BIC criterion of equation (7.35).
Therefore, choosing the model with minimum BIC is equivalen t to choos-
ing the model with largest (approximate) posterior probabi lity. But this
framework gives us more. If we compute the BIC criterion for a set ofM,

7.8 Minimum Description Length 235
models, giving BIC m,m= 1,2,...,M, then we can estimate the posterior
probability of each model Mmas
e−1
2·BICm
/summationtextM
ℓ=1e−1
2·BICℓ. (7.41)
Thus we can estimate not only the best model, but also assess t he relative
merits of the models considered.
For model selection purposes, there is no clear choice betwe en AIC and
BIC. BIC is asymptotically consistent as a selection criter ion. What this
means is that given a family of models, including the true mod el, the prob-
ability that BIC will select the correct model approaches on e as the sample
sizeN→∞. This is not the case for AIC, which tends to choose models
which are too complex as N→∞. On the other hand, for ﬁnite samples,
BIC often chooses models that are too simple, because of its h eavy penalty
on complexity.
7.8 Minimum Description Length
The minimum description length (MDL) approach gives a selec tion cri-
terion formally identical to the BIC approach, but is motiva ted from an
optimal coding viewpoint. We ﬁrst review the theory of codin g for data
compression, and then apply it to model selection.
We think of our datum zas a message that we want to encode and
send to someone else (the “receiver”). We think of our model a s a way of
encoding the datum, and will choose the most parsimonious mo del, that is
the shortest code, for the transmission.
Suppose ﬁrst that the possible messages we might want to tran smit are
z1,z2,...,z m. Our code uses a ﬁnite alphabet of length A: for example, we
might use a binary code {0,1}of lengthA= 2. Here is an example with
four possible messages and a binary coding:
Message z1z2z3z4
Code 010110111(7.42)
This code is known as an instantaneous preﬁx code: no code is t he pre-
ﬁx of any other, and the receiver (who knows all of the possibl e codes),
knows exactly when the message has been completely sent. We r estrict our
discussion to such instantaneous preﬁx codes.
One could use the coding in (7.42) or we could permute the code s, for
example use codes 110 ,10,111,0 forz1,z2,z3,z4. How do we decide which
to use? It depends on how often we will be sending each of the me ssages.
If, for example, we will be sending z1most often, it makes sense to use the
shortest code 0 for z1. Using this kind of strategy—shorter codes for more
frequent messages—the average message length will be shorte r.

236 7. Model Assessment and Selection
In general, if messages are sent with probabilities Pr( zi),i= 1,2,...,4,
a famous theorem due to Shannon says we should use code length sli=
−log2Pr(zi) and the average message length satisﬁes
E(length)≥−/summationdisplay
Pr(zi)log2(Pr(zi)). (7.43)
The right-hand side above is also called the entropy of the di stribution
Pr(zi). The inequality is an equality when the probabilities sati sfypi=
A−li. In our example, if Pr( zi) = 1/2,1/4,1/8,1/8,respectively, then the
coding shown in (7.42) is optimal and achieves the entropy lo wer bound.
In general the lower bound cannot be achieved, but procedure s like the
Huﬀman coding scheme can get close to the bound. Note that wit h an
inﬁnite set of messages, the entropy is replaced by −/integraltext
Pr(z)log2Pr(z)dz.
From this result we glean the following:
To transmit a random variable zhaving probability density func-
tionPr(z), we require about −log2Pr(z)bits of information.
We henceforth change notation from log2Pr(z) to logPr(z) = logePr(z);
this is for convenience, and just introduces an unimportant multiplicative
constant.
Now we apply this result to the problem of model selection. We have
a modelMwith parameters θ, and data Z= (X,y) consisting of both
inputs and outputs. Let the (conditional) probability of th e outputs under
the model be Pr( y|θ,M,X), assume the receiver knows all of the inputs,
and we wish to transmit the outputs. Then the message length r equired to
transmit the outputs is
length =−logPr(y|θ,M,X)−logPr(θ|M), (7.44)
the log-probability of the target values given the inputs. T he second term
is the average code length for transmitting the model parame tersθ, while
the ﬁrst term is the average code length for transmitting the discrepancy
between the model and actual target values. For example supp ose we have
a single target ywithy∼N(θ,σ2), parameter θ∼N(0,1) and no input
(for simplicity). Then the message length is
length = constant+log σ+(y−θ)2
2σ2+θ2
2. (7.45)
Note that the smaller σis, the shorter on average is the message length,
sinceyis more concentrated around θ.
The MDL principle says that we should choose the model that mi ni-
mizes (7.44). We recognize (7.44) as the (negative) log-pos terior distribu-
tion, and hence minimizing description length is equivalen t to maximizing
posterior probability. Hence the BIC criterion, derived as approximation to
log-posterior probability, can also be viewed as a device fo r (approximate)
model choice by minimum description length.

7.9 Vapnik–Chervonenkis Dimension 237
0.0 0.2 0.4 0.6 0.8 1.0-1.0 0.0 1.0
xsin(50·x)
FIGURE 7.5. The solid curve is the function sin(50x)forx∈[0,1]. The green
(solid) and blue (hollow) points illustrate how the associated i ndicator function
I(sin(αx)>0)can shatter (separate) an arbitrarily large number of points b y
choosing an appropriately high frequency α.
Note that we have ignored the precision with which a random va riable
zis coded. With a ﬁnite code length we cannot code a continuous variable
exactly. However, if we code zwithin a tolerance δz, the message length
neededisthelogoftheprobabilityintheinterval[ z,z+δz]whichiswellap-
proximated by δzPr(z) ifδzis small. Since log δzPr(z) = logδz+logPr(z),
this means we can just ignore the constant log δzand use logPr( z) as our
measure of message length, as we did above.
The preceding view of MDL for model selection says that we sho uld
choose the model with highest posterior probability. Howev er, many Bayes-
ianswouldinsteaddoinferencebysamplingfromtheposteri ordistribution.
7.9 Vapnik–Chervonenkis Dimension
A diﬃculty in using estimates of in-sample error is the need t o specify the
number of parameters (or the complexity) dused in the ﬁt. Although the
eﬀective number of parameters introduced in Section 7.6 is u seful for some
nonlinear models, it is not fully general. The Vapnik–Cherv onenkis (VC)
theory provides such a general measure of complexity, and gi ves associated
bounds on the optimism. Here we give a brief review of this the ory.
Suppose we have a class of functions {f(x,α)}indexed by a parameter
vectorα, withx∈IRp. Assume for now that fis an indicator function,
that is, takes the values 0 or 1. If α= (α0,α1) andfis the linear indi-
cator function I(α0+αT
1x >0), then it seems reasonable to say that the
complexity of the class fis the number of parameters p+ 1. But what
aboutf(x,α) =I(sinα·x) whereαis any real number and x∈IR? The
function sin(50·x) is shown in Figure 7.5. This is a very wiggly function
that gets even rougher as the frequency αincreases, but it has only one
parameter: despite this, it doesn’t seem reasonable to conc lude that it has
less complexity than the linear indicator function I(α0+α1x) inp= 1
dimension.

238 7. Model Assessment and Selection
FIGURE 7.6. The ﬁrst three panels show that the class of lines in the plane
can shatter three points. The last panel shows that this class c annot shatter four
points, as no line will put the hollow points on one side and the s olid points on
the other. Hence the VC dimension of the class of straight lines in the plane is
three. Note that a class of nonlinear curves could shatter four p oints, and hence
has VC dimension greater than three.
The Vapnik–Chervonenkis dimension is a way of measuring the com-
plexity of a class of functions by assessing how wiggly its me mbers can
be.
TheVC dimension of the class{f(x,α)}is deﬁned to be the
largest number of points (in some conﬁguration) that can be
shattered by members of{f(x,α)}.
A set of points is said to be shattered by a class of functions i f, no matter
how we assign a binary label to each point, a member of the clas s can
perfectly separate them.
Figure 7.6 shows that the VC dimension of linear indicator fu nctions
in the plane is 3 but not 4, since no four points can be shattere d by a
set of lines. In general, a linear indicator function in pdimensions has VC
dimensionp+1, which is also the number of free parameters. On the other
hand, it can be shown that the family sin( αx) has inﬁnite VC dimension,
as Figure 7.5 suggests. By appropriate choice of α, any set of points can be
shattered by this class (Exercise 7.8).
So far we have discussed the VC dimension only of indicator fu nctions,
but this can be extended to real-valued functions. The VC dim ension of a
class of real-valued functions {g(x,α)}is deﬁned to be the VC dimension
of the indicator class {I(g(x,α)−β >0)}, whereβtakes values over the
range ofg.
One can use the VC dimension in constructing an estimate of (e xtra-
sample) prediction error; diﬀerent types of results are ava ilable. Using the
concept of VC dimension, one can prove results about the opti mism of the
trainingerrorwhenusingaclassoffunctions.Anexampleof sucharesultis
the following. If we ﬁt Ntraining points using a class of functions {f(x,α)}
having VC dimension h, then with probability at least 1 −ηover training

7.9 Vapnik–Chervonenkis Dimension 239
sets:
ErrT≤err+ǫ
2/parenleftig
1+/radicalbigg
1+4·err
ǫ/parenrightig
(binary classiﬁcation)
ErrT≤err
(1−c√ǫ)+(regression) (7.46)
whereǫ=a1h[log(a2N/h)+1]−log(η/4)
N,
and 0<a1≤4,0<a2≤2
These bounds hold simultaneously for all members f(x,α), and are taken
from Cherkassky and Mulier (2007, pages 116–118). They reco mmend the
valuec= 1. For regression they suggest a1=a2= 1, and for classiﬁcation
they make no recommendation, with a1= 4 anda2= 2 corresponding
to worst-case scenarios. They also give an alternative practical bound for
regression
ErrT≤err/parenleftigg
1−/radicalbigg
ρ−ρlogρ+logN
2N/parenrightigg−1
+(7.47)
withρ=h
N, which is free of tuning constants. The bounds suggest that t he
optimism increases with hand decreases with Nin qualitative agreement
with the AIC correction d/Ngiven in (7.24). However, the results in (7.46)
are stronger: rather than giving the expected optimism for e ach ﬁxed func-
tionf(x,α), they give probabilistic upper bounds for all functions f(x,α),
and hence allow for searching over the class.
Vapnik’s structural risk minimization (SRM) approach ﬁts a nested se-
quence of models of increasing VC dimensions h1< h2<···, and then
chooses the model with the smallest value of the upper bound.
We note that upper bounds like the ones in (7.46) are often ver y loose,
but that doesn’t rule them out as good criteria for model sele ction, where
the relative (not absolute) size of the test error is importa nt. The main
drawback of this approach is the diﬃculty in calculating the VC dimension
of a class of functions. Often only a crude upper bound for VC d imension
is obtainable, and this may not be adequate. An example in whi ch the
structural risk minimization program can be successfully c arried out is the
support vector classiﬁer, discussed in Section 12.2.
7.9.1 Example (Continued)
Figure 7.7 shows the results when AIC, BIC and SRM are used to s elect
the model size for the examples of Figure 7.3. For the example s labeled KNN,
the model index αrefers to neighborhood size, while for those labeled REGα
refers to subset size. Using each selection method (e.g., AI C) we estimated
the best model ˆ αand found its true prediction error Err T(ˆα) on a test
set. For the same training set we computed the prediction err or of the best

240 7. Model Assessment and Selection
reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestAIC
reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBIC
reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestSRM
FIGURE 7.7. Boxplots show the distribution of the relative error
100×[ErrT(ˆα)−minαErrT(α)]/[maxαErrT(α)−minαErrT(α)]over the four
scenarios of Figure 7.3. This is the error in using the chosen model relative to
the best model. There are 100training sets each of size 80represented in each
boxplot, with the errors computed on test sets of size 10,000.

7.10 Cross-Validation 241
and worst possible model choices: min αErrT(α) and max αErrT(α). The
boxplots show the distribution of the quantity
100×ErrT(ˆα)−minαErrT(α)
maxαErrT(α)−minαErrT(α),
which represents the error in using the chosen model relativ e to the best
model. For linear regression the model complexity was measu red by the
number of features; as mentioned in Section 7.5, this undere stimates the
df, since it does not chargefor the search for the best model of that size.
This was also used for the VC dimension of the linear classiﬁe r. Fork-
nearest neighbors, we used the quantity N/k. Under an additive-error re-
gression model, this can be justiﬁed as the exact eﬀective de grees of free-
dom (Exercise 7.6); we do not know if it corresponds to the VC d imen-
sion. We used a1=a2= 1 for the constants in (7.46); the results for SRM
changedwithdiﬀerentconstants,andthischoicegavethemo stfavorablere-
sults. We repeated the SRM selection using the alternative p ractical bound
(7.47), and got almost identical results. For misclassiﬁca tion error we used
ˆσε2= [N/(N−d)]·err(α) for the least restrictive model ( k= 5 for KNN,
sincek= 1 results in zero training error). The AIC criterion seems t o work
well in all four scenarios, despite the lack of theoretical s upport with 0–1
loss. BIC does nearly as well, while the performance of SRM is mixed.
7.10 Cross-Validation
Probably the simplest and most widely used method for estima ting predic-
tion error is cross-validation. This method directly estim ates the expected
extra-sample error Err = E[ L(Y,ˆf(X))], the average generalization error
when the method ˆf(X) is applied to an independent test sample from the
joint distribution of XandY. As mentioned earlier, we might hope that
cross-validation estimates the conditional error, with th e training setT
held ﬁxed. But as we will see in Section 7.12, cross-validati on typically
estimates well only the expected prediction error.
7.10.1K-Fold Cross-Validation
Ideally, if we had enough data, we would set aside a validatio n set and use
it to assess the performance of our prediction model. Since d ata are often
scarce, this is usually not possible. To ﬁnesse the problem, K-fold cross-
validation uses part of the available data to ﬁt the model, an d a diﬀerent
part to test it. We split the data into Kroughly equal-sized parts; for
example, when K= 5, the scenario looks like this:

242 7. Model Assessment and Selection
Validation Train1 2 3 4 5
Train Train Train
For thekth part (third above), we ﬁt the model to the other K−1 parts
of the data, and calculate the prediction error of the ﬁtted m odel when
predicting the kth part of the data. We do this for k= 1,2,...,Kand
combine the Kestimates of prediction error.
Here are more details. Let κ:{1,...,N}∝ma√sto→{1,...,K}be an indexing
function that indicates the partition to which observation iis allocated by
the randomization. Denote by ˆf−k(x) the ﬁtted function, computed with
thekth part of the data removed. Then the cross-validation estim ate of
prediction error is
CV(ˆf) =1
NN/summationdisplay
i=1L(yi,ˆf−κ(i)(xi)). (7.48)
Typical choices of Kare 5 or 10 (see below). The case K=Nis known
asleave-one-out cross-validation. In this case κ(i) =i, and for the ith
observation the ﬁt is computed using all the data except the ith.
Given a set of models f(x,α) indexed by a tuning parameter α, denote
byˆf−k(x,α) theαth model ﬁt with the kth part of the data removed. Then
for this set of models we deﬁne
CV(ˆf,α) =1
NN/summationdisplay
i=1L(yi,ˆf−κ(i)(xi,α)). (7.49)
The function CV( ˆf,α) provides an estimate of the test error curve, and we
ﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model is
f(x,ˆα), which we then ﬁt to all the data.
It is interesting to wonder about what quantity K-fold cross-validation
estimates. With K= 5 or 10, we might guess that it estimates the ex-
pected error Err, since the training sets in each fold are qui te diﬀerent
from the original training set. On the other hand, if K=Nwe might
guess that cross-validation estimates the conditional err or Err T. It turns
out that cross-validation only estimates eﬀectively the av erage error Err,
as discussed in Section 7.12.
What value should we choose for K? WithK=N, the cross-validation
estimator is approximately unbiased for the true (expected ) prediction er-
ror, but can have high variance because the N“training sets” are so similar
to one another. The computational burden is also considerab le, requiring
Napplications of the learning method. In certain special pro blems, this
computation can be done quickly—see Exercises 7.3 and 5.13.

7.10 Cross-Validation 243
Size of Training Set1-Err
0 50 100 150 2000.0 0.2 0.4 0.6 0.8
FIGURE 7.8. Hypothetical learning curve for a classiﬁer on a given task: a
plot of1−Errversus the size of the training set N. With a dataset of 200
observations, 5-fold cross-validation would use training sets of size 160, which
would behave much like the full set. However, with a dataset of 50observations
ﬁvefold cross-validation would use training sets of size 40, and this would result
in a considerable overestimate of prediction error.
On the other hand, with K= 5 say, cross-validation has lower variance.
But bias could be a problem, depending on how the performance of the
learning method varies with the size of the training set. Fig ure 7.8 shows
a hypothetical “learning curve” for a classiﬁer on a given ta sk, a plot of
1−Err versus the size of the training set N. The performance of the
classiﬁer improves as the training set size increases to 100 observations;
increasing the number further to 200 brings only a small bene ﬁt. If our
training set had 200 observations, ﬁvefold cross-validati on would estimate
the performance of our classiﬁer over training sets of size 1 60, which from
Figure 7.8 is virtually the same as the performance for train ing set size
200. Thus cross-validation would not suﬀer from much bias. H owever if the
training set had 50 observations, ﬁvefold cross-validatio n would estimate
the performance of our classiﬁer over training sets of size 4 0, and from the
ﬁgure that would be an underestimate of 1 −Err. Hence as an estimate of
Err, cross-validation would be biased upward.
To summarize, if the learning curve has a considerable slope at the given
training set size, ﬁve- or tenfold cross-validation will ov erestimate the true
prediction error. Whether this bias is a drawback in practic e depends on
the objective. On the other hand, leave-one-out cross-vali dation has low
bias but can have high variance. Overall, ﬁve- or tenfold cro ss-validation
are recommended as a good compromise: see Breiman and Specto r (1992)
and Kohavi (1995).
Figure 7.9 shows the prediction error and tenfold cross-val idation curve
estimated from a single training set, from the scenario in th e bottom right
panel of Figure 7.3. This is a two-class classiﬁcation probl em, using a lin-

244 7. Model Assessment and Selection
Subset Size pMisclassification Error
5 10 15 200.0 0.1 0.2 0.3 0.4 0.5 0.6•••
••
••
•
•
•••••• • •••••
•
••
•
•
••
••• • •• ••• • • •
FIGURE 7.9. Prediction error (orange) and tenfold cross-validation curv e
(blue) estimated from a single training set, from the scenario in the bottom right
panel of Figure 7.3.
ear model with best subsets regression of subset size p. Standard error bars
are shown, which are the standard errors of the individual mi sclassiﬁcation
error rates for each of the ten parts. Both curves have minima atp= 10,
although the CV curve is rather ﬂat beyond 10. Often a “one-st andard
error” rule is used with cross-validation, in which we choos e the most par-
simonious model whose error is no more than one standard erro r above
the error of the best model. Here it looks like a model with abo utp= 9
predictors would be chosen, while the true model uses p= 10.
Generalized cross-validation providesaconvenientapproximationtoleave-
one out cross-validation, for linear ﬁtting under squared- error loss. As de-
ﬁned in Section 7.6, a linear ﬁtting method is one for which we can write
ˆy=Sy. (7.50)
Now for many linear ﬁtting methods,
1
NN/summationdisplay
i=1[yi−ˆf−i(xi)]2=1
NN/summationdisplay
i=1/bracketleftigyi−ˆf(xi)
1−Sii/bracketrightig2
,(7.51)
whereSiiis theith diagonal element of S(see Exercise 7.3). The GCV
approximation is
GCV(ˆf) =1
NN/summationdisplay
i=1/bracketleftigg
yi−ˆf(xi)
1−trace(S)/N/bracketrightigg2
. (7.52)

7.10 Cross-Validation 245
The quantity trace( S) is the eﬀective number of parameters, as deﬁned in
Section 7.6.
GCV can have a computational advantage in some settings, whe re the
trace ofScan be computed more easily than the individual elements Sii.
In smoothing problems, GCV can also alleviate the tendency o f cross-
validation to undersmooth. The similarity between GCV and A IC can be
seen from the approximation 1 /(1−x)2≈1+2x(Exercise 7.7).
7.10.2 The Wrong and Right Way to Do Cross-validation
Consider a classiﬁcation problem with a large number of pred ictors, as may
arise, for example, in genomic or proteomic applications. A typical strategy
for analysis might be as follows:
1. Screen the predictors: ﬁnd a subset of “good” predictors t hat show
fairly strong (univariate) correlation with the class labe ls
2. Using just this subset of predictors, build a multivariat e classiﬁer.
3. Use cross-validation to estimate the unknown tuning para meters and
to estimate the prediction error of the ﬁnal model.
Is this a correct application of cross-validation? Conside r a scenario with
N= 50 samples in two equal-sized classes, and p= 5000 quantitative
predictors (standard Gaussian) that are independent of the class labels.
The true (test) error rate of any classiﬁer is 50%. We carried out the above
recipe, choosing in step (1) the 100 predictors having highe st correlation
with the class labels, and then using a 1-nearest neighbor cl assiﬁer, based
on just these 100 predictors, in step (2). Over 50 simulation s from this
setting, the average CV error rate was 3%. This is far lower th an the true
error rate of 50%.
What has happened? The problem is that the predictors have an unfair
advantage, as they were chosen in step (1) on the basis of all of the samples .
Leaving samples out afterthe variables have been selected does not cor-
rectly mimic the application of the classiﬁer to a completel y independent
test set, since these predictors “have already seen” the lef t out samples.
Figure 7.10 (top panel) illustrates the problem. We selecte d the 100 pre-
dictors having largest correlation with the class labels ov er all 50 samples.
Thenwechosearandomsetof10samples,aswewoulddoinﬁve-f oldcross-
validation, and computed the correlations of the pre-selec ted 100 predictors
with the class labels over just these 10 samples (top panel). We see that
the correlations average about 0.28, rather than 0, as one mi ght expect.
Here is the correct way to carry out cross-validation in this example:
1. Divide the samples into Kcross-validation folds (groups) at random.
2. For each fold k= 1,2,...,K

246 7. Model Assessment and Selection
Correlations of Selected Predictors with OutcomeFrequency
−1.0 −0.5 0.0 0.5 1.00 10 20 30Wrong way
Correlations of Selected Predictors with OutcomeFrequency
−1.0 −0.5 0.0 0.5 1.00 10 20 30Right way
FIGURE 7.10. Cross-validation the wrong and right way: histograms shows t he
correlation of class labels, in 10randomly chosen samples, with the 100predic-
tors chosen using the incorrect (upper red) and correct (lowe r green) versions of
cross-validation.
(a) Find a subset of “good” predictors that show fairly stron g (uni-
variate) correlation with the class labels, using all of the samples
except those in fold k.
(b) Using just this subset of predictors, build a multivaria te classi-
ﬁer, using all of the samples except those in fold k.
(c) Use the classiﬁer to predict the class labels for the samp les in
foldk.
Theerrorestimatesfromstep2(c)arethenaccumulated over allKfolds,to
produce the cross-validation estimate of prediction error . The lower panel
of Figure 7.10 shows the correlations of class labels with th e 100 predictors
chosen in step 2(a) of the correct procedure, over the sample s in a typical
foldk. We see that they average about zero, as they should.
In general, with a multistep modeling procedure, cross-val idation must
be applied to the entire sequence of modeling steps. In parti cular, samples
must be “left out” before any selection or ﬁltering steps are applied. There
is one qualiﬁcation: initial unsupervised screening steps can be done be-
fore samples are left out. For example, we could select the 10 00 predictors

7.10 Cross-Validation 247
with highest variance across all 50 samples, before startin g cross-validation.
Since this ﬁltering does not involve the class labels, it doe s not give the
predictors an unfair advantage.
While this point may seem obvious to the reader, we have seen t his
blunder committed many times in published papers in top rank journals.
With the large numbers of predictors that are so common in gen omic and
other areas, the potential consequences of this error have a lso increased
dramatically; see Ambroise and McLachlan (2002) for a detai led discussion
of this issue.
7.10.3 Does Cross-Validation Really Work?
Weonceagainexaminethebehaviorofcross-validationinah igh-dimensional
classiﬁcation problem. Consider a scenario with N= 20 samples in two
equal-sized classes, and p= 500 quantitative predictors that are indepen-
dent of the class labels. Once again, the true error rate of an y classiﬁer is
50%. Consider a simple univariate classiﬁer: a single split that minimizes
the misclassiﬁcation error (a “stump”). Stumps are trees wi th a single split,
and are used in boosting methods (Chapter 10). A simple argum ent sug-
gests that cross-validation will not work properly in this s etting2:
Fitting to the entire training set, we will ﬁnd a predictor th at
splits the data very well. If we do 5-fold cross-validation, this
same predictor should split any 4/5ths and 1/5th of the data
well too, and hence its cross-validation error will be small (much
less than 50%.) Thus CV does not give an accurate estimate of
error.
To investigate whether this argument is correct, Figure 7.1 1 shows the
result of a simulation from this setting. There are 500 predi ctors and 20
samples, in each of two equal-sized classes, with all predic tors having a
standard Gaussian distribution. The panel in the top left sh ows the number
of training errors for each of the 500 stumps ﬁt to the trainin g data. We
have marked in color the six predictors yielding the fewest e rrors. In the top
right panel, the training errors are shown for stumps ﬁt to a r andom 4/5ths
partition of the data (16 samples), and tested on the remaini ng 1/5th (four
samples). The colored points indicate the same predictors m arked in the
top left panel. We see that the stump for the blue predictor (w hose stump
was the best in the top left panel), makes two out of four test e rrors (50%),
and is no better than random.
What has happened? The preceding argument has ignored the fa ct that
in cross-validation, the model must be completely retraine d for each fold
2This argument was made to us by a scientist at a proteomics lab meeting, a nd led
to material in this section.

248 7. Model Assessment and Selection
0 100 200 300 400 5002 3 4 5 6 7 8 9
PredictorError on Full Training Set
1 2 3 4 5 6 7 80 1 2 3 4
Error on 4/5Error on 1/5
−1 0 1 2
Predictor 436 (blue)Class Label
0 1
full
4/5
0.0 0.2 0.4 0.6 0.8 1.0
CV Errors
FIGURE 7.11. Simulation study to investigate the performance of cross vali -
dation in a high-dimensional problem where the predictors ar e independent of the
class labels. The top-left panel shows the number of errors made b y individual
stump classiﬁers on the full training set ( 20observations). The top right panel
shows the errors made by individual stumps trained on a rando m split of the
dataset into 4/5ths (16observations) and tested on the remaining 1/5th (4ob-
servations). The best performers are depicted by colored dot s in each panel. The
bottom left panel shows the eﬀect of re-estimating the split po int in each fold: the
colored points correspond to the four samples in the 1/5th validation set. The split
point derived from the full dataset classiﬁes all four samples correctly, but when
the split point is re-estimated on the 4/5ths data (as it should be), it commits
two errors on the four validation samples. In the bottom right w e see the overall
result of ﬁve-fold cross-validation applied to 50simulated datasets. The average
error rate is about 50%, as it should be.

7.11 Bootstrap Methods 249
of the process. In the present example, this means that the be st predictor
and corresponding split point are found from 4 /5ths of the data. The eﬀect
of predictor choice is seen in the top right panel. Since the c lass labels are
independent of the predictors, the performance of a stump on the 4/5ths
training data contains no information about its performanc e in the remain-
ing 1/5th. The eﬀect of the choice of split point is shown in the bott om left
panel. Here we see the data for predictor 436, corresponding to the blue
dot in the top left plot. The colored points indicate the 1 /5th data, while
the remaining points belong to the 4 /5ths. The optimal split points for this
predictor based on both the full training set and 4 /5ths data are indicated.
The split based on the full data makes no errors on the 1 /5ths data. But
cross-validation must base its split on the 4 /5ths data, and this incurs two
errors out of four samples.
The results of applying ﬁve-fold cross-validation to each o f 50 simulated
datasets is shown in the bottom right panel. As we would hope, the average
cross-validation error is around 50%, which is the true expe cted prediction
error for this classiﬁer. Hence cross-validation has behav ed as it should.
On the other hand, there is considerable variability in the e rror, underscor-
ing the importance of reporting the estimated standard erro r of the CV
estimate. See Exercise 7.10 for another variation of this pr oblem.
7.11 Bootstrap Methods
The bootstrap is a general tool for assessing statistical ac curacy. First we
describe the bootstrap in general, and then show how it can be used to
estimate extra-sample prediction error. As with cross-val idation, the boot-
strap seeks to estimate the conditional error Err T, but typically estimates
well only the expected prediction error Err.
Suppose we have a model ﬁt to a set of training data. We denote t he
training set by Z= (z1,z2,...,z N) wherezi= (xi,yi). The basic idea is
to randomly draw datasets with replacement from the trainin g data, each
sample the same size as the original training set. This is don eBtimes
(B= 100 say), producing Bbootstrap datasets, as shown in Figure 7.12.
Then we reﬁt the model to each of the bootstrap datasets, and e xamine
the behavior of the ﬁts over the Breplications.
In the ﬁgure, S(Z) is any quantity computed from the data Z, for ex-
ample, the prediction at some input point. From the bootstra p sampling
we can estimate any aspect of the distribution of S(Z), for example, its
variance,
/hatwidestVar[S(Z)] =1
B−1B/summationdisplay
b=1(S(Z∗b)−¯S∗)2, (7.53)

250 7. Model Assessment and Selection
  Bootstrap
Bootstrapreplications
samples
sampleTrainingZ= (z1,z2,...,z N)Z∗1Z∗2Z∗BS(Z∗1)S(Z∗2) S(Z∗B)
FIGURE 7.12. Schematic of the bootstrap process. We wish to assess the sta -
tistical accuracy of a quantity S(Z)computed from our dataset. Btraining sets
Z∗b, b= 1,...,Beach of size Nare drawn with replacement from the original
dataset. The quantity of interest S(Z)is computed from each bootstrap training
set, and the values S(Z∗1),...,S(Z∗B)are used to assess the statistical accuracy
ofS(Z).
where¯S∗=/summationtext
bS(Z∗b)/B. Note that/hatwidestVar[S(Z)] can be thought of as a
Monte-Carlo estimate of the variance of S(Z) under sampling from the
empirical distribution function ˆFfor the data ( z1,z2,...,z N).
How can we apply the bootstrap to estimate prediction error? One ap-
proach would be to ﬁt the model in question on a set of bootstra p samples,
and then keep track of how well it predicts the original train ing set. If
ˆf∗b(xi) is the predicted value at xi, from the model ﬁtted to the bth boot-
strap dataset, our estimate is
/hatwidestErrboot=1
B1
NB/summationdisplay
b=1N/summationdisplay
i=1L(yi,ˆf∗b(xi)). (7.54)
However,itiseasytoseethat /hatwidestErrbootdoesnotprovideagoodestimatein
general. The reason is that the bootstrap datasets are actin g as the training
samples, while the original training set is acting as the tes t sample, and
these two samples have observations in common. This overlap can make
overﬁt predictions look unrealistically good, and is the re ason that cross-
validation explicitly uses non-overlapping data for the tr aining and test
samples. Consider for example a 1-nearest neighbor classiﬁ er applied to a
two-class classiﬁcation problem with the same number of obs ervations in

7.11 Bootstrap Methods 251
each class, in which the predictors and class labels are in fa ct independent.
Then the true error rate is 0 .5. But the contributions to the bootstrap
estimate/hatwidestErrbootwill be zero unless the observation idoes not appear in the
bootstrap sample b. In this latter case it will have the correct expectation
0.5. Now
Pr{observation i∈bootstrap sample b}= 1−/parenleftig
1−1
N/parenrightigN
≈1−e−1
= 0.632. (7.55)
Hence the expectation of /hatwidestErrbootis about 0.5×0.368 = 0.184, far below
the correct error rate 0 .5.
By mimicking cross-validation, a better bootstrap estimat e can be ob-
tained. For each observation, we only keep track of predicti ons from boot-
strap samples not containing that observation. The leave-o ne-out bootstrap
estimate of prediction error is deﬁned by
/hatwidestErr(1)=1
NN/summationdisplay
i=11
|C−i|/summationdisplay
b∈C−iL(yi,ˆf∗b(xi)). (7.56)
HereC−iis the set of indices of the bootstrap samples bthat donotcontain
observation i,and|C−i|isthenumberofsuchsamples.Incomputing /hatwidestErr(1),
we either have to choose Blarge enough to ensure that all of the |C−i|are
greaterthanzero,orwecanjustleaveoutthetermsin(7.56) corresponding
to|C−i|’s that are zero.
The leave-one out bootstrap solves the overﬁtting problem s uﬀered by
/hatwidestErrboot, but has the training-set-size bias mentioned in the discus sion of
cross-validation. The average number of distinct observat ions in each boot-
strap sample is about 0 .632·N, so its bias will roughly behave like that of
twofold cross-validation. Thus if the learning curve has co nsiderable slope
at sample size N/2, the leave-one out bootstrap will be biased upward as
an estimate of the true error.
The “.632 estimator” is designed to alleviate this bias. It is deﬁn ed by
/hatwidestErr(.632)=.368·err+.632·/hatwidestErr(1). (7.57)
The derivation of the .632 estimator is complex; intuitively it pulls the
leave-one out bootstrap estimate down toward the training e rror rate, and
hencereducesitsupwardbias.Theuseoftheconstant.632re latesto(7.55).
The.632 estimator works well in “light ﬁtting” situations, but c an break
down in overﬁt ones. Here is an example due to Breiman et al. (1 984).
Suppose we have two equal-size classes, with the targets ind ependent of
the class labels, and we apply a one-nearest neighbor rule. T henerr = 0,

252 7. Model Assessment and Selection
/hatwidestErr(1)= 0.5 and so/hatwidestErr(.632)=.632×0.5 =.316. However, the true error
rate is 0.5.
One can improve the .632 estimator by taking into account the amount
of overﬁtting. First we deﬁne γto be the no-information error rate : this
is the error rate of our prediction rule if the inputs and clas s labels were
independent. An estimate of γis obtained by evaluating the prediction rule
on all possible combinations of targets yiand predictors xi′
ˆγ=1
N2N/summationdisplay
i=1N/summationdisplay
i′=1L(yi,ˆf(xi′)). (7.58)
For example, consider the dichotomous classiﬁcation probl em: let ˆp1be
the observed proportion of responses yiequaling 1, and let ˆ q1be the ob-
served proportion of predictions ˆf(xi′) equaling 1. Then
ˆγ= ˆp1(1−ˆq1)+(1−ˆp1)ˆq1. (7.59)
With a rule like 1-nearest neighbors for which ˆ q1= ˆp1the value of ˆ γis
2ˆp1(1−ˆp1).Themulti-category generalization of(7.59) is ˆ γ=/summationtext
ℓˆpℓ(1−ˆqℓ).
Using this, the relative overﬁtting rate is deﬁned to be
ˆR=/hatwidestErr(1)−err
ˆγ−err, (7.60)
a quantity that ranges from 0 if there is no overﬁtting ( /hatwidestErr(1)=err) to 1
if the overﬁtting equals the no-information value ˆ γ−err. Finally, we deﬁne
the “.632+” estimator by
/hatwidestErr(.632+)= (1−ˆw)·err+ ˆw·/hatwidestErr(1)(7.61)
with ˆw=.632
1−.368ˆR.
The weight wranges from .632 ifˆR= 0 to 1 if ˆR= 1, so/hatwidestErr(.632+)
ranges from/hatwidestErr(.632)to/hatwidestErr(1). Again, the derivation of (7.61) is compli-
cated: roughly speaking, it produces a compromise between t he leave-one-
out bootstrap and the training error rate that depends on the amount of
overﬁtting. For the 1-nearest-neighbor problem with class labels indepen-
dent of the inputs, ˆ w=ˆR= 1, so/hatwidestErr(.632+)=/hatwidestErr(1), which has the correct
expectation of 0.5. In other problems with less overﬁtting, /hatwidestErr(.632+)will
lie somewhere between err and/hatwidestErr(1).
7.11.1 Example (Continued)
Figure7.13showstheresultsoftenfoldcross-validationa ndthe.632+boot-
strap estimate in the same four problems of Figures 7.7. As in that ﬁgure,

7.11 Bootstrap Methods 253
reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestCross−validation
reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBootstrap
FIGURE 7.13. Boxplots show the distribution of the relative error
100·[Errˆα−minαErr(α)]/[maxαErr(α)−minαErr(α)]over the four scenar-
ios of Figure 7.3. This is the error in using the chosen model r elative to the best
model. There are 100training sets represented in each boxplot.
Figure 7.13 shows boxplots of 100 ·[Errˆα−minαErr(α)]/[maxαErr(α)−
minαErr(α)],theerrorinusingthechosenmodelrelativetothebestmo del.
Thereare100diﬀerent trainingsets representedin eachbox plot. Both mea-
sures perform well overall, perhaps the same or slightly wor se than the AIC
in Figure 7.7.
Our conclusion is that for these particular problems and ﬁtt ing methods,
minimization of either AIC, cross-validation or bootstrap yields a model
fairly close to the best available. Note that for the purpose of model selec-
tion, any of the measures could be biased and it wouldn’t aﬀec t things, as
long as the bias did not change the relative performance of th e methods.
For example, the addition of a constant to any of the measures would not
change the resulting chosen model. However, for many adapti ve, nonlinear
techniques (like trees), estimation of the eﬀective number of parameters is
very diﬃcult. This makes methods like AIC impractical and le aves us with
cross-validation or bootstrap as the methods of choice.
A diﬀerent question is: how well does each method estimate te st error?
On the average the AIC criterion overestimated prediction e rror of its cho-

254 7. Model Assessment and Selection
senmodelby38%,37%,51%,and30%,respectively,overthefo urscenarios,
with BIC performing similarly. In contrast, cross-validat ion overestimated
the error by 1%, 4%, 0%, and 4%, with the bootstrap doing about the
same. Hence the extra work involved in computing a cross-val idation or
bootstrap measure is worthwhile, if an accurate estimate of test error is
required. With other ﬁtting methods like trees, cross-vali dation and boot-
strap can underestimate the true error by 10%, because the se arch for best
tree is strongly aﬀected by the validation set. In these situ ations only a
separate test set will provide an unbiased estimate of test e rror.
7.12 Conditional or Expected Test Error?
Figures7.14and7.15examinethequestionofwhethercross- validationdoes
a good job in estimating Err T, the error conditional on a given training set
T(expression (7.15) on page 228), as opposed to the expected t est error.
For each of 100 training sets generated from the “reg/linear ” setting in
the top-right panel of Figure 7.3, Figure 7.14 shows the cond itional error
curvesErr Tasafunctionofsubsetsize(topleft).Thenexttwopanelssh ow
10-fold and N-fold cross-validation, the latter also known as leave-one -out
(LOO). The thick red curve in each plot is the expected error E rr, while
the thick black curves are the expected cross-validation cu rves. The lower
right panel shows how well cross-validation approximates t he conditional
and expected error.
One might have expected N-fold CV to approximate Err Twell, since it
almostuses the full training sample to ﬁt a new test point. 10-fold C V, on
the other hand, might be expected to estimate Err well, since it averages
over somewhat diﬀerent training sets. From the ﬁgure it appe ars 10-fold
does a better job than N-fold in estimating Err T, and estimates Err even
better. Indeed, the similarity of the two black curves with t he red curve
suggests both CV curves are approximately unbiased for Err, with 10-fold
having less variance. Similar trends were reported by Efron (1983).
Figure7.15showsscatterplotsofboth10-foldand N-foldcross-validation
error estimates versus the true conditional error for the 10 0 simulations.
Although the scatterplots do not indicate much correlation , the lower right
panel shows that for the most part the correlations are negat ive, a curi-
ous phenomenon that has been observed before. This negative correlation
explains why neither form of CV estimates Err Twell. The broken lines in
each plot are drawn at Err( p), the expected error for the best subset of
sizep. We see again that both forms of CV are approximately unbiase d for
expected error, but the variation in test error for diﬀerent training sets is
quite substantial.
Amongthefourexperimental conditions in7.3,this“reg/li near”scenario
showedthehighestcorrelationbetweenactualandpredicte dtesterror.This

7.12 Conditional or Expected Test Error? 255
5 10 15 200.1 0.2 0.3 0.4Prediction Error
Subset Size pError
5 10 15 200.1 0.2 0.3 0.410−Fold CV Error
Subset Size pError
5 10 15 200.1 0.2 0.3 0.4Leave−One−Out CV Error
Subset Size pError
5 10 15 200.015 0.025 0.035 0.045Approximation Error
Subset Size pMean Absolute DeviationET|CV10−Err|
ET|CV10−ErrT|
ET|CVN−ErrT|
FIGURE 7.14. Conditional prediction-error ErrT,10-fold cross-validation, and
leave-one-out cross-validation curves for a 100simulations from the top-right
panel in Figure 7.3. The thick red curve is the expected predi ction error Err,
while the thick black curves are the expected CV curves ETCV10andETCVN.
The lower-right panel shows the mean absolute deviation of the CV curves from
the conditional error, ET|CVK−ErrT|forK= 10(blue) andK=N(green),
as well as from the expected error ET|CV10−Err|(orange).

256 7. Model Assessment and Selection
0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 1
Prediction ErrorCV Error
0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 5
Prediction ErrorCV Error
0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 10
Prediction ErrorCV Error
5 10 15 20−0.6 −0.4 −0.2 0.0 0.2
Subset SizeCorrelation
Leave−one−out
10−Fold
FIGURE 7.15. Plots of the CV estimates of error versus the true conditional
error for each of the 100training sets, for the simulation setup in the top right
panel Figure 7.3. Both 10-fold and leave-one-out CV are depicted in diﬀerent
colors. The ﬁrst three panels correspond to diﬀerent subset si zesp, and vertical
and horizontal lines are drawn at Err(p). Although there appears to be little cor-
relation in these plots, we see in the lower right panel that for t he most part the
correlation is negative.

Exercises 257
phenomenon also occurs for bootstrap estimates of error, an d we would
guess, for any other estimate of conditional prediction err or.
We conclude that estimation of test error for a particular tr aining set is
noteasyingeneral,givenjustthedatafromthatsametraini ngset.Instead,
cross-validation and related methods may provide reasonab le estimates of
theexpected error Err.
Bibliographic Notes
Key references for cross-validation are Stone (1974), Ston e (1977) and
Allen (1974). The AIC was proposed by Akaike (1973), while th e BIC
was introduced by Schwarz (1978). Madigan and Raftery (1994 ) give an
overview of Bayesian model selection. The MDL criterion is d ue to Rissa-
nen(1983).CoverandThomas(1991)containsagooddescript ionofcoding
theory and complexity. VC dimension is described in Vapnik ( 1996). Stone
(1977) showed that the AIC and leave-one out cross-validati on are asymp-
totically equivalent. Generalized cross-validation is de scribed by Golub et
al. (1979) and Wahba (1980); a further discussion of the topi c may be found
in the monograph by Wahba (1990). See also Hastie and Tibshir ani (1990),
Chapter 3. The bootstrap is due to Efron (1979); see Efron and Tibshi-
rani (1993) for an overview. Efron (1983) proposes a number o f bootstrap
estimates of prediction error, including the optimism and . 632 estimates.
Efron (1986) compares CV, GCV and bootstrap estimates of err or rates.
The use of cross-validation and the bootstrap for model sele ction is stud-
ied by Breiman and Spector (1992), Breiman (1992), Shao (199 6), Zhang
(1993) and Kohavi (1995). The .632+ estimator was proposed by Efron
and Tibshirani (1997).
Cherkassky and Ma (2003) published a study on the performanc e of
SRM for model selection in regression, in response to our stu dy of section
7.9.1. They complained that we had been unfair to SRM because had not
applied it properly. Our response can be found in the same iss ue of the
journal (Hastie et al. (2003)).
Exercises
Ex. 7.1Derive the estimate of in-sample error (7.24).
Ex. 7.2For 0–1 loss with Y∈{0,1}and Pr(Y= 1|x0) =f(x0), show that
Err(x0) = Pr(Y∝ne}ationslash=ˆG(x0)|X=x0)
= Err B(x0)+|2f(x0)−1|Pr(ˆG(x0)∝ne}ationslash=G(x0)|X=x0),
(7.62)

258 7. Model Assessment and Selection
whereˆG(x) =I(ˆf(x)>1
2),G(x) =I(f(x)>1
2) is the Bayes classiﬁer,
and Err B(x0) = Pr(Y∝ne}ationslash=G(x0)|X=x0), the irreducible Bayes error atx0.
Using the approximation ˆf(x0)∼N(Eˆf(x0),Var(ˆf(x0)), show that
Pr(ˆG(x0)∝ne}ationslash=G(x0)|X=x0)≈Φ/parenleftigg
sign(1
2−f(x0))(Eˆf(x0)−1
2)/radicalig
Var(ˆf(x0))/parenrightigg
.(7.63)
In the above,
Φ(t) =1√
2π/integraldisplayt
−∞exp(−t2/2)dt,
the cumulative Gaussian distribution function. This is an i ncreasing func-
tion, with value 0 at t=−∞and value 1 at t= +∞.
We can think of sign(1
2−f(x0))(Eˆf(x0)−1
2) as a kind of boundary-
biasterm, as it depends on the true f(x0) only through which side of the
boundary (1
2) that it lies. Notice also that the bias and variance combine
in a multiplicative rather than additive fashion. If E ˆf(x0) is on the same
side of1
2asf(x0), then the bias is negative, and decreasing the variance
will decrease the misclassiﬁcation error. On the other hand , if Eˆf(x0) is
on the opposite side of1
2tof(x0), then the bias is positive and it pays to
increase the variance! Such an increase will improve the cha nce that ˆf(x0)
falls on the correct side of1
2(Friedman, 1997).
Ex. 7.3Letˆf=Sybe a linear smoothing of y.
(a)IfSiiistheithdiagonalelementof S,showthatfor Sarisingfromleast
squares projections and cubic smoothing splines, the cross -validated
residual can be written as
yi−ˆf−i(xi) =yi−ˆf(xi)
1−Sii. (7.64)
(b) Use this result to show that |yi−ˆf−i(xi)|≥|yi−ˆf(xi)|.
(c) Find general conditions on any smoother Sto make result (7.64) hold.
Ex. 7.4Consider the in-sample prediction error (7.18) and the trai ning
errorerr in the case of squared-error loss:
Errin=1
NN/summationdisplay
i=1EY0(Y0
i−ˆf(xi))2
err =1
NN/summationdisplay
i=1(yi−ˆf(xi))2.

Exercises 259
Add and subtract f(xi) and Eˆf(xi) in each expression and expand. Hence
establish that the average optimism in the training error is
2
NN/summationdisplay
i=1Cov(ˆyi,yi),
as given in (7.21).
Ex. 7.5For a linear smoother ˆy=Sy, show that
N/summationdisplay
i=1Cov(ˆyi,yi) = trace( S)σ2
ε, (7.65)
which justiﬁes its use as the eﬀective number of parameters.
Ex. 7.6Show that for an additive-error model, the eﬀective degrees -of-
freedom for the k-nearest-neighbors regression ﬁt is N/k.
Ex.7.7Usetheapproximation1 /(1−x)2≈1+2xtoexposetherelationship
between Cp/AIC (7.26) and GCV (7.52), the main diﬀerence being the
model used to estimate the noise variance σ2
ε.
Ex. 7.8Show that the set of functions {I(sin(αx)>0)}can shatter the
following points on the line:
z1= 10−1,...,zℓ= 10−ℓ, (7.66)
for anyℓ. Hence the VC dimension of the class {I(sin(αx)>0)}is inﬁnite.
Ex. 7.9For the prostate data of Chapter 3, carry out a best-subset li near
regression analysis, as in Table 3.3 (third column from left ). Compute the
AIC, BIC, ﬁve- and tenfold cross-validation, and bootstrap .632 estimates
of prediction error. Discuss the results.
Ex. 7.10 Referring to the example in Section 7.10.3, suppose instead that
all of theppredictors are binary, and hence there is no need to estimate
split points. The predictors are independent of the class la bels as before.
Then ifpis very large, we can probably ﬁnd a predictor that splits the
entire training data perfectly, and hence would split the va lidation data
(one-ﬁfth of data) perfectly as well. This predictor would t herefore have
zero cross-validation error. Does this mean that cross-val idation does not
provide a good estimate of test error in this situation? [Thi s question was
suggested by Li Ma.]

260 7. Model Assessment and Selection

This is page 261
Printer: Opaque this
8
Model Inference and Averaging
8.1 Introduction
For most of this book, the ﬁtting (learning) of models has bee n achieved by
minimizing a sum of squares for regression, or by minimizing cross-entropy
for classiﬁcation. In fact, both of these minimizations are instances of the
maximum likelihood approach to ﬁtting.
In this chapter we provide a general exposition of the maximu m likeli-
hood approach, as well as the Bayesian method for inference. The boot-
strap, introduced in Chapter 7, is discussed in this context , and its relation
to maximum likelihood and Bayes is described. Finally, we pr esent some
related techniques for model averaging and improvement, in cluding com-
mittee methods, bagging, stacking and bumping.
8.2 The Bootstrap and Maximum Likelihood
Methods
8.2.1 A Smoothing Example
The bootstrap method provides a direct computational way of assessing
uncertainty, by sampling from the training data. Here we ill ustrate the
bootstrap in a simple one-dimensional smoothing problem, a nd show its
connection to maximum likelihood.

262 8. Model Inference and Averaging
0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5
xy
••••••
••••••••••
••••
•
•
•••
•••••••••
••••••••
••••
••
••
0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.2 0.4 0.6 0.8 1.0
xB-spline Basis
FIGURE 8.1. (Left panel): Data for smoothing example. (Right panel:) Set of
sevenB-spline basis functions. The broken vertical lines indicate t he placement
of the three knots.
Denote the training data by Z={z1,z2,...,z N}, withzi= (xi,yi),
i= 1,2,...,N. Herexiis a one-dimensional input, and yithe outcome,
either continuous or categorical. As an example, consider t heN= 50 data
points shown in the left panel of Figure 8.1.
Suppose we decide to ﬁt a cubic spline to the data, with three k nots
placed at the quartiles of the Xvalues. This is a seven-dimensional lin-
ear space of functions, and can be represented, for example, by a linear
expansion of B-spline basis functions (see Section 5.9.2):
µ(x) =7/summationdisplay
j=1βjhj(x). (8.1)
Here thehj(x),j= 1,2,...,7 are the seven functions shown in the right
panel of Figure 8.1. We can think of µ(x) as representing the conditional
mean E(Y|X=x).
LetHbe theN×7 matrix with ijth element hj(xi). The usual estimate
ofβ, obtained by minimizing the squared error over the training set, is
given by
ˆβ= (HTH)−1HTy. (8.2)
The corresponding ﬁt ˆ µ(x) =/summationtext7
j=1ˆβjhj(x) is shown in the top left panel
of Figure 8.2.
The estimated covariance matrix of ˆβis
/hatwidestVar(ˆβ) = (HTH)−1ˆσ2, (8.3)
where we have estimated the noise variance by ˆ σ2=/summationtextN
i=1(yi−ˆµ(xi))2/N.
Lettingh(x)T= (h1(x),h2(x),...,h 7(x)), the standard error of a predic-

8.2 The Bootstrap and Maximum Likelihood Methods 263
0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••
••••••••••
••••
•
•
•••
•••••••••
••••••••
••••
••
••
xy
0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5
xy
••••••
••••••••••
••••
•
•
•••
•••••••••
••••••••
••••
••
••
0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5
xy
••••••
••••••••••
••••
•
•
•••
•••••••••
••••••••
••••
••
••
0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5
xy
••••••
••••••••••
••••
•
•
•••
•••••••••
••••••••
••••
••
••
FIGURE 8.2. (Top left:)B-spline smooth of data. (Top right:) B-spline smooth
plus and minus 1.96×standard error bands. (Bottom left:) Ten bootstrap repli-
cates of the B-spline smooth. (Bottom right:) B-spline smooth with 95% standard
error bands computed from the bootstrap distribution.

264 8. Model Inference and Averaging
tion ˆµ(x) =h(x)Tˆβis
/hatwidese[ˆµ(x)] = [h(x)T(HTH)−1h(x)]1
2ˆσ. (8.4)
In the top right panel of Figure 8.2 we have plotted ˆ µ(x)±1.96·/hatwidese[ˆµ(x)].
Since 1.96 is the 97.5% point of the standard normal distribu tion, these
represent approximate 100 −2×2.5% = 95% pointwise conﬁdence bands
forµ(x).
Here is how we could apply the bootstrap in this example. We dr awB
datasets each of size N= 50 with replacement from our training data, the
sampling unit being the pair zi= (xi,yi). To each bootstrap dataset Z∗
we ﬁt a cubic spline ˆ µ∗(x); the ﬁts from ten such samples are shown in the
bottom left panel of Figure 8.2. Using B= 200 bootstrap samples, we can
form a 95% pointwise conﬁdence band from the percentiles at e achx: we
ﬁnd the 2.5%×200 = ﬁfth largest and smallest values at each x. These are
plotted in the bottom right panel of Figure 8.2. The bands loo k similar to
those in the top right, being a little wider at the endpoints.
There is actually a close connection between the least squar es estimates
(8.2)and(8.3),thebootstrap,andmaximumlikelihood.Sup posewefurther
assume that the model errors are Gaussian,
Y=µ(X)+ε;ε∼N(0,σ2),
µ(x) =7/summationdisplay
j=1βjhj(x). (8.5)
The bootstrap method described above, in which we sample wit h re-
placement from the training data, is called the nonparametric bootstrap .
This really means that the method is “model-free,” since it u ses the raw
data, not a speciﬁc parametric model, to generate new datase ts. Consider
a variation of the bootstrap, called the parametric bootstrap , in which we
simulate new responses by adding Gaussian noise to the predi cted values:
y∗
i= ˆµ(xi)+ε∗
i;ε∗
i∼N(0,ˆσ2);i= 1,2,...,N. (8.6)
This process is repeated Btimes, where B= 200 say. The resulting boot-
strap datasets have the form ( x1,y∗
1),...,(xN,y∗
N) and we recompute the
B-spline smooth on each. The conﬁdence bands from this method will ex-
actly equal the least squares bands in the top right panel, as the number of
bootstrap samples goes to inﬁnity. A function estimated fro m a bootstrap
sampley∗is given by ˆ µ∗(x) =h(x)T(HTH)−1HTy∗, and has distribution
ˆµ∗(x)∼N(ˆµ(x),h(x)T(HTH)−1h(x)ˆσ2). (8.7)
Notice that the mean of this distribution is the least square s estimate, and
the standard deviation is the same as the approximate formul a (8.4).

8.2 The Bootstrap and Maximum Likelihood Methods 265
8.2.2 Maximum Likelihood Inference
It turns out that the parametric bootstrap agrees with least squares in the
previous example because the model (8.5) has additive Gauss ian errors. In
general, the parametric bootstrap agrees not with least squ ares but with
maximum likelihood, which we now review.
We begin by specifying a probability density or probability mass function
for our observations
zi∼gθ(z). (8.8)
In this expression θrepresents one or more unknown parameters that gov-
ern the distribution of Z. This is called a parametric model forZ. As an
example, if Zhas a normal distribution with mean µand variance σ2, then
θ= (µ,σ2), (8.9)
and
gθ(z) =1√
2πσe−1
2(z−µ)2/σ2. (8.10)
Maximum likelihood is based on the likelihood function , given by
L(θ;Z) =N/productdisplay
i=1gθ(zi), (8.11)
the probability of the observed data under the model gθ. The likelihood is
deﬁned only up to a positive multiplier, which we have taken t o be one.
We think of L(θ;Z) as a function of θ, with our data Zﬁxed.
Denote the logarithm of L(θ;Z) by
ℓ(θ;Z) =N/summationdisplay
i=1ℓ(θ;zi)
=N/summationdisplay
i=1loggθ(zi), (8.12)
which we will sometimes abbreviate as ℓ(θ). This expression is called the
log-likelihood, and each value ℓ(θ;zi) = loggθ(zi) is called a log-likelihood
component. The method of maximum likelihood chooses the val ueθ=ˆθ
to maximize ℓ(θ;Z).
The likelihood function can be used to assess the precision o fˆθ. We need
a few more deﬁnitions. The score function is deﬁned by
˙ℓ(θ;Z) =N/summationdisplay
i=1˙ℓ(θ;zi), (8.13)

266 8. Model Inference and Averaging
where˙ℓ(θ;zi) =∂ℓ(θ;zi)/∂θ. Assuming that the likelihood takes its maxi-
mum in the interior of the parameter space, ˙ℓ(ˆθ;Z) = 0. The information
matrixis
I(θ) =−N/summationdisplay
i=1∂2ℓ(θ;zi)
∂θ∂θT. (8.14)
WhenI(θ) is evaluated at θ=ˆθ, it is often called the observed information .
TheFisher information (or expected information) is
i(θ) = Eθ[I(θ)]. (8.15)
Finally, let θ0denote the true value of θ.
A standard result says that the sampling distribution of the maximum
likelihood estimator has a limiting normal distribution
ˆθ→N(θ0,i(θ0)−1), (8.16)
asN→∞. Here we are independently sampling from gθ0(z). This suggests
that the sampling distribution of ˆθmay be approximated by
N(ˆθ,i(ˆθ)−1) orN(ˆθ,I(ˆθ)−1), (8.17)
whereˆθrepresents the maximum likelihood estimate from the observ ed
data.
The corresponding estimates for the standard errors of ˆθjare obtained
from
/radicalig
i(ˆθ)−1
jjand/radicalig
I(ˆθ)−1
jj. (8.18)
Conﬁdence points for θjcan be constructed from either approximation
in (8.17). Such a conﬁdence point has the form
ˆθj−z(1−α)·/radicalig
i(ˆθ)−1
jjorˆθj−z(1−α)·/radicalig
I(ˆθ)−1
jj,
respectively, where z(1−α)is the 1−αpercentile of the standard normal
distribution. More accurate conﬁdence intervals can be der ived from the
likelihood function, by using the chi-squared approximati on
2[ℓ(ˆθ)−ℓ(θ0)]∼χ2
p, (8.19)
wherepis the number of components in θ. The resulting 1 −2αconﬁ-
dence interval is the set of all θ0such that 2[ ℓ(ˆθ)−ℓ(θ0)]≤χ2
p(1−2α),
whereχ2
p(1−2α)is the 1−2αpercentile of the chi-squared distribution with
pdegrees of freedom.

8.3 Bayesian Methods 267
Let’s return to our smoothing example to see what maximum lik elihood
yields. The parameters are θ= (β,σ2). The log-likelihood is
ℓ(θ) =−N
2logσ22π−1
2σ2N/summationdisplay
i=1(yi−h(xi)Tβ)2. (8.20)
The maximum likelihood estimate is obtained by setting ∂ℓ/∂β= 0 and
∂ℓ/∂σ2= 0, giving
ˆβ= (HTH)−1HTy,
ˆσ2=1
N/summationdisplay
(yi−ˆµ(xi))2,(8.21)
which are the same as the usual estimates given in (8.2) and be low (8.3).
The information matrix for θ= (β,σ2) is block-diagonal, and the block
corresponding to βis
I(β) = (HTH)/σ2, (8.22)
so that the estimated variance ( HTH)−1ˆσ2agrees with the least squares
estimate (8.3).
8.2.3 Bootstrap versus Maximum Likelihood
In essence the bootstrap is a computer implementation of non parametric or
parametric maximum likelihood. The advantage of the bootst rap over the
maximum likelihood formula is that it allows us to compute ma ximum like-
lihood estimates of standard errors and other quantities in settings where
no formulas are available.
In our example, suppose that we adaptively choose by cross-v alidation
the number and position of the knots that deﬁne the B-splines, rather
than ﬁx them in advance. Denote by λthe collection of knots and their
positions. Then the standard errors and conﬁdence bands sho uld account
for the adaptive choice of λ, but there is no way to do this analytically.
With the bootstrap, we compute the B-spline smooth with an adaptive
choice of knots for each bootstrap sample. The percentiles o f the resulting
curves capture the variability from both the noise in the tar gets as well as
that from ˆλ. In this particular example the conﬁdence bands (not shown)
don’t look much diﬀerent than the ﬁxed λbands. But in other problems,
where more adaptation is used, this can be an important eﬀect to capture.
8.3 Bayesian Methods
In the Bayesian approach to inference, we specify a sampling model Pr( Z|θ)
(density or probability mass function) for our data given th e parameters,

268 8. Model Inference and Averaging
and a prior distribution for the parameters Pr( θ) reﬂecting our knowledge
aboutθbefore we see the data. We then compute the posterior distrib ution
Pr(θ|Z) =Pr(Z|θ)·Pr(θ)/integraltext
Pr(Z|θ)·Pr(θ)dθ, (8.23)
which represents our updated knowledge about θafter we see the data. To
understand this posterior distribution, one might draw sam ples from it or
summarize by computing its mean or mode. The Bayesian approa ch diﬀers
from the standard (“frequentist”) method for inference in i ts use of a prior
distribution to express the uncertainty present before see ing the data, and
to allow the uncertainty remaining after seeing the data to b e expressed in
the form of a posterior distribution.
Theposteriordistributionalsoprovidesthebasisforpred ictingthevalues
of a future observation znew, via the predictive distribution :
Pr(znew|Z) =/integraldisplay
Pr(znew|θ)·Pr(θ|Z)dθ. (8.24)
In contrast, the maximum likelihood approach would use Pr( znew|ˆθ),
the data density evaluated at the maximum likelihood estima te, to predict
future data. Unlike the predictive distribution (8.24), th is does not account
for the uncertainty in estimating θ.
Let’s walk through the Bayesian approach in our smoothing ex ample.
We start with the parametric model given by equation (8.5), a nd assume
for the moment that σ2is known. We assume that the observed feature
valuesx1,x2,...,x Nare ﬁxed, so that the randomness in the data comes
solely from yvarying around its mean µ(x).
The second ingredient we need is a prior distribution. Distr ibutions on
functions are fairly complex entities: one approach is to us e a Gaussian
process prior in which we specify the prior covariance betwe en any two
function values µ(x) andµ(x′) (Wahba, 1990; Neal, 1996).
Here we take a simpler route: by considering a ﬁnite B-spline basis for
µ(x),wecaninsteadprovideapriorforthecoeﬃcients β,andthisimplicitly
deﬁnes a prior for µ(x). We choose a Gaussian prior centered at zero
β∼N(0,τΣ) (8.25)
with the choices of the prior correlation matrix Σand variance τto be
discussed below. The implicit process prior for µ(x) is hence Gaussian,
with covariance kernel
K(x,x′) = cov[ µ(x),µ(x′)]
=τ·h(x)TΣh(x′). (8.26)

8.3 Bayesian Methods 269
0.0 0.5 1.0 1.5 2.0 2.5 3.0-3 -2 -1 0 1 2 3µ(x)
x
FIGURE 8.3. Smoothing example: Ten draws from the Gaussian prior distri-
bution for the function µ(x).
The posterior distribution for βis also Gaussian, with mean and covariance
E(β|Z) =/parenleftbigg
HTH+σ2
τΣ−1/parenrightbigg−1
HTy,
cov(β|Z) =/parenleftbigg
HTH+σ2
τΣ−1/parenrightbigg−1
σ2,(8.27)
with the corresponding posterior values for µ(x),
E(µ(x)|Z) =h(x)T/parenleftbigg
HTH+σ2
τΣ−1/parenrightbigg−1
HTy,
cov[µ(x),µ(x′)|Z] =h(x)T/parenleftbigg
HTH+σ2
τΣ−1/parenrightbigg−1
h(x′)σ2.(8.28)
How do we choose the prior correlation matrix Σ? In some settings the
prior can be chosen from subject matter knowledge about the p arameters.
Here we are willing to say the function µ(x) should be smooth, and have
guaranteed this by expressing µin a smooth low-dimensional basis of B-
splines. Hence we can take the prior correlation matrix to be the identity
Σ=I. When the number of basis functions is large, this might not b e suf-
ﬁcient, and additional smoothness can be enforced by imposi ng restrictions
onΣ; this is exactly the case with smoothing splines (Section 5. 8.1).
Figure 8.3 shows ten draws from the corresponding prior for µ(x). To
generateposteriorvaluesofthefunction µ(x),wegeneratevalues β′fromits
posterior (8.27), giving corresponding posterior value µ′(x) =/summationtext7
1β′
jhj(x).
Ten such posterior curves are shown in Figure 8.4. Two diﬀere nt values
were used for the prior variance τ, 1 and 1000. Notice how similar the
right panel looks to the bootstrap distribution in the botto m left panel

270 8. Model Inference and Averaging
0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••
••••••••••
••••
•
•
•••
•••••••••
••••••••
••••
••
••
0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••
••••••••••
••••
•
•
•••
•••••••••
••••••••
••••
••
••µ(x)µ(x)
x xτ= 1 τ= 1000
FIGURE 8.4. Smoothing example: Ten draws from the posterior distributio n
for the function µ(x), for two diﬀerent values of the prior variance τ. The purple
curves are the posterior means.
of Figure 8.2 on page 263. This similarity is no accident. As τ→∞, the
posterior distribution (8.27) and the bootstrap distribut ion (8.7) coincide.
On the other hand, for τ= 1, the posterior curves µ(x) in the left panel
of Figure 8.4 are smoother than the bootstrap curves, becaus e we have
imposed more prior weight on smoothness.
The distribution (8.25) with τ→∞is called a noninformative prior for
θ.InGaussianmodels,maximumlikelihoodandparametricboo tstrapanal-
yses tend to agree with Bayesian analyses that use a noninfor mative prior
for the free parameters. These tend to agree, because with a c onstant prior,
the posterior distribution is proportional to the likeliho od. This correspon-
dence also extends to the nonparametric case, where the nonp arametric
bootstrap approximates a noninformative Bayes analysis; S ection 8.4 has
the details.
We have, however, done some things that are not proper from a B ayesian
point of view. We have used a noninformative (constant) prio r forσ2and
replaced it with the maximum likelihood estimate ˆ σ2in the posterior. A
more standard Bayesian analysis would also put a prior on σ(typically
g(σ)∝1/σ), calculate a joint posterior for µ(x) andσ, and then integrate
outσ, rather than just extract the maximum of the posterior distr ibution
(“MAP” estimate).

8.4 Relationship Between the Bootstrap and Bayesian Infere nce 271
8.4 Relationship Between the Bootstrap and
Bayesian Inference
Consider ﬁrst a very simple example, in which we observe a sin gle obser-
vationzfrom a normal distribution
z∼N(θ,1). (8.29)
To carry out a Bayesian analysis for θ, we need to specify a prior. The
most convenient and common choice would be θ∼N(0,τ) giving posterior
distribution
θ|z∼N/parenleftbiggz
1+1/τ,1
1+1/τ/parenrightbigg
. (8.30)
Now the larger we take τ, the more concentrated the posterior becomes
around the maximum likelihood estimate ˆθ=z. In the limit as τ→∞we
obtain a noninformative (constant) prior, and the posterio r distribution is
θ|z∼N(z,1). (8.31)
This is the same as a parametric bootstrap distribution in wh ich we gen-
erate bootstrap values z∗from the maximum likelihood estimate of the
sampling density N(z,1).
There are three ingredients that make this correspondence w ork:
1. The choice of noninformative prior for θ.
2. The dependence of the log-likelihood ℓ(θ;Z) on the data Zonly
through the maximum likelihood estimate ˆθ. Hence we can write the
log-likelihood as ℓ(θ;ˆθ).
3. The symmetry of the log-likelihood in θandˆθ, that is,ℓ(θ;ˆθ) =
ℓ(ˆθ;θ)+constant.
Properties (2) and (3) essentially only hold for the Gaussia n distribu-
tion. However, they also hold approximately for the multino mial distribu-
tion, leading to a correspondence between the nonparametri c bootstrap
and Bayes inference, which we outline next.
Assumethatwehaveadiscretesamplespacewith Lcategories.Let wjbe
the probability that a sample point falls in category j, and ˆwjthe observed
proportion in category j. Letw= (w1,w2,...,w L),ˆw= (ˆw1,ˆw2,...,ˆwL).
Denote our estimator by S(ˆw); take as a prior distribution for wa sym-
metric Dirichlet distribution with parameter a:
w∼DiL(a1), (8.32)

272 8. Model Inference and Averaging
that is, the prior probability mass function is proportiona l to/producttextL
ℓ=1wa−1
ℓ.
Then the posterior density of wis
w∼DiL(a1+Nˆw), (8.33)
whereNis the sample size. Letting a→0 to obtain a noninformative prior
gives
w∼DiL(Nˆw). (8.34)
Now the bootstrap distribution, obtained by sampling with r eplacement
from the data, can be expressed as sampling the category prop ortions from
a multinomial distribution. Speciﬁcally,
Nˆw∗∼Mult(N,ˆw), (8.35)
where Mult( N,ˆw) denotes a multinomial distribution, having probability
massfunction/parenleftbigN
Nˆw∗
1,...,Nˆw∗
L/parenrightbig/producttextˆwNˆw∗
ℓ
ℓ.Thisdistributionissimilartothepos-
terior distribution above, having the same support, same me an, and nearly
the same covariance matrix. Hence the bootstrap distributi on ofS(ˆw∗) will
closely approximate the posterior distribution of S(w).
In this sense, the bootstrap distribution represents an (ap proximate)
nonparametric, noninformative posterior distribution fo r our parameter.
But this bootstrap distribution is obtained painlessly—wit hout having to
formally specify a prior and without having to sample from th e posterior
distribution. Hence we might think of the bootstrap distrib ution as a “poor
man’s” Bayes posterior. By perturbing the data, the bootstr ap approxi-
mates the Bayesian eﬀect of perturbing the parameters, and i s typically
much simpler to carry out.
8.5 The EM Algorithm
The EM algorithm is a popular tool for simplifying diﬃcult ma ximum
likelihood problems. We ﬁrst describe it in the context of a s imple mixture
model.
8.5.1 Two-Component Mixture Model
In this section we describe a simple mixture model for densit y estimation,
and the associated EM algorithm for carrying out maximum lik elihood
estimation. This has a natural connection to Gibbs sampling methods for
Bayesian inference. Mixture models are discussed and demon strated in sev-
eral other parts of the book, in particular Sections 6.8, 12. 7 and 13.2.3.
The left panel of Figure 8.5 shows a histogram of the 20 ﬁctiti ous data
points in Table 8.1.

8.5 The EM Algorithm 273
0 2 4 60.0 0.2 0.4 0.6 0.8 1.0
y ydensity
0 2 4 60.0 0.2 0.4 0.6 0.8 1.0• •• •••••••
•
•
••••• •• •
FIGURE 8.5. Mixture example. (Left panel:) Histogram of data. (Right pane l:)
Maximum likelihood ﬁt of Gaussian densities (solid red) and resp onsibility (dotted
green) of the left component density for observation y, as a function of y.
TABLE 8.1. Twenty ﬁctitious data points used in the two-component mixt ure
example in Figure 8.5.
-0.39 0.12 0.94 1.67 1.76 2.44 3.72 4.28 4.92 5.53
0.06 0.48 1.01 1.68 1.80 3.25 4.12 4.60 5.28 6.22
We would like to model the density of the data points, and due t o the
apparent bi-modality, a Gaussian distribution would not be appropriate.
There seems to be two separate underlying regimes, so instea d we model
Yas a mixture of two normal distributions:
Y1∼N(µ1,σ2
1),
Y2∼N(µ2,σ2
2), (8.36)
Y= (1−∆)·Y1+∆·Y2,
where ∆∈{0,1}with Pr(∆ = 1) = π. Thisgenerative representation is
explicit: generate a ∆ ∈{0,1}with probability π, and then depending on
the outcome, deliver either Y1orY2. Letφθ(x) denote the normal density
with parameters θ= (µ,σ2). Then the density of Yis
gY(y) = (1−π)φθ1(y)+πφθ2(y). (8.37)
Now suppose we wish to ﬁt this model to the data in Figure 8.5 by maxi-
mum likelihood. The parameters are
θ= (π,θ1,θ2) = (π,µ1,σ2
1,µ2,σ2
2). (8.38)
The log-likelihood based on the Ntraining cases is
ℓ(θ;Z) =N/summationdisplay
i=1log[(1−π)φθ1(yi)+πφθ2(yi)]. (8.39)

274 8. Model Inference and Averaging
Direct maximization of ℓ(θ;Z) is quite diﬃcult numerically, because of
the sum of terms inside the logarithm. There is, however, a si mpler ap-
proach. We consider unobserved latent variables ∆ itaking values 0 or 1 as
in (8.36): if ∆ i= 1 thenYicomes from model 2, otherwise it comes from
model 1. Suppose we knew the values of the ∆ i’s. Then the log-likelihood
would be
ℓ0(θ;Z,∆) =N/summationdisplay
i=1[(1−∆i)logφθ1(yi)+∆ilogφθ2(yi)]
+N/summationdisplay
i=1[(1−∆i)log(1−π)+∆ilogπ],(8.40)
and the maximum likelihood estimates of µ1andσ2
1would be the sample
mean and variance for those data with ∆ i= 0, and similarly those for µ2
andσ2
2would be the sample mean and variance of the data with ∆ i= 1.
The estimate of πwould be the proportion of ∆ i= 1.
Since the values of the ∆ i’s are actually unknown, we proceed in an
iterative fashion, substituting for each ∆ iin (8.40) its expected value
γi(θ) = E(∆ i|θ,Z) = Pr(∆ i= 1|θ,Z), (8.41)
also called the responsibility of model 2 for observation i. We use a proce-
dure called the EM algorithm, given in Algorithm 8.1 for the s pecial case of
Gaussian mixtures. In the expectation step, we do a soft assignment of each
observation to each model: the current estimates of the para meters are used
to assign responsibilities according to the relative densi ty of the training
points under each model. In the maximization step, these responsibilities
are used in weighted maximum-likelihood ﬁts to update the es timates of
the parameters.
A good way to construct initial guesses for ˆ µ1and ˆµ2is simply to choose
two of the yiat random. Both ˆ σ2
1and ˆσ2
2can be set equal to the overall
sample variance/summationtextN
i=1(yi−¯y)2/N. The mixing proportion ˆ πcan be started
at the value 0 .5.
Note that the actual maximizer of the likelihood occurs when we put a
spike of inﬁnite height at any one data point, that is, ˆ µ1=yifor some
iand ˆσ2
1= 0. This gives inﬁnite likelihood, but is not a useful soluti on.
Hence we are actually looking for a good local maximum of the l ikelihood,
one for which ˆ σ2
1,ˆσ2
2>0. To further complicate matters, there can be
more than one local maximum having ˆ σ2
1,ˆσ2
2>0. In our example, we
ran the EM algorithm with a number of diﬀerent initial guesse s for the
parameters, all having ˆ σ2
k>0.5, and chose the run that gave us the highest
maximized likelihood. Figure 8.6 shows the progress of the E M algorithm in
maximizingthelog-likelihood.Table8.2shows ˆ π=/summationtext
iˆγi/N,themaximum
likelihood estimate of the proportion of observations in cl ass 2, at selected
iterations of the EM procedure.

8.5 The EM Algorithm 275
Algorithm 8.1 EM Algorithm for Two-component Gaussian Mixture.
1. Take initial guesses for the parameters ˆ µ1,ˆσ2
1,ˆµ2,ˆσ2
2,ˆπ(see text).
2.Expectation Step : compute the responsibilities
ˆγi=ˆπφˆθ2(yi)
(1−ˆπ)φˆθ1(yi)+ ˆπφˆθ2(yi), i= 1,2,...,N. (8.42)
3.Maximization Step : compute the weighted means and variances:
ˆµ1=/summationtextN
i=1(1−ˆγi)yi/summationtextN
i=1(1−ˆγi),ˆσ2
1=/summationtextN
i=1(1−ˆγi)(yi−ˆµ1)2
/summationtextN
i=1(1−ˆγi),
ˆµ2=/summationtextN
i=1ˆγiyi/summationtextN
i=1ˆγi,ˆσ2
2=/summationtextN
i=1ˆγi(yi−ˆµ2)2
/summationtextN
i=1ˆγi,
and the mixing probability ˆ π=/summationtextN
i=1ˆγi/N.
4. Iterate steps 2 and 3 until convergence.
TABLE 8.2. Selected iterations of the EM algorithm for mixture example.
Iteration ˆ π
1 0.485
5 0.493
10 0.523
15 0.544
20 0.546
The ﬁnal maximum likelihood estimates are
ˆµ1= 4.62, ˆσ2
1= 0.87,
ˆµ2= 1.06, ˆσ2
2= 0.77,
ˆπ= 0.546.
TherightpanelofFigure8.5showstheestimatedGaussianmi xturedensity
fromthisprocedure(solidredcurve),alongwiththerespon sibilities(dotted
green curve). Note that mixtures are also useful for supervi sed learning; in
Section 6.7 we show how the Gaussian mixture model leads to a v ersion of
radial basis functions.

276 8. Model Inference and Averaging
IterationObserved Data Log-likelihood
5 10 15 20-44 -43 -42 -41 -40 -39
ooooooooooooooo o o o o o
FIGURE 8.6. EM algorithm: observed data log-likelihood as a function of the
iteration number.
8.5.2 The EM Algorithm in General
The above procedure is an example of the EM (or Baum–Welch) al gorithm
for maximizing likelihoods in certain classes of problems. These problems
are ones for which maximization of the likelihood is diﬃcult , but made
easier by enlarging the sample with latent (unobserved) dat a. This is called
data augmentation . Here the latent data are the model memberships ∆ i.
In other problems, the latent data are actual data that shoul d have been
observed but are missing.
Algorithm 8.2 gives the general formulation of the EM algori thm. Our
observed data is Z, having log-likelihood ℓ(θ;Z) depending on parameters
θ. The latent or missing data is Zm, so that the complete data is T=
(Z,Zm) with log-likelihood ℓ0(θ;T),ℓ0based on the complete density. In
the mixture problem ( Z,Zm) = (y,∆), andℓ0(θ;T) is given in (8.40).
In our mixture example, E( ℓ0(θ′;T)|Z,ˆθ(j)) is simply (8.40) with the ∆ i
replaced by the responsibilities ˆ γi(ˆθ), and the maximizers in step 3 are just
weighted means and variances.
We now give an explanation of why the EM algorithm works in gen eral.
Since
Pr(Zm|Z,θ′) =Pr(Zm,Z|θ′)
Pr(Z|θ′), (8.44)
we can write
Pr(Z|θ′) =Pr(T|θ′)
Pr(Zm|Z,θ′). (8.45)
Intermsoflog-likelihoods,wehave ℓ(θ′;Z) =ℓ0(θ′;T)−ℓ1(θ′;Zm|Z),where
ℓ1is based on the conditional density Pr( Zm|Z,θ′). Taking conditional
expectations with respect to the distribution of T|Zgoverned by parameter
θgives
ℓ(θ′;Z) = E[ℓ0(θ′;T)|Z,θ]−E[ℓ1(θ′;Zm|Z)|Z,θ]

8.5 The EM Algorithm 277
Algorithm 8.2 The EM Algorithm.
1. Start with initial guesses for the parameters ˆθ(0).
2.Expectation Step : at thejth step, compute
Q(θ′,ˆθ(j)) = E(ℓ0(θ′;T)|Z,ˆθ(j)) (8.43)
as a function of the dummy argument θ′.
3.Maximization Step : determine the new estimate ˆθ(j+1)as the maxi-
mizer ofQ(θ′,ˆθ(j)) overθ′.
4. Iterate steps 2 and 3 until convergence.
≡Q(θ′,θ)−R(θ′,θ). (8.46)
In theMstep, the EM algorithm maximizes Q(θ′,θ) overθ′, rather than
the actual objective function ℓ(θ′;Z). Why does it succeed in maximizing
ℓ(θ′;Z)?Notethat R(θ∗,θ)istheexpectationofalog-likelihoodofadensity
(indexed by θ∗), with respect to the same density indexed by θ, and hence
(by Jensen’s inequality) is maximized as a function of θ∗, whenθ∗=θ(see
Exercise 8.1). So if θ′maximizesQ(θ′,θ), we see that
ℓ(θ′;Z)−ℓ(θ;Z) = [Q(θ′,θ)−Q(θ,θ)]−[R(θ′,θ)−R(θ,θ)]
≥0. (8.47)
Hence the EM iteration never decreases the log-likelihood.
This argument also makes it clear that a full maximization in theM
step is not necessary: we need only to ﬁnd a value ˆθ(j+1)so thatQ(θ′,ˆθ(j))
increases as a function of the ﬁrst argument, that is, Q(ˆθ(j+1),ˆθ(j))>
Q(ˆθ(j),ˆθ(j)).Suchproceduresarecalled GEM (generalized EM) algorithms.
The EM algorithm can also be viewed as a minorization procedu re: see
Exercise 8.7.
8.5.3 EM as a Maximization–Maximization Procedure
Here is a diﬀerent view of the EM procedure, as a joint maximiz ation
algorithm. Consider the function
F(θ′,˜P) = E˜P[ℓ0(θ′;T)]−E˜P[log˜P(Zm)]. (8.48)
Here˜P(Zm) is any distribution over the latent data Zm. In the mixture
example, ˜P(Zm) comprises the set of probabilities γi= Pr(∆ i= 1|θ,Z).
Note thatFevaluated at ˜P(Zm) = Pr(Zm|Z,θ′), is the log-likelihood of

278 8. Model Inference and Averaging
1 2 3 4 50 1 2 3 40.10.3
0.50.7
0.9Model Parameters
Latent Data ParametersEMEM
FIGURE 8.7. Maximization–maximization view of the EM algorithm. Shown
are the contours of the (augmented) observed data log-likeliho odF(θ′,˜P). The
Estep is equivalent to maximizing the log-likelihood over the par ameters of the
latent data distribution. The Mstep maximizes it over the parameters of the
log-likelihood. The red curve corresponds to the observed data log-likelihood, a
proﬁleobtained by maximizing F(θ′,˜P)for each value of θ′.
the observed data, from (8.46)1. The function Fexpands the domain of
the log-likelihood, to facilitate its maximization.
The EM algorithm can be viewed as a joint maximization method forF
overθ′and˜P(Zm), by ﬁxing one argument and maximizing over the other.
The maximizer over ˜P(Zm) for ﬁxedθ′can be shown to be
˜P(Zm) = Pr(Zm|Z,θ′) (8.49)
(Exercise8.2).Thisisthedistributioncomputedbythe Estep,forexample,
(8.42) in the mixture example. In the Mstep, we maximize F(θ′,˜P) overθ′
with˜Pﬁxed:thisisthesameasmaximizingtheﬁrsttermE ˜P[ℓ0(θ′;T)|Z,θ]
since the second term does not involve θ′.
Finally, since F(θ′,˜P) and the observed data log-likelihood agree when
˜P(Zm) = Pr(Zm|Z,θ′), maximization of the former accomplishes maxi-
mization of the latter. Figure 8.7 shows a schematic view of t his process.
This view of the EM algorithm leads to alternative maximizat ion proce-
1(8.46) holds for all θ, including θ=θ′.

8.6 MCMC for Sampling from the Posterior 279
Algorithm 8.3 Gibbs Sampler.
1. Take some initial values U(0)
k,k= 1,2,...,K.
2. Repeat for t= 1,2,...,.:
Fork= 1,2,...,KgenerateU(t)
kfrom
Pr(U(t)
k|U(t)
1,...,U(t)
k−1,U(t−1)
k+1,...,U(t−1)
K).
3. Continue step 2 until the joint distribution of ( U(t)
1,U(t)
2,...,U(t)
K)
does not change.
dures. For example, one does not need to maximize with respec t to all of
the latent data parameters at once, but could instead maximi ze over one
of them at a time, alternating with the Mstep.
8.6 MCMC for Sampling from the Posterior
Having deﬁned a Bayesian model, one would like to draw sample s from
the resulting posterior distribution, in order to make infe rences about the
parameters. Except for simple models, this is often a diﬃcul t computa-
tional problem. In this section we discuss the Markov chain Monte Carlo
(MCMC) approach to posterior sampling. We will see that Gibb s sampling,
an MCMC procedure, is closely related to the EM algorithm: th e main dif-
ference is that it samples from the conditional distributio ns rather than
maximizing over them.
Consider ﬁrst the following abstract problem. We have rando m variables
U1,U2,...,U Kand we wish to draw a sample from their joint distribution.
Supposethisisdiﬃcult todo,butitiseasytosimulate fromt heconditional
distributions Pr( Uj|U1,U2,...,U j−1,Uj+1,...,U K), j= 1,2,...,K. The
Gibbs sampling procedure alternatively simulates from eac h of these distri-
butions and when the process stabilizes, provides a sample f rom the desired
joint distribution. The procedure is deﬁned in Algorithm 8. 3.
Under regularity conditions it can be shown that this proced ure even-
tually stabilizes, and the resulting random variables are i ndeed a sample
from the joint distribution of U1,U2,...,U K. This occurs despite the fact
that the samples ( U(t)
1,U(t)
2,...,U(t)
K) are clearly not independent for dif-
ferentt. More formally, Gibbs sampling produces a Markov chain whos e
stationary distribution is the true joint distribution, an d hence the term
“Markov chain Monte Carlo.” It is not surprising that the tru e joint dis-
tribution is stationary under this process, as the successi ve steps leave the
marginal distributions of the Uk’s unchanged.

280 8. Model Inference and Averaging
Note that we don’t need to know the explicit form of the condit ional
densities,butjustneedtobeabletosamplefromthem.After theprocedure
reaches stationarity, the marginal density of any subset of the variables
can be approximated by a density estimate applied to the samp le values.
However if the explicit form of the conditional density Pr( Uk,|Uℓ,ℓ∝ne}ationslash=k)
is available, a better estimate of say the marginal density o fUkcan be
obtained from (Exercise 8.3):
/hatwiderPrUk(u) =1
(M−m+1)M/summationdisplay
t=mPr(u|U(t)
ℓ,ℓ∝ne}ationslash=k). (8.50)
Here we have averaged over the last M−m+1 members of the sequence,
to allow for an initial “burn-in” period before stationarit y is reached.
NowgettingbacktoBayesianinference,ourgoalistodrawas amplefrom
the joint posterior of the parameters given the data Z. Gibbs sampling will
be helpful if it is easy to sample from the conditional distri bution of each
parameter given the other parameters and Z. An example—the Gaussian
mixture problem—is detailed next.
There is a close connection between Gibbs sampling from a pos terior and
the EM algorithm in exponential family models. The key is to c onsider the
latent data Zmfrom the EM procedure to be another parameter for the
Gibbs sampler. To make this explicit for the Gaussian mixtur e problem,
we take our parameters to be ( θ,Zm). For simplicity we ﬁx the variances
σ2
1,σ2
2and mixing proportion πat their maximum likelihood values so that
the only unknown parameters in θare the means µ1andµ2. The Gibbs
sampler for the mixture problem is given in Algorithm 8.4. We see that
steps 2(a) and 2(b) are the same as the EandMsteps of the EM pro-
cedure, except that we sample rather than maximize. In step 2 (a), rather
than compute the maximum likelihood responsibilities γi= E(∆ i|θ,Z),
the Gibbs sampling procedure simulates the latent data ∆ ifrom the distri-
butions Pr(∆ i|θ,Z). In step 2(b), rather than compute the maximizers of
the posterior Pr( µ1,µ2,∆|Z) we simulate from the conditional distribution
Pr(µ1,µ2|∆,Z).
Figure 8.8 shows 200 iterations of Gibbs sampling, with the m ean param-
etersµ1(lower) and µ2(upper) shown in the left panel, and the proportion
of class 2 observations/summationtext
i∆i/Non the right. Horizontal broken lines have
been drawn at the maximum likelihood estimate values ˆ µ1,ˆµ2and/summationtext
iˆγi/N
in each case. The values seem to stabilize quite quickly, and are distributed
evenly around the maximum likelihood values.
The above mixture model was simpliﬁed, in order to make the cl ear
connection between Gibbs sampling and the EM algorithm. Mor e realisti-
cally, one would put a prior distribution on the variances σ2
1,σ2
2and mixing
proportion π, and include separate Gibbs sampling steps in which we sam-
ple from their posterior distributions, conditional on the other parameters.
One can also incorporate proper (informative) priors for th e mean param-

8.6 MCMC for Sampling from the Posterior 281
Algorithm 8.4 Gibbs sampling for mixtures.
1. Take some initial values θ(0)= (µ(0)
1,µ(0)
2).
2. Repeat for t= 1,2,...,.
(a) Fori= 1,2,...,Ngenerate ∆(t)
i∈{0,1}with Pr(∆(t)
i= 1) =
ˆγi(θ(t)), from equation (8.42).
(b) Set
ˆµ1=/summationtextN
i=1(1−∆(t)
i)·yi/summationtextN
i=1(1−∆(t)
i),
ˆµ2=/summationtextN
i=1∆(t)
i·yi/summationtextN
i=1∆(t)
i,
and generate µ(t)
1∼N(ˆµ1,ˆσ2
1) andµ(t)
2∼N(ˆµ2,ˆσ2
2).
3. Continue step 2 until the joint distribution of ( ∆(t),µ(t)
1,µ(t)
2) doesn’t
change
Gibbs IterationMean Parameters
0 50 100 150 2000 2 4 6 8
Gibbs IterationMixing Proportion
0 50 100 150 2000.3 0.4 0.5 0.6 0.7
FIGURE 8.8. Mixture example. (Left panel:) 200values of the two mean param-
eters from Gibbs sampling; horizontal lines are drawn at the max imum likelihood
estimates ˆµ1,ˆµ2. (Right panel:) Proportion of values with ∆i= 1, for each of the
200Gibbs sampling iterations; a horizontal line is drawn at/summationtext
iˆγi/N.

282 8. Model Inference and Averaging
eters. These priors must not be improper as this will lead to a degenerate
posterior, with all the mixing weight on one component.
Gibbs sampling is just one of a number of recently developed p rocedures
for sampling from posterior distributions. It uses conditi onal sampling of
each parameter given the rest, and is useful when the structu re of the prob-
lem makes this sampling easy to carry out. Other methods do no t require
such structure, for example the Metropolis–Hastings algorithm. These and
other computational Bayesian methods have been applied to s ophisticated
learning algorithms such as Gaussian process models and neu ral networks.
Details may be found in the references given in the Bibliogra phic Notes at
the end of this chapter.
8.7 Bagging
Earlier we introduced the bootstrap as a way of assessing the accuracy of a
parameter estimate or a prediction. Here we show how to use th e bootstrap
to improve the estimate or prediction itself. In Section 8.4 we investigated
the relationship between the bootstrap and Bayes approache s, and found
that the bootstrap mean is approximately a posterior averag e. Bagging
further exploits this connection.
Consider ﬁrst the regression problem. Suppose we ﬁt a model t o our
training data Z={(x1,y1),(x2,y2),...,(xN,yN)}, obtaining the predic-
tionˆf(x) at inputx. Bootstrap aggregation or baggingaverages this predic-
tion over a collection of bootstrap samples, thereby reduci ng its variance.
For each bootstrap sample Z∗b,b= 1,2,...,B, we ﬁt our model, giving
prediction ˆf∗b(x). The bagging estimate is deﬁned by
ˆfbag(x) =1
BB/summationdisplay
b=1ˆf∗b(x). (8.51)
Denote by ˆPthe empirical distribution putting equal probability 1 /Non
each of the data points ( xi,yi). In fact the “true” bagging estimate is
deﬁned by E ˆPˆf∗(x), where Z∗={(x∗
1,y∗
1),(x∗
2,y∗
2),...,(x∗
N,y∗
N)}and each
(x∗
i,y∗
i)∼ˆP. Expression (8.51) is a Monte Carlo estimate of the true
bagging estimate, approaching it as B→∞.
The bagged estimate (8.51) will diﬀer from the original esti mateˆf(x)
only when the latter is a nonlinear or adaptive function of th e data. For
example, to bag the B-spline smooth of Section 8.2.1, we average the curves
in the bottom left panel of Figure 8.2 at each value of x. TheB-spline
smoother is linear in the data if we ﬁx the inputs; hence if we s ample using
the parametric bootstrap in equation (8.6), then ˆfbag(x)→ˆf(x) asB→∞
(Exercise 8.4). Hence bagging just reproduces the original smooth in the

8.7 Bagging 283
top left panel of Figure 8.2. The same is approximately true i f we were to
bag using the nonparametric bootstrap.
A more interesting example is a regression tree, where ˆf(x) denotes the
tree’s prediction at input vector x(regression trees are described in Chap-
ter 9). Each bootstrap tree will typically involve diﬀerent features than the
original, and might have a diﬀerent number of terminal nodes . The bagged
estimate is the average prediction at xfrom these Btrees.
Now suppose our tree produces a classiﬁer ˆG(x) for aK-class response.
Here it is useful to consider an underlying indicator-vecto r function ˆf(x),
with value a single one and K−1 zeroes, such that ˆG(x) = argmax kˆf(x).
Then the bagged estimate ˆfbag(x) (8.51) is a K-vector [p1(x),p2(x),...,
pK(x)], withpk(x) equal to the proportion of trees predicting class katx.
The bagged classiﬁer selects the class with the most “votes” from theB
trees,ˆGbag(x) = argmax kˆfbag(x).
Often we require the class-probability estimates at x, rather than the
classiﬁcations themselves. It is tempting to treat the voti ng proportions
pk(x) as estimates of these probabilities. A simple two-class ex ample shows
that they fail in this regard. Suppose the true probability o f class 1 at xis
0.75, and each of the bagged classiﬁers accurately predict a 1. Thenp1(x) =
1, which is incorrect. For many classiﬁers ˆG(x), however, there is already
an underlying function ˆf(x) that estimates the class probabilities at x(for
trees, the class proportions in the terminal node). An alter native bagging
strategy is to average these instead, rather than the vote in dicator vectors.
Not only does this produce improved estimates of the class pr obabilities,
butitalsotendstoproducebaggedclassiﬁerswithlowervar iance,especially
for smallB(see Figure 8.10 in the next example).
8.7.1 Example: Trees with Simulated Data
We generated a sample of size N= 30, with two classes and p= 5 features,
each having a standard Gaussian distribution with pairwise correlation
0.95.Theresponse YwasgeneratedaccordingtoPr( Y= 1|x1≤0.5) = 0.2,
Pr(Y= 1|x1>0.5) = 0.8. The Bayes error is 0 .2. A test sample of size 2000
was also generated from the same population. We ﬁt classiﬁca tion trees to
the training sample and to each of 200 bootstrap samples (cla ssiﬁcation
trees are described in Chapter 9). No pruning was used. Figur e 8.9 shows
the original tree and eleven bootstrap trees. Notice how the trees are all
diﬀerent, with diﬀerent splitting features and cutpoints. The test error for
the original tree and the bagged tree is shown in Figure 8.10. In this ex-
ample the trees have high variance due to the correlation in t he predictors.
Bagging succeeds in smoothing out this variance and hence re ducing the
test error.
Bagging can dramatically reduce the variance of unstable pr ocedures
like trees, leading to improved prediction. A simple argume nt shows why

284 8. Model Inference and Averaging
|x.1 < 0.395
010
101
10Original Tree
|x.1 < 0.555
0
1001b = 1
|x.2 < 0.205
0101
01b = 2
|x.2 < 0.285
1 1010b = 3
|x.3 < 0.985
0
1
011 1b = 4
|x.4 < −1.36
0
1
1010
10b = 5
|x.1 < 0.395
1 10 01b = 6
|x.1 < 0.395
01011b = 7
|x.3 < 0.985
010 010b = 8
|x.1 < 0.395
0
1
0110b = 9
|x.1 < 0.555
101
01b = 10
|x.1 < 0.555
0 101b = 11
FIGURE 8.9. Bagging trees on simulated dataset. The top left panel shows th e
original tree. Eleven trees grown on bootstrap samples are sho wn. For each tree,
the top split is annotated.

8.7 Bagging 285
0 50 100 150 2000.20 0.25 0.30 0.35 0.40 0.45 0.50
Number of Bootstrap SamplesTest ErrorBagged TreesOriginal Tree
BayesConsensus
Probability
FIGURE 8.10. Error curves for the bagging example of Figure 8.9. Shown is
the test error of the original tree and bagged trees as a funct ion of the number of
bootstrap samples. The orange points correspond to the conse nsus vote, while the
green points average the probabilities.
bagging helps under squared-error loss, in short because av eraging reduces
variance and leaves bias unchanged.
Assume our training observations ( xi,yi), i= 1,...,Nare indepen-
dently drawn from a distribution P, and consider the ideal aggregate es-
timatorfag(x) = EPˆf∗(x). Herexis ﬁxed and the bootstrap dataset Z∗
consists of observations x∗
i,y∗
i,i= 1,2,...,Nsampled fromP. Note that
fag(x) is a bagging estimate, drawing bootstrap samples from the a ctual
populationPrather than the data. It is not an estimate that we can use
in practice, but is convenient for analysis. We can write
EP[Y−ˆf∗(x)]2= EP[Y−fag(x)+fag(x)−ˆf∗(x)]2
= EP[Y−fag(x)]2+EP[ˆf∗(x)−fag(x)]2
≥EP[Y−fag(x)]2. (8.52)
The extra error on the right-hand side comes from the varianc e ofˆf∗(x)
around its mean fag(x). Therefore true population aggregation never in-
creases mean squared error. This suggests that bagging—draw ing samples
from the training data— will often decrease mean-squared er ror.
The above argument does not hold for classiﬁcation under 0-1 loss, be-
cause of the nonadditivity of bias and variance. In that sett ing, bagging a

286 8. Model Inference and Averaging
good classiﬁer can make it better, but bagging a bad classiﬁe r can make it
worse. Here is a simple example, using a randomized rule. Sup poseY= 1
for allx, and the classiﬁer ˆG(x) predictsY= 1 (for all x) with proba-
bility 0.4 and predicts Y= 0 (for all x) with probability 0.6. Then the
misclassiﬁcation error of ˆG(x) is 0.6 but that of the bagged classiﬁer is 1.0.
For classiﬁcation we can understand the bagging eﬀect in ter ms of a
consensus of independent weak learners (Dietterich, 2000a). Let the Bayes
optimal decision at xbeG(x) = 1 in a two-class example. Suppose each
of the weak learners G∗
bhave an error-rate eb=e <0.5, and letS1(x) =/summationtextB
b=1I(G∗
b(x) = 1) be the consensus vote for class 1. Since the weak learn-
ers are assumed to be independent, S1(x)∼Bin(B,1−e), and Pr(S1>
B/2)→1 asBgets large. This concept has been popularized outside of
statistics as the “Wisdom of Crowds” (Surowiecki, 2004) — th e collective
knowledge of a diverse and independent body of people typica lly exceeds
the knowledge of any single individual, and can be harnessed by voting.
Of course, the main caveat here is “independent,” and bagged trees are
not. Figure 8.11 illustrates the power of a consensus vote in a simulated
example, where only 30% of the voters have some knowledge.
InChapter15weseehowrandomforestsimproveonbaggingbyr educing
the correlation between the sampled trees.
Note that when we bag a model, any simple structure in the mode l is
lost. As an example, a bagged tree is no longer a tree. For inte rpretation
of the model this is clearly a drawback. More stable procedur es like near-
est neighbors are typically not aﬀected much by bagging. Unf ortunately,
the unstable models most helped by bagging are unstable beca use of the
emphasis on interpretability, and this is lost in the baggin g process.
Figure 8.12 shows an example where bagging doesn’t help. The 100 data
points shown have two features and two classes, separated by the gray
linear boundary x1+x2= 1. We choose as our classiﬁer ˆG(x) a single
axis-oriented split, choosing the split along either x1orx2that produces
the largest decrease in training misclassiﬁcation error.
The decision boundary obtained from bagging the 0-1 decisio n rule over
B= 50 bootstrap samples is shown by the blue curve in the left pa nel.
It does a poor job of capturing the true boundary. The single s plit rule,
derived from the training data, splits near 0 (the middle of t he range of x1
orx2), and hence has little contribution away from the center. Av eraging
the probabilities rather than the classiﬁcations does not h elp here. Bagging
estimates the expected class probabilities from the single split rule, that is,
averaged over many replications. Note that the expected cla ss probabilities
computed by bagging cannot be realized on any single replica tion, in the
same way that a woman cannot have 2.4 children. In this sense, bagging
increases somewhat the space of models of the individual bas e classiﬁer.
However, it doesn’t help in this and many other examples wher e a greater
enlargement of the model class is needed. “Boosting” is a way of doing this

8.7 Bagging 2870 2 4 6 8 10
P −  Probability of Informed Person Being CorrectExpected Correct out of 10Wisdom of Crowds
Consensus
Individual
0.25 0.50 0.75 1.00
FIGURE 8.11. Simulated academy awards voting. 50members vote in 10 cat-
egories, each with 4nominations. For any category, only 15voters have some
knowledge, represented by their probability of selecting the “ correct” candidate in
that category (so P= 0.25means they have no knowledge). For each category, the
15experts are chosen at random from the 50. Results show the expected correct
(based on 50simulations) for the consensus, as well as for the individuals . The
error bars indicate one standard deviation. We see, for exam ple, that if the 15
informed for a category have a 50% chance of selecting the correct candidate, the
consensus doubles the expected performance of an individual.

288 8. Model Inference and Averaging
•••
••••
•••
••
••
•
••••
•••
•••
•••
•
••
•••
•
•• •• •••
•••
••
•
•
••
•••
••
•
•
••••
•••
••
•
••
••
•••
••
••• •••
••
•••
•••
•
••••
•
• ••
••
••
••• •••
••••
••
••
•••
••
••
•
••
•
•
••
•
••
••
••
•
••
••
•••
••
•••
••
•••••
•
••
•••••
••
•••
•••••
••••
••
•••
•••
••
•
•
••
•••
•
•Bagged Decision Rule
•••
••••
•••
••
••
•
••••
•••
•••
•••
•
••
•••
•
•• •• •••
•••
••
•
•
••
•••
••
•
•
••••
•••
••
•
••
••
•••
••
••• •••
••
•••
•••
•
••••
•
• ••
••
••
••• •••
••••
••
••
•••
••
••
•
••
•
•
••
•
••
••
••
•
••
••
•••
••
•••
••
•••••
•
••
•••••
••
•••
•••••
••••
••
•••
•••
••
•
•
••
•••
•
•Boosted Decision Rule
FIGURE 8.12. Data with two features and two classes, separated by a linear
boundary. (Left panel:) Decision boundary estimated from ba gging the decision
rule from a single split, axis-oriented classiﬁer. (Right panel: ) Decision boundary
from boosting the decision rule of the same classiﬁer. The test error rates are
0.166, and0.065, respectively. Boosting is described in Chapter 10.
and is described in Chapter 10. The decision boundary in the r ight panel is
the result of the boosting procedure, and it roughly capture s the diagonal
boundary.
8.8 Model Averaging and Stacking
In Section 8.4 we viewed bootstrap values of an estimator as a pproximate
posterior values of a corresponding parameter, from a kind o f nonparamet-
ric Bayesian analysis. Viewed in this way, the bagged estima te (8.51) is
an approximate posterior Bayesian mean. In contrast, the tr aining sample
estimate ˆf(x) corresponds to the mode of the posterior. Since the posteri or
mean (not mode) minimizes squared-error loss, it is not surp rising that
bagging can often reduce mean squared-error.
Here we discuss Bayesian model averaging more generally. We have a
set of candidate models Mm, m= 1,...,Mfor our training set Z. These
models may be of the same type with diﬀerent parameter values (e.g.,
subsets in linear regression), or diﬀerent models for the sa me task (e.g.,
neural networks and regression trees).
Supposeζis some quantity of interest, for example, a prediction f(x) at
some ﬁxed feature value x. The posterior distribution of ζis
Pr(ζ|Z) =M/summationdisplay
m=1Pr(ζ|Mm,Z)Pr(Mm|Z), (8.53)

8.8 Model Averaging and Stacking 289
with posterior mean
E(ζ|Z) =M/summationdisplay
m=1E(ζ|Mm,Z)Pr(Mm|Z). (8.54)
ThisBayesianpredictionisaweightedaverageoftheindivi dualpredictions,
with weights proportional to the posterior probability of e ach model.
This formulation leads to a number of diﬀerent model-averag ing strate-
gies.Committee methods take a simple unweighted average of the predic-
tions from each model, essentially giving equal probabilit y to each model.
More ambitiously, the development in Section 7.7 shows the B IC criterion
can be used to estimate posterior model probabilities. This is applicable
in cases where the diﬀerent models arise from the same parame tric model,
with diﬀerent parameter values. The BIC gives weight to each model de-
pending on how well it ﬁts and how many parameters it uses. One can also
carry out the Bayesian recipe in full. If each model Mmhas parameters
θm, we write
Pr(Mm|Z)∝Pr(Mm)·Pr(Z|Mm)
∝Pr(Mm)·/integraldisplay
Pr(Z|θm,Mm)Pr(θm|Mm)dθm.
(8.55)
In principle one can specify priors Pr( θm|Mm) and numerically com-
pute the posterior probabilities from (8.55), to be used as m odel-averaging
weights. However, we have seen no real evidence that this is w orth all of
the eﬀort, relative to the much simpler BIC approximation.
How can we approach model averaging from a frequentist viewp oint?
Given predictions ˆf1(x),ˆf2(x),...,ˆfM(x), under squared-error loss, we can
seek the weights w= (w1,w2,...,w M) such that
ˆw= argmin
wEP/bracketleftig
Y−M/summationdisplay
m=1wmˆfm(x)/bracketrightig2
. (8.56)
Heretheinputvalue xisﬁxedandthe Nobservationsinthedataset Z(and
the targetY) are distributed according to P. The solution is the population
linear regression of YonˆF(x)T≡[ˆf1(x),ˆf2(x),...,ˆfM(x)]:
ˆw= EP[ˆF(x)ˆF(x)T]−1EP[ˆF(x)Y]. (8.57)
Now the full regression has smaller error than any single mod el
EP/bracketleftigg
Y−M/summationdisplay
m=1ˆwmˆfm(x)/bracketrightigg2
≤EP/bracketleftig
Y−ˆfm(x)/bracketrightig2
∀m (8.58)
so combining models never makes things worse, at the populat ion level.

290 8. Model Inference and Averaging
Of course the population linear regression (8.57) is not ava ilable, and it
is natural to replace it with the linear regression over the t raining set. But
there are simple examples where this does not work well. For e xample, if
ˆfm(x), m= 1,2,...,Mrepresent the prediction from the best subset of
inputs of size mamongMtotal inputs, then linear regression would put all
of the weight on the largest model, that is, ˆ wM= 1,ˆwm= 0, m<M . The
problem is that we have not put each of the models on the same fo oting
by taking into account their complexity (the number of input smin this
example).
Stacked generalization , orstacking, is a way of doing this. Let ˆf−i
m(x)
be the prediction at x, using model m, applied to the dataset with the
ith training observation removed. The stacking estimate of t he weights is
obtained from the least squares linear regression of yionˆf−i
m(xi), m=
1,2,...,M. In detail the stacking weights are given by
ˆwst= argmin
wN/summationdisplay
i=1/bracketleftigg
yi−M/summationdisplay
m=1wmˆf−i
m(xi)/bracketrightigg2
. (8.59)
The ﬁnal prediction is/summationtext
mˆwst
mˆfm(x). By using the cross-validated pre-
dictions ˆf−i
m(x), stacking avoids giving unfairly high weight to models wit h
higher complexity. Better results can be obtained by restri cting the weights
to be nonnegative, and to sum to 1. This seems like a reasonabl e restriction
if we interpret the weights as posterior model probabilitie s as in equation
(8.54), and it leads to a tractable quadratic programming pr oblem.
There is a close connection between stacking and model selec tion via
leave-one-outcross-validation(Section7.10).Ifwerest ricttheminimization
in (8.59) to weight vectors wthat have one unit weight and the rest zero,
this leads to a model choice ˆ mwith smallest leave-one-out cross-validation
error. Rather than choose a single model, stacking combines them with
estimated optimal weights. This will often lead to better pr ediction, but
less interpretability than the choice of only one of the Mmodels.
The stacking idea is actually more general than described ab ove. One
can use any learning method, not just linear regression, to c ombine the
models as in (8.59); the weights could also depend on the inpu t location
x. In this way, learning methods are “stacked” on top of one ano ther, to
improve prediction performance.
8.9 Stochastic Search: Bumping
The ﬁnal method described in this chapter does not involve av eraging or
combining models, but rather is a technique for ﬁnding a bett er single
model.Bumping uses bootstrap sampling to move randomly through model
space. For problems where ﬁtting method ﬁnds many local mini ma, bump-
ing can help the method to avoid getting stuck in poor solutio ns.

8.9 Stochastic Search: Bumping 291
Regular 4-Node Tree
••
•••
••
•
•••
• ••
••
••
••
•
••
•••
•
••
•
•
•••
••
•
••
••
••
••
••••••
••••
•
•
• ••
•••
•
••
••
•••
••
• •••
••
•
••
•••
••
•••
•
••••
•
•••
•
•
•••
••
•
•••••
•••
••
•
•
••
•
••
••••• •
••
•
•••
••
••
••
•••••
•
••••
•
••
•
••
•
••
••
••
••
••••
•
•• •••
••
•
•••
••
••
•
•••
•••
••
•••
••
••
•••
••
•
••
••
•••
••
••
••
••
•
•
••
••
•••
••
•••
•• •
•••
••
••
•
•••
••
•
•••
••
••
•••
••••
••
•
••
•
•••
•
•••
•••
•••
••••
••
•
••
•
••••••
•
••
••
••
•
•
••
••••
•••
••
•••
•••••
•••
••
••
••
•
••
•
••
•
••
••
•••
•••
••••
•••••
•
• •• ••
•
•••
•
••
•••
•
•
•••
••
••
•
•
••
••
••Bumped 4-Node Tree
••
•••
••
•
•••
• ••
••
••
••
•
••
•••
•
••
•
•
•••
••
•
••
••
••
••
••••••
••••
•
•
• ••
•••
•
••
••
•••
••
• •••
••
•
••
•••
••
•••
•
••••
•
•••
•
•
•••
••
•
•••••
•••
••
•
•
••
•
••
••••• •
••
•
•••
••
••
••
•••••
•
••••
•
••
•
••
•
••
••
••
••
••••
•
•• •••
••
•
•••
••
••
•
•••
•••
••
•••
••
••
•••
••
•
••
••
•••
••
••
••
••
•
•
••
••
•••
••
•••
•• •
•••
••
••
•
•••
••
•
•••
••
••
•••
••••
••
•
••
•
•••
•
•••
•••
•••
••••
••
•
••
•
••••••
•
••
••
••
•
•
••
••••
•••
••
•••
•••••
•••
••
••
••
•
••
•
••
•
••
••
•••
•••
••••
•••••
•
• •• ••
•
•••
•
••
•••
•
•
•••
••
••
•
•
••
••
••
FIGURE 8.13. Data with two features and two classes (blue and orange), dis-
playing a pure interaction. The left panel shows the partition found by three splits
of a standard, greedy, tree-growing algorithm. The vertical grey line near the left
edge is the ﬁrst split, and the broken lines are the two subseque nt splits. The al-
gorithm has no idea where to make a good initial split, and make s a poor choice.
The right panel shows the near-optimal splits found by bumpin g the tree-growing
algorithm 20times.
As in bagging, we draw bootstrap samples and ﬁt a model to each . But
rather than average the predictions, we choose the model est imated from a
bootstrap sample that best ﬁts the training data. In detail, we draw boot-
strap samples Z∗1,...,Z∗Band ﬁt our model to each, giving predictions
ˆf∗b(x), b= 1,2,...,Bat input point x. We then choose the model that
produces the smallest prediction error, averaged over the original training
set. For squared error, for example, we choose the model obtaine d from
bootstrap sample ˆb, where
ˆb= argmin
bN/summationdisplay
i=1[yi−ˆf∗b(xi)]2. (8.60)
The corresponding model predictions are ˆf∗ˆb(x). By convention we also
include the original training sample in the set of bootstrap samples, so that
the method is free to pick the original model if it has the lowe st training
error.
By perturbing the data, bumping tries to move the ﬁtting proc edure
around to good areas of model space. For example, if a few data points are
causing the procedure to ﬁnd a poor solution, any bootstrap s ample that
omits those data points should procedure a better solution.
For another example, consider the classiﬁcation data in Fig ure 8.13, the
notorious exclusive or (XOR) problem. There are two classes (blue and
orange) and two input features, with the features exhibitin g a pure inter-

292 8. Model Inference and Averaging
action. By splitting the data at x1= 0 and then splitting each resulting
strata atx2= 0, (or vice versa) a tree-based classiﬁer could achieve per -
fect discrimination. However, the greedy, short-sighted C ART algorithm
(Section 9.2) tries to ﬁnd the best split on either feature, a nd then splits
the resulting strata. Because of the balanced nature of the d ata, all initial
splits onx1orx2appear to be useless, and the procedure essentially gener-
ates a random split at the top level. The actual split found fo r these data is
shownintheleftpanelofFigure8.13.Bybootstrapsampling fromthedata,
bumping breaks the balance in the classes, and with a reasona ble number
of bootstrap samples (here 20), it will by chance produce at l east one tree
with initial split near either x1= 0 orx2= 0. Using just 20 bootstrap
samples, bumping found the near optimal splits shown in the r ight panel
of Figure 8.13. This shortcoming of the greedy tree-growing algorithm is
exacerbated if we add a number of noise features that are inde pendent of
the class label. Then the tree-growing algorithm cannot dis tinguishx1or
x2from the others, and gets seriously lost.
Since bumping compares diﬀerent models on the training data , one must
ensure that the models have roughly the same complexity. In t he case of
trees, this would mean growing trees with the same number of t erminal
nodes on each bootstrap sample. Bumping can also help in prob lems where
it is diﬃcult to optimize the ﬁtting criterion, perhaps beca use of a lack of
smoothness. The trick is to optimize a diﬀerent, more conven ient criterion
over the bootstrap samples, and then choose the model produc ing the best
results for the desired criterion on the training sample.
Bibliographic Notes
There are many books on classical statistical inference: Co x and Hink-
ley (1974) and Silvey (1975) give nontechnical accounts. Th e bootstrap
is due to Efron (1979) and is described more fully in Efron and Tibshi-
rani (1993) and Hall (1992). A good modern book on Bayesian in ference
is Gelman et al. (1995). A lucid account of the application of Bayesian
methods to neural networks is given in Neal (1996). The stati stical appli-
cation of Gibbs sampling is due to Geman and Geman (1984), and Gelfand
and Smith (1990), with related work by Tanner and Wong (1987) . Markov
chain Monte Carlo methods, including Gibbs sampling and the Metropolis–
Hastings algorithm, are discussed in Spiegelhalter et al. ( 1996). The EM
algorithm is due to Dempster et al. (1977); as the discussant s in that pa-
per make clear, there was much related, earlier work. The vie w of EM as
a joint maximization scheme for a penalized complete-data l og-likelihood
was elucidated by Neal and Hinton (1998); they credit Csisza r and Tusn´ ady
(1984) and Hathaway (1986) as having noticed this connectio n earlier. Bag-
ging was proposed by Breiman (1996a). Stacking is due to Wolp ert (1992);

Exercises 293
Breiman (1996b) contains an accessible discussion for stat isticians. Leblanc
and Tibshirani (1996) describe variations on stacking base d on the boot-
strap. Model averaging in the Bayesian framework has been re cently advo-
cated by Madigan and Raftery (1994). Bumping was proposed by Tibshi-
rani and Knight (1999).
Exercises
Ex. 8.1Letr(y) andq(y) be probability density functions. Jensen’s in-
equality states that for a random variable Xand a convex function φ(x),
E[φ(X)]≥φ[E(X)]. Use Jensen’s inequality to show that
Eqlog[r(Y)/q(Y)] (8.61)
is maximized as a function of r(y) whenr(y) =q(y). Hence show that
R(θ,θ)≥R(θ′,θ) as stated below equation (8.46).
Ex. 8.2Consider the maximization of the log-likelihood (8.48), ov er dis-
tributions ˜P(Zm) such that ˜P(Zm)≥0 and/summationtext
Zm˜P(Zm) = 1. Use La-
grange multipliers to show that the solution is the conditio nal distribution
˜P(Zm) = Pr(Zm|Z,θ′), as in (8.49).
Ex. 8.3Justify the estimate (8.50), using the relationship
Pr(A) =/integraldisplay
Pr(A|B)d(Pr(B)).
Ex. 8.4Consider the bagging method of Section 8.7. Let our estimate ˆf(x)
be theB-spline smoother ˆ µ(x) of Section 8.2.1. Consider the parametric
bootstrap of equation (8.6), applied to this estimator. Sho w that if we bag
ˆf(x), using the parametric bootstrap to generate the bootstrap samples,
the bagging estimate ˆfbag(x) converges to the original estimate ˆf(x) as
B→∞.
Ex. 8.5Suggest generalizations of each of the loss functions in Fig ure 10.4
to more than two classes, and design an appropriate plot to co mpare them.
Ex. 8.6Consider the bone mineral density data of Figure 5.6.
(a) Fit a cubic smooth spline to the relative change in spinal BMD, as a
function of age. Use cross-validation to estimate the optim al amount
of smoothing. Construct pointwise 90% conﬁdence bands for t he un-
derlying function.
(b) Compute the posterior mean and covariance for the true fu nction via
(8.28), and compare the posterior bands to those obtained in (a).

294 8. Model Inference and Averaging
(c)Compute100bootstrapreplicates oftheﬁttedcurves,as inthebottom
left panel of Figure 8.2. Compare the results to those obtain ed in (a)
and (b).
Ex. 8.7EM as a minorization algorithm (Hunter and Lange, 2004; Wu and
Lange, 2007). A function g(x,y) to said to minorize a functionf(x) if
g(x,y)≤f(x), g(x,x) =f(x) (8.62)
for allx,yin the domain. This is useful for maximizing f(x) since it is easy
to show that f(x) is non-decreasing under the update
xs+1= argmaxxg(x,xs) (8.63)
There are analogous deﬁnitions for majorization , for minimizing a function
f(x).Theresultingalgorithmsareknownas MMalgorithms,for“Minorize-
Maximize” or “Majorize-Minimize.”
Show that the EM algorithm (Section 8.5.2) is an example of an MM al-
gorithm, using Q(θ′,θ)+logPr( Z|θ)−Q(θ,θ) to minorize the observed data
log-likelihood ℓ(θ′;Z). (Note that only the ﬁrst term involves the relevant
parameterθ′).

This is page 295
Printer: Opaque this
9
Additive Models, Trees, and Related
Methods
In this chapter we begin our discussion of some speciﬁc metho ds for super-
vised learning. These techniques each assume a (diﬀerent) s tructured form
for the unknown regression function, and by doing so they ﬁne sse the curse
of dimensionality. Of course, they pay the possible price of misspecifying
the model, and so in each case there is a tradeoﬀ that has to be m ade. They
take oﬀ where Chapters 3–6 left oﬀ. We describe ﬁve related te chniques:
generalized additive models, trees, multivariate adaptiv e regression splines,
the patient rule induction method, and hierarchical mixtur es of experts.
9.1 Generalized Additive Models
Regression models play an important role in many data analys es, providing
prediction and classiﬁcation rules, and data analytic tool s for understand-
ing the importance of diﬀerent inputs.
Although attractively simple, the traditional linear mode l often fails in
these situations: in real life, eﬀects are often not linear. In earlier chapters
we described techniques that used predeﬁned basis function s to achieve
nonlinearities. This section describes more automatic ﬂex ible statistical
methods that may be used to identify and characterize nonlin ear regression
eﬀects. These methods are called “generalized additive mod els.”
In the regression setting, a generalized additive model has the form
E(Y|X1,X2,...,X p) =α+f1(X1)+f2(X2)+···+fp(Xp).(9.1)

296 9. Additive Models, Trees, and Related Methods
As usualX1,X2,...,X prepresentpredictors and Yis the outcome; the fj’s
are unspeciﬁed smooth (“nonparametric”) functions. If we w ere to model
each function using an expansion of basis functions (as in Ch apter 5), the
resulting model could then be ﬁt by simple least squares. Our approach
here is diﬀerent: we ﬁt each function using a scatterplot smo other (e.g., a
cubic smoothing spline or kernel smoother), and provide an a lgorithm for
simultaneously estimating all pfunctions (Section 9.1.1).
For two-class classiﬁcation, recall the logistic regressi on model for binary
data discussed in Section 4.4. We relate the mean of the binar y response
µ(X) = Pr(Y= 1|X) to the predictors via a linear regression model and
thelogitlink function:
log/parenleftbiggµ(X)
1−µ(X)/parenrightbigg
=α+β1X1+···+βpXp. (9.2)
Theadditivelogisticregressionmodelreplaceseachlineartermbyamor e
general functional form
log/parenleftbiggµ(X)
1−µ(X)/parenrightbigg
=α+f1(X1)+···+fp(Xp), (9.3)
where again each fjis an unspeciﬁed smooth function. While the non-
parametric form for the functions fjmakes the model more ﬂexible, the
additivity is retained and allows us to interpret the model i n much the
same way as before. The additive logistic regression model i s an example
of a generalized additive model. In general, the conditiona l meanµ(X) of
a response Yis related to an additive function of the predictors via a link
functiong:
g[µ(X)] =α+f1(X1)+···+fp(Xp). (9.4)
Examples of classical link functions are the following:
•g(µ) =µis the identity link, used for linear and additive models for
Gaussian response data.
•g(µ) = logit(µ)asabove,or g(µ) = probit(µ),theprobitlinkfunction,
for modeling binomial probabilities. The probit function i s the inverse
Gaussian cumulative distribution function: probit( µ) = Φ−1(µ).
•g(µ) = log(µ) for log-linear or log-additive models for Poisson count
data.
All three of these arise from exponential family sampling mo dels, which
in addition include the gamma and negative-binomial distri butions. These
families generate the well-known class of generalized line ar models, which
are all extended in the same way to generalized additive mode ls.
The functions fjare estimated in a ﬂexible manner, using an algorithm
whose basic building block is a scatterplot smoother. The es timated func-
tionˆfjcan then reveal possible nonlinearities in the eﬀect of Xj. Not all

9.1 Generalized Additive Models 297
of the functions fjneed to be nonlinear. We can easily mix in linear and
other parametric forms with the nonlinear terms, a necessit y when some of
the inputs are qualitative variables (factors). The nonlin ear terms are not
restricted to main eﬀects either; we can have nonlinear comp onents in two
or more variables, or separate curves in Xjfor each level of the factor Xk.
Thus each of the following would qualify:
•g(µ) =XTβ+αk+f(Z)—asemiparametric model, where Xis a
vector of predictors to be modeled linearly, αkthe eﬀect for the kth
level of a qualitative input V, and the eﬀect of predictor Zis modeled
nonparametrically.
•g(µ) =f(X) +gk(Z)—againkindexes the levels of a qualitative
inputV, and thus creates an interaction term g(V,Z) =gk(Z) for
the eﬀect of VandZ.
•g(µ) =f(X)+g(Z,W) wheregis a nonparametric function in two
features.
Additive models can replace linear models in a wide variety o f settings,
for example an additive decomposition of time series,
Yt=St+Tt+εt, (9.5)
whereStis a seasonal component, Ttis a trend and εis an error term.
9.1.1 Fitting Additive Models
In this section we describe a modular algorithm for ﬁtting ad ditive models
and their generalizations. The building block is the scatte rplot smoother
for ﬁtting nonlinear eﬀects in a ﬂexible way. For concretene ss we use as our
scatterplot smoother the cubic smoothing spline described in Chapter 5.
The additive model has the form
Y=α+p/summationdisplay
j=1fj(Xj)+ε, (9.6)
where the error term εhas mean zero. Given observations xi,yi, a criterion
like the penalized sum of squares (5.9) of Section 5.4 can be s peciﬁed for
this problem,
PRSS(α,f1,f2,...,f p) =N/summationdisplay
i=1/parenleftigg
yi−α−p/summationdisplay
j=1fj(xij)/parenrightigg2
+p/summationdisplay
j=1λj/integraldisplay
f′′
j(tj)2dtj,
(9.7)
wheretheλj≥0aretuningparameters.Itcanbeshownthattheminimizer
of (9.7) is an additive cubic spline model; each of the functi onsfjis a

298 9. Additive Models, Trees, and Related Methods
Algorithm 9.1 The Backﬁtting Algorithm for Additive Models.
1. Initialize: ˆ α=1
N/summationtextN
1yi,ˆfj≡0,∀i,j.
2. Cycle:j= 1,2,...,p,..., 1,2,...,p,...,
ˆfj← Sj/bracketleftigg
{yi−ˆα−/summationdisplay
k/ne}ationslash=jˆfk(xik)}N
1/bracketrightigg
,
ˆfj←ˆfj−1
NN/summationdisplay
i=1ˆfj(xij).
until the functions ˆfjchange less than a prespeciﬁed threshold.
cubic spline in the component Xj, with knots at each of the unique values
ofxij, i= 1,...,N. However, without further restrictions on the model,
the solution is not unique. The constant αis not identiﬁable, since we
can add or subtract any constants to each of the functions fj, and adjust
αaccordingly. The standard convention is to assume that/summationtextN
1fj(xij) =
0∀j—the functions average zero over the data. It is easily seen th at ˆα=
ave(yi) in this case. If in addition to this restriction, the matrix of input
values (having ijth entryxij) has full column rank, then (9.7) is a strictly
convex criterion and the minimizer is unique. If the matrix i s singular, then
thelinear part of the components fjcannot be uniquely determined (while
the nonlinear parts can!)(Buja et al., 1989).
Furthermore, a simple iterative procedure exists for ﬁndin g the solution.
We set ˆα= ave(yi), and it never changes. We apply a cubic smoothing
splineSjto the targets{yi−ˆα−/summationtext
k/ne}ationslash=jˆfk(xik)}N
1, as a function of xij,
to obtain a new estimate ˆfj. This is done for each predictor in turn, using
the current estimates of the other functions ˆfkwhen computing yi−ˆα−/summationtext
k/ne}ationslash=jˆfk(xik).Theprocessiscontinueduntiltheestimates ˆfjstabilize.This
procedure, given in detail in Algorithm 9.1, is known as “bac kﬁtting” and
the resulting ﬁt is analogous to a multiple regression for li near models.
In principle, the second step in (2) of Algorithm 9.1 is not ne eded, since
the smoothing spline ﬁt to a mean-zero response has mean zero (Exer-
cise 9.1). In practice, machine rounding can cause slippage , and the ad-
justment is advised.
This same algorithm can accommodate other ﬁtting methods in exactly
the same way, by specifying appropriate smoothing operator sSj:
•other univariate regression smoothers such as local polyno mial re-
gression and kernel methods;

9.1 Generalized Additive Models 299
•linear regression operators yielding polynomial ﬁts, piec ewise con-
stant ﬁts, parametric spline ﬁts, series and Fourier ﬁts;
•more complicated operators such as surface smoothers for se cond or
higher-order interactions or periodic smoothers for seaso nal eﬀects.
If we consider the operation of smoother Sjonly at the training points, it
can be represented by an N×Noperator matrix Sj(see Section 5.4.1).
Thenthedegreesoffreedomforthe jthtermare(approximately)computed
as dfj= trace[Sj]−1, by analogy with degrees of freedom for smoothers
discussed in Chapters 5 and 6.
For a large class of linear smoothers Sj, backﬁtting is equivalent to a
Gauss–Seidel algorithm for solving a certain linear system of equations.
Details are given in Exercise 9.2.
For the logistic regression model and other generalized add itive models,
the appropriate criterion is a penalized log-likelihood. T o maximize it, the
backﬁtting procedure is used in conjunction with a likeliho od maximizer.
The usual Newton–Raphson routine for maximizing log-likel ihoods in gen-
eralized linear models can be recast as an IRLS (iteratively reweighted
least squares) algorithm. This involves repeatedly ﬁtting a weighted linear
regression of a working response variable on the covariates ; each regression
yields a new value of the parameter estimates, which in turn g ive new work-
ing responses and weights, and the process is iterated (see S ection 4.4.1).
In the generalized additive model, the weighted linear regr ession is simply
replaced by a weighted backﬁtting algorithm. We describe th e algorithm in
more detail for logistic regression below, and more general ly in Chapter 6
of Hastie and Tibshirani (1990).
9.1.2 Example: Additive Logistic Regression
Probably the most widely used model in medical research is th e logistic
model for binary data. In this model the outcome Ycan be coded as 0
or 1, with 1 indicating an event (like death or relapse of a dis ease) and
0 indicating no event. We wish to model Pr( Y= 1|X), the probability of
an event given values of the prognostic factors XT= (X1,...,X p). The
goal is usually to understand the roles of the prognostic fac tors, rather
than to classify new individuals. Logistic models are also u sed in applica-
tions where one is interested in estimating the class probab ilities, for use
in risk screening. Apart from medical applications, credit risk screening is
a popular application.
The generalized additive logistic model has the form
logPr(Y= 1|X)
Pr(Y= 0|X)=α+f1(X1)+···+fp(Xp). (9.8)
The functions f1,f2,...,f pare estimated by a backﬁtting algorithm
within a Newton–Raphson procedure, shown in Algorithm 9.2.

300 9. Additive Models, Trees, and Related Methods
Algorithm 9.2 Local Scoring Algorithm for the Additive Logistic Regres-
sion Model.
1. Compute starting values: ˆ α= log[¯y/(1−¯y)], where ¯y= ave(yi), the
sample proportion of ones, and set ˆfj≡0∀j.
2. Deﬁne ˆηi= ˆα+/summationtext
jˆfj(xij) and ˆpi= 1/[1+exp(−ˆηi)].
Iterate:
(a) Construct the working target variable
zi= ˆηi+(yi−ˆpi)
ˆpi(1−ˆpi).
(b) Construct weights wi= ˆpi(1−ˆpi)
(c) Fit an additive model to the targets ziwith weights wi, us-
ing a weighted backﬁtting algorithm. This gives new estimat es
ˆα,ˆfj,∀j
3. Continue step 2. until the change in the functions falls be low a pre-
speciﬁed threshold.
Theadditivemodelﬁttinginstep(2)ofAlgorithm9.2requir esaweighted
scatterplot smoother. Most smoothing procedures can accep t observation
weights (Exercise 5.12); see Chapter 3 of Hastie and Tibshir ani (1990) for
further details.
The additive logistic regression model can be generalized f urther to han-
dle more than two classes, using the multilogit formulation as outlined in
Section 4.4. While the formulation is a straightforward ext ension of (9.8),
the algorithms for ﬁtting such models are more complex. See Y ee and Wild
(1996) for details, and the VGAMsoftware currently available from:
http://www.stat.auckland.ac.nz/ ∼yee.
Example: Predicting Email Spam
We apply a generalized additive model to the spamdata introduced in
Chapter 1. The data consists of information from 4601 email m essages, in
a study to screen email for “spam” (i.e., junk email). The dat a is publicly
available at ftp.ics.uci.edu , and was donated by George Forman from
Hewlett-Packard laboratories, Palo Alto, California.
The response variable is binary, with values emailorspam, and there are
57 predictors as described below:
•48 quantitative predictors—the percentage of words in the em ail that
match a given word. Examples include business ,address,internet ,

9.1 Generalized Additive Models 301
TABLE 9.1. Test data confusion matrix for the additive logistic regress ion model
ﬁt to the spam training data. The overall test error rate is 5.5%.
Predicted Class
True Class email(0)spam(1)
email(0) 58.3% 2.5%
spam(1) 3.0% 36.3%
free, andgeorge. The idea was that these could be customized for
individual users.
•6 quantitative predictors—the percentage of characters in t he email
that match a given character. The characters are ch;,ch(,ch[,ch!,
ch$, andch#.
•The average length of uninterrupted sequences of capital le tters:
CAPAVE.
•The length of the longest uninterrupted sequence of capital letters:
CAPMAX.
•The sum of the length of uninterrupted sequences of capital l etters:
CAPTOT.
Wecoded spamas1and emailaszero.Atestsetofsize1536wasrandomly
chosen, leaving 3065 observations in the training set. A gen eralized additive
modelwasﬁt,usingacubicsmoothingsplinewithanominalfo urdegreesof
freedom for each predictor. What this means is that for each p redictorXj,
thesmoothing-splineparameter λjwaschosensothattrace[ Sj(λj)]−1 = 4,
whereSj(λ) is the smoothing spline operator matrix constructed using the
observed values xij, i= 1,...,N. This is a convenient way of specifying
the amount of smoothing in such a complex model.
Most of the spampredictors have a very long-tailed distribution. Before
ﬁtting the GAM model, we log-transformed each variable (act ually log(x+
0.1)), but the plots in Figure 9.1 are shown as a function of the o riginal
variables.
The test error rates are shown in Table 9.1; the overall error rate is 5.3%.
By comparison, a linear logistic regression has a test error rate of 7.6%.
Table 9.2 shows the predictors that are highly signiﬁcant in the additive
model.
For ease of interpretation, in Table 9.2 the contribution fo r each variable
is decomposed into a linear component and the remaining nonl inear com-
ponent. The top block of predictors are positively correlat ed with spam,
while the bottom block is negatively correlated. The linear component is a
weighted least squares linear ﬁt of the ﬁtted curve on the pre dictor, while
the nonlinear part is the residual. The linear component of a n estimated

302 9. Additive Models, Trees, and Related Methods
TABLE 9.2. Signiﬁcant predictors from the additive model ﬁt to the spam train-
ing data. The coeﬃcients represent the linear part of ˆfj, along with their standard
errors and Z-score. The nonlinear P-value is for a test of nonlin earity of ˆfj.
Name Num. df Coeﬃcient Std. Error ZScore Nonlinear
P-value
Positive eﬀects
our 53.9 0.566 0.114 4.970 0.052
over 63.9 0.244 0.195 1.249 0.004
remove 74.0 0.949 0.183 5.201 0.093
internet 84.0 0.524 0.176 2.974 0.028
free 163.9 0.507 0.127 4.010 0.065
business 173.8 0.779 0.186 4.179 0.194
hpl 263.8 0.045 0.250 0.181 0.002
ch! 524.0 0.674 0.128 5.283 0.164
ch$ 533.9 1.419 0.280 5.062 0.354
CAPMAX 563.8 0.247 0.228 1.080 0.000
CAPTOT 574.0 0.755 0.165 4.566 0.063
Negative eﬀects
hp 253.9−1.404 0.224 −6.262 0.140
george 273.7−5.003 0.744 −6.722 0.045
1999 373.8−0.672 0.191 −3.512 0.011
re 453.9−0.620 0.133 −4.649 0.597
edu 464.0−1.183 0.209 −5.647 0.000
function is summarized by the coeﬃcient, standard error and Z-score; the
latter is the coeﬃcient divided by its standard error, and is considered
signiﬁcant if it exceeds the appropriate quantile of a stand ard normal dis-
tribution. The column labeled nonlinearP-valueis a test of nonlinearity
of the estimated function. Note, however, that the eﬀect of e ach predictor
is fully adjusted for the entire eﬀects of the other predicto rs, not just for
their linear parts. The predictors shown in the table were ju dged signiﬁ-
cant by at least one of the tests (linear or nonlinear) at the p= 0.01 level
(two-sided).
Figure 9.1 shows the estimated functions for the signiﬁcant predictors
appearing in Table 9.2. Many of the nonlinear eﬀects appear t o account for
a strong discontinuity at zero. For example, the probabilit y ofspamdrops
signiﬁcantly as the frequency of georgeincreases from zero, but then does
not change much after that. This suggests that one might repl ace each of
thefrequencypredictorsbyanindicatorvariableforazero count,andresort
to a linear logistic model. This gave a test error rate of 7 .4%; including the
linear eﬀects of the frequencies as well dropped the test err or to 6.6%. It
appears that the nonlinearities in the additive model have a n additional
predictive power.

9.1 Generalized Additive Models 303
0 2 4 6 8-5 0 5
0 1 2 3-5 0 5
0 2 4 6-5 0 5 10
0 2 4 6 8 10-5 0 5 10
0 2 4 6 8 10-5 0 5 10
0 2 4 6-5 0 5 10
0 5 10 15 20-10 -5 0
0 5 10-10 -5 0
0 10 20 30-10 -5 0 5
0 2 4 6-5 0 5
0 5 10 15 20-10 -5 0 5
0 5 10 15-10 -5 0
0 10 20 30-5 0 5 10
0 1 2 3 4 5 6-5 0 5 10
0 2000 6000 10000-5 0 5
0 5000 10000 15000-5 0 5our over remove internet
free business hp hpl
george 1999 re edu
ch! ch$ CAPMAX CAPTOTˆf(our)
ˆf(over)
ˆf(remove)
ˆf(internet )ˆf(free)
ˆf(business )
ˆf(hp)
ˆf(hpl)ˆf(george)
ˆf(1999)
ˆf(re)
ˆf(edu)ˆf(ch!)
ˆf(ch$)
ˆf(CAPMAX)
ˆf(CAPTOT)
FIGURE 9.1. Spam analysis: estimated functions for signiﬁcant predicto rs. The
rug plot along the bottom of each frame indicates the observed values of the cor-
responding predictor. For many of the predictors the nonline arity picks up the
discontinuity at zero.

304 9. Additive Models, Trees, and Related Methods
It is more serious to classify a genuine emailmessage as spam, since then
a good email would be ﬁltered out and would not reach the user. We can
alter the balance between the class error rates by changing t he losses (see
Section 2.4). If we assign a loss L01for predicting a true class 0 as class 1,
andL10for predicting a true class 1 as class 0, then the estimated Ba yes
rule predicts class 1 if its probability is greater than L01/(L01+L10). For
example, if we take L01= 10,L10= 1 then the (true) class 0 and class 1
error rates change to 0.8% and 8.7%.
More ambitiously, we can encourage the model to ﬁt better dat a in the
class 0 by using weights L01for the class 0 observations and L10for the
class 1 observations. As above, we then use the estimated Bay es rule to
predict. This gave error rates of 1.2% and 8.0% in (true) clas s 0 and class 1,
respectively. We discuss below the issue of unequal losses f urther, in the
context of tree-based models.
After ﬁtting an additive model, one should check whether the inclusion
of some interactions can signiﬁcantly improve the ﬁt. This c an be done
“manually,” by inserting products of some or all of the signi ﬁcant inputs,
or automatically via the MARS procedure (Section 9.4).
This example uses the additive model in an automatic fashion . As a data
analysis tool, additive models are often used in a more inter active fashion,
adding and dropping terms to determine their eﬀect. By calib rating the
amount of smoothing in terms of df j, one can move seamlessly between
linear models (df j= 1) and partially linear models, where some terms are
modeled more ﬂexibly. See Hastie and Tibshirani (1990) for m ore details.
9.1.3 Summary
Additive models provide a useful extension of linear models , making them
moreﬂexiblewhilestillretainingmuchoftheirinterpreta bility.Thefamiliar
tools for modeling and inference in linear models are also av ailable for
additive models, seen for example in Table 9.2. The backﬁtti ng procedure
for ﬁtting these models is simple and modular, allowing one t o choose a
ﬁtting method appropriate for each input variable. As a resu lt they have
become widely used in the statistical community.
However additive models can have limitations for large data -mining ap-
plications. The backﬁtting algorithm ﬁts all predictors, w hich is not feasi-
ble or desirable when a large number are available. The BRUTO procedure
(Hastie and Tibshirani, 1990, Chapter 9) combines backﬁtti ng with selec-
tion of inputs, but is not designed for large data-mining pro blems. There
has also been recent work using lasso-type penalties to esti mate sparse ad-
ditive models, for example the COSSO procedure of Lin and Zha ng (2006)
and the SpAM proposal of Ravikumar et al. (2008). For large pr oblems a
forward stagewise approach suchas boosting(Chapter 10) is moreeﬀective,
and also allows for interactions to be included in the model.

9.2 Tree-Based Methods 305
9.2 Tree-Based Methods
9.2.1 Background
Tree-based methods partition the feature space into a set of rectangles, and
then ﬁt a simple model (like a constant) in each one. They are c onceptually
simple yet powerful. We ﬁrst describe a popular method for tr ee-based
regression and classiﬁcation called CART, and later contra st it with C4.5,
a major competitor.
Let’s consider a regression problem with continuous respon seYand in-
putsX1andX2, each taking values in the unit interval. The top left panel
of Figure 9.2 shows a partition of the feature space by lines t hat are parallel
to the coordinate axes. In each partition element we can mode lYwith a
diﬀerent constant. However, there is a problem: although ea ch partitioning
line has a simple description like X1=c, some of the resulting regions are
complicated to describe.
To simplify matters, we restrict attention to recursive bin ary partitions
like that in the top right panel of Figure 9.2. We ﬁrst split th e space into
two regions, and model the response by the mean of Yin each region.
We choose the variable and split-point to achieve the best ﬁt . Then one
or both of these regions are split into two more regions, and t his process
is continued, until some stopping rule is applied. For examp le, in the top
right panel of Figure 9.2, we ﬁrst split at X1=t1. Then the region X1≤t1
is split atX2=t2and the region X1>t1is split atX1=t3. Finally, the
regionX1>t3is split atX2=t4. The result of this process is a partition
into the ﬁve regions R1,R2,...,R 5shown in the ﬁgure. The corresponding
regression model predicts Ywith a constant cmin regionRm, that is,
ˆf(X) =5/summationdisplay
m=1cmI{(X1,X2)∈Rm}. (9.9)
This same model can be represented by the binary tree in the bo ttom left
panel of Figure 9.2. The full dataset sits at the top of the tre e. Observations
satisfying the condition at each junction are assigned to th e left branch,
and the others to the right branch. The terminal nodes or leav es of the
tree correspond to the regions R1,R2,...,R 5. The bottom right panel of
Figure 9.2 is a perspective plot of the regression surface fr om this model.
For illustration, we chose the node means c1=−5,c2=−7,c3= 0,c4=
2,c5= 4 to make this plot.
A key advantage of the recursive binary tree is its interpret ability. The
feature space partition is fully described by a single tree. With more than
two inputs, partitions like that in the top right panel of Fig ure 9.2 are
diﬃcult to draw, but the binary tree representation works in the same
way. This representation is also popular among medical scie ntists, perhaps
because it mimics the way that a doctor thinks. The tree strat iﬁes the

306 9. Additive Models, Trees, and Related Methods
|t1t2
t3t4
R1R1
R2R2
R3R3
R4R4
R5R5
X1X1 X1
X2X2X2
X1≤t1
X2≤t2 X1≤t3
X2≤t4
FIGURE 9.2. Partitions and CART. Top right panel shows a partition of a
two-dimensional feature space by recursive binary splittin g, as used in CART,
applied to some fake data. Top left panel shows a general partit ion that cannot
be obtained from recursive binary splitting. Bottom left pane l shows the tree cor-
responding to the partition in the top right panel, and a persp ective plot of the
prediction surface appears in the bottom right panel.

9.2 Tree-Based Methods 307
population into strata of high and low outcome, on the basis o f patient
characteristics.
9.2.2 Regression Trees
We now turn to the question of how to grow a regression tree. Ou r data
consists of pinputs and a response, for each of Nobservations: that is,
(xi,yi) fori= 1,2,...,N, withxi= (xi1,xi2,...,x ip). The algorithm
needs to automatically decide on the splitting variables an d split points,
and also what topology (shape) the tree should have. Suppose ﬁrst that we
have a partition into MregionsR1,R2,...,R M, and we model the response
as a constant cmin each region:
f(x) =M/summationdisplay
m=1cmI(x∈Rm). (9.10)
If we adopt as our criterion minimization of the sum of square s/summationtext(yi−
f(xi))2, it is easy to see that the best ˆ cmis just the average of yiin region
Rm:
ˆcm= ave(yi|xi∈Rm). (9.11)
Now ﬁnding the best binary partition in terms of minimum sum o f squares
is generally computationally infeasible. Hence we proceed with a greedy
algorithm. Starting with all of the data, consider a splitti ng variable jand
split points, and deﬁne the pair of half-planes
R1(j,s) ={X|Xj≤s}andR2(j,s) ={X|Xj>s}. (9.12)
Then we seek the splitting variable jand split point sthat solve
min
j, s/bracketleftig
min
c1/summationdisplay
xi∈R1(j,s)(yi−c1)2+min
c2/summationdisplay
xi∈R2(j,s)(yi−c2)2/bracketrightig
.(9.13)
For any choice jands, the inner minimization is solved by
ˆc1= ave(yi|xi∈R1(j,s)) and ˆc2= ave(yi|xi∈R2(j,s)).(9.14)
For each splitting variable, the determination of the split pointscan
be done very quickly and hence by scanning through all of the i nputs,
determination of the best pair ( j,s) is feasible.
Having found the best split, we partition the data into the tw o resulting
regions and repeat the splitting process on each of the two re gions. Then
this process is repeated on all of the resulting regions.
Howlargeshouldwegrowthetree?Clearlyaverylargetreemi ghtoverﬁt
the data, while a small tree might not capture the important s tructure.

308 9. Additive Models, Trees, and Related Methods
Tree size is a tuning parameter governing the model’s comple xity, and the
optimal tree size should be adaptively chosen from the data. One approach
wouldbetosplittreenodesonlyifthedecreaseinsum-of-sq uaresduetothe
split exceeds some threshold. This strategy is too short-si ghted, however,
since a seemingly worthless split might lead to a very good sp lit below it.
The preferred strategy is to grow a large tree T0, stopping the splitting
process only when some minimum node size (say 5) is reached. T hen this
large tree is pruned using cost-complexity pruning , which we now describe.
We deﬁne a subtree T⊂T0to be any tree that can be obtained by
pruningT0, that is, collapsing any number of its internal (non-termin al)
nodes. We index terminal nodes by m, with node mrepresenting region
Rm. Let|T|denote the number of terminal nodes in T. Letting
Nm= #{xi∈Rm},
ˆcm=1
Nm/summationdisplay
xi∈Rmyi,
Qm(T) =1
Nm/summationdisplay
xi∈Rm(yi−ˆcm)2,(9.15)
we deﬁne the cost complexity criterion
Cα(T) =|T|/summationdisplay
m=1NmQm(T)+α|T|. (9.16)
The idea is to ﬁnd, for each α, the subtree Tα⊆T0to minimize Cα(T).
The tuning parameter α≥0 governs the tradeoﬀ between tree size and its
goodness of ﬁt to the data. Large values of αresult in smaller trees Tα, and
conversely for smaller values of α. As the notation suggests, with α= 0 the
solution is the full tree T0. We discuss how to adaptively choose αbelow.
For eachαone can show that there is a unique smallest subtree Tαthat
minimizesCα(T). To ﬁndTαwe useweakest link pruning : we successively
collapse the internal node that produces the smallest per-n ode increase in/summationtext
mNmQm(T), and continue until we produce the single-node (root) tree .
This gives a (ﬁnite) sequence of subtrees, and one can show th is sequence
must contain Tα. See Breiman et al. (1984) or Ripley (1996) for details.
Estimation of αis achieved by ﬁve- or tenfold cross-validation: we choose
the value ˆαto minimize the cross-validated sum of squares. Our ﬁnal tre e
isTˆα.
9.2.3 Classiﬁcation Trees
If the target is a classiﬁcation outcome taking values 1 ,2,...,K, the only
changes needed in the tree algorithm pertain to the criteria for splitting
nodes and pruning the tree. For regression we used the square d-error node

9.2 Tree-Based Methods 309
0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5
pEntropy
Gini index
Misclassification error
FIGURE 9.3. Node impurity measures for two-class classiﬁcation, as a func tion
of the proportion pin class 2. Cross-entropy has been scaled to pass through
(0.5,0.5).
impurity measure Qm(T) deﬁned in (9.15), but this is not suitable for
classiﬁcation. In a node m, representing a region RmwithNmobservations,
let
ˆpmk=1
Nm/summationdisplay
xi∈RmI(yi=k),
the proportion of class kobservations in node m. We classify the obser-
vations in node mto classk(m) = argmax kˆpmk, the majority class in
nodem. Diﬀerent measures Qm(T) of node impurity include the following:
Misclassiﬁcation error:1
Nm/summationtext
i∈RmI(yi∝ne}ationslash=k(m)) = 1−ˆpmk(m).
Gini index:/summationtext
k/ne}ationslash=k′ˆpmkˆpmk′=/summationtextK
k=1ˆpmk(1−ˆpmk).
Cross-entropy or deviance: −/summationtextK
k=1ˆpmklog ˆpmk.
(9.17)
For two classes, if pis the proportion in the second class, these three mea-
sures are 1−max(p,1−p), 2p(1−p) and−plogp−(1−p)log(1−p),
respectively. They are shown in Figure 9.3. All three are sim ilar, but cross-
entropy and the Gini index are diﬀerentiable, and hence more amenable to
numerical optimization. Comparing (9.13) and (9.15), we se e that we need
to weight the node impurity measures by the number NmLandNmRof
observations in the two child nodes created by splitting nod em.
Inaddition,cross-entropyandtheGiniindexaremoresensi tivetochanges
in the node probabilities than the misclassiﬁcation rate. F or example, in
a two-class problem with 400 observations in each class (den ote this by
(400,400)), suppose one split created nodes (300 ,100) and (100 ,300), while

310 9. Additive Models, Trees, and Related Methods
the other created nodes (200 ,400) and (200 ,0). Both splits produce a mis-
classiﬁcation rate of 0.25, but the second split produces a p ure node and is
probablypreferable.BoththeGiniindexandcross-entropy arelowerforthe
second split. For this reason, either the Gini index or cross -entropy should
be used when growing the tree. To guide cost-complexity prun ing, any of
the three measures can be used, but typically it is the miscla ssiﬁcation rate.
The Gini index can be interpreted in two interesting ways. Ra ther than
classify observations to the majority class in the node, we c ould classify
them to class kwith probability ˆ pmk. Then the expected training error
rate of this rule in the node is/summationtext
k/ne}ationslash=k′ˆpmkˆpmk′—the Gini index. Similarly,
if we code each observation as 1 for class kand zero otherwise, the variance
over the node of this 0-1 response is ˆ pmk(1−ˆpmk). Summing over classes
kagain gives the Gini index.
9.2.4 Other Issues
Categorical Predictors
When splitting a predictor having qpossible unordered values, there are
2q−1−1 possible partitions of the qvalues into two groups, and the com-
putations become prohibitive for large q. However, with a 0 −1 outcome,
this computation simpliﬁes. We order the predictor classes according to the
proportion falling in outcome class 1. Then we split this pre dictor as if it
were an ordered predictor. One can show this gives the optima l split, in
termsofcross-entropyorGiniindex,amongallpossible2q−1−1splits.This
result also holds for a quantitative outcome and square erro r loss—the cat-
egories are ordered by increasing mean of the outcome. Altho ugh intuitive,
the proofs of these assertions are not trivial. The proof for binary outcomes
is given in Breiman et al. (1984) and Ripley (1996); the proof for quantita-
tive outcomes can be found in Fisher (1958). For multicatego ry outcomes,
no such simpliﬁcations are possible, although various appr oximations have
been proposed (Loh and Vanichsetakul, 1988).
The partitioning algorithm tends to favor categorical pred ictors with
many levels q; the number of partitions grows exponentially in q, and the
more choices we have, the more likely we can ﬁnd a good one for t he data
at hand. This can lead to severe overﬁtting if qis large, and such variables
should be avoided.
The Loss Matrix
In classiﬁcation problems, the consequences of misclassif ying observations
are more serious in some classes than others. For example, it is probably
worse to predict that a person will not have a heart attack whe n he/she
actually will, than vice versa. To account for this, we deﬁne aK×Kloss
matrixL, withLkk′being the loss incurred for classifying a class kobser-
vation as class k′. Typically no loss is incurred for correct classiﬁcations,

9.2 Tree-Based Methods 311
that is,Lkk= 0∀k. To incorporate the losses into the modeling process,
we could modify the Gini index to/summationtext
k/ne}ationslash=k′Lkk′ˆpmkˆpmk′; this would be the
expected loss incurred by the randomized rule. This works fo r the multi-
class case, but in the two-class case has no eﬀect, since the c oeﬃcient of
ˆpmkˆpmk′isLkk′+Lk′k. For two classes a better approach is to weight the
observations in class kbyLkk′. This can be used in the multiclass case only
if, as a function of k,Lkk′doesn’t depend on k′. Observation weighting can
be used with the deviance as well. The eﬀect of observation we ighting is to
alter the prior probability on the classes. In a terminal nod e, the empirical
Bayes rule implies that we classify to class k(m) = argmin k/summationtext
ℓLℓkˆpmℓ.
Missing Predictor Values
Suppose our data has some missing predictor values in some or all of the
variables. We might discard any observation with some missi ng values, but
this could lead to serious depletion of the training set. Alt ernatively we
might try to ﬁll in (impute) the missing values, with say the m ean of that
predictor over the nonmissing observations. For tree-base d models, there
are two better approaches. The ﬁrst is applicable to categor ical predictors:
we simply make a new category for “missing.” From this we migh t dis-
cover that observations with missing values for some measur ement behave
diﬀerently than those with nonmissing values. The second mo re general
approach is the construction of surrogate variables. When c onsidering a
predictor for a split, we use only the observations for which that predictor
is not missing. Having chosen the best (primary) predictor a nd split point,
we form a list of surrogate predictors and split points. The ﬁ rst surrogate
is the predictor and corresponding split point that best mim ics the split of
the training data achieved by the primary split. The second s urrogate is
the predictor and corresponding split point that does secon d best, and so
on. When sending observations down the tree either in the tra ining phase
or during prediction, we use the surrogate splits in order, i f the primary
splitting predictor is missing. Surrogate splits exploit c orrelations between
predictorstotryandalleviatetheeﬀectofmissingdata.Th ehigherthecor-
relation between the missing predictor and the other predic tors, the smaller
the loss of information due to the missing value. The general problem of
missing data is discussed in Section 9.6.
Why Binary Splits?
Rather than splitting each node into just two groups at each s tage (as
above),wemightconsidermultiwaysplitsintomorethantwo groups.While
this can sometimes be useful, it is not a good general strateg y. The problem
is that multiway splits fragment the data too quickly, leavi ng insuﬃcient
data at the next level down. Hence we would want to use such spl its only
when needed. Since multiway splits can be achieved by a serie s of binary
splits, the latter are preferred.

312 9. Additive Models, Trees, and Related Methods
Other Tree-Building Procedures
The discussion above focuses on the CART (classiﬁcation and regression
tree) implementation of trees. The other popular methodolo gy is ID3 and
its later versions, C4.5 and C5.0 (Quinlan, 1993). Early ver sions of the
program were limited to categorical predictors, and used a t op-down rule
with no pruning. With more recent developments, C5.0 has bec ome quite
similar to CART. The most signiﬁcant feature unique to C5.0 i s a scheme
forderivingrulesets.Afteratreeisgrown,thesplittingr ulesthatdeﬁnethe
terminal nodes can sometimes be simpliﬁed: that is, one or mo re condition
can be dropped without changing the subset of observations t hat fall in
the node. We end up with a simpliﬁed set of rules deﬁning each t erminal
node; these no longer follow a tree structure, but their simp licity might
make them more attractive to the user.
Linear Combination Splits
Rather than restricting splits to be of the form Xj≤s, one can allow splits
along linear combinations of the form/summationtextajXj≤s. The weights ajand
split pointsare optimized to minimize the relevant criterion (such as th e
Gini index). While this can improve the predictive power of t he tree, it can
hurt interpretability. Computationally, the discretenes s of the split point
search precludes the use of a smooth optimization for the wei ghts. A better
way to incorporate linear combination splits is in the hiera rchical mixtures
of experts (HME) model, the topic of Section 9.5.
Instability of Trees
One major problem with trees is their high variance. Often a s mall change
in the data can result in a very diﬀerent series of splits, mak ing interpre-
tation somewhat precarious. The major reason for this insta bility is the
hierarchical nature of the process: the eﬀect of an error in t he top split
is propagated down to all of the splits below it. One can allev iate this to
some degree by trying to use a more stable split criterion, bu t the inherent
instability is not removed. It is the price to be paid for esti mating a simple,
tree-based structure from the data. Bagging (Section 8.7) averages many
trees to reduce this variance.
Lack of Smoothness
Another limitation of trees is the lack of smoothness of the p rediction sur-
face, as can be seen in the bottom right panel of Figure 9.2. In classiﬁcation
with 0/1 loss, this doesn’t hurt much, since bias in estimati on of the class
probabilities has a limited eﬀect. However, this can degrad e performance
in the regression setting, where we would normally expect th e underlying
function to be smooth. The MARS procedure, described in Sect ion 9.4,

9.2 Tree-Based Methods 313
TABLE 9.3. Spam data: confusion rates for the 17-node tree (chosen by cross–
validation) on the test data. Overall error rate is 9.3%.
Predicted
Trueemail spam
email 57.3% 4.0%
spam 5.3% 33.4%
can be viewed as a modiﬁcation of CART designed to alleviate t his lack of
smoothness.
Diﬃculty in Capturing Additive Structure
Another problem with trees is their diﬃculty in modeling add itive struc-
ture.Inregression,suppose,forexample,that Y=c1I(X1<t1)+c2I(X2<
t2)+εwhereεis zero-mean noise. Then a binary tree might make its ﬁrst
split onX1neart1. At the next level down it would have to split both nodes
onX2att2in order to capture the additive structure. This might happe n
withsuﬃcientdata,butthemodelisgivennospecialencoura gementtoﬁnd
such structure. If there were ten rather than two additive eﬀ ects, it would
take many fortuitous splits to recreate the structure, and t he data analyst
would be hard pressed to recognize it in the estimated tree. T he “blame”
here can again be attributed to the binary tree structure, wh ich has both
advantages and drawbacks. Again the MARS method (Section 9. 4) gives
up this tree structure in order to capture additive structur e.
9.2.5 Spam Example (Continued)
We applied the classiﬁcation tree methodology to the spamexample intro-
duced earlier. We used the deviance measure to grow the tree a nd mis-
classiﬁcation rate to prune it. Figure 9.4 shows the 10-fold cross-validation
error rate as a function of the size of the pruned tree, along w ith±2 stan-
dard errors of the mean, from the ten replications. The test e rror curve is
shown in orange. Note that the cross-validation error rates are indexed by
a sequence of values of αandnottree size; for trees grown in diﬀerent folds,
a value ofαmight imply diﬀerent sizes. The sizes shown at the base of the
plot refer to|Tα|, the sizes of the pruned originaltree.
Theerrorﬂattensoutataround17terminalnodes,givingthe prunedtree
in Figure 9.5. Of the 13 distinct features chosen by the tree, 11 overlap with
the 16 signiﬁcant features in the additive model (Table 9.2) . The overall
error rate shown in Table 9.3 is about 50% higher than for the a dditive
model in Table 9.1.
Consider the rightmost branches of the tree. We branch to the right
with aspamwarning if more than 5.5% of the characters are the $ sign.

314 9. Additive Models, Trees, and Related Methods
0 10 20 30 400.0 0.1 0.2 0.3 0.4
Tree SizeMisclassification Rate176 21 7 5 3 2 0α
FIGURE 9.4. Results for spamexample. The blue curve is the 10-fold cross-val-
idation estimate of misclassiﬁcation rate as a function of tr ee size, with standard
error bars. The minimum occurs at a tree size with about 17terminal nodes (using
the “one-standard-error” rule). The orange curve is the test error, which tracks
the CV error quite closely. The cross-validation is indexed by v alues ofα, shown
above. The tree sizes shown below refer to |Tα|, the size of the original tree indexed
byα.
However, if in addition the phrase hpoccurs frequently, then this is likely
to be company business and we classify as email. All of the 22 cases in
the test set satisfying these criteria were correctly class iﬁed. If the second
condition is not met, and in addition the average length of re peated capital
lettersCAPAVEis larger than 2.9, then we classify as spam. Of the 227 test
cases, only seven were misclassiﬁed.
In medical classiﬁcation problems, the terms sensitivity andspeciﬁcity
are used to characterize a rule. They are deﬁned as follows:
Sensitivity: probability of predicting disease given true state is disea se.
Speciﬁcity: probability of predicting non-disease given true state is n on-
disease.

9.2 Tree-Based Methods 315
600/1536
280/1177
180/1065
 80/861
 80/652
 77/423
 20/238
 19/236   1/2 57/185
 48/113
 37/101   1/12  9/72  3/229  0/209100/204
 36/123
 16/94
 14/89   3/5  9/29 16/81  9/112
  6/109   0/3 48/359
 26/337
 19/110
 18/109   0/1  7/227  0/22
spam
spamspamspamspam
spamspam
spam
spam
spam
spamspamemail
emailemail
emailemailemailemail
email
email
email
emailemailemail
emailemailemailemailemailemailemailemail
ch$<0.0555
remove<0.06
ch!<0.191
george<0.005
hp<0.03
CAPMAX<10.5
receive<0.125 edu<0.045
our<1.2CAPAVE<2.7505
free<0.065
business<0.145george<0.15hp<0.405
CAPAVE<2.907
1999<0.58ch$>0.0555
remove>0.06
ch!>0.191
george>0.005
hp>0.03
CAPMAX>10.5
receive>0.125 edu>0.045
our>1.2CAPAVE>2.7505
free>0.065
business>0.145george>0.15hp>0.405
CAPAVE>2.907
1999>0.58
FIGURE 9.5. The pruned tree for the spamexample. The split variables are
shown in blue on the branches, and the classiﬁcation is shown in every node.The
numbers under the terminal nodes indicate misclassiﬁcation rates on the test data.

316 9. Additive Models, Trees, and Related Methods
SpecificitySensitivity
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0• •
•• •
•
•
•
••
•
•••• ••••••••••••••• ••••• ••••••••• •••••••••••••
••
•
•
••
••
•••
•
•
•
•
•Tree (0.95)
GAM (0.98)
Weighted Tree (0.90)
FIGURE 9.6. ROC curves for the classiﬁcation rules ﬁt to the spamdata. Curves
that are closer to the northeast corner represent better class iﬁers. In this case the
GAM classiﬁer dominates the trees. The weighted tree achieves better sensitivity
for higher speciﬁcity than the unweighted tree. The numbers in the legend repre-
sent the area under the curve.
If we think of spamandemailas the presence and absence of disease, re-
spectively, then from Table 9.3 we have
Sensitivity = 100×33.4
33.4+5.3= 86.3%,
Speciﬁcity = 100×57.3
57.3+4.0= 93.4%.
In this analysis we have used equal losses. As before let Lkk′be the
loss associated with predicting a class kobject as class k′. By varying the
relative sizes of the losses L01andL10, we increase the sensitivity and
decrease the speciﬁcity of the rule, or vice versa. In this ex ample, we want
to avoid marking good emailasspam, and thus we want the speciﬁcity to
be very high. We can achieve this by setting L01>1 say, with L10= 1.
The Bayes’ rule in each terminal node classiﬁes to class 1 ( spam) if the
proportion of spamis≥L01/(L10+L01), and class zero otherwise. The

9.3 PRIM: Bump Hunting 317
receiveroperatingcharacteristiccurve(ROC)isacommonl yusedsummary
for assessing the tradeoﬀ between sensitivity and speciﬁci ty. It is a plot of
thesensitivityversusspeciﬁcityaswevarytheparameters ofaclassiﬁcation
rule. Varying the loss L01between 0.1 and 10, and applying Bayes’ rule to
the 17-node tree selected in Figure 9.4, produced the ROC cur ve shown
in Figure 9.6. The standard error of each curve near 0.9 is app roximately/radicalbig
0.9(1−0.9)/1536 = 0.008, and hence the standard error of the diﬀerence
is about 0.01. We see that in order to achieve a speciﬁcity of close to 100 %,
the sensitivity has to drop to about 50%. The area under the cu rve is a
commonly used quantitative summary; extending the curve li nearly in each
direction so that it is deﬁned over [0 ,100], the area is approximately 0 .95.
For comparison, we have included the ROC curve for the GAM mod el ﬁt
to these data in Section 9.2; it gives a better classiﬁcation rule for any loss,
with an area of 0 .98.
Rather than just modifying the Bayes rule in the nodes, it is b etter to
take full account of the unequal losses in growing the tree, a s was done
in Section 9.2. With just two classes 0 and 1, losses may be inc orporated
into the tree-growing process by using weight Lk,1−kfor an observation in
classk. Here we chose L01= 5,L10= 1 and ﬁt the same size tree as before
(|Tα|= 17). This tree has higher sensitivity at high values of the s peciﬁcity
than the original tree, but does more poorly at the other extr eme. Its top
few splits are the same as the original tree, and then it depar ts from it.
For this application the tree grown using L01= 5 is clearly better than the
original tree.
The area under the ROC curve, used above, is sometimes called thec-
statistic. Interestingly, it can be shown that the area under the ROC cu rve
isequivalenttotheMann-WhitneyUstatistic(orWilcoxonr ank-sumtest),
for the median diﬀerence between the prediction scores in th e two groups
(HanleyandMcNeil,1982).Forevaluatingthecontribution ofanadditional
predictor when added to a standard model, the c-statistic may not be an
informative measure. The new predictor can be very signiﬁca nt in terms
of the change in model deviance, but show only a small increas e in thec-
statistic. For example, removal of the highly signiﬁcant te rmgeorgefrom
the model of Table 9.2 results in a decrease in the c-statistic of less than
0.01. Instead, it is useful to examine how the additional predi ctor changes
the classiﬁcation on an individual sample basis. A good disc ussion of this
point appears in Cook (2007).
9.3 PRIM: Bump Hunting
Tree-based methods (for regression) partition the feature space into box-
shaped regions, to try to make the response averages in each b ox as diﬀer-

318 9. Additive Models, Trees, and Related Methods
ent as possible. The splitting rules deﬁning the boxes are re lated to each
through a binary tree, facilitating their interpretation.
Thepatientruleinductionmethod(PRIM)alsoﬁndsboxesint hefeature
space, but seeks boxes in which the response average is high. Hence it looks
for maxima in the target function, an exercise known as bump hunting . (If
minimaratherthanmaximaaredesired,onesimplyworkswith thenegative
response values.)
PRIM also diﬀers from tree-based partitioning methods in th at the box
deﬁnitions are not described by a binary tree. This makes int erpretation of
the collection of rules more diﬃcult; however, by removing t he binary tree
constraint, the individual rules are often simpler.
The main box construction method in PRIM works from the top do wn,
starting with a box containing all of the data. The box is comp ressed along
one face by a small amount, and the observations then falling outside the
box arepeeledoﬀ. The face chosen for compression is the one resulting in
the largest box mean, after the compression is performed. Th en the process
isrepeated,stoppingwhenthecurrentboxcontainssomemin imumnumber
of data points.
This process is illustrated in Figure 9.7. There are 200 data points uni-
formly distributed over the unit square. The color-coded pl ot indicates the
responseYtaking the value 1 (red) when 0 .5<X1<0.8 and 0.4<X2<
0.6. and zero (blue) otherwise. The panels shows the successiv e boxes found
by the top-down peeling procedure, peeling oﬀ a proportion α= 0.1 of the
remaining data points at each stage.
Figure 9.8 shows the mean of the response values in the box, as the box
is compressed.
After the top-down sequence is computed, PRIM reverses the p rocess,
expanding along any edge, if such an expansion increases the box mean.
This is called pasting. Since the top-down procedure is greedy at each step,
such an expansion is often possible.
The result of these steps is a sequence of boxes, with diﬀeren t numbers
of observation in each box. Cross-validation, combined wit h the judgment
of the data analyst, is used to choose the optimal box size.
Denote byB1the indices of the observations in the box found in step 1.
ThePRIMprocedurethenremovestheobservationsin B1fromthetraining
set, and the two-step process—top down peeling, followed by b ottom-up
pasting—is repeated on the remaining dataset. This entire pr ocess is re-
peated several times, producing a sequence of boxes B1,B2,...,B k. Each
box is deﬁned by a set of rules involving a subset of predictor s like
(a1≤X1≤b1) and (b1≤X3≤b2).
A summary of the PRIM procedure is given Algorithm 9.3.
PRIM can handle a categorical predictor by considering all p artitions of
the predictor, as in CART. Missing values are also handled in a manner
similar to CART. PRIM is designed for regression (quantitat ive response

9.3 PRIM: Bump Hunting 319
1
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o2
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o3
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o4
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o
5
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o6
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o7
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o8
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o
12
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o17
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o22
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o27
ooo
oo
oooo
ooo
oo
oo
oo
oo
o
oooo
oo
oooo
o oo
oo
oo
o
oo
o
o
oooo
ooo
ooo
oo
oo
ooo
oo
ooo
o
ooooo
o
ooo o
oo
oo
ooo
ooo
ooo
o
oo
oo
ooo
o
oo
o
ooo
oooo
oo
ooo
o
oo
oo oo
oo
o
ooo
ooo
ooo
oo
oo
o
ooo
oooo
ooo
ooo
oo
oo
ooo
ooo
oo
oo
oo oo
o
ooo
oo
o
oooo
oo
o
FIGURE 9.7. Illustration of PRIM algorithm. There are two classes, indicat ed
by the blue (class 0) and red (class 1) points. The procedure starts with a rectangle
(broken black lines) surrounding all of the data, and then peels away points along
one edge by a prespeciﬁed amount in order to maximize the mean of the points
remaining in the box. Starting at the top left panel, the sequen ce of peelings is
shown, until a pure red region is isolated in the bottom right p anel. The iteration
number is indicated at the top of each panel.
Number of Observations in BoxBox Mean
50 100 1500.2 0.4 0.6 0.8 1.0
•••••••••••••••••••••••••••
FIGURE 9.8. Box mean as a function of number of observations in the box.

320 9. Additive Models, Trees, and Related Methods
Algorithm 9.3 Patient Rule Induction Method.
1. Start with all of the training data, and a maximal box conta ining all
of the data.
2. Consider shrinking the box by compressing one face, so as t o peel oﬀ
the proportion αof observations having either the highest values of
a predictor Xj, or the lowest. Choose the peeling that produces the
highest response mean in the remaining box. (Typically α= 0.05 or
0.10.)
3. Repeat step 2 until some minimal number of observations (s ay 10)
remain in the box.
4. Expand the box along any face, as long as the resulting box m ean
increases.
5. Steps 1–4 give a sequence of boxes, with diﬀerent numbers o f obser-
vations in each box. Use cross-validation to choose a member of the
sequence. Call the box B1.
6. Remove the data in box B1from the dataset and repeat steps 2–5 to
obtain a second box, and continue to get as many boxes as desir ed.
variable); a two-class outcome can be handled simply by codi ng it as 0 and
1. There is no simple way to deal with k >2 classes simultaneously: one
approach is to run PRIM separately for each class versus a bas eline class.
An advantage of PRIM over CART is its patience. Because of its bi-
nary splits, CART fragments the data quite quickly. Assumin g splits of
equal size, with Nobservations it can only make log2(N)−1 splits before
running out of data. If PRIM peels oﬀ a proportion αof training points
at each stage, it can perform approximately −log(N)/log(1−α) peeling
steps before running out of data. For example, if N= 128 and α= 0.10,
then log2(N)−1 = 6 while−log(N)/log(1−α)≈46. Taking into account
that there must be an integer number of observations at each s tage, PRIM
in fact can peel only 29 times. In any case, the ability of PRIM to be more
patient should help the top-down greedy algorithm ﬁnd a bett er solution.
9.3.1 Spam Example (Continued)
We applied PRIM to the spamdata, with the response coded as 1 for spam
and 0 for email.
The ﬁrst two boxes found by PRIM are summarized below:

9.4 MARS: Multivariate Adaptive Regression Splines 321
Rule 1 Global Mean Box Mean Box Support
Training 0.3931 0.9607 0.1413
Test 0.3958 1.0000 0.1536
Rule 1

ch!>0.029
CAPAVE>2.331
your>0.705
1999<0.040
CAPTOT>79.50
edu<0.070
re<0.535
ch;<0.030
Rule 2 Remain Mean Box Mean Box Support
Training 0.2998 0.9560 0.1043
Test 0.2862 0.9264 0.1061
Rule 2/braceleftbigg
remove>0.010
george<0.110
The box support is the proportion of observations falling in the box.
The ﬁrst box is purely spam, and contains about 15% of the test data.
The second box contains 10.6% of the test observations, 92.6 % of which
arespam. Together the two boxes contain 26% of the data and are about
97%spam. The next few boxes (not shown) are quite small, containing o nly
about 3% of the data.
The predictors are listed in order of importance. Interesti ngly the top
splitting variables in the CART tree (Figure 9.5) do not appe ar in PRIM’s
ﬁrst box.
9.4 MARS: Multivariate Adaptive Regression
Splines
MARS is an adaptive procedure for regression, and is well sui ted for high-
dimensional problems (i.e., a large number of inputs). It ca n be viewed as a
generalization of stepwise linear regression or a modiﬁcat ion of the CART
method to improve the latter’s performance in the regressio n setting. We
introduceMARSfromtheﬁrstpointofview,andlatermakethe connection
to CART.
MARS uses expansions in piecewise linear basis functions of the form
(x−t)+and (t−x)+. The “+” means positive part, so
(x−t)+=/braceleftbigg
x−t,ifx>t,
0,otherwise,and (t−x)+=/braceleftbigg
t−x,ifx<t,
0,otherwise.

322 9. Additive Models, Trees, and Related Methods
0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5(x−t)+ (t−x)+
xtBasis Function
FIGURE 9.9. The basis functions (x−t)+(solid orange) and (t−x)+(broken
blue) used by MARS.
As an example, the functions ( x−0.5)+and (0.5−x)+are shown in Fig-
ure 9.9.
Each function is piecewise linear, with a knotat the value t. In the
terminology of Chapter 5, these are linear splines. We call t he two functions
areﬂected pair in the discussion below. The idea is to form reﬂected pairs
for each input Xjwith knots at each observed value xijof that input.
Therefore, the collection of basis functions is
C={(Xj−t)+,(t−Xj)+}t∈ {x1j,x2j,...,x Nj}
j= 1,2,...,p.(9.18)
If all of the input values are distinct, there are 2 Npbasis functions alto-
gether. Note that although each basis function depends only on a single
Xj, for example, h(X) = (Xj−t)+, it is considered as a function over the
entire input space IRp.
The model-building strategy is like a forward stepwise line ar regression,
but instead of using the original inputs, we are allowed to us e functions
from the setCand their products. Thus the model has the form
f(X) =β0+M/summationdisplay
m=1βmhm(X), (9.19)
where each hm(X) is a function in C, or a product of two or more such
functions.
Given a choice for the hm, the coeﬃcients βmare estimated by minimiz-
ing the residual sum-of-squares, that is, by standard linea r regression. The
real art, however, is in the construction of the functions hm(x). We start
with only the constant function h0(X) = 1 in our model, and all functions
in the setCare candidate functions. This is depicted in Figure 9.10.
At each stage we consider as a new basis function pair all prod ucts of a
functionhmin the model set Mwith one of the reﬂected pairs in C. We
add to the model Mthe term of the form
ˆβM+1hℓ(X)·(Xj−t)++ˆβM+2hℓ(X)·(t−Xj)+, hℓ∈M,

9.4 MARS: Multivariate Adaptive Regression Splines 323
X1
X1X1
X1
X2X2
X2X2
X2
XpXpXpConstant
FIGURE 9.10. Schematic of the MARS forward model-building procedure. On
the left are the basis functions currently in the model: initial ly, this is the constant
functionh(X) = 1. On the right are all candidate basis functions to be conside red
in building the model. These are pairs of piecewise linear basis functions as in
Figure 9.9, with knots tat all unique observed values xijof each predictor Xj.
At each stage we consider all products of a candidate pair wit h a basis function
in the model. The product that decreases the residual error th e most is added into
the current model. Above we illustrate the ﬁrst three steps of t he procedure, with
the selected functions shown in red.

324 9. Additive Models, Trees, and Related Methods
X1X2h(X1,X2)
FIGURE 9.11. The function h(X1,X2) = (X1−x51)+·(x72−X2)+, resulting
from multiplication of two piecewise linear MARS basis functio ns.
that produces the largest decrease in training error. Here ˆβM+1andˆβM+2
are coeﬃcients estimated by least squares, along with all th e otherM+1
coeﬃcients in the model. Then the winning products are added to the
model and the process is continued until the model set Mcontains some
preset maximum number of terms.
Forexample,attheﬁrststageweconsideraddingtothemodel afunction
of the form β1(Xj−t)++β2(t−Xj)+;t∈{xij}, since multiplication by
the constant function just produces the function itself. Su ppose the best
choice is ˆβ1(X2−x72)++ˆβ2(x72−X2)+. Then this pair of basis functions
is added to the set M, and at the next stage we consider including a pair
of products the form
hm(X)·(Xj−t)+andhm(X)·(t−Xj)+, t∈{xij},
where forhmwe have the choices
h0(X) = 1,
h1(X) = (X2−x72)+,or
h2(X) = (x72−X2)+.
The third choice produces functions such as ( X1−x51)+·(x72−X2)+,
depicted in Figure 9.11.
At the end of this process we have a large model of the form (9.1 9). This
model typically overﬁts the data, and so a backward deletion procedure
is applied. The term whose removal causes the smallest incre ase in resid-
ual squared error is deleted from the model at each stage, pro ducing an
estimated best model ˆfλof each size (number of terms) λ. One could use
cross-validation to estimate the optimal value of λ, but for computational

9.4 MARS: Multivariate Adaptive Regression Splines 325
savingstheMARSprocedureinsteadusesgeneralizedcross- validation.This
criterion is deﬁned as
GCV(λ) =/summationtextN
i=1(yi−ˆfλ(xi))2
(1−M(λ)/N)2. (9.20)
The valueM(λ) is the eﬀective number of parameters in the model: this
accounts both for the number of terms in the models, plus the n umber
of parameters used in selecting the optimal positions of the knots. Some
mathematical and simulation results suggest that one shoul d pay a price
of three parameters for selecting a knot in a piecewise linea r regression.
Thus if there are rlinearly independent basis functions in the model, and
Kknotswereselectedintheforwardprocess,theformulais M(λ) =r+cK,
wherec= 3. (When the model is restricted to be additive—details belo w—
a penalty of c= 2 is used). Using this, we choose the model along the
backward sequence that minimizes GCV( λ).
Whythesepiecewiselinearbasisfunctions,andwhythispar ticularmodel
strategy? A key property of the functions of Figure 9.9 is the ir ability to
operate locally; they are zero over part of their range. When they are mul-
tiplied together, as in Figure 9.11, the result is nonzero on ly over the small
part of the feature space where both component functions are nonzero. As
a result, the regression surface is built up parsimoniously , using nonzero
components locally—only where they are needed. This is impor tant, since
one should “spend” parameters carefully in high dimensions , as they can
runoutquickly.Theuseofotherbasisfunctionssuchaspoly nomials,would
produce a nonzero product everywhere, and would not work as w ell.
The second important advantage of the piecewise linear basi s function
concerns computation. Consider the product of a function in Mwith each
of theNreﬂected pairs for an input Xj. This appears to require the ﬁtting
ofNsingle-input linear regression models, each of which uses O(N) oper-
ations, making a total of O(N2) operations. However, we can exploit the
simple form of the piecewise linear function. We ﬁrst ﬁt the r eﬂected pair
with rightmost knot. As the knot is moved successively one po sition at a
time to the left, the basis functions diﬀer by zero over the le ft part of the
domain, and by a constant over the right part. Hence after eac h such move
we can update the ﬁt in O(1) operations. This allows us to try every knot
in onlyO(N) operations.
The forward modeling strategy in MARS is hierarchical, in th e sense that
multiway products are built up from products involving term s already in
themodel.Forexample,afour-wayproductcanonlybeaddedt othemodel
if one of its three-way components is already in the model. Th e philosophy
hereisthatahigh-orderinteractionwilllikelyonlyexist ifsomeofitslower-
order “footprints” exist as well. This need not be true, but i s a reasonable
working assumption and avoids the search over an exponentia lly growing
space of alternatives.

326 9. Additive Models, Trees, and Related Methods
Rank of ModelTest Misclassification Error
0 20 40 60 80 1000.1 0.2 0.3 0.4
• • • • • • • • • • •••••• •• •• • • •••• • ••• • • •• • ••••• ••••••• ••••••••••••••• ••• ••••• ••••••••••••••••••••••••••••••
0.055GCV choice
FIGURE 9.12. Spam data: test error misclassiﬁcation rate for the MARS pro-
cedure, as a function of the rank (number of independent basi s functions) in the
model.
There is one restriction put on the formation of model terms: each input
can appear at most once in a product. This prevents the format ion of
higher-order powers of an input, which increase or decrease too sharply
near the boundaries of the feature space. Such powers can be a pproximated
in a more stable way with piecewise linear functions.
A useful option in the MARS procedure is to set an upper limit o n
the order of interaction. For example, one can set a limit of t wo, allowing
pairwise products of piecewise linear functions, but not th ree- or higher-
way products. This can aid in the interpretation of the ﬁnal m odel. An
upper limit of one results in an additive model.
9.4.1 Spam Example (Continued)
We applied MARS to the “spam” data analyzed earlier in this ch apter. To
enhanceinterpretability,werestrictedMARStosecond-de greeinteractions.
Although the target is a two-class variable, we used the squa red-error loss
function nonetheless (see Section 9.4.3). Figure 9.12 show s the test error
misclassiﬁcation rate as a function of the rank (number of in dependent ba-
sis functions) in the model. The error rate levels oﬀ at about 5.5%, which is
slightly higher than that of the generalized additive model (5.3%) discussed
earlier. GCV chose a model size of 60, which is roughly the sma llest model
giving optimal performance. The leading interactions foun d by MARS in-
volved inputs ( ch$, remove ), (ch$, free ) and (hp, CAPTOT ). However, these
interactions give no improvement in performance over the ge neralized ad-
ditive model.

9.4 MARS: Multivariate Adaptive Regression Splines 327
9.4.2 Example (Simulated Data)
Here we examine the performance of MARS in three contrasting scenarios.
There areN= 100 observations, and the predictors X1,X2,...,X pand
errorsεhave independent standard normal distributions.
Scenario 1: The data generation model is
Y= (X1−1)++(X1−1)+·(X2−.8)++0.12·ε.(9.21)
The noise standard deviation 0.12 was chosen so that the sign al-to-
noise ratio was about 5. We call this the tensor-product scen ario; the
product term gives a surface that looks like that of Figure 9. 11.
Scenario2: Thisisthesameasscenario1,butwith p= 20totalpredictors;
that is, there are 18 inputs that are independent of the respo nse.
Scenario 3: This has the structure of a neural network:
ℓ1=X1+X2+X3+X4+X5,
ℓ2=X6−X7+X8−X9+X10,
σ(t) = 1/(1+e−t),
Y=σ(ℓ1)+σ(ℓ2)+0.12·ε.(9.22)
Scenarios 1 and 2 are ideally suited for MARS, while scenario 3 contains
high-order interactions and may be diﬃcult for MARS to appro ximate. We
ran ﬁve simulations from each model, and recorded the result s.
In scenario 1, MARS typically uncovered the correct model al most per-
fectly. In scenario 2, it found the correct structure but als o found a few
extraneous terms involving other predictors.
Letµ(x) be the true mean of Y, and let
MSE0= ave x∈Test(¯y−µ(x))2,
MSE = ave x∈Test(ˆf(x)−µ(x))2.(9.23)
These represent the mean-square error of the constant model and the ﬁtted
MARSmodel,estimatedbyaveragingatthe1000testvaluesof x.Table9.4
shows the proportional decrease in model error or R2for each scenario:
R2=MSE0−MSE
MSE0. (9.24)
The values shown are means and standard error over the ﬁve sim ulations.
The performance of MARS is degraded only slightly by the incl usion of the
useless inputs in scenario 2; it performs substantially wor se in scenario 3.

328 9. Additive Models, Trees, and Related Methods
TABLE 9.4. Proportional decrease in model error ( R2) when MARS is applied
to three diﬀerent scenarios.
Scenario Mean (S.E.)
1: Tensor product p= 2 0.97 (0.01)
2: Tensor product p= 20 0.96 (0.01)
3: Neural network 0.79 (0.01)
9.4.3 Other Issues
MARS for Classiﬁcation
The MARS method and algorithm can be extended to handle class iﬁcation
problems. Several strategies have been suggested.
For two classes, one can code the output as 0/1 and treat the pr oblem as
a regression; we did this for the spamexample. For more than two classes,
one can use the indicator response approach described in Sec tion 4.2. One
codes theKresponse classes via 0/1 indicator variables, and then per-
forms a multi-response MARS regression. For the latter we us e a common
set of basis functions for all response variables. Classiﬁc ation is made to
the class with the largest predicted response value. There a re, however, po-
tential masking problems with this approach, as described i n Section 4.2.
A generally superior approach is the “optimal scoring” meth od discussed
in Section 12.5.
Stoneetal.(1997)developedahybridofMARScalledPolyMAR Sspecif-
icallydesignedtohandleclassiﬁcationproblems.Itusest hemultiplelogistic
framework described in Section 4.4. It grows the model in a fo rward stage-
wise fashion like MARS, but at each stage uses a quadratic app roximation
to the multinomial log-likelihood to search for the next bas is-function pair.
Once found, the enlarged model is ﬁt by maximum likelihood, a nd the
process is repeated.
Relationship of MARS to CART
Although they might seem quite diﬀerent, the MARS and CART st rategies
actuallyhavestrongsimilarities.SupposewetaketheMARS procedureand
make the following changes:
•Replacethepiecewiselinearbasisfunctionsbystepfuncti onsI(x−t>
0) andI(x−t≤0).
•When a model term is involved in a multiplication by a candida te
term, it gets replaced by the interaction, and hence is not av ailable
for further interactions.
Withthesechanges,theMARSforwardprocedureisthesameas theCART
tree-growing algorithm. Multiplying a step function by a pa ir of reﬂected

9.5 Hierarchical Mixtures of Experts 329
step functions is equivalent to splitting a node at the step. The second
restriction implies that a node may not be split more than onc e, and leads
to the attractive binary-tree representation of the CART mo del. On the
other hand, it is this restriction that makes it diﬃcult for C ART to model
additive structures. MARS forgoes the tree structure and ga ins the ability
to capture additive eﬀects.
Mixed Inputs
Mars can handle “mixed” predictors—quantitative and qualit ative—in a
natural way, much like CART does. MARS considers all possibl e binary
partitions of the categories for a qualitative predictor in to two groups.
Each such partition generates a pair of piecewise constant b asis functions—
indicator functions for the two sets of categories. This bas is pair is now
treated as any other, and is used in forming tensor products w ith other
basis functions already in the model.
9.5 Hierarchical Mixtures of Experts
The hierarchical mixtures of experts (HME) procedure can be viewed as a
variant of tree-based methods. The main diﬀerence is that th e tree splits
are not hard decisions but rather soft probabilistic ones. A t each node an
observation goes left or right with probabilities dependin g on its input val-
ues. This has some computational advantages since the resul ting parameter
optimization problem is smooth, unlike the discrete split p oint search in the
tree-based approach. The soft splits might also help in pred iction accuracy
and provide a useful alternative description of the data.
There are other diﬀerences between HMEs and the CART impleme nta-
tion of trees. In an HME, a linear (or logistic regression) mo del is ﬁt in
each terminal node, instead of a constant as in CART. The spli ts can be
multiway, not just binary, and the splits are probabilistic functions of a
linear combination of inputs, rather than a single input as i n the standard
use of CART. However, the relative merits of these choices ar e not clear,
and most were discussed at the end of Section 9.2.
Asimpletwo-levelHMEmodelinshowninFigure9.13.Itcanbe thought
of as a tree with soft splits at each non-terminal node. Howev er, the inven-
tors of this methodology use a diﬀerent terminology. The ter minal nodes
are called experts, and the non-terminal nodes are called gating networks .
The idea is that each expert provides an opinion (prediction ) about the
response, and these are combined together by the gating netw orks. As we
will see, the model is formally a mixture model, and the two-l evel model
in the ﬁgure can be extend to multiple levels, hence the name hierarchical
mixtures of experts .

330 9. Additive Models, Trees, and Related Methods
g1 g2
g1|1 g2|1g1|2 g2|2Gating GatingGatingGating
Gating GatingGating GatingGating
Network Network NetworkNetwork
Network
NetworkNetwork
Network Network NetworkNetwork
NetworkNetwork NetworkNetwork Network
Network Network NetworkNetwork
Expert Expert Expert Expert Expert Expert Expert Expert Expert Expert Expert
Pr(y|x,θ11) Pr( y|x,θ21) Pr( y|x,θ12) Pr(y|x,θ22)
FIGURE 9.13. A two-level hierarchical mixture of experts (HME) model.
Consider the regression or classiﬁcation problem, as descr ibed earlier in
the chapter. The data is ( xi,yi),i= 1,2,...,N, withyieither a continuous
or binary-valued response, and xia vector-valued input. For ease of nota-
tion we assume that the ﬁrst element of xiis one, to account for intercepts.
Here is how an HME is deﬁned. The top gating network has the out put
gj(x,γj) =eγT
jx
/summationtextK
k=1eγT
kx, j= 1,2,...,K, (9.25)
where each γjis a vector of unknown parameters. This represents a soft
K-way split ( K= 2 in Figure 9.13.) Each gj(x,γj) is the probability of
assigning an observation with feature vector xto thejth branch. Notice
that withK= 2 groups, if we take the coeﬃcient of one of the elements of
xto be +∞, then we get a logistic curve with inﬁnite slope. In this case ,
the gating probabilities are either 0 or 1, corresponding to a hard split on
that input.
At the second level, the gating networks have a similar form:
gℓ|j(x,γjℓ) =eγT
jℓx
/summationtextK
k=1eγT
jkx, ℓ= 1,2,...,K. (9.26)

9.5 Hierarchical Mixtures of Experts 331
This is the probability of assignment to the ℓth branch, given assignment
to thejth branch at the level above.
Ateachexpert(terminalnode),wehaveamodelfortherespon sevariable
of the form
Y∼Pr(y|x,θjℓ). (9.27)
This diﬀers according to the problem.
Regression: The Gaussian linear regression model is used, with θjℓ=
(βjℓ,σ2
jℓ):
Y=βT
jℓx+εandε∼N(0,σ2
jℓ). (9.28)
Classiﬁcation: The linear logistic regression model is used:
Pr(Y= 1|x,θjℓ) =1
1+e−θT
jℓx. (9.29)
Denoting the collection of all parameters by Ψ = {γj,γjℓ,θjℓ}, the total
probability that Y=yis
Pr(y|x,Ψ) =K/summationdisplay
j=1gj(x,γj)K/summationdisplay
ℓ=1gℓ|j(x,γjℓ)Pr(y|x,θjℓ).(9.30)
This is a mixture model, with the mixture probabilities dete rmined by the
gating network models.
To estimate the parameters, we maximize the log-likelihood of the data,/summationtext
ilogPr(yi|xi,Ψ), over the parameters in Ψ. The most convenient method
for doing this is the EM algorithm, which we describe for mixt ures in
Section 8.5. We deﬁne latent variables ∆ j, all of which are zero except for
a single one. We interpret these as the branching decisions m ade by the top
level gating network. Similarly we deﬁne latent variables ∆ ℓ|jto describe
the gating decisions at the second level.
In the E-step, the EM algorithm computes the expectations of the ∆j
and ∆ ℓ|jgiven the current values of the parameters. These expectati ons
are then used as observation weights in the M-step of the proc edure, to
estimate the parameters in the expert networks. The paramet ers in the
internal nodes are estimated by a version of multiple logist ic regression.
The expectations of the ∆ jor ∆ℓ|jare probability proﬁles, and these are
used as the response vectors for these logistic regressions .
The hierarchical mixtures of experts approach is a promisin g competitor
to CART trees. By using soft splits rather than hard decision rules it can
capturesituationswherethetransitionfromlowtohighres ponseisgradual.
The log-likelihood is a smooth function of the unknown weigh ts and hence
is amenable to numerical optimization. The model is similar to CART with
linear combination splits, but the latter is more diﬃcult to optimize. On

332 9. Additive Models, Trees, and Related Methods
the other hand, to our knowledge there are no methods for ﬁndi ng a good
tree topology for the HME model, as there are in CART. Typical ly one uses
a ﬁxed tree of some depth, possibly the output of the CART proc edure.
The emphasis in the research on HMEs has been on prediction ra ther than
interpretation of the ﬁnal model. A close cousin of the HME is thelatent
class model (Lin et al., 2000), which typically has only one layer; here
the nodes or latent classes are interpreted as groups of subj ects that show
similar response behavior.
9.6 Missing Data
It is quite common to have observations with missing values f or one or more
input features. The usual approach is to impute (ﬁll-in) the missing values
in some way.
However,theﬁrstissueindealingwiththeproblemisdeterm iningwheth-
er the missing data mechanism has distorted the observed dat a. Roughly
speaking, data are missing at random if the mechanism result ing in its
omission is independent of its (unobserved) value. A more pr ecise deﬁnition
is given in Little and Rubin (2002). Suppose yis the response vector and X
is theN×pmatrix of inputs (some of which are missing). Denote by Xobs
the observed entries in Xand letZ= (y,X),Zobs= (y,Xobs). Finally, if R
is an indicator matrix with ijth entry 1 if xijis missing and zero otherwise,
then the data is said to be missing at random (MAR) if the distribution of
Rdepends on the data Zonly through Zobs:
Pr(R|Z,θ) = Pr(R|Zobs,θ). (9.31)
Hereθare any parameters in the distribution of R. Data are said to be
missing completely at random (MCAR) if the distribution of Rdoesn’t
depend on the observed or missing data:
Pr(R|Z,θ) = Pr(R|θ). (9.32)
MCAR is a stronger assumption than MAR: most imputation meth ods rely
on MCAR for their validity.
Forexample,ifapatient’smeasurementwasnottakenbecaus ethedoctor
felt he was too sick, that observation would not be MAR or MCAR . In this
casethemissingdatamechanismcausesourobservedtrainin gdatatogivea
distorted picture of the true population, and data imputati on is dangerous
in this instance. Often the determination of whether featur es are MCAR
must be made from information about the data collection proc ess. For
categorical features, one way to diagnose this problem is to code “missing”
as an additional class. Then we ﬁt our model to the training da ta and see
if class “missing” is predictive of the response.

9.6 Missing Data 333
Assuming the features are missing completely at random, the re are a
number of ways of proceeding:
1. Discard observations with any missing values.
2. Rely on the learning algorithm to deal with missing values in its
training phase.
3. Impute all missing values before training.
Approach (1) can be used if the relative amount of missing dat a is small,
but otherwise should be avoided. Regarding (2), CART is one l earning
algorithmthatdealseﬀectivelywithmissingvalues,throu ghsurrogate splits
(Section 9.2.4). MARS and PRIM use similar approaches. In ge neralized
additive modeling, all observations missing for a given inp ut feature are
omitted when the partial residuals are smoothed against tha t feature in
the backﬁtting algorithm, and their ﬁtted values are set to z ero. Since the
ﬁtted curves have mean zero (when the model includes an inter cept), this
amounts to assigning the average ﬁtted value to the missing o bservations.
For most learning methods, the imputation approach (3) is ne cessary.
The simplest tactic is to impute the missing value with the me an or median
of the nonmissing values for that feature. (Note that the abo ve procedure
for generalized additive models is analogous to this.)
If the features have at least some moderate degree of depende nce, one
can do better by estimating a predictive model for each featu re given the
other features and then imputing each missing value by its pr ediction from
the model. In choosing the learning method for imputation of the features,
one must remember that this choice is distinct from the metho d used for
predicting yfromX. Thus a ﬂexible, adaptive method will often be pre-
ferred, even for the eventual purpose of carrying out a linea r regression of y
onX. In addition, if there are many missing feature values in the training
set, the learning method must itself be able to deal with miss ing feature
values. CART therefore is an ideal choice for this imputatio n “engine.”
After imputation, missing values are typically treated as i f they were ac-
tually observed. This ignores the uncertainty due to the imp utation, which
will itself introduce additional uncertainty into estimat es and predictions
from the response model. One can measure this additional unc ertainty by
doing multiple imputations and hence creating many diﬀeren t training sets.
The predictive model for ycan be ﬁt to each training set, and the variation
across training sets can be assessed. If CART was used for the imputation
engine,themultipleimputations couldbedonebysamplingf romthevalues
in the corresponding terminal nodes.

334 9. Additive Models, Trees, and Related Methods
9.7 Computational Considerations
WithNobservations and ppredictors, additive model ﬁtting requires some
numbermpof applications of a one-dimensional smoother or regressio n
method. The required number of cycles mof the backﬁtting algorithm is
usually less than 20 and often less than 10, and depends on the amount
of correlation in the inputs. With cubic smoothing splines, for example,
NlogNoperations are needed for an initial sort and Noperations for the
spline ﬁt. Hence the total operations for an additive model ﬁ t ispNlogN+
mpN.
Trees require pNlogNoperations for an initial sort for each predictor,
and typically another pNlogNoperations for the split computations. If the
splits occurred near the edges of the predictor ranges, this number could
increase to N2p.
MARS requires Nm2+pmNoperations to add a basis function to a
model with mterms already present, from a pool of ppredictors. Hence to
build anM-term model requires NM3+pM2Ncomputations, which can
be quite prohibitive if Mis a reasonable fraction of N.
Each of the components of an HME are typically inexpensive to ﬁt at
each M-step: Np2for the regressions, and Np2K2for aK-class logistic
regression. The EM algorithm, however, can take a long time t o converge,
and so sizable HME models are considered costly to ﬁt.
Bibliographic Notes
The most comprehensive source for generalized additive mod els is the text
of that name by Hastie and Tibshirani (1990). Diﬀerent appli cations of
this work in medical problems are discussed in Hastie et al. ( 1989) and
Hastie and Herman (1990), and the software implementation i n Splus is
described in Chambers and Hastie (1991). Green and Silverma n (1994)
discuss penalization and spline models in a variety of setti ngs. Efron and
Tibshirani (1991) give an exposition of modern development s in statistics
(including generalized additive models), for a nonmathema tical audience.
Classiﬁcation and regression trees date back at least as far as Morgan and
Sonquist (1963). We have followed the modern approaches of B reiman et
al. (1984) and Quinlan (1993). The PRIM method is due to Fried man
and Fisher (1999), while MARS is introduced in Friedman (199 1), with an
additiveprecursorinFriedmanandSilverman(1989).Hiera rchicalmixtures
of experts were proposed in Jordan and Jacobs (1994); see als o Jacobs et
al. (1991).

Exercises 335
Exercises
Ex. 9.1Show that a smoothing spline ﬁt of yitoxipreserves the linear
partof the ﬁt. In other words, if yi= ˆyi+ri, where ˆyirepresents the
linear regression ﬁts, and Sis the smoothing matrix, then Sy=ˆy+Sr.
Show that the same is true for local linear regression (Secti on 6.1.1). Hence
argue that the adjustment step in the second line of (2) in Alg orithm 9.1
is unnecessary.
Ex. 9.2LetAbe a known k×kmatrix,bbe a known k-vector, and z
be an unknown k-vector. A Gauss–Seidel algorithm for solving the linear
system of equations Az=bworks by successively solving for element zjin
thejth equation, ﬁxing all other zj’s at their current guesses. This process
is repeated for j= 1,2,...,k,1,2,...,k,..., until convergence (Golub and
Van Loan, 1983).
(a) Consider an additive model with Nobservations and pterms, with
thejth term to be ﬁt by a linear smoother Sj. Consider the following
system of equations:

I S1S1···S1
S2I S2···S2
...............
SpSpSp···I

f1
f2
...
fp
=
S1y
S2y
...
Spy
. (9.33)
Here each fjis anN-vector of evaluations of the jth function at
the data points, and yis anN-vector of the response values. Show
that backﬁtting is a blockwise Gauss–Seidel algorithm for s olving this
system of equations.
(b) LetS1andS2be symmetric smoothing operators (matrices) with
eigenvalues in [0 ,1). Consider a backﬁtting algorithm with response
vectoryand smoothers S1,S2. Show that with any starting values,
the algorithm converges and give a formula for the ﬁnal itera tes.
Ex.9.3Backﬁtting equations. Considerabackﬁttingprocedurewithorthog-
onal projections, and let Dbe the overall regression matrix whose columns
spanV=Lcol(S1)⊕Lcol(S2)⊕···⊕L col(Sp), whereLcol(S) denotes the
column space of a matrix S. Show that the estimating equations

I S1S1···S1
S2I S2···S2
...............
SpSpSp···I

f1
f2
...
fp
=
S1y
S2y
...
Spy

are equivalent to the least squares normal equations DTDβ=DTywhere
βis the vector of coeﬃcients.

336 9. Additive Models, Trees, and Related Methods
Ex. 9.4Suppose the same smoother Sis used to estimate both terms in a
two-term additive model (i.e., both variables are identica l). Assume that S
is symmetric with eigenvalues in [0 ,1). Show that the backﬁtting residual
converges to ( I+S)−1(I−S)y, and that the residual sum of squares con-
verges upward. Can the residual sum of squares converge upwa rd in less
structured situations? How does this ﬁt compare to the ﬁt wit h a single
term ﬁt by S? [Hint: Use the eigen-decomposition of Sto help with this
comparison.]
Ex. 9.5Degrees of freedom of a tree . Given data yiwith mean f(xi) and
varianceσ2, and a ﬁtting operation y→ˆy, let’s deﬁne the degrees of
freedom of a ﬁt by/summationtext
icov(yi,ˆyi)/σ2.
Consider a ﬁt ˆyestimated by a regression tree, ﬁt to a set of predictors
X1,X2,...,X p.
(a) In terms of the number of terminal nodes m, give a rough formula for
the degrees of freedom of the ﬁt.
(b) Generate 100 observations with predictors X1,X2,...,X 10as inde-
pendent standard Gaussian variates and ﬁx these values.
(c) Generate response values also as standard Gaussian ( σ2= 1), indepen-
dent of the predictors. Fit regression trees to the data of ﬁx ed size 1,5
and 10 terminal nodes and hence estimate the degrees of freed om of
each ﬁt. [Do ten simulations of the response and average the r esults,
to get a good estimate of degrees of freedom.]
(d) Compare your estimates of degrees of freedom in (a) and (c ) and
discuss.
(e) If the regression tree ﬁt were a linear operation, we coul d writeˆy=Sy
for some matrix S. Then the degrees of freedom would be tr( S).
Suggest a way to compute an approximate Smatrix for a regression
tree, compute it and compare the resulting degrees of freedo m to
those in (a) and (c).
Ex. 9.6Consider the ozone data of Figure 6.9.
(a) Fit an additive model to the cube root of ozone concentrat ion. as a
function of temperature, wind speed, and radiation. Compar e your
results to those obtained via the trellis display in Figure 6 .9.
(b)Fittrees,MARS,andPRIMtothesamedata,andcomparethe results
to those found in (a) and in Figure 6.9.

This is page 337
Printer: Opaque this
10
Boosting and Additive Trees
10.1 Boosting Methods
Boosting is one of the most powerful learning ideas introduc ed in the last
twenty years. It was originally designed for classiﬁcation problems, but as
will be seen in this chapter, it can proﬁtably be extended to r egression
as well. The motivation for boosting was a procedure that com bines the
outputs of many “weak” classiﬁers to produce a powerful “com mittee.”
From this perspective boosting bears a resemblance to baggi ng and other
committee-based approaches (Section 8.8). However we shal l see that the
connection is at best superﬁcial and that boosting is fundam entally diﬀer-
ent.
We begin by describing the most popular boosting algorithm d ue to
Freund and Schapire (1997) called “AdaBoost.M1.” Consider a two-class
problem, with the output variable coded as Y∈{−1,1}. Given a vector of
predictor variables X, a classiﬁer G(X) produces a prediction taking one
of the two values {−1,1}. The error rate on the training sample is
err =1
NN/summationdisplay
i=1I(yi∝ne}ationslash=G(xi)),
and the expected error rate on future predictions is E XYI(Y∝ne}ationslash=G(X)).
A weak classiﬁer is one whose error rate is only slightly bett er than
random guessing. The purpose of boosting is to sequentially apply the
weak classiﬁcation algorithm to repeatedly modiﬁed versio ns of the data,
thereby producing a sequence of weak classiﬁers Gm(x),m= 1,2,...,M.

338 10. Boosting and Additive Trees
Training  SampleWeighted  SampleWeighted  SampleWeighted  Sample
Training  SampleWeighted  SampleWeighted  SampleWeighted  SampleWeighted  Sample
Training  SampleWeighted  Sample
Training  SampleWeighted  SampleWeighted  SampleWeighted  Sample
Weighted  SampleWeighted  SampleWeighted  Sample
Training  SampleWeighted  SampleG(x) = sign/bracketleftig/summationtextM
m=1αmGm(x)/bracketrightig
GM(x)
G3(x)
G2(x)
G1(x)Final Classifier
FIGURE 10.1. Schematic of AdaBoost. Classiﬁers are trained on weighted ve r-
sions of the dataset, and then combined to produce a ﬁnal pred iction.
The predictions from all of them are then combined through a w eighted
majority vote to produce the ﬁnal prediction:
G(x) = sign/parenleftiggM/summationdisplay
m=1αmGm(x)/parenrightigg
. (10.1)
Hereα1,α2,...,α Mare computed by the boosting algorithm, and weight
the contribution of each respective Gm(x). Their eﬀect is to give higher
inﬂuence to the more accurate classiﬁers in the sequence. Fi gure 10.1 shows
a schematic of the AdaBoost procedure.
The data modiﬁcations at each boosting step consist of apply ing weights
w1,w2,...,w Nto each of the training observations ( xi,yi), i= 1,2,...,N.
Initially all of the weights are set to wi= 1/N, so that the ﬁrst step simply
trains the classiﬁer on the data in the usual manner. For each successive
iterationm= 2,3,...,Mthe observation weights are individually modi-
ﬁed and the classiﬁcation algorithm is reapplied to the weig hted observa-
tions. At step m, those observations that were misclassiﬁed by the classiﬁe r
Gm−1(x)inducedatthepreviousstephavetheirweightsincreased, whereas
the weights are decreased for those that were classiﬁed corr ectly. Thus as
iterations proceed, observations that are diﬃcult to class ify correctly re-
ceive ever-increasing inﬂuence. Each successive classiﬁe r is thereby forced

10.1 Boosting Methods 339
Algorithm 10.1 AdaBoost.M1.
1. Initialize the observation weights wi= 1/N, i= 1,2,...,N.
2. Form= 1 toM:
(a) Fit a classiﬁer Gm(x) to the training data using weights wi.
(b) Compute
errm=/summationtextN
i=1wiI(yi∝ne}ationslash=Gm(xi))
/summationtextN
i=1wi.
(c) Compute αm= log((1−errm)/errm).
(d) Setwi←wi·exp[αm·I(yi∝ne}ationslash=Gm(xi))], i= 1,2,...,N.
3. OutputG(x) = sign/bracketleftig/summationtextM
m=1αmGm(x)/bracketrightig
.
to concentrate on those training observations that are miss ed by previous
ones in the sequence.
Algorithm 10.1 shows the details of the AdaBoost.M1 algorit hm. The
current classiﬁer Gm(x) is induced on the weighted observations at line 2a.
The resulting weighted error rate is computed at line 2b. Lin e 2c calculates
the weight αmgiven toGm(x) in producing the ﬁnal classiﬁer G(x) (line
3). The individual weights of each of the observations are up dated for the
next iteration at line 2d. Observations misclassiﬁed by Gm(x) have their
weights scaled by a factor exp( αm), increasing their relative inﬂuence for
inducing the next classiﬁer Gm+1(x) in the sequence.
The AdaBoost.M1 algorithm is known as “Discrete AdaBoost” i n Fried-
man et al. (2000), because the base classiﬁer Gm(x) returns a discrete class
label. If the base classiﬁer instead returns a real-valued p rediction (e.g.,
a probability mapped to the interval [ −1,1]), AdaBoost can be modiﬁed
appropriately (see “Real AdaBoost” in Friedman et al. (2000 )).
The power of AdaBoost to dramatically increase the performa nce of even
a very weak classiﬁer is illustrated in Figure 10.2. The feat uresX1,...,X 10
are standard independent Gaussian, and the deterministic t argetYis de-
ﬁned by
Y=/braceleftbigg
1 if/summationtext10
j=1X2
j>χ2
10(0.5),
−1 otherwise .(10.2)
Hereχ2
10(0.5) = 9.34 is the median of a chi-squared random variable with
10 degrees of freedom (sum of squares of 10 standard Gaussian s). There are
2000trainingcases,withapproximately1000casesineachc lass,and10,000
testobservations. Heretheweak classiﬁeris justa“stump” : atwoterminal-
node classiﬁcation tree. Applying this classiﬁer alone to t he training data
set yields a very poor test set error rate of 45.8%, compared t o 50% for

340 10. Boosting and Additive Trees
0 100 200 300 4000.0 0.1 0.2 0.3 0.4 0.5
Boosting IterationsTest ErrorSingle Stump
244 Node Tree
FIGURE 10.2. Simulated data (10.2): test error rate for boosting with stum ps,
as a function of the number of iterations. Also shown are the te st error rate for
a single stump, and a 244-node classiﬁcation tree.
random guessing. However, as boosting iterations proceed t he error rate
steadily decreases, reaching 5.8% after 400 iterations. Th us, boosting this
simple very weak classiﬁer reduces its prediction error rat e by almost a
factor of four. It also outperforms a single large classiﬁca tion tree (error
rate 24.7%). Since its introduction, much has been written to explai n the
success of AdaBoost in producing accurate classiﬁers. Most of this work
has centered on using classiﬁcation trees as the “base learn er”G(x), where
improvements are often most dramatic. In fact, Breiman (NIP S Workshop,
1996) referred to AdaBoost with trees as the “best oﬀ-the-sh elf classiﬁer in
the world” (see also Breiman (1998)). This is especially the case for data-
mining applications, as discussed more fully in Section 10. 7 later in this
chapter.
10.1.1 Outline of This Chapter
Here is an outline of the developments in this chapter:
•We show that AdaBoost ﬁts an additive model in a base learner,
optimizing a novel exponential loss function. This loss fun ction is

10.2 Boosting Fits an Additive Model 341
very similar to the (negative) binomial log-likelihood (Se ctions 10.2–
10.4).
•The population minimizer of the exponential loss function i s shown
to be the log-odds of the class probabilities (Section 10.5) .
•We describe loss functions for regression and classiﬁcatio n that are
more robust than squared error or exponential loss (Section 10.6).
•It is argued that decision trees are an ideal base learner for data
mining applications of boosting (Sections 10.7 and 10.9).
•We develop a class of gradient boosted models (GBMs), for boo sting
trees with any loss function (Section 10.10).
•The importance of “slow learning” is emphasized, and implem ented
by shrinkage of each new term that enters the model (Section 1 0.12),
as well as randomization (Section 10.12.2).
•Toolsforinterpretationoftheﬁttedmodelaredescribed(S ection10.13).
10.2 Boosting Fits an Additive Model
The success of boosting is really not very mysterious. The ke y lies in ex-
pression (10.1). Boosting is a way of ﬁtting an additive expa nsion in a set
of elementary “basis” functions. Here the basis functions a re the individual
classiﬁersGm(x)∈{−1,1}. More generally, basis function expansions take
the form
f(x) =M/summationdisplay
m=1βmb(x;γm), (10.3)
whereβm,m= 1,2,...,Mare the expansion coeﬃcients, and b(x;γ)∈IR
are usually simple functions of the multivariate argument x, characterized
by a set of parameters γ. We discuss basis expansions in some detail in
Chapter 5.
Additive expansions like this are at the heart of many of the l earning
techniques covered in this book:
•In single-hidden-layer neural networks (Chapter 11), b(x;γ) =σ(γ0+
γT
1x), whereσ(t) = 1/(1+e−t) is the sigmoid function, and γparam-
eterizes a linear combination of the input variables.
•Insignalprocessing,wavelets(Section5.9.1)areapopula rchoicewith
γparameterizing the location and scale shifts of a “mother” w avelet.
•Multivariate adaptive regression splines (Section 9.4) us es truncated-
power spline basis functions where γparameterizes the variables and
values for the knots.

342 10. Boosting and Additive Trees
Algorithm 10.2 Forward Stagewise Additive Modeling.
1. Initialize f0(x) = 0.
2. Form= 1 toM:
(a) Compute
(βm,γm) = argmin
β,γN/summationdisplay
i=1L(yi,fm−1(xi)+βb(xi;γ)).
(b) Setfm(x) =fm−1(x)+βmb(x;γm).
•For trees,γparameterizes the split variables and split points at the
internal nodes, and the predictions at the terminal nodes.
Typically these models are ﬁt by minimizing a loss function a veraged
over the training data, such as the squared-error or a likeli hood-based loss
function,
min
{βm,γm}M
1N/summationdisplay
i=1L/parenleftigg
yi,M/summationdisplay
m=1βmb(xi;γm)/parenrightigg
. (10.4)
For many loss functions L(y,f(x)) and/or basis functions b(x;γ), this re-
quires computationally intensive numerical optimization techniques. How-
ever, a simple alternative often can be found when it is feasi ble to rapidly
solve the subproblem of ﬁtting just a single basis function,
min
β,γN/summationdisplay
i=1L(yi,βb(xi;γ)). (10.5)
10.3 Forward Stagewise Additive Modeling
Forward stagewise modeling approximates the solution to (1 0.4) by sequen-
tially adding new basis functions to the expansion without a djusting the
parameters and coeﬃcients of those that have already been ad ded. This is
outlined in Algorithm 10.2. At each iteration m, one solves for the optimal
basis function b(x;γm) and corresponding coeﬃcient βmto add to the cur-
rent expansion fm−1(x). This produces fm(x), and the process is repeated.
Previously added terms are not modiﬁed.
For squared-error loss
L(y,f(x)) = (y−f(x))2, (10.6)

10.4 Exponential Loss and AdaBoost 343
one has
L(yi,fm−1(xi)+βb(xi;γ)) = (yi−fm−1(xi)−βb(xi;γ))2
= (rim−βb(xi;γ))2, (10.7)
whererim=yi−fm−1(xi) is simply the residual of the current model
on theith observation. Thus, for squared-error loss, the term βmb(x;γm)
that best ﬁts the current residuals is added to the expansion at each step.
This idea is the basis for “least squares” regression boosti ng discussed in
Section 10.10.2. However, as we show near the end of the next s ection,
squared-error loss is generally not a good choice for classi ﬁcation; hence
the need to consider other loss criteria.
10.4 Exponential Loss and AdaBoost
We now show that AdaBoost.M1 (Algorithm 10.1) is equivalent to forward
stagewise additive modeling (Algorithm 10.2) using the los s function
L(y,f(x)) = exp(−yf(x)). (10.8)
The appropriateness of this criterion is addressed in the ne xt section.
For AdaBoost the basis functions are the individual classiﬁ ersGm(x)∈
{−1,1}. Using the exponential loss function, one must solve
(βm,Gm) = argmin
β,GN/summationdisplay
i=1exp[−yi(fm−1(xi)+βG(xi))]
for the classiﬁer Gmand corresponding coeﬃcient βmto be added at each
step. This can be expressed as
(βm,Gm) = argmin
β,GN/summationdisplay
i=1w(m)
iexp(−βyiG(xi)) (10.9)
withw(m)
i= exp(−yifm−1(xi)). Since each w(m)
idepends neither on β
norG(x), it can be regarded as a weight that is applied to each observ a-
tion. This weight depends on fm−1(xi), and so the individual weight values
change with each iteration m.
The solution to (10.9) can be obtained in two steps. First, fo r any value
ofβ >0, the solution to (10.9) for Gm(x) is
Gm= argmin
GN/summationdisplay
i=1w(m)
iI(yi∝ne}ationslash=G(xi)), (10.10)

344 10. Boosting and Additive Trees
which is the classiﬁer that minimizes the weighted error rat e in predicting
y. This can be easily seen by expressing the criterion in (10.9 ) as
e−β·/summationdisplay
yi=G(xi)w(m)
i+eβ·/summationdisplay
yi/ne}ationslash=G(xi)w(m)
i,
which in turn can be written as
/parenleftbig
eβ−e−β/parenrightbig
·N/summationdisplay
i=1w(m)
iI(yi∝ne}ationslash=G(xi))+e−β·N/summationdisplay
i=1w(m)
i.(10.11)
Plugging this Gminto (10.9) and solving for βone obtains
βm=1
2log1−errm
errm, (10.12)
where err mis the minimized weighted error rate
errm=/summationtextN
i=1w(m)
iI(yi∝ne}ationslash=Gm(xi))
/summationtextN
i=1w(m)
i. (10.13)
The approximation is then updated
fm(x) =fm−1(x)+βmGm(x),
which causes the weights for the next iteration to be
w(m+1)
i=w(m)
i·e−βmyiGm(xi). (10.14)
Using the fact that −yiGm(xi) = 2·I(yi∝ne}ationslash=Gm(xi))−1, (10.14) becomes
w(m+1)
i=w(m)
i·eαmI(yi/ne}ationslash=Gm(xi))·e−βm, (10.15)
whereαm= 2βmis the quantity deﬁned at line 2(c) of AdaBoost.M1
(Algorithm 10.1). The factor e−βmin (10.15) multiplies all weights by the
same value, so it has no eﬀect. Thus (10.15) is equivalent to l ine 2(d) of
Algorithm 10.1.
One can view line 2(a) of the Adaboost.M1 algorithm as a metho d for
approximatelysolvingtheminimizationin(10.11)andhenc e(10.10).Hence
we conclude that AdaBoost.M1 minimizes the exponential los s criterion
(10.8) via a forward-stagewise additive modeling approach .
Figure 10.3 shows the training-set misclassiﬁcation error rate and aver-
age exponential loss for the simulated data problem (10.2) o f Figure 10.2.
The training-set misclassiﬁcation error decreases to zero at around 250 it-
erations (and remains there), but the exponential loss keep s decreasing.
Notice also in Figure 10.2 that the test-set misclassiﬁcati on error continues
to improve after iteration 250. Clearly Adaboost is not opti mizing training-
set misclassiﬁcation error; the exponential loss is more se nsitive to changes
in the estimated class probabilities.

10.5 Why Exponential Loss? 345
0 100 200 300 4000.0 0.2 0.4 0.6 0.8 1.0
Boosting IterationsTraining Error
Misclassification RateExponential Loss
FIGURE 10.3. Simulated data, boosting with stumps: misclassiﬁcation erro r
rate on the training set, and average exponential loss: (1/N)/summationtextN
i=1exp(−yif(xi)).
After about 250iterations, the misclassiﬁcation error is zero, while the exp onential
loss continues to decrease.
10.5 Why Exponential Loss?
The AdaBoost.M1 algorithm was originally motivated from a v ery diﬀer-
ent perspective than presented in the previous section. Its equivalence to
forward stagewise additive modeling based on exponential l oss was only
discovered ﬁve years after its inception. By studying the pr operties of the
exponential loss criterion, one can gain insight into the pr ocedure and dis-
cover ways it might be improved.
The principal attraction of exponential loss in the context of additive
modeling is computational; it leads to the simple modular re weighting Ad-
aBoost algorithm. However, it is of interest to inquire abou t its statistical
properties. What does it estimate and how well is it being est imated? The
ﬁrst question is answered by seeking its population minimiz er.
It is easy to show (Friedman et al., 2000) that
f∗(x) = argmin
f(x)EY|x(e−Yf(x)) =1
2logPr(Y= 1|x)
Pr(Y=−1|x),(10.16)

346 10. Boosting and Additive Trees
or equivalently
Pr(Y= 1|x) =1
1+e−2f∗(x).
Thus, the additive expansion produced by AdaBoost is estima ting one-
half the log-odds of P(Y= 1|x). This justiﬁes using its sign as the classiﬁ-
cation rule in (10.1).
Another loss criterion with the same population minimizer i s the bi-
nomial negative log-likelihood or deviance (also known as cross-entropy),
interpreting fas the logit transform. Let
p(x) = Pr(Y= 1|x) =ef(x)
e−f(x)+ef(x)=1
1+e−2f(x)(10.17)
and deﬁne Y′= (Y+1)/2∈{0,1}. Then the binomial log-likelihood loss
function is
l(Y,p(x)) =Y′logp(x)+(1−Y′)log(1−p(x)),
or equivalently the deviance is
−l(Y,f(x)) = log/parenleftig
1+e−2Yf(x)/parenrightig
. (10.18)
Since the population maximizer of log-likelihood is at the t rue probabilities
p(x) = Pr(Y= 1|x), we see from (10.17) that the population minimizers of
the deviance E Y|x[−l(Y,f(x))] and E Y|x[e−Yf(x)]are thesame. Thus, using
either criterion leads to the same solution at the populatio n level. Note that
e−Yfitself is not a proper log-likelihood, since it is not the log arithm of
any probability mass function for a binary random variable Y∈{−1,1}.
10.6 Loss Functions and Robustness
In this section we examine the diﬀerent loss functions for cl assiﬁcation and
regression more closely, and characterize them in terms of t heir robustness
to extreme data.
Robust Loss Functions for Classiﬁcation
Although both the exponential (10.8) and binomial deviance (10.18) yield
the same solution when applied to the population joint distr ibution, the
same is not true for ﬁnite data sets. Both criteria are monoto ne decreasing
functions of the “margin” yf(x). In classiﬁcation (with a −1/1 response)
themarginplaysaroleanalogoustotheresiduals y−f(x)inregression.The
classiﬁcation rule G(x) = sign[f(x)] implies that observations with positive
marginyif(xi)>0 are classiﬁed correctly whereas those with negative
marginyif(xi)<0 are misclassiﬁed. The decision boundary is deﬁned by

10.6 Loss Functions and Robustness 347
−2 −1 0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0Misclassification
Exponential
Binomial Deviance
Squared Error
Support VectorLoss
yf
FIGURE 10.4. Loss functions for two-class classiﬁcation. The response is
y=±1; the prediction is f, with class prediction sign(f). The losses are
misclassiﬁcation: I(sign(f)/ne}ationslash=y); exponential: exp(−yf); binomial deviance:
log(1+exp( −2yf)); squared error: (y−f)2; and support vector: (1−yf)+(see
Section 12.3). Each function has been scaled so that it passes through the point
(0,1).
f(x) = 0. The goal of the classiﬁcation algorithm is to produce po sitive
margins as frequently as possible. Any loss criterion used f or classiﬁcation
should penalize negative margins more heavily than positiv e ones since
positive margin observations are already correctly classi ﬁed.
Figure 10.4 shows both the exponential (10.8) and binomial d eviance
criteria as a function of the margin yf(x). Also shown is misclassiﬁcation
lossL(y,f(x)) =I(yf(x)<0), which gives unit penalty for negative mar-
gin values, and no penalty at all for positive ones. Both the e xponential
and deviance loss can be viewed as monotone continuous appro ximations
to misclassiﬁcation loss. They continuously penalize incr easingly negative
margin values more heavily than they reward increasingly po sitive ones.
The diﬀerence between them is in degree. The penalty associa ted with bi-
nomial deviance increases linearly for large increasingly negative margin,
whereas the exponential criterion increases the inﬂuence o f such observa-
tions exponentially.
At any point in the training process the exponential criteri on concen-
trates much more inﬂuence on observations with large negati ve margins.
Binomial deviance concentrates relatively less inﬂuence o n such observa-

348 10. Boosting and Additive Trees
tions, more evenly spreading the inﬂuence among all of the da ta. It is
therefore far more robust in noisy settings where the Bayes e rror rate is
not close to zero, and especially in situations where there i s misspeciﬁcation
of the class labels in the training data. The performance of A daBoost has
been empirically observed to dramatically degrade in such s ituations.
Also shown in the ﬁgure is squared-error loss. The minimizer of the cor-
responding risk on the population is
f∗(x) = argmin
f(x)EY|x(Y−f(x))2= E(Y|x) = 2·Pr(Y= 1|x)−1.(10.19)
As before the classiﬁcation rule is G(x) = sign[f(x)]. Squared-error loss
is not a good surrogate for misclassiﬁcation error. As seen i n Figure 10.4, it
is not a monotone decreasing function of increasing margin yf(x). For mar-
gin valuesyif(xi)>1 it increases quadratically, thereby placing increasing
inﬂuence (error) on observations that are correctly classi ﬁed with increas-
ing certainty, thereby reducing the relative inﬂuence of th ose incorrectly
classiﬁedyif(xi)<0. Thus, if class assignment is the goal, a monotone de-
creasing criterion serves as a better surrogate loss functi on. Figure 12.4 on
page 426 in Chapter 12 includes a modiﬁcation of quadratic lo ss, the “Hu-
berized” square hinge loss (Rosset et al., 2004b), which enj oys the favorable
properties of the binomial deviance, quadratic loss and the SVM hinge loss.
It has the same population minimizer as the quadratic (10.19 ), is zero for
yf(x)>1, and becomes linear for yf(x)<−1. Since quadratic functions
are easier to compute with than exponentials, our experienc e suggests this
to be a useful alternative to the binomial deviance.
WithK-class classiﬁcation, the response Ytakes values in the unordered
setG={G1,...,Gk}(see Sections 2.4 and 4.4). We now seek a classiﬁer
G(x) taking values in G. It is suﬃcient to know the class conditional proba-
bilitiespk(x) = Pr(Y=Gk|x),k= 1,2,...,K, for then the Bayes classiﬁer
is
G(x) =Gkwherek= argmax
ℓpℓ(x). (10.20)
In principal, though, we need not learn the pk(x), but simply which one is
largest. However, in data mining applications the interest is often more in
the class probabilities pℓ(x), ℓ= 1,...,Kthemselves, rather than in per-
forming a class assignment. As in Section 4.4, the logistic m odel generalizes
naturally to Kclasses,
pk(x) =efk(x)
/summationtextK
l=1efl(x), (10.21)
which ensures that 0 ≤pk(x)≤1 and that they sum to one. Note that
here we have Kdiﬀerent functions, one per class. There is a redundancy
in the functions fk(x), since adding an arbitrary h(x) to each leaves the
model unchanged. Traditionally one of them is set to zero: fo r example,

10.6 Loss Functions and Robustness 349
fK(x) = 0, as in (4.17). Here we prefer to retain the symmetry, and i mpose
the constraint/summationtextK
k=1fk(x) = 0. The binomial deviance extends naturally
to theK-classmultinomial deviance loss function:
L(y,p(x)) =−K/summationdisplay
k=1I(y=Gk)logpk(x)
=−K/summationdisplay
k=1I(y=Gk)fk(x)+log/parenleftiggK/summationdisplay
ℓ=1efℓ(x)/parenrightigg
.(10.22)
Asinthetwo-classcase,thecriterion(10.22)penalizesin correctpredictions
only linearly in their degree of incorrectness.
Zhu et al. (2005) generalize the exponential loss for K-class problems.
See Exercise 10.5 for details.
Robust Loss Functions for Regression
In the regression setting, analogous to the relationship be tween exponential
loss and binomial log-likelihood is the relationship betwe en squared-error
lossL(y,f(x)) = (y−f(x))2and absolute loss L(y,f(x)) =|y−f(x)|. The
population solutions are f(x) = E(Y|x) for squared-error loss, and f(x) =
median(Y|x) for absolute loss; for symmetric error distributions thes e are
the same. However, on ﬁnite samples squared-error loss plac es much more
emphasis on observations with large absolute residuals |yi−f(xi)|during
the ﬁtting process. It is thus far less robust, and its perfor mance severely
degrades for long-tailed error distributions and especial ly for grossly mis-
measuredy-values (“outliers”). Other more robust criteria, such as a bso-
lute loss, perform much better in these situations. In the st atistical ro-
bustness literature, a variety of regression loss criteria have been proposed
that provide strong resistance (if not absolute immunity) t o gross outliers
while being nearly as eﬃcient as least squares for Gaussian e rrors. They
are often better than either for error distributions with mo derately heavy
tails. One such criterion is the Huber loss criterion used fo r M-regression
(Huber, 1964)
L(y,f(x)) =/braceleftbigg
[y−f(x)]2for|y−f(x)|≤δ,
2δ|y−f(x)|−δ2otherwise.(10.23)
Figure 10.5 compares these three loss functions.
These considerations suggest that when robustness is a conc ern, as is
especially the case in data mining applications (see Sectio n 10.7), squared-
error loss for regression and exponential loss for classiﬁc ation are not the
best criteria from a statistical perspective. However, the y both lead to the
elegant modular boosting algorithms in the context of forwa rd stagewise
additive modeling. For squared-error loss one simply ﬁts th e base learner
to the residuals from the current model yi−fm−1(xi) at each step. For

350 10. Boosting and Additive Trees
−3 −2 −1 0 1 2 30 2 4 6 8Squared Error
Absolute Error
HuberLoss
y−f
FIGURE 10.5. A comparison of three loss functions for regression, plotted a s a
function of the margin y−f. The Huber loss function combines the good properties
of squared-error loss near zero and absolute error loss when |y−f|is large.
exponential loss one performs a weighted ﬁt of the base learn er to the
output values yi, with weights wi= exp(−yifm−1(xi)). Using other more
robust criteria directly in their place does not give rise to such simple
feasible boosting algorithms. However, in Section 10.10.2 we show how one
can derive simple elegant boosting algorithms based on any d iﬀerentiable
losscriterion,therebyproducinghighlyrobustboostingp roceduresfordata
mining.
10.7 “Oﬀ-the-Shelf” Procedures for Data Mining
Predictive learning is an important aspect of data mining. A s can be seen
from this book, a wide variety of methods have been developed for predic-
tive learning from data. For each particular method there ar e situations
for which it is particularly well suited, and others where it performs badly
compared to the best that can be done with that data. We have at tempted
to characterize appropriate situations in our discussions of each of the re-
spective methods. However, it is seldom known in advance whi ch procedure
willperformbestorevenwellforanygivenproblem.Table10 .1summarizes
some of the characteristics of a number of learning methods.
Industrial and commercial data mining applications tend to be especially
challenging in terms of the requirements placed on learning procedures.
Data sets are often very large in terms of number of observati ons and
number of variables measured on each of them. Thus, computat ional con-

10.7 “Oﬀ-the-Shelf” Procedures for Data Mining 351
TABLE 10.1. Some characteristics of diﬀerent learning methods. Key: ▲= good,
◆=fair, and ▼=poor.
Characteristic Neural SVM Trees MARS k-NN,
Nets Kernels
Natural handling of data
of “mixed” type▼ ▼ ▲ ▲ ▼
Handlingofmissingvalues ▼ ▼ ▲ ▲ ▲
Robustness to outliers in
input space▼ ▼ ▲ ▼ ▲
Insensitive to monotone
transformations of inputs▼ ▼ ▲ ▼ ▼
Computational scalability
(largeN)▼ ▼ ▲ ▲ ▼
Ability to deal with irrel-
evant inputs▼ ▼ ▲ ▲ ▼
Ability to extract linear
combinations of features▲ ▲ ▼ ▼ ◆
Interpretability ▼ ▼ ◆ ▲ ▼
Predictive power ▲ ▲ ▼ ◆ ▲
siderations play an important role. Also, the data are usual lymessy: the
inputs tend to be mixtures of quantitative, binary, and cate gorical vari-
ables, the latter often with many levels. There are generall y many missing
values, complete observations being rare. Distributions o f numeric predic-
tor and response variables are often long-tailed and highly skewed. This
is the case for the spam data (Section 9.1.2); when ﬁtting a ge neralized
additive model, we ﬁrst log-transformed each of the predict ors in order to
get a reasonable ﬁt. In addition they usually contain a subst antial fraction
of gross mis-measurements (outliers). The predictor varia bles are generally
measured on very diﬀerent scales.
In data mining applications, usually only a small fraction o f the large
number of predictor variables that have been included in the analysis are
actually relevant to prediction. Also, unlike many applica tions such as pat-
tern recognition, there is seldom reliable domain knowledg e to help create
especially relevant features and/or ﬁlter out the irreleva nt ones, the inclu-
sion of which dramatically degrades the performance of many methods.
Inaddition,dataminingapplicationsgenerallyrequirein terpretablemod-
els. It is not enough to simply produce predictions. It is als o desirable to
have information providing qualitative understanding of t he relationship

352 10. Boosting and Additive Trees
between joint values of the input variables and the resultin g predicted re-
sponse value. Thus, black box methods such as neural networks, which can
be quite useful in purely predictive settings such as patter n recognition,
are far less useful for data mining.
These requirements of speed, interpretability and the mess y nature of
the data sharply limit the usefulness of most learning proce dures as oﬀ-
the-shelf methods for data mining. An “oﬀ-the-shelf” metho d is one that
can be directly applied to the data without requiring a great deal of time-
consuming data preprocessing or careful tuning of the learn ing procedure.
Of all the well-known learning methods, decision trees come closest to
meeting the requirements for serving as an oﬀ-the-shelf pro cedure for data
mining.Theyarerelativelyfasttoconstructandtheyprodu ceinterpretable
models (if the trees are small). As discussed in Section 9.2, they naturally
incorporate mixtures of numeric and categorical predictor variables and
missing values. They are invariant under (strictly monoton e) transforma-
tions of the individual predictors. As a result, scaling and /or more general
transformations are not an issue, and they are immune to the e ﬀects of pre-
dictor outliers. They perform internal feature selection a s an integral part
of the procedure. They are thereby resistant, if not complet ely immune,
to the inclusion of many irrelevant predictor variables. Th ese properties of
decision trees are largely the reason that they have emerged as the most
popular learning method for data mining.
Trees have one aspect that prevents them from being the ideal tool for
predictive learning, namely inaccuracy. They seldom provi de predictive ac-
curacy comparable to the best that can be achieved with the da ta at hand.
As seen in Section 10.1, boosting decision trees improves th eir accuracy,
often dramatically. At the same time it maintains most of the ir desirable
properties for data mining. Some advantages of trees that ar e sacriﬁced by
boosting are speed, interpretability, and, for AdaBoost, r obustness against
overlapping class distributions and especially mislabeli ng of the training
data. A gradient boosted model (GBM) is a generalization of t ree boosting
that attempts to mitigate these problems, so as to produce an accurate and
eﬀective oﬀ-the-shelf procedure for data mining.
10.8 Example: Spam Data
Before we go into the details of gradient boosting, we demons trate its abili-
ties on a two-class classiﬁcation problem. The spam data are introduced in
Chapter 1, and used as an example for many of the procedures in Chapter 9
(Sections 9.1.2, 9.2.5, 9.3.1 and 9.4.1).
Applying gradient boosting to these data resulted in a test e rror rate of
4.5%, using the same test set as was used in Section 9.1.2. By c omparison,
an additive logistic regression achieved 5.5%, a CART tree f ully grown and

10.9 Boosting Trees 353
pruned by cross-validation 8.7%, and MARS 5.5%. The standar d error of
these estimates is around 0.6%, although gradient boosting is signiﬁcantly
better than all of them using the McNemar test (Exercise 10.6 ).
In Section 10.13 below we develop a relative importance meas ure for
each predictor, as well as a partial dependence plot describ ing a predictor’s
contribution to the ﬁtted model. We now illustrate these for the spam data.
Figure 10.6 displays the relative importance spectrum for a ll 57 predictor
variables. Clearly some predictors are more important than others in sep-
aratingspamfromemail. The frequencies of the character strings !,$,hp,
andremoveare estimated to be the four most relevant predictor variabl es.
At the other end of the spectrum, the character strings 857,415,table, and
3dhave virtually no relevance.
The quantity being modeled here is the log-odds of spamversusemail
f(x) = logPr(spam|x)
Pr(email|x)(10.24)
(see Section 10.13 below). Figure 10.7 shows the partial dep endence of the
log-odds on selected important predictors, two positively associated with
spam(!andremove), and two negatively associated ( eduandhp). These
particular dependencies are seen to be essentially monoton ic. There is a
general agreement with the corresponding functions found b y the additive
logistic regression model; see Figure 9.1 on page 303.
Running a gradient boosted model on these data with J= 2 terminal-
node trees produces a purely additive (main eﬀects) model fo r the log-
odds, with a corresponding error rate of 4.7%, as compared to 4.5% for the
full gradient boosted model (with J= 5 terminal-node trees). Although
not signiﬁcant, this slightly higher error rate suggests th at there may be
interactions among some of the important predictor variabl es. This can
be diagnosed through two-variable partial dependence plot s. Figure 10.8
shows one of the several such plots displaying strong intera ction eﬀects.
One sees that for very low frequencies of hp, the log-odds of spamare
greatly increased. For high frequencies of hp, the log-odds of spamtend to
be much lower and roughly constant as a function of !. As the frequency
ofhpdecreases, the functional relationship with !strengthens.
10.9 Boosting Trees
Regression and classiﬁcation trees are discussed in detail in Section 9.2.
They partition the space of all joint predictor variable val ues into disjoint
regionsRj,j= 1,2,...,J, as represented by the terminal nodes of the tree.
A constant γjis assigned to each such region and the predictive rule is
x∈Rj⇒f(x) =γj.

354 10. Boosting and Additive Trees
!$hpremovefreeCAPAVEyourCAPMAXgeorgeCAPTOTeduyouourmoneywill1999businessre(receiveinternet000emailmeeting;650overmailpmpeopletechnologyhplallorderaddressmakefontprojectdataoriginalreportconferencelab[creditparts#85tablecsdirect415857telnetlabsaddresses3d
0 20 40 60 80 100
Relative Importance
FIGURE 10.6. Predictor variable importance spectrum for the spamdata. The
variable names are written on the vertical axis.

10.9 Boosting Trees 355
!Partial Dependence
0.0 0.2 0.4 0.6 0.8 1.0-0.2 0.0 0.2 0.4 0.6 0.8 1.0
removePartial Dependence
0.0 0.2 0.4 0.6-0.2 0.0 0.2 0.4 0.6 0.8 1.0
eduPartial Dependence
0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.6 -0.2 0.0 0.2
hpPartial Dependence
0.0 0.5 1.0 1.5 2.0 2.5 3.0-1.0 -0.6 -0.2 0.0 0.2
FIGURE 10.7. Partial dependence of log-odds of spamon four important pre-
dictors. The red ticks at the base of the plots are deciles of the input variable.
0.51.01.52.02.53.00.20.40.60.81.0-1.0-0.5 0.0 0.5 1.0
hp!
FIGURE 10.8. Partial dependence of the log-odds of spamvs.emailas a func-
tion of joint frequencies of hpand the character !.

356 10. Boosting and Additive Trees
Thus a tree can be formally expressed as
T(x;Θ) =J/summationdisplay
j=1γjI(x∈Rj), (10.25)
with parameters Θ = {Rj,γj}J
1.Jis usually treated as a meta-parameter.
The parameters are found by minimizing the empirical risk
ˆΘ = argmin
ΘJ/summationdisplay
j=1/summationdisplay
xi∈RjL(yi,γj). (10.26)
This is a formidable combinatorial optimization problem, a nd we usually
settle for approximate suboptimal solutions. It is useful t o divide the opti-
mization problem into two parts:
FindingγjgivenRj:Given theRj, estimating the γjis typically trivial,
and often ˆγj= ¯yj, the mean of the yifalling in region Rj. For mis-
classiﬁcation loss, ˆ γjis the modal class of the observations falling in
regionRj.
FindingRj:This is the diﬃcult part, for which approximate solutions ar e
found. Note also that ﬁnding the Rjentails estimating the γjas well.
A typical strategy is to use a greedy, top-down recursive par titioning
algorithm to ﬁnd the Rj. In addition, it is sometimes necessary to
approximate (10.26) by a smoother and more convenient crite rion for
optimizing the Rj:
˜Θ = argmin
ΘN/summationdisplay
i=1˜L(yi,T(xi,Θ)). (10.27)
Then given the ˆRj=˜Rj, theγjcan be estimated more precisely
using the original criterion.
In Section 9.2 we described such a strategy for classiﬁcatio n trees. The Gini
index replaced misclassiﬁcation loss in the growing of the t ree (identifying
theRj).
The boosted tree model is a sum of such trees,
fM(x) =M/summationdisplay
m=1T(x;Θm), (10.28)
induced in a forward stagewise manner (Algorithm 10.2). At e ach step in
the forward stagewise procedure one must solve
ˆΘm= argmin
ΘmN/summationdisplay
i=1L(yi,fm−1(xi)+T(xi;Θm)) (10.29)

10.9 Boosting Trees 357
for the region set and constants Θ m={Rjm,γjm}Jm
1of the next tree, given
the current model fm−1(x).
Given the regions Rjm, ﬁnding the optimal constants γjmin each region
is typically straightforward:
ˆγjm= argmin
γjm/summationdisplay
xi∈RjmL(yi,fm−1(xi)+γjm).(10.30)
Finding the regions is diﬃcult, and even more diﬃcult than fo r a single
tree. For a few special cases, the problem simpliﬁes.
For squared-error loss, the solution to (10.29) is no harder than for a
single tree. It is simply the regression tree that best predi cts the current
residualsyi−fm−1(xi), and ˆγjmis the mean of these residuals in each
corresponding region.
For two-class classiﬁcation and exponential loss, this sta gewise approach
gives rise to the AdaBoost method for boosting classiﬁcatio n trees (Algo-
rithm 10.1). In particular, if the trees T(x;Θm) are restricted to be scaled
classiﬁcation trees, then we showed in Section 10.4 that the solution to
(10.29) is the tree that minimizes the weighted error rate/summationtextN
i=1w(m)
iI(yi∝ne}ationslash=
T(xi;Θm)) with weights w(m)
i=e−yifm−1(xi). By a scaled classiﬁcation
tree, we mean βmT(x;Θm), with the restriction that γjm∈{−1,1}).
Without this restriction, (10.29) still simpliﬁes for expo nential loss to a
weighted exponential criterion for the new tree:
ˆΘm= argmin
ΘmN/summationdisplay
i=1w(m)
iexp[−yiT(xi;Θm)]. (10.31)
Itisstraightforwardtoimplementagreedyrecursive-part itioningalgorithm
using this weighted exponential loss as a splitting criteri on. Given the Rjm,
one can show (Exercise 10.7) that the solution to (10.30) is t he weighted
log-odds in each corresponding region
ˆγjm=1
2log/summationtext
xi∈Rjmw(m)
iI(yi= 1)
/summationtext
xi∈Rjmw(m)
iI(yi=−1). (10.32)
This requires a specialized tree-growing algorithm; in pra ctice, we prefer
the approximation presented below that uses a weighted leas t squares re-
gression tree.
Using loss criteria such as the absolute error or the Huber lo ss (10.23) in
place of squared-error loss for regression, and the devianc e (10.22) in place
of exponential loss for classiﬁcation, will serve to robust ify boosting trees.
Unfortunately, unlike their nonrobust counterparts, thes e robust criteria
do not give rise to simple fast boosting algorithms.
For more general loss criteria the solution to (10.30), give n theRjm,
is typically straightforward since it is a simple “location ” estimate. For

358 10. Boosting and Additive Trees
absolute loss it is just the median of the residuals in each re spective region.
For the other criteria fast iterative algorithms exist for s olving (10.30),
and usually their faster “single-step” approximations are adequate. The
problem is tree induction. Simple fast algorithms do not exi st for solving
(10.29) for these more general loss criteria, and approxima tions like (10.27)
become essential.
10.10 Numerical Optimization via Gradient
Boosting
Fast approximate algorithms for solving (10.29) with any di ﬀerentiable loss
criterion can be derived by analogy to numerical optimizati on. The loss in
usingf(x) to predict yon the training data is
L(f) =N/summationdisplay
i=1L(yi,f(xi)). (10.33)
The goal is to minimize L(f) with respect to f, where here f(x) is con-
strained to be a sum of trees (10.28). Ignoring this constrai nt, minimizing
(10.33) can be viewed as a numerical optimization
ˆf= argmin
fL(f), (10.34)
where the “parameters” f∈IRNare the values of the approximating func-
tionf(xi) at each of the Ndata points xi:
f={f(x1),f(x2),...,f(xN)}T.
Numerical optimization procedures solve (10.34) as a sum of component
vectors
fM=M/summationdisplay
m=0hm,hm∈IRN,
wheref0=h0is an initial guess, and each successive fmis induced based
on the current parameter vector fm−1, which is the sum of the previously
induced updates. Numerical optimization methods diﬀer in t heir prescrip-
tions for computing each increment vector hm(“step”).
10.10.1 Steepest Descent
Steepest descent chooses hm=−ρmgmwhereρmis a scalar and gm∈IRN
is the gradient of L(f) evaluated at f=fm−1. The components of the
gradient gmare
gim=/bracketleftbigg∂L(yi,f(xi))
∂f(xi)/bracketrightbigg
f(xi)=fm−1(xi)(10.35)

10.10 Numerical Optimization via Gradient Boosting 359
Thestep length ρmis the solution to
ρm= argmin
ρL(fm−1−ρgm). (10.36)
The current solution is then updated
fm=fm−1−ρmgm
and the process repeated at the next iteration. Steepest des cent can be
viewed as a very greedy strategy, since −gmis the local direction in IRN
for whichL(f) is most rapidly decreasing at f=fm−1.
10.10.2 Gradient Boosting
Forward stagewise boosting (Algorithm 10.2) is also a very g reedy strategy.
At each step the solution tree is the one that maximally reduc es (10.29),
given the current model fm−1and its ﬁts fm−1(xi). Thus, the tree predic-
tionsT(xi;Θm) are analogous to the components of the negative gradient
(10.35). The principal diﬀerence between them is that the tr ee components
tm={T(x1;Θm),...,T(xN;Θm)}Tare not independent. They are con-
strained to be the predictions of a Jm-terminal node decision tree, whereas
the negative gradient is the unconstrained maximal descent direction.
The solution to (10.30) in the stagewise approach is analogo us to the line
search (10.36) in steepest descent. The diﬀerence is that (1 0.30) performs
a separate line search for those components of tmthat correspond to each
separate terminal region {T(xi;Θm)}xi∈Rjm.
If minimizing loss on the training data (10.33) were the only goal, steep-
est descent would be the preferred strategy. The gradient (1 0.35) is trivial
to calculate for any diﬀerentiable loss function L(y,f(x)), whereas solving
(10.29) is diﬃcult for the robust criteria discussed in Sect ion 10.6. Unfor-
tunately the gradient (10.35) is deﬁned only at the training data points xi,
whereas the ultimate goal is to generalize fM(x) to new data not repre-
sented in the training set.
A possible resolution to this dilemma is to induce a tree T(x;Θm) at the
mth iteration whose predictions tmare as close as possible to the negative
gradient. Using squared error to measure closeness, this le ads us to
˜Θm= argmin
ΘN/summationdisplay
i=1(−gim−T(xi;Θ))2. (10.37)
That is, one ﬁts the tree Tto the negative gradient values (10.35) by least
squares. As noted in Section 10.9 fast algorithms exist for l east squares
decision tree induction. Although the solution regions ˜Rjmto (10.37) will
not be identical to the regions Rjmthat solve (10.29), it is generally sim-
ilar enough to serve the same purpose. In any case, the forwar d stagewise

360 10. Boosting and Additive Trees
TABLE 10.2. Gradients for commonly used loss functions.
Setting Loss Function −∂L(yi,f(xi))/∂f(xi)
Regression1
2[yi−f(xi)]2yi−f(xi)
Regression|yi−f(xi)|sign[yi−f(xi)]
Regression Huber yi−f(xi) for|yi−f(xi)|≤δm
δmsign[yi−f(xi)] for|yi−f(xi)|>δm
whereδm=αth-quantile{|yi−f(xi)|}
Classiﬁcation Deviance kth component: I(yi=Gk)−pk(xi)
boosting procedure, and top-down decision tree induction, are themselves
approximation procedures. After constructing the tree (10 .37), the corre-
sponding constants in each region are given by (10.30).
Table 10.2 summarizes the gradients for commonly used loss f unctions.
For squared error loss, the negative gradient is just the ord inary residual
−gim=yi−fm−1(xi), so that (10.37) on its own is equivalent to standard
least-squares boosting. With absolute error loss, the nega tive gradient is
thesignof the residual, so at each iteration (10.37) ﬁts the tree to t he
sign of the current residuals by least squares. For Huber M-r egression, the
negative gradient is a compromise between these two (see the table).
For classiﬁcation the loss function is the multinomial devi ance (10.22),
andKleast squares trees are constructed at each iteration. Each treeTkm
is ﬁt to its respective negative gradient vector gkm,
−gikm=/bracketleftbigg∂L(yi,f1(xi),...,f K(xi))
∂fk(xi)/bracketrightbigg
f(xi)=fm−1(xi)
=I(yi=Gk)−pk(xi), (10.38)
withpk(x) given by (10.21). Although Kseparate trees are built at each
iteration, they are related through (10.21). For binary cla ssiﬁcation ( K=
2), only one tree is needed (exercise 10.10).
10.10.3 Implementations of Gradient Boosting
Algorithm 10.3 presents the generic gradient tree-boostin g algorithm for
regression. Speciﬁc algorithms are obtained by inserting d iﬀerent loss cri-
teriaL(y,f(x)). The ﬁrst line of the algorithm initializes to the optimal
constant model, which is just a single terminal node tree. Th e components
of the negative gradient computed at line 2(a) are referred t o as general-
ized orpseudoresiduals,r. Gradients for commonly used loss functions are
summarized in Table 10.2.

10.11 Right-Sized Trees for Boosting 361
Algorithm 10.3 Gradient Tree Boosting Algorithm.
1. Initialize f0(x) = argmin γ/summationtextN
i=1L(yi,γ).
2. Form= 1 toM:
(a) Fori= 1,2,...,Ncompute
rim=−/bracketleftbigg∂L(yi,f(xi))
∂f(xi)/bracketrightbigg
f=fm−1.
(b) Fit a regression tree to the targets rimgiving terminal regions
Rjm, j= 1,2,...,J m.
(c) Forj= 1,2,...,J mcompute
γjm= argmin
γ/summationdisplay
xi∈RjmL(yi,fm−1(xi)+γ).
(d) Update fm(x) =fm−1(x)+/summationtextJm
j=1γjmI(x∈Rjm).
3. Output ˆf(x) =fM(x).
The algorithm for classiﬁcation is similar. Lines 2(a)–(d) are repeated
Ktimes at each iteration m, once for each class using (10.38). The result
at line 3 is Kdiﬀerent (coupled) tree expansions fkM(x),k= 1,2,...,K.
These produce probabilities via (10.21) or do classiﬁcatio n as in (10.20).
Details are given in Exercise 10.9. Two basic tuning paramet ers are the
number of iterations Mand the sizes of each of the constituent trees
Jm, m= 1,2,...,M.
The original implementation of this algorithm was called MA RT for
“multiple additive regression trees,” and was referred to i n the ﬁrst edi-
tion of this book. Many of the ﬁgures in this chapter were prod uced by
MART. Gradient boosting as described here is implemented in the Rgbm
package (Ridgeway, 1999, “Gradient Boosted Models”), and i s freely avail-
able. The gbmpackage is used in Section 10.14.2, and extensively in Chap-
ters 16 and 15. Another R implementation of boosting is mboost(Hothorn
and B¨ uhlmann, 2006). A commercial implementation of gradi ent boost-
ing/MART called TreeNetis available from Salford Systems, Inc.
10.11 Right-Sized Trees for Boosting
Historically, boosting was considered to be a technique for combining mod-
els, here trees. As such, the tree building algorithm was reg arded as a

362 10. Boosting and Additive Trees
primitive that produced models to be combined by the boostin g proce-
dure. In this scenario, the optimal size of each tree is estim ated separately
in the usual manner when it is built (Section 9.2). A very larg e (oversized)
tree is ﬁrst induced, and then a bottom-up procedure is emplo yed to prune
it to the estimated optimal number of terminal nodes. This ap proach as-
sumes implicitly that each tree is the last one in the expansi on (10.28).
Except perhaps for the very last tree, this is clearly a very p oor assump-
tion. The result is that trees tend to be much too large, espec ially during
the early iterations. This substantially degrades perform ance and increases
computation.
The simplest strategy for avoiding this problem is to restri ct all trees
to be the same size, Jm=J∀m. At each iteration a J-terminal node
regression tree is induced. Thus Jbecomes a meta-parameter of the entire
boosting procedure, to be adjusted to maximize estimated pe rformance for
the data at hand.
One can get an idea of useful values for Jby considering the properties
of the “target” function
η= argmin
fEXYL(Y,f(X)). (10.39)
Here the expected value is over the population joint distrib ution of (X,Y).
The target function η(x) is the one with minimum prediction risk on future
data. This is the function we are trying to approximate.
One relevant property of η(X) is the degree to which the coordinate vari-
ablesXT= (X1,X2,...,X p) interact with one another. This is captured
by its ANOVA (analysis of variance) expansion
η(X) =/summationdisplay
jηj(Xj)+/summationdisplay
jkηjk(Xj,Xk)+/summationdisplay
jklηjkl(Xj,Xk,Xl)+···.(10.40)
The ﬁrst sum in (10.40) is over functions of only a single pred ictor variable
Xj.Theparticularfunctions ηj(Xj)arethosethatjointlybestapproximate
η(X) under the loss criterion being used. Each such ηj(Xj) is called the
“main eﬀect” of Xj. The second sum is over those two-variable functions
that when added to the main eﬀects best ﬁt η(X). These are called the
second-order interactions of each respective variable pai r (Xj,Xk). The
thirdsumrepresentsthird-orderinteractions,andsoon.F ormanyproblems
encountered in practice, low-order interaction eﬀects ten d to dominate.
When this is the case, models that produce strong higher-ord er interaction
eﬀects, such as large decision trees, suﬀer in accuracy.
The interaction level of tree-based approximations is limi ted by the tree
sizeJ. Namely, no interaction eﬀects of level greater than J−1 are pos-
sible. Since boosted models are additive in the trees (10.28 ), this limit
extends to them as well. Setting J= 2 (single split “decision stump”)
produces boosted models with only main eﬀects; no interacti ons are per-
mitted. With J= 3, two-variable interaction eﬀects are also allowed, and

10.11 Right-Sized Trees for Boosting 363
Number of TermsTest Error
0 100 200 300 4000.0 0.1 0.2 0.3 0.4Stumps
10 Node
100 Node
Adaboost
FIGURE 10.9. Boosting with diﬀerent sized trees, applied to the example (10 .2)
used in Figure 10.2. Since the generative model is additive, stumps perform the
best. The boosting algorithm used the binomial deviance loss i n Algorithm 10.3;
shown for comparison is the AdaBoost Algorithm 10.1.
so on. This suggests that the value chosen for Jshould reﬂect the level
of dominant interactions of η(x). This is of course generally unknown, but
in most situations it will tend to be low. Figure 10.9 illustr ates the eﬀect
of interaction order (choice of J) on the simulation example (10.2). The
generative function is additive (sum of quadratic monomial s), so boosting
models with J >2 incurs unnecessary variance and hence the higher test
error. Figure 10.10 compares the coordinate functions foun d by boosted
stumps with the true functions.
Although in many applications J= 2 will be insuﬃcient, it is unlikely
thatJ >10 will be required. Experience so far indicates that 4 ≤J≤8
works well in the context of boosting, with results being fai rly insensitive
to particular choices in this range. One can ﬁne-tune the val ue forJby
trying several diﬀerent values and choosing the one that pro duces the low-
est risk on a validation sample. However, this seldom provid es signiﬁcant
improvement over using J≃6.

364 10. Boosting and Additive Trees
Coordinate Functions for Additive Logistic Trees
f1(x1) f2(x2)f3(x3)f4(x4)f5(x5)
f6(x6) f7(x7)f8(x8)f9(x9)f10(x10)
FIGURE 10.10. Coordinate functions estimated by boosting stumps for the s im-
ulated example used in Figure 10.9. The true quadratic functio ns are shown for
comparison.
10.12 Regularization
Besides the size of the constituent trees, J, the other meta-parameter of
gradient boosting is the number of boosting iterations M. Each iteration
usuallyreducesthetrainingrisk L(fM),sothatfor Mlargeenoughthisrisk
can be made arbitrarily small. However, ﬁtting the training data too well
can lead to overﬁtting, which degrades the risk on future pre dictions. Thus,
there is an optimal number M∗minimizing future risk that is application
dependent. A convenient way to estimate M∗is to monitor prediction risk
as a function of Mon a validation sample. The value of Mthat minimizes
this risk is taken to be an estimate of M∗. This is analogous to the early
stopping strategy often used with neural networks (Section 11.4).
10.12.1 Shrinkage
Controlling the value of Mis not the only possible regularization strategy.
As with ridge regression and neural networks, shrinkage tec hniques can be
employedaswell(seeSections3.4.1and11.5).Thesimplest implementation
of shrinkage in the context of boosting is to scale the contri bution of each
tree by a factor 0 <ν <1 when it is added to the current approximation.
That is, line 2(d) of Algorithm 10.3 is replaced by
fm(x) =fm−1(x)+ν·J/summationdisplay
j=1γjmI(x∈Rjm). (10.41)
The parameter νcan be regarded as controlling the learning rate of the
boosting procedure. Smaller values of ν(more shrinkage) result in larger
training risk for the same number of iterations M. Thus, both νandM
control prediction risk on the training data. However, thes e parameters do

10.12 Regularization 365
not operate independently. Smaller values of νlead to larger values of M
for the same training risk, so that there is a tradeoﬀ between them.
Empirically it has been found (Friedman, 2001) that smaller values ofν
favor better test error, and require correspondingly large r values ofM. In
fact, the best strategy appears to be to set νto be very small ( ν <0.1)
and then choose Mby early stopping. This yields dramatic improvements
(over no shrinkage ν= 1) for regression and for probability estimation. The
corresponding improvements in misclassiﬁcation risk via ( 10.20) are less,
but still substantial. The price paid for these improvement s is computa-
tional: smaller values of νgive rise to larger values of M, and computation
is proportional to the latter. However, as seen below, many i terations are
generally computationally feasible even on very large data sets. This is
partly due to the fact that small trees are induced at each ste p with no
pruning.
Figure 10.11 shows test error curves for the simulated examp le (10.2) of
Figure 10.2. A gradient boosted model (MART) was trained usi ng binomial
deviance, using either stumps or six terminal-node trees, a nd with or with-
out shrinkage. The beneﬁts of shrinkage are evident, especi ally when the
binomial deviance is tracked. With shrinkage, each test err or curve reaches
a lower value, and stays there for many iterations.
Section 16.2.1 draws a connection between forward stagewis e shrinkage
in boosting and the use of an L1penalty for regularizing model parame-
ters (the “lasso”). We argue that L1penalties may be superior to the L2
penalties used by methods such as the support vector machine .
10.12.2 Subsampling
We saw in Section 8.7 that bootstrap averaging (bagging) imp roves the
performance of a noisy classiﬁer through averaging. Chapte r 15 discusses
in some detail the variance-reduction mechanism of this sam pling followed
by averaging. We can exploit the same device in gradient boos ting, both
to improve performance and computational eﬃciency.
Withstochastic gradient boosting (Friedman, 1999), at each iteration we
sample a fraction ηof the training observations (without replacement),
and grow the next tree using that subsample. The rest of the al gorithm is
identical. A typical value for ηcan be1
2, although for large N,ηcan be
substantially smaller than1
2.
Not only does the sampling reduce the computing time by the sa me
fractionη, but in many cases it actually produces a more accurate model .
Figure 10.12 illustrates the eﬀect of subsampling using the simulated
example (10.2), both as a classiﬁcation and as a regression e xample. We
see in both cases that sampling along with shrinkage slightl y outperformed
the rest. It appears here that subsampling without shrinkag e does poorly.

366 10. Boosting and Additive Trees
Boosting IterationsTest Set Deviance
0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage
Shrinkage=0.2Stumps
Deviance
Boosting IterationsTest Set Misclassification Error
0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage
Shrinkage=0.2Stumps
Misclassification Error
Boosting IterationsTest Set Deviance
0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage
Shrinkage=0.66-Node Trees
Deviance
Boosting IterationsTest Set Misclassification Error
0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage
Shrinkage=0.66-Node Trees
Misclassification Error
FIGURE 10.11. Test error curves for simulated example (10.2) of Figure 10.9,
using gradient boosting (MART). The models were trained usin g binomial de-
viance, either stumps or six terminal-node trees, and with or without shrinkage.
The left panels report test deviance, while the right panels show misclassiﬁcation
error. The beneﬁcial eﬀect of shrinkage can be seen in all cas es, especially for
deviance in the left panels.

10.13 Interpretation 367
0 200 400 600 800 10000.4 0.6 0.8 1.0 1.2 1.4
Boosting IterationsTest Set DevianceDeviance4−Node Trees
0 200 400 600 800 10000.30 0.35 0.40 0.45 0.50
Boosting IterationsTest Set Absolute ErrorNo shrinkage
Shrink=0.1
Sample=0.5
Shrink=0.1 Sample=0.5Absolute Error
FIGURE 10.12. Test-error curves for the simulated example (10.2), showing
the eﬀect of stochasticity. For the curves labeled “Sample = 0.5”, a diﬀerent 50%
subsample of the training data was used each time a tree was gro wn. In the left
panel the models were ﬁt by gbmusing a binomial deviance loss function; in the
right-hand panel using square-error loss.
The downside is that we now have four parameters to set: J,M,νand
η. Typically some early explorations determine suitable val ues forJ,νand
η, leavingMas the primary parameter.
10.13 Interpretation
Single decision trees are highly interpretable. The entire model can be com-
pletely represented by a simple two-dimensional graphic (b inary tree) that
is easily visualized. Linear combinations of trees (10.28) lose this important
feature, and must therefore be interpreted in a diﬀerent way .
10.13.1 Relative Importance of Predictor Variables
Indataminingapplicationstheinputpredictorvariablesa reseldomequally
relevant. Often only a few of them have substantial inﬂuence on the re-
sponse; the vast majority are irrelevant and could just as we ll have not
been included. It is often useful to learn the relative impor tance or contri-
bution of each input variable in predicting the response.

368 10. Boosting and Additive Trees
For a single decision tree T, Breiman et al. (1984) proposed
I2
ℓ(T) =J−1/summationdisplay
t=1ˆı2
tI(v(t) =ℓ) (10.42)
as a measure of relevance for each predictor variable Xℓ. The sum is over
theJ−1 internal nodes of the tree. At each such node t, one of the input
variablesXv(t)isusedtopartitiontheregionassociatedwiththatnodeint o
twosubregions;withineachaseparateconstantisﬁttother esponsevalues.
The particular variable chosen is the one that gives maximal estimated
improvement ˆ ı2
tin squared error risk over that for a constant ﬁt over the
entire region. The squared relative importance of variable Xℓis the sum of
such squared improvements over all internal nodes for which it was chosen
as the splitting variable.
This importance measure is easily generalized to additive t ree expansions
(10.28); it is simply averaged over the trees
I2
ℓ=1
MM/summationdisplay
m=1I2
ℓ(Tm). (10.43)
Due to the stabilizing eﬀect of averaging, this measure turn s out to be more
reliable than is its counterpart (10.42) for a single tree. A lso, because of
shrinkage (Section 10.12.1) the masking of important varia bles by others
with which they are highly correlated is much less of a proble m. Note
that (10.42) and (10.43) refer to squaredrelevance; the actual relevances
are their respective square roots. Since these measures are relative, it is
customary to assign the largest a value of 100 and then scale t he others
accordingly. Figure 10.6 shows the relevant importance of t he 57 inputs in
predicting spamversusemail.
ForK-class classiﬁcation, Kseparate models fk(x),k= 1,2,...,Kare
induced, each consisting of a sum of trees
fk(x) =M/summationdisplay
m=1Tkm(x). (10.44)
In this case (10.43) generalizes to
I2
ℓk=1
MM/summationdisplay
m=1I2
ℓ(Tkm). (10.45)
HereIℓkis the relevance of Xℓin separating the class kobservations from
the other classes. The overall relevance of Xℓis obtained by averaging over
all of the classes
I2
ℓ=1
KK/summationdisplay
k=1I2
ℓk. (10.46)

10.13 Interpretation 369
Figures 10.23 and 10.24 illustrate the use of these averaged and separate
relative importances.
10.13.2 Partial Dependence Plots
After the most relevant variables have been identiﬁed, the n ext step is to
attempt to understand the nature of the dependence of the app roximation
f(X) on their joint values. Graphical renderings of the f(X) as a function
of its arguments provides a comprehensive summary of its dep endence on
the joint values of the input variables.
Unfortunately, such visualization is limited to low-dimen sional views.
We can easily display functions of one or two arguments, eith er continuous
or discrete (or mixed), in a variety of diﬀerent ways; this bo ok is ﬁlled
with such displays. Functions of slightly higher dimension s can be plotted
by conditioning on particular sets of values of all but one or two of the
arguments, producing a trellisof plots (Becker et al., 1996).1
For more than two or three variables, viewing functions of th e corre-
sponding higher-dimensional arguments is more diﬃcult. A u seful alterna-
tive can sometimes be to view a collection of plots, each one o f which shows
the partial dependence of the approximation f(X) on a selected small sub-
set of the input variables. Although such a collection can se ldom provide a
comprehensive depiction of the approximation, it can often produce helpful
clues, especially when f(x) is dominated by low-order interactions (10.40).
Considerthesubvector XSofℓ<poftheinputpredictorvariables XT=
(X1,X2,...,X p), indexed byS⊂{1,2,...,p}. LetCbe the complement
set, withS∪C={1,2,...,p}. A general function f(X) will in principle
depend on all of the input variables: f(X) =f(XS,XC). One way to deﬁne
the average or partialdependence of f(X) onXSis
fS(XS) = EXCf(XS,XC). (10.47)
This is a marginal average of f, and can serve as a useful description of the
eﬀect of the chosen subset on f(X) when, for example, the variables in XS
do not have strong interactions with those in XC.
Partial dependence functions can be used to interpret the re sults of any
“black box” learning method. They can be estimated by
¯fS(XS) =1
NN/summationdisplay
i=1f(XS,xiC), (10.48)
where{x1C,x2C,...,x NC}are the values of XCoccurring in the training
data. This requires a pass over the data for each set of joint v alues ofXSfor
which¯fS(XS) is to be evaluated. This can be computationally intensive,
1lattice in R.

370 10. Boosting and Additive Trees
evenformoderatelysizeddatasets.Fortunatelywithdecis iontrees, ¯fS(XS)
(10.48) can be rapidly computed from the tree itself without reference to
the data (Exercise 10.11).
It is important to note that partial dependence functions de ﬁned in
(10.47) represent the eﬀect of XSonf(X) after accounting for the (av-
erage) eﬀects of the other variables XConf(X). They are notthe eﬀect
ofXSonf(X)ignoring the eﬀects of XC. The latter is given by the con-
ditional expectation
˜fS(XS) = E(f(XS,XC)|XS), (10.49)
and is the best least squares approximation to f(X) by a function of XS
alone. The quantities ˜fS(XS) and¯fS(XS) will be the same only in the
unlikely event that XSandXCare independent. For example, if the eﬀect
of the chosen variable subset happens to be purely additive,
f(X) =h1(XS)+h2(XC). (10.50)
Then (10.47) produces the h1(XS) up to an additive constant. If the eﬀect
is purely multiplicative,
f(X) =h1(XS)·h2(XC), (10.51)
then (10.47) produces h1(XS) up to a multiplicative constant factor. On
the other hand, (10.49) will not produce h1(XS) in either case. In fact,
(10.49) can produce strong eﬀects on variable subsets for wh ichf(X) has
no dependence at all.
Viewing plots of the partial dependence of the boosted-tree approxima-
tion (10.28) on selected variables subsets can help to provi de a qualitative
description of its properties. Illustrations are shown in S ections 10.8 and
10.14. Owing to the limitations of computer graphics, and hu man percep-
tion, the size of the subsets XSmust be small ( l≈1,2,3). There are of
course a large number of such subsets, but only those chosen f rom among
the usually much smaller set of highly relevant predictors a re likely to be
informative. Also, those subsets whose eﬀect on f(X) is approximately
additive (10.50) or multiplicative (10.51) will be most rev ealing.
ForK-class classiﬁcation, there are Kseparate models (10.44), one for
eachclass.Eachoneisrelatedtotherespectiveprobabilit ies(10.21)through
fk(X) = logpk(X)−1
KK/summationdisplay
l=1logpl(X). (10.52)
Thus eachfk(X) is a monotone increasing function of its respective prob-
ability on a logarithmic scale. Partial dependence plots of each respective
fk(X) (10.44) on its most relevant predictors (10.45) can help re veal how
thelog-oddsofrealizingthatclassdependontherespectiv einputvariables.

10.14 Illustrations 371
10.14 Illustrations
Inthissectionweillustrategradientboostingonanumbero flargerdatasets,
using diﬀerent loss functions as appropriate.
10.14.1 California Housing
This data set (Pace and Barry, 1997) is available from the Car negie-Mellon
StatLibrepository2. It consists of aggregated data from each of 20,460
neighborhoods (1990 census block groups) in California. Th e response vari-
ableYis the median house value in each neighborhood measured in un its of
$100,000. The predictor variables are demographics such as median income
MedInc, housing density as reﬂected by the number of houses House, and the
average occupancy in each house AveOccup . Also included as predictors are
the location of each neighborhood ( longitude andlatitude ), and several
quantities reﬂecting the properties of the houses in the nei ghborhood: av-
erage number of rooms AveRooms and bedrooms AveBedrms . There are thus
a total of eight predictors, all numeric.
We ﬁt a gradient boosting model using the MART procedure, wit hJ= 6
terminal nodes, a learning rate (10.41) of ν= 0.1, and the Huber loss
criterion for predicting the numeric response. We randomly divided the
dataset into a training set (80%) and a test set (20%).
Figure 10.13 shows the average absolute error
AAE = E|y−ˆfM(x)| (10.53)
as a function for number of iterations Mon both the training data and test
data. The test error is seen to decrease monotonically with i ncreasingM,
more rapidly during the early stages and then leveling oﬀ to b eing nearly
constant as iterations increase. Thus, the choice of a parti cular value of M
is not critical, as long as it is not too small. This tends to be the case in
many applications. The shrinkage strategy (10.41) tends to eliminate the
problem of overﬁtting, especially for larger data sets.
The value of AAE after 800 iterations is 0.31. This can be comp ared to
thatoftheoptimal constantpredictormedian {yi}whichis0.89.Intermsof
more familiar quantities, the squared multiple correlatio n coeﬃcient of this
model isR2= 0.84. Pace and Barry (1997) use a sophisticated spatial auto-
regression procedure, where prediction for each neighborh ood is based on
median house values in nearby neighborhoods, using the othe r predictors as
covariates. Experimenting with transformations they achi evedR2= 0.85,
predicting log Y. Using log Yas the response the corresponding value for
gradient boosting was R2= 0.86.
2http://lib.stat.cmu.edu.

372 10. Boosting and Additive Trees
0 200 400 600 8000.0 0.2 0.4 0.6 0.8
Iterations MAbsolute ErrorTraining and Test Absolute Error
Train Error
Test Error
FIGURE 10.13. Average-absolute error as a function of number of iterations
for the California housing data.
Figure 10.14 displays the relative variable importances fo r each of the
eight predictor variables. Not surprisingly, median incom e in the neigh-
borhood is the most relevant predictor. Longitude, latitud e, and average
occupancy all have roughly half the relevance of income, whe reas the others
are somewhat less inﬂuential.
Figure 10.15 shows single-variable partial dependence plo ts on the most
relevantnonlocation predictors.Notethattheplotsareno tstrictlysmooth.
This is a consequence of using tree-based models. Decision t rees produce
discontinuous piecewise constant models (10.25). This car ries over to sums
of trees (10.28), with of course many more pieces. Unlike mos t of the meth-
ods discussed in this book, there is no smoothness constrain t imposed on
the result. Arbitrarily sharp discontinuities can be model ed. The fact that
these curves generally exhibit a smooth trend is because tha t is what is
estimated to best predict the response for this problem. Thi s is often the
case.
The hash marks at the base of each plot delineate the deciles o f the
data distribution of the corresponding variables. Note tha t here the data
density is lower near the edges, especially for larger value s. This causes the
curves to be somewhat less well determined in those regions. The vertical
scales of the plots are the same, and give a visual comparison of the relative
importance of the diﬀerent variables.
The partial dependence of median house value on median incom e is
monotonicincreasing,beingnearlylinearoverthemainbod yofdata.House
value is generally monotonic decreasing with increasing av erage occupancy,
except perhaps for average occupancy rates less than one. Me dian house

10.14 Illustrations 373
MedIncLongitudeAveOccupLatitudeHouseAgeAveRoomsAveBedrmsPopulation
0 20 40 60 80 100
Relative importance
FIGURE 10.14. Relative importance of the predictors for the California hous ing
data.
value has a nonmonotonic partial dependence on average numb er of rooms.
It has a minimum at approximately three rooms and is increasi ng both for
smaller and larger values.
Median house value is seen to have a very weak partial depende nce on
house age that is inconsistent with its importance ranking ( Figure 10.14).
This suggests that this weak main eﬀect may be masking strong er interac-
tioneﬀectswithothervariables.Figure10.16showsthetwo -variablepartial
dependence of housing value on joint values of median age and average oc-
cupancy. An interaction between these two variables is appa rent. For values
of average occupancy greater than two, house value is nearly independent
ofmedianage,whereasforvalueslessthantwothereisastro ngdependence
on age.
Figure 10.17 shows the two-variable partial dependence of t he ﬁtted
model on joint values of longitude and latitude, displayed a s a shaded
contour plot. There is clearly a very strong dependence of me dian house
value on the neighborhood location in California. Note that Figure 10.17 is
nota plot of house value versus location ignoring the eﬀects of the other
predictors (10.49). Like all partial dependence plots, it r epresents the eﬀect
of location after accounting for the eﬀects of the other neig hborhood and
houseattributes(10.47).Itcanbeviewedasrepresentinga nextrapremium
one pays for location. This premium is seen to be relatively l arge near the
Paciﬁc coast especially in the Bay Area and Los Angeles–San D iego re-

374 10. Boosting and Additive Trees
MedIncPartial Dependence
2 4 6 8 10-0.5 0.0 0.5 1.0 1.5 2.0
AveOccupPartial Dependence
2 3 4 5-1.0 -0.5 0.0 0.5 1.0 1.5
HouseAgePartial Dependence
10 20 30 40 50-1.0 -0.5 0.0 0.5 1.0
AveRoomsPartial Dependence
4 6 8 10-1.0 -0.5 0.0 0.5 1.0 1.5
FIGURE 10.15. Partial dependence of housing value on the nonlocation vari-
ables for the California housing data. The red ticks at the base of the plot are
deciles of the input variables.
2
3
4
510203040500.00.51.0
AveOccupHouseAge
FIGURE 10.16. Partial dependence of house value on median age and aver-
age occupancy. There appears to be a strong interaction eﬀec t between these two
variables.

10.14 Illustrations 375
−124 −122 −120 −118 −116 −11434 36 38 40 42
LongitudeLatitude
−1.0−0.5 0.0 0.5 1.0
FIGURE 10.17. Partial dependence of median house value on location in Cal-
ifornia. One unit is $100,000, at1990prices, and the values plotted are relative
to the overall median of $180,000.
gions. In the northern, central valley, and southeastern de sert regions of
California, location costs considerably less.
10.14.2 New Zealand Fish
Plant and animal ecologists use regression models to predic t species pres-
ence, abundance and richness as a function of environmental variables.
Although for many years simple linear and parametric models were popu-
lar, recent literature shows increasing interest in more so phisticated mod-
els such as generalized additive models (Section 9.1, GAM), multivariate
adaptive regression splines (Section 9.4, MARS) and booste d regression
trees (Leathwick et al., 2005; Leathwick et al., 2006). Here we model the

376 10. Boosting and Additive Trees
presence and abundance of the Black Oreo Dory , a marine ﬁsh found in the
oceanic waters around New Zealand.3
Figure 10.18 shows the locations of 17,000 trawls (deep-wat er net ﬁshing,
with a maximum depth of 2km), and the red points indicate thos e 2353
trawls for which the Black Oreo was present, one of over a hund red species
regularly recorded. The catch size in kg for each species was recorded for
each trawl. Along with the species catch, a number of environ mental mea-
surements are available for each trawl. These include the av erage depth of
the trawl ( AvgDepth ), and the temperature and salinity of the water. Since
the latter two are strongly correlated with depth, Leathwic k et al. (2006)
derived instead TempResid andSalResid , the residuals obtained when these
two measures are adjusted for depth (via separate non-param etric regres-
sions).SSTGradis a measure of the gradient of the sea surface temperature,
andChlais a broad indicator of ecosytem productivity via satellite -image
measurements. SusPartMatter provides a measure of suspended particulate
matter, particularly in coastal waters, and is also satelli te derived.
The goal of this analysis is to estimate the probability of ﬁn ding Black
Oreo in a trawl, as well as the expected catch size, standardi zed to take
into account the eﬀects of variation in trawl speed and dista nce, as well
as the mesh size of the trawl net. The authors used logistic re gression
for estimating the probability. For the catch size, it might seem natural
to assume a Poisson distribution and model the log of the mean count,
but this is often not appropriate because of the excessive nu mber of zeros.
Although specialized approaches have been developed, such as thezero-
inﬂatedPoisson (Lambert, 1992), they chose a simpler approach. If Yis
the (non-negative) catch size,
E(Y|X) = E(Y|Y >0,X)·Pr(Y >0|X). (10.54)
The second term is estimated by the logistic regression, and the ﬁrst term
can be estimated using only the 2353 trawls with a positive ca tch.
For the logistic regression the authors used a gradient boos ted model
(GBM)4with binomial deviance loss function, depth-10 trees, and a shrink-
age factor ν= 0.025. For the positive-catch regression, they modeled
log(Y) using a GBM with squared-error loss (also depth-10 trees, b ut
ν= 0.01), and un-logged the predictions. In both cases they used 1 0-fold
cross-validation for selecting the number of terms, as well as the shrinkage
factor.
3The models, data, and maps shown here were kindly provided by Dr John Leathwick
of the National Institute of Water and Atmospheric Research in New Zeal and, and Dr
Jane Elith, School of Botany, University of Melbourne. The collectio n of the research
trawl data took place from 1979–2005, and was funded by the New Zea land Ministry of
Fisheries.
4Version 1.5-7 of package gbmin R, ver. 2.2.0.

10.14 Illustrations 377
FIGURE 10.18. Map of New Zealand and its surrounding exclusive economic
zone, showing the locations of 17,000 trawls (small blue dots) t aken between 1979
and 2005. The red points indicate trawls for which the species Black Oreo Dory
were present.

378 10. Boosting and Additive Trees
0 500 1000 15000.24 0.26 0.28 0.30 0.32 0.34
Number of TreesMean DevianceGBM Test
GBM CV
GAM Test
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0
SpecificitySensitivity
AUC
GAM 0.97
GBM 0.98
FIGURE 10.19. The left panel shows the mean deviance as a function of the
number of trees for the GBM logistic regression model ﬁt to the p resence/absence
data. Shown are 10-fold cross-validation on the training data (and1×s.e. bars),
and test deviance on the test data. Also shown for comparison i s the test deviance
using a GAM model with 8dffor each term. The right panel shows ROC curves
on the test data for the chosen GBM model (vertical line in left plo t) and the
GAM model.
Figure 10.19 (left panel) shows the mean binomial deviance f or the se-
quence of GBM models, both for 10-fold CV and test data. There is a mod-
estimprovementovertheperformanceofaGAMmodel,ﬁtusing smoothing
splines with 8 degrees-of-freedom (df) per term. The right p anel shows the
ROC curves (see Section 9.2.5) for both models, which measur es predictive
performance. From this point of view, the performance looks very simi-
lar, with GBM perhaps having a slight edge as summarized by th e AUC
(area under the curve). At the point of equal sensitivity/sp eciﬁcity, GBM
achieves 91%, and GAM 90%.
Figure 10.20 summarizes the contributions of the variables in the logistic
GBM ﬁt. We see that there is a well-deﬁned depth range over whi ch Black
Oreo are caught, with much more frequent capture in colder wa ters. We do
not give details of the quantitative catch model; the import ant variables
were much the same.
All the predictors used in these models are available on a ﬁne geographi-
cal grid; in fact they were derived from environmental atlas es, satellite im-
ages and the like—see Leathwick et al. (2006) for details. Thi s also means
that predictions can be made on this grid, and imported into G IS mapping
systems. Figure 10.21 shows prediction maps for both presen ce and catch
size, with both standardized to a common set of trawl conditi ons; since the
predictors vary in a continuous fashion with geographical l ocation, so do
the predictions.

10.14 Illustrations 379
OrbVelSpeedDistanceDisOrgMatterCodendSizePentadeTidalCurrSlopeChlaCase2SSTGradSalResidSusPartMatterAvgDepthTempResid
Relative influence0 10 25 −4 0 2 4 6−7 −5 −3 −1
TempResidf(TempResid)
0 500 1000 2000−6 −4 −2
AvgDepthf(AvgDepth)
0 5 10 15−7 −5 −3
SusPartMatterf(SusPartMatter)
−0.8 −0.4 0.0 0.4−7 −5 −3 −1
SalResidf(SalResid)
0.00 0.05 0.10 0.15−7 −5 −3 −1
SSTGradf(SSTGrad)
FIGURE 10.20. The top-left panel shows the relative inﬂuence computed from
the GBM logistic regression model. The remaining panels show the partial de-
pendence plots for the leading ﬁve variables, all plotted on the s ame scale for
comparison.
Because of their ability to model interactions and automati cally select
variables, as well as robustness to outliers and missing dat a, GBM models
arerapidlygainingpopularityinthisdata-richandenthus iasticcommunity.
10.14.3 Demographics Data
In this section we illustrate gradient boosting on a multicl ass classiﬁca-
tion problem, using MART. The data come from 9243 questionna ires ﬁlled
out by shopping mall customers in the San Francisco Bay Area ( Impact
Resources, Inc., Columbus, OH). Among the questions are 14 c oncerning
demographics. For this illustration the goal is to predict o ccupation us-
ing the other 13 variables as predictors, and hence identify demographic
variables that discriminate between diﬀerent occupationa l categories. We
randomly divided the data into a training set (80%) and test s et (20%),
and usedJ= 6 node trees with a learning rate ν= 0.1.
Figure 10.22 shows the K= 9 occupation class values along with their
corresponding error rates. The overall error rate is 42.5%, which can be
comparedtothenullrateof69%obtainedbypredictingthemo stnumerous

380 10. Boosting and Additive Trees
FIGURE 10.21. Geological prediction maps of the presence probability (left
map) and catch size (right map) obtained from the gradient bo osted models.
classProf/Man (Professional/Managerial). The four best predicted class es
are seen to be Retired,Student,Prof/Man , andHomemaker .
Figure 10.23 shows the relative predictor variable importa nces as aver-
aged over all classes (10.46). Figure 10.24 displays the ind ividual relative
importance distributions (10.45) for each of the four best p redicted classes.
One sees that the most relevant predictors are generally diﬀ erent for each
respective class. An exception is agewhich is among the threemost relevant
for predicting Retired,Student, andProf/Man .
Figure 10.25 shows the partial dependence of the log-odds (1 0.52) on age
for these three classes. The abscissa values are ordered cod es for respective
equally spaced age intervals. One sees that after accountin g for the contri-
butions of the other variables, the odds of being retired are higher for older
people, whereas the opposite is the case for being a student. The odds of
being professional/managerial are highest for middle-age d people. These
results are of course not surprising. They illustrate that i nspecting partial
dependences separately for each class can lead to sensible r esults.
Bibliographic Notes
Schapire (1990) developed the ﬁrst simple boosting procedu re in the PAC
learning framework (Valiant, 1984; Kearns and Vazirani, 19 94). Schapire

10.14 Illustrations 381
SalesUnemployedMilitaryClericalLaborHomemakerProf/ManRetiredStudent
0.0 0.2 0.4 0.6 0.8 1.0
Error RateOverall Error Rate = 0.425
FIGURE 10.22. Error rate for each occupation in the demographics data.
ageincomeeduhsld-statmar-dlincsexethnicmar-stattyp-homelangnum-hsldchildrenyrs-BA
0 20 40 60 80 100
Relative Importance
FIGURE 10.23. Relative importance of the predictors as averaged over all
classes for the demographics data.

382 10. Boosting and Additive Trees
agemar-dlincsexethnicincomehsld-statmar-statlangtyp-homechildrenedunum-hsldyrs-BA
0 20 40 60 80 100
Relative ImportanceClass =  Retired
hsld-statageincomemar-stateduethnicnum-hsldtyp-homesexmar-dlinclangyrs-BAchildren
0 20 40 60 80 100
Relative ImportanceClass =  Student
eduincomeagemar-dlincethnichsld-stattyp-homesexnum-hsldlangmar-statyrs-BAchildren
0 20 40 60 80 100
Relative ImportanceClass =  Prof/Man
sexmar-dlincchildrenethnicnum-hsldedumar-statlangtyp-homeincomeagehsld-statyrs-BA
0 20 40 60 80 100
Relative ImportanceClass =  Homemaker
FIGURE 10.24. Predictor variable importances separately for each of the fou r
classes with lowest error rate for the demographics data.

10.14 Illustrations 383
agePartial Dependence
1 2 3 4 5 6 70 1 2 3 4Retired
agePartial Dependence
1 2 3 4 5 6 7-2 -1 0 1 2Student
agePartial Dependence
1 2 3 4 5 6 7-2 -1 0 1 2Prof/Man
FIGURE 10.25. Partial dependence of the odds of three diﬀerent occupation s
on age, for the demographics data.
showed that a weak learner could always improve its performance by train-
ing two additional classiﬁers on ﬁltered versions of the inp ut data stream.
A weak learner is an algorithm for producing a two-class clas siﬁer with
performance guaranteed (with high probability) to be signi ﬁcantly better
than a coin-ﬂip. After learning an initial classiﬁer G1on the ﬁrst Ntraining
points,
•G2is learned on a new sample of Npoints, half of which are misclas-
siﬁed byG1;
•G3is learned on Npoints for which G1andG2disagree;
•the boosted classiﬁer is GB=majority vote (G1,G2,G3).
Schapire’s “Strength of Weak Learnability” theorem proves thatGBhas
improved performance over G1.
Freund (1995) proposed a “boost by majority” variation whic h combined
many weak learners simultaneously and improved the perform ance of the
simple boosting algorithm of Schapire. The theory supporti ng both of these

384 10. Boosting and Additive Trees
algorithms requires the weak learner to produce a classiﬁer with a ﬁxed
error rate. This led to the more adaptive and realistic AdaBo ost (Freund
and Schapire, 1996a) and its oﬀspring, where this assumptio n was dropped.
Freund and Schapire (1996a) and Schapire and Singer (1999) p rovide
some theory to support their algorithms, in the form of upper bounds on
generalization error. This theory has evolved in the comput ational learning
community, initially based on the concepts of PAC learning. Other theo-
ries attempting to explain boosting come from game theory (F reund and
Schapire, 1996b; Breiman, 1999; Breiman, 1998), and VC theo ry (Schapire
et al., 1998). The bounds and the theory associated with the A daBoost
algorithms are interesting, but tend to be too loose to be of p ractical im-
portance. In practice, boosting achieves results far more i mpressive than
the bounds would imply. Schapire (2002) and Meir and R¨ atsch (2003) give
useful overviews more recent than the ﬁrst edition of this bo ok.
Friedman et al. (2000) and Friedman (2001) form the basis for our expo-
sitioninthischapter.Friedmanetal.(2000)analyzeAdaBo oststatistically,
derive the exponential criterion, and show that it estimate s the log-odds
of the class probability. They propose additive tree models , the right-sized
trees and ANOVA representation of Section 10.11, and the mul ticlass logit
formulation. Friedman (2001) developed gradient boosting and shrinkage
for classiﬁcation and regression, while Friedman (1999) ex plored stochastic
variants ofboosting.Masonetal.(2000) alsoembracedagra dientapproach
to boosting. As the published discussions of Friedman et al. (2000) shows,
there is some controversy about how and why boosting works.
Since the publication of the ﬁrst edition of this book, these debates have
continued, and spread into the statistical community with a series of papers
on consistency of boosting (Jiang, 2004; Lugosi and Vayatis , 2004; Zhang
and Yu, 2005; Bartlett and Traskin, 2007). Mease and Wyner (2 008),
through a series of simulation examples, challenge some of o ur interpre-
tations of boosting; our response (Friedman et al., 2008a) p uts most of
these objections to rest. A recent survey by B¨ uhlmann and Ho thorn (2007)
supports our approach to boosting.
Exercises
Ex. 10.1 Derive expression (10.12) for the update parameter in AdaBo ost.
Ex. 10.2 Prove result (10.16), that is, the minimizer of the populati on
version of the AdaBoost criterion, is one-half of the log odd s.
Ex. 10.3 Show that the marginal average (10.47) recovers additive an d
multiplicative functions (10.50) and (10.51), while the co nditional expec-
tation (10.49) does not.

Exercises 385
Ex. 10.4
(a) Write a program implementing AdaBoost with trees.
(b) Redo the computations for the example of Figure 10.2. Plo t the train-
ing error as well as test error, and discuss its behavior.
(c) Investigate the number of iterations needed to make the t est error
ﬁnally start to rise.
(d) Change the setup of this example as follows: deﬁne two cla sses, with
the features in Class 1 being X1,X2,...,X 10, standard indepen-
dent Gaussian variates. In Class 2, the features X1,X2,...,X 10are
also standard independent Gaussian, but conditioned on the event/summationtext
jX2
j>12.Nowtheclasseshavesigniﬁcantoverlapinfeaturespace .
Repeat the AdaBoost experiments as in Figure 10.2 and discus s the
results.
Ex. 10.5 Multiclass exponential loss (Zhu et al., 2005). For a K-class clas-
siﬁcation problem, consider the coding Y= (Y1,...,Y K)Twith
Yk=/braceleftbigg1, ifG=Gk
−1
K−1,otherwise.(10.55)
Letf= (f1,...,f K)Twith/summationtextK
k=1fk= 0, and deﬁne
L(Y,f) = exp/parenleftbigg
−1
KYTf/parenrightbigg
. (10.56)
(a) Using Lagrange multipliers, derive the population mini mizerf∗of
L(Y,f), subject to the zero-sum constraint, and relate these to th e
class probabilities.
(b) Show that a multiclass boosting using this loss function leads to a
reweighting algorithm similar to Adaboost, as in Section 10 .4.
Ex. 10.6 McNemar test (Agresti, 1996). We report the test error rates on
the spam data to be 5.5% for a generalized additive model (GAM ), and
4.5% for gradient boosting (GBM), with a test sample of size 1 536.
(a) Show that the standard error of these estimates is about 0 .6%.
Since the same test data are used for both methods, the error r ates are
correlated, and we cannot perform a two-sample t-test. We ca n compare
the methods directly on each test observation, leading to th e summary
GBM
GAM Correct Error
Correct 1434 18
Error 33 51

386 10. Boosting and Additive Trees
The McNemar test focuses on the discordant errors, 33 vs. 18.
(b) Conduct a test to show that GAM makes signiﬁcantly more er rors
than gradient boosting, with a two-sided p-value of 0 .036.
Ex. 10.7 Derive expression (10.32).
Ex. 10.8 Consider a K-class problem where the targets yikare coded as
1 if observation iis in class kand zero otherwise. Suppose we have a
current model fk(x), k= 1,...,K, with/summationtextK
k=1fk(x) = 0 (see (10.21) in
Section 10.6). We wish to update the model for observations i n a regionR
in predictor space, by adding constants fk(x)+γk, withγK= 0.
(a) Write down the multinomial log-likelihood for this prob lem, and its
ﬁrst and second derivatives.
(b) Using only the diagonal of the Hessian matrix in (1), and s tarting
fromγk= 0∀k, show that a one-step approximate Newton update
forγkis
γ1
k=/summationtext
xi∈R(yik−pik)/summationtext
xi∈Rpik(1−pik), k= 1,...,K−1, (10.57)
wherepik= exp(fk(xi))/exp(/summationtextK
ℓ=1fℓ(xi)).
(c) We prefer our update to sum to zero, as the current model do es. Using
symmetry arguments, show that
ˆγk=K−1
K(γ1
k−1
KK/summationdisplay
ℓ=1γ1
ℓ), k= 1,...,K (10.58)
is an appropriate update, where γ1
kis deﬁned as in (10.57) for all
k= 1,...,K.
Ex. 10.9 Consider a K-class problem where the targets yikare coded as
1 if observation iis in classkand zero otherwise. Using the multinomial
deviance loss function (10.22) and the symmetric logistic t ransform, use
the arguments leading to the gradient boosting Algorithm 10 .3 to derive
Algorithm 10.4. Hint: See exercise 10.8 for step 2(b)iii.
Ex. 10.10 Show that for K= 2 class classiﬁcation, only one tree needs to
be grown at each gradient-boosting iteration.
Ex. 10.11 Show how to compute the partial dependence function fS(XS)
in (10.47) eﬃciently.
Ex. 10.12 Referring to (10.49), let S={1}andC={2}, withf(X1,X2) =
X1. AssumeX1andX2are bivariate Gaussian, each with mean zero, vari-
ance one, and E( X1X2) =ρ. Show that E( f(X1,X2)|X2) =ρX2, even
thoughfis not a function of X2.

Exercises 387
Algorithm 10.4 Gradient Boosting for K-class Classiﬁcation.
1. Initialize fk0(x) = 0, k= 1,2,...,K.
2. Form=1 toM:
(a) Set
pk(x) =efk(x)
/summationtextK
ℓ=1efℓ(x), k= 1,2,...,K.
(b) Fork= 1 toK:
i. Compute rikm=yik−pk(xi), i= 1,2,...,N.
ii. Fit a regression tree to the targets rikm, i= 1,2,...,N,
giving terminal regions Rjkm, j= 1,2,...,J m.
iii. Compute
γjkm=K−1
K/summationtext
xi∈Rjkmrikm/summationtext
xi∈Rjkm|rikm|(1−|rikm|), j= 1,2,...,J m.
iv. Update fkm(x) =fk,m−1(x)+/summationtextJm
j=1γjkmI(x∈Rjkm).
3. Output ˆfk(x) =fkM(x), k= 1,2,...,K.

388 10. Boosting and Additive Trees

This is page 389
Printer: Opaque this
11
Neural Networks
11.1 Introduction
In this chapter we describe a class of learning methods that w as developed
separately in diﬀerent ﬁelds—statistics and artiﬁcial inte lligence—based
on essentially identical models. The central idea is to extr act linear com-
binations of the inputs as derived features, and then model t he target as
a nonlinear function of these features. The result is a power ful learning
method, with widespread applications in many ﬁelds. We ﬁrst discuss the
projection pursuit model, which evolved in the domain of sem iparamet-
ric statistics and smoothing. The rest of the chapter is devo ted to neural
network models.
11.2 Projection Pursuit Regression
As in our generic supervised learning problem, assume we hav e an input
vectorXwithpcomponents, and a target Y. Letωm, m= 1,2,...,M, be
unitp-vectors of unknown parameters. The projection pursuit reg ression
(PPR) model has the form
f(X) =M/summationdisplay
m=1gm(ωT
mX). (11.1)
This is an additive model, but in the derived features Vm=ωT
mXrather
than the inputs themselves. The functions gmare unspeciﬁed and are esti-

390 Neural Networks
g(V)
X1X2g(V)
X1X2
FIGURE 11.1. Perspective plots of two ridge functions.
(Left:)g(V) = 1/[1+exp( −5(V−0.5))], whereV= (X1+X2)/√
2.
(Right:)g(V) = (V+0.1)sin(1/(V/3+0.1)), whereV=X1.
mated along with the directions ωmusing some ﬂexible smoothing method
(see below).
The function gm(ωT
mX) is called a ridge function in IRp. It varies only
in the direction deﬁned by the vector ωm. The scalar variable Vm=ωT
mX
is the projection of Xonto the unit vector ωm, and we seek ωmso that
the model ﬁts well, hence the name “projection pursuit.” Fig ure 11.1 shows
someexamplesofridgefunctions.Intheexampleontheleft ω= (1/√
2)(1,1)T,
so that the function only varies in the direction X1+X2. In the example
on the right, ω= (1,0).
The PPR model (11.1) is very general, since the operation of f orming
nonlinear functions of linear combinations generates a sur prisingly large
class of models. For example, the product X1·X2can be written as [( X1+
X2)2−(X1−X2)2]/4, and higher-order products can be represented simi-
larly.
In fact, ifMis taken arbitrarily large, for appropriate choice of gmthe
PPR model can approximate any continuous function in IRparbitrarily
well. Such a class of models is called a universal approximator . However
this generality comes at a price. Interpretation of the ﬁtte d model is usually
diﬃcult, because each input enters into the model in a comple x and multi-
faceted way. As a result, the PPR model is most useful for pred iction, and
not very useful for producing an understandable model for th e data. The
M= 1 model, known as the single index model in econometrics, is an
exception. It is slightly more general than the linear regre ssion model, and
oﬀers a similar interpretation.
How do we ﬁt a PPR model, given training data ( xi,yi),i= 1,2,...,N?
We seek the approximate minimizers of the error function
N/summationdisplay
i=1/bracketleftigg
yi−M/summationdisplay
m=1gm(ωT
mxi)/bracketrightigg2
(11.2)

11.2 Projection Pursuit Regression 391
over functions gmand direction vectors ωm,m= 1,2,...,M. As in other
smoothing problems, we need either explicitly or implicitl y to impose com-
plexity constraints on the gm, to avoid overﬁt solutions.
Consider just one term ( M= 1, and drop the subscript). Given the
direction vector ω, we form the derived variables vi=ωTxi. Then we have
a one-dimensional smoothing problem, and we can apply any sc atterplot
smoother, such as a smoothing spline, to obtain an estimate o fg.
On the other hand, given g, we want to minimize (11.2) over ω. A Gauss–
Newton search is convenient for this task. This is a quasi-Ne wton method,
in which the part of the Hessian involving the second derivat ive ofgis
discarded. It can be simply derived as follows. Let ωoldbe the current
estimate for ω. We write
g(ωTxi)≈g(ωT
oldxi)+g′(ωT
oldxi)(ω−ωold)Txi (11.3)
to give
N/summationdisplay
i=1/bracketleftbig
yi−g(ωTxi)/bracketrightbig2≈N/summationdisplay
i=1g′(ωT
oldxi)2/bracketleftbigg/parenleftbigg
ωT
oldxi+yi−g(ωT
oldxi)
g′(ωT
oldxi)/parenrightbigg
−ωTxi/bracketrightbigg2
.
(11.4)
To minimize the right-hand side, we carry out a least squares regression
with target ωT
oldxi+(yi−g(ωT
oldxi))/g′(ωT
oldxi) on the input xi, with weights
g′(ωT
oldxi)2and no intercept (bias) term. This produces the updated coef -
ﬁcient vector ωnew.
These two steps, estimation of gandω, are iterated until convergence.
With more than one term in the PPR model, the model is built in a forward
stage-wise manner, adding a pair ( ωm,gm) at each stage.
There are a number of implementation details.
•Although any smoothing method can in principle be used, it is conve-
nientifthemethodprovidesderivatives.Localregression andsmooth-
ing splines are convenient.
•After each step the gm’s from previous steps can be readjusted using
the backﬁtting procedure described in Chapter 9. While this may
lead ultimately to fewer terms, it is not clear whether it imp roves
prediction performance.
•Usually the ωmare not readjusted (partly to avoid excessive compu-
tation), although in principle they could be as well.
•The number of terms Mis usually estimated as part of the forward
stage-wise strategy. The model building stops when the next term
does not appreciably improve the ﬁt of the model. Cross-vali dation
can also be used to determine M.

392 Neural Networks
There are many other applications, such as density estimati on (Friedman
etal.,1984;Friedman,1987),wheretheprojectionpursuit ideacanbeused.
In particular, see the discussion of ICA in Section 14.7 and i ts relationship
with exploratory projection pursuit. However the projecti on pursuit re-
gression model has not been widely used in the ﬁeld of statist ics, perhaps
because at the time of its introduction (1981), its computat ional demands
exceeded the capabilities of most readily available comput ers. But it does
represent an important intellectual advance, one that has b lossomed in its
reincarnation in the ﬁeld of neural networks, the topic of th e rest of this
chapter.
11.3 Neural Networks
The term neural network has evolved to encompass a large class of models
andlearningmethods.Herewedescribethemostwidelyused“ vanilla”neu-
ral net, sometimes called the single hidden layer back-prop agation network,
or single layer perceptron. There has been a great deal of hypesurrounding
neural networks, making them seem magical and mysterious. A s we make
clear in this section, they are just nonlinear statistical m odels, much like
the projection pursuit regression model discussed above.
A neural network is a two-stage regression or classiﬁcation model, typ-
ically represented by a network diagram as in Figure 11.2. This network
applies both to regression or classiﬁcation. For regressio n, typically K= 1
and there is only one output unit Y1at the top. However, these networks
can handle multiple quantitative responses in a seamless fa shion, so we will
deal with the general case.
ForK-class classiﬁcation, there are Kunits at the top, with the kth
unit modeling the probability of class k. There are Ktarget measurements
Yk, k= 1,...,K, each being coded as a 0 −1 variable for the kth class.
Derived features Zmare created from linear combinations of the inputs,
and then the target Ykis modeled as a function of linear combinations of
theZm,
Zm=σ(α0m+αT
mX), m= 1,...,M,
Tk=β0k+βT
kZ, k= 1,...,K,
fk(X) =gk(T), k= 1,...,K,(11.5)
whereZ= (Z1,Z2,...,Z M), andT= (T1,T2,...,T K).
The activation function σ(v) is usually chosen to be the sigmoidσ(v) =
1/(1+e−v); see Figure 11.3 for a plot of 1 /(1+e−v). Sometimes Gaussian
radial basis functions (Chapter 6) are used for the σ(v), producing what is
known as a radial basis function network .
Neural network diagrams like Figure 11.2 are sometimes draw n with an
additional biasunit feeding into every unit in the hidden and output layers.

11.3 Neural Networks 393
 Y  Y Y 2 1 K
 Z  Z  Z1 Z2 3 m
 X  X Z  Z1 Z2 3
1  Xp  X p-1  X2  X3M
 X p-1 3 X 2 X 1p Z Y  Y Y
 XK 1 2
                                                                                                                                                /0/0/1/1/0/0/1/1/0/0/1/1/0/0/1/1/0/0/1/1/0/0/1/1
FIGURE 11.2. Schematic of a single hidden layer, feed-forward neural netwo rk.
Thinking of the constant “1” as an additional input feature, this bias unit
captures the intercepts α0mandβ0kin model (11.5).
The output function gk(T) allows a ﬁnal transformation of the vector of
outputsT. For regression we typically choose the identity function gk(T) =
Tk. Early work in K-class classiﬁcation also used the identity function, but
this was later abandoned in favor of the softmaxfunction
gk(T) =eTk
/summationtextK
ℓ=1eTℓ. (11.6)
This is of course exactly the transformation used in the mult ilogit model
(Section 4.4), and produces positive estimates that sum to o ne. In Sec-
tion 4.2 we discuss other problems with linear activation fu nctions, in par-
ticular potentially severe masking eﬀects.
The units in the middle of the network, computing the derived features
Zm, are called hidden units because the values Zmare not directly ob-
served. In general there can be more than one hidden layer, as illustrated
in the example at the end of this chapter. We can think of the Zmas a
basis expansion of the original inputs X; the neural network is then a stan-
dard linear model, or linear multilogit model, using these t ransformations
as inputs. There is, however, an important enhancement over the basis-
expansion techniques discussed in Chapter 5; here the param eters of the
basis functions are learned from the data.

394 Neural Networks
-10 -5 0 5 100.0 0.5 1.01/(1+e−v)
v
FIGURE 11.3. Plot of the sigmoid function σ(v) = 1/(1+exp( −v))(red curve),
commonly used in the hidden layer of a neural network. Included a reσ(sv)for
s=1
2(blue curve) and s= 10(purple curve). The scale parameter scontrols
the activation rate, and we can see that large samounts to a hard activation at
v= 0. Note that σ(s(v−v0))shifts the activation threshold from 0tov0.
Notice that if σis the identity function, then the entire model collapses
to a linear model in the inputs. Hence a neural network can be t hought of
as a nonlinear generalization of the linear model, both for r egression and
classiﬁcation. By introducing the nonlinear transformati onσ, it greatly
enlarges the class of linear models. In Figure 11.3 we see tha t the rate of
activation of the sigmoid depends on the norm of αm, and if∝⌊a∇⌈⌊lαm∝⌊a∇⌈⌊lis very
small, the unit will indeed be operating in the linear part of its activation
function.
Notice also that the neural network model with one hidden lay er has
exactly the same form as the projection pursuit model descri bed above.
The diﬀerence is that the PPR model uses nonparametric funct ionsgm(v),
while the neural network uses a far simpler function based on σ(v), with
three free parameters in its argument. In detail, viewing th e neural network
model as a PPR model, we identify
gm(ωT
mX) =βmσ(α0m+αT
mX)
=βmσ(α0m+∝⌊a∇⌈⌊lαm∝⌊a∇⌈⌊l(ωT
mX)), (11.7)
whereωm=αm/∝⌊a∇⌈⌊lαm∝⌊a∇⌈⌊lis themth unit-vector. Since σβ,α0,s(v) =βσ(α0+
sv) has lower complexity than a more general nonparametric g(v), it is not
surprising that a neural network might use 20 or 100 such func tions, while
the PPR model typically uses fewer terms ( M= 5 or 10, for example).
Finally, we note that the name “neural networks” derives fro m the fact
that they were ﬁrst developed as models for the human brain. E ach unit
represents a neuron, and the connections (links in Figure 11 .2) represent
synapses. In early models, the neurons ﬁred when the total si gnal passed to
thatunitexceededacertainthreshold.Inthemodelabove,t hiscorresponds

11.4 Fitting Neural Networks 395
to use of a step function for σ(Z) andgm(T). Later the neural network was
recognized as a useful tool for nonlinear statistical model ing, and for this
purpose the step function is not smooth enough for optimizat ion. Hence the
step function was replaced by a smoother threshold function , the sigmoid
in Figure 11.3.
11.4 Fitting Neural Networks
The neural network model has unknown parameters, often call edweights,
and we seek values for them that make the model ﬁt the training data well.
We denote the complete set of weights by θ, which consists of
{α0m,αm;m= 1,2,...,M}M(p+1) weights ,
{β0k,βk;k= 1,2,...,K}K(M+1) weights .(11.8)
For regression, we use sum-of-squared errors as our measure of ﬁt (error
function)
R(θ) =K/summationdisplay
k=1N/summationdisplay
i=1(yik−fk(xi))2. (11.9)
For classiﬁcation we use either squared error or cross-entr opy (deviance):
R(θ) =−N/summationdisplay
i=1K/summationdisplay
k=1yiklogfk(xi), (11.10)
and the corresponding classiﬁer is G(x) = argmaxkfk(x). With the softmax
activationfunctionandthecross-entropyerrorfunction, theneuralnetwork
model is exactly a linear logistic regression model in the hi dden units, and
all the parameters are estimated by maximum likelihood.
Typically we don’t want the global minimizer of R(θ), as this is likely
to be an overﬁt solution. Instead some regularization is nee ded: this is
achieved directly through a penalty term, or indirectly by e arly stopping.
Details are given in the next section.
The generic approach to minimizing R(θ) is by gradient descent, called
back-propagation in this setting. Because of the compositional form of the
model, the gradient can be easily derived using the chain rul e for diﬀeren-
tiation. This can be computed by a forward and backward sweep over the
network, keeping track only of quantities local to each unit .

396 Neural Networks
Here is back-propagation in detail for squared error loss. L etzmi=
σ(α0m+αT
mxi), from (11.5) and let zi= (z1i,z2i,...,z Mi). Then we have
R(θ)≡N/summationdisplay
i=1Ri
=N/summationdisplay
i=1K/summationdisplay
k=1(yik−fk(xi))2, (11.11)
with derivatives
∂Ri
∂βkm=−2(yik−fk(xi))g′
k(βT
kzi)zmi,
∂Ri
∂αmℓ=−K/summationdisplay
k=12(yik−fk(xi))g′
k(βT
kzi)βkmσ′(αT
mxi)xiℓ.(11.12)
Given these derivatives, a gradient descent update at the ( r+1)st iter-
ation has the form
β(r+1)
km=β(r)
km−γrN/summationdisplay
i=1∂Ri
∂β(r)
km,
α(r+1)
mℓ=α(r)
mℓ−γrN/summationdisplay
i=1∂Ri
∂α(r)
mℓ,(11.13)
whereγris thelearning rate , discussed below.
Now write (11.12) as
∂Ri
∂βkm=δkizmi,
∂Ri
∂αmℓ=smixiℓ.(11.14)
The quantities δkiandsmiare “errors” from the current model at the
output and hidden layer units, respectively. From their deﬁ nitions, these
errors satisfy
smi=σ′(αT
mxi)K/summationdisplay
k=1βkmδki, (11.15)
known as the back-propagation equations . Using this, the updates in (11.13)
can be implemented with a two-pass algorithm. In the forward pass , the
current weights are ﬁxed and the predicted values ˆfk(xi) are computed
from formula (11.5). In the backward pass , the errors δkiare computed,
and then back-propagated via (11.15) to give the errors smi. Both sets of
errors are then used to compute the gradients for the updates in (11.13),
via (11.14).

11.5 Some Issues in Training Neural Networks 397
This two-pass procedure is what is known as back-propagatio n. It has
also been called the delta rule (Widrow and Hoﬀ, 1960). The computational
components for cross-entropy have the same form as those for the sum of
squares error function, and are derived in Exercise 11.3.
The advantages of back-propagation are its simple, local na ture. In the
back propagation algorithm, each hidden unit passes and rec eives infor-
mation only to and from units that share a connection. Hence i t can be
implemented eﬃciently on a parallel architecture computer .
The updates in (11.13) are a kind of batch learning , with the parame-
ter updates being a sum over all of the training cases. Learni ng can also
be carried out online—processing each observation one at a ti me, updat-
ing the gradient after each training case, and cycling throu gh the training
cases many times. In this case, the sums in equations (11.13) are replaced
by a single summand. A training epoch refers to one sweep through the
entire training set. Online training allows the network to h andle very large
training sets, and also to update the weights as new observat ions come in.
The learning rate γrfor batch learning is usually taken to be a con-
stant, and can also be optimized by a line search that minimiz es the error
function at each update. With online learning γrshould decrease to zero
as the iteration r→∞. This learning is a form of stochastic approxima-
tion(Robbins and Munro, 1951); results in this ﬁeld ensure conve rgence if
γr→0,/summationtext
rγr=∞, and/summationtext
rγ2
r<∞(satisﬁed, for example, by γr= 1/r).
Back-propagation can be very slow, and for that reason is usu ally not
the method of choice. Second-order techniques such as Newto n’s method
are not attractive here, because the second derivative matr ix ofR(the
Hessian) can be very large. Better approaches to ﬁtting incl ude conjugate
gradients and variable metric methods. These avoid explici t computation
of the second derivative matrix while still providing faste r convergence.
11.5 Some Issues in Training Neural Networks
There is quite an art in training neural networks. The model i s generally
overparametrized, and the optimization problem is nonconv ex and unstable
unless certain guidelines are followed. In this section we s ummarize some
of the important issues.
11.5.1 Starting Values
Notethatiftheweightsarenearzero,thentheoperativepar tofthesigmoid
(Figure 11.3) is roughly linear, and hence the neural networ k collapses into
an approximately linear model (Exercise 11.2). Usually sta rting values for
weights are chosen to be random values near zero. Hence the mo del starts
out nearly linear, and becomes nonlinear as the weights incr ease. Individual

398 Neural Networks
units localize to directions and introduce nonlinearities where needed. Use
of exact zero weights leads to zero derivatives and perfect s ymmetry, and
the algorithm never moves. Starting instead with large weig hts often leads
to poor solutions.
11.5.2 Overﬁtting
Often neural networks have too many weights and will overﬁt t he data at
the global minimum of R. In early developments of neural networks, either
by design or by accident, an early stopping rule was used to av oid over-
ﬁtting. Here we train the model only for a while, and stop well before we
approach the global minimum. Since the weights start at a hig hly regular-
ized (linear) solution, this has the eﬀect of shrinking the ﬁ nal model toward
a linear model. A validation dataset is useful for determini ng when to stop,
since we expect the validation error to start increasing.
A more explicit method for regularization is weight decay , which is anal-
ogous to ridge regression used for linear models (Section 3. 4.1). We add a
penalty to the error function R(θ)+λJ(θ), where
J(θ) =/summationdisplay
k,mβ2
km+/summationdisplay
m,ℓα2
mℓ (11.16)
andλ≥0 is a tuning parameter. Larger values of λwill tend to shrink
the weights toward zero: typically cross-validation is use d to estimate λ.
The eﬀect of the penalty is to simply add terms 2 βkmand 2αmℓto the
respective gradient expressions (11.13). Other forms for t he penalty have
been proposed, for example,
J(θ) =/summationdisplay
k,mβ2
km
1+β2
km+/summationdisplay
m,ℓα2
mℓ
1+α2
mℓ, (11.17)
known as the weight elimination penalty. This has the eﬀect of shrinking
smaller weights more than (11.16) does.
Figure 11.4 shows the result of training a neural network wit h ten hidden
units, without weight decay (upper panel) and with weight de cay (lower
panel), to the mixture example of Chapter 2. Weight decay has clearly
improved the prediction. Figure 11.5 shows heat maps of the e stimated
weights from the training (grayscale versions of these are c alledHinton
diagrams. ) We see that weight decay has dampened the weights in both
layers: the resulting weights are spread fairly evenly over the ten hidden
units.
11.5.3 Scaling of the Inputs
Sincethescalingoftheinputsdeterminestheeﬀectivescal ingoftheweights
in the bottom layer, it can have a large eﬀect on the quality of the ﬁnal

11.5 Some Issues in Training Neural Networks 399
Neural Network - 10 Units, No Weight Decay
. . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . .. . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.100
Test Error:       0.259
Bayes Error:    0.210
Neural Network - 10 Units, Weight Decay=0.02 
. .. .. .. .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.160
Test Error:       0.223
Bayes Error:    0.210
FIGURE 11.4. A neural network on the mixture example of Chapter 2. The
upper panel uses no weight decay, and overﬁts the training da ta. The lower panel
uses weight decay, and achieves close to the Bayes error rate ( broken purple
boundary). Both use the softmax activation function and cro ss-entropy error.

400 Neural Networks
1 1
11
x1 x1x2 x2y1 y1y2 y2
z1z1
z1z1
z2z2
z2z2
z3z3
z3z3
z1z1
z1z1
z5z5
z5z5
z6z6
z6z6
z7z7
z7z7
z8z8
z8z8
z9z9
z9z9
z10z10
z10z10No weight decay Weight decay
FIGURE 11.5. Heat maps of the estimated weights from the training of neura l
networks from Figure 11.4. The display ranges from bright gre en (negative) to
bright red (positive).
solution. At the outset it is best to standardize all inputs t o have mean zero
and standard deviation one. This ensures all inputs are trea ted equally in
the regularization process, and allows one to choose a meani ngful range for
the random starting weights. With standardized inputs, it i s typical to take
random uniform weights over the range [ −0.7,+0.7].
11.5.4 Number of Hidden Units and Layers
Generally speaking it is better to have too many hidden units than too few.
With too few hidden units, the model might not have enough ﬂex ibility to
capture the nonlinearities in the data; with too many hidden units, the
extra weights can be shrunk toward zero if appropriate regul arization is
used. Typically the number of hidden units is somewhere in th e range of
5 to 100, with the number increasing with the number of inputs and num-
ber of training cases. It is most common to put down a reasonab ly large
number of units and train them with regularization. Some res earchers use
cross-validation to estimate the optimal number, but this s eems unneces-
sary if cross-validation is used to estimate the regulariza tion parameter.
Choice of the number of hidden layers is guided by background knowledge
and experimentation. Each layer extracts features of the in put for regres-
sion or classiﬁcation. Use of multiple hidden layers allows construction of
hierarchical features at diﬀerent levels of resolution. An example of the
eﬀective use of multiple layers is given in Section 11.6.
11.5.5 Multiple Minima
The error function R(θ) is nonconvex, possessing many local minima. As a
result, the ﬁnal solution obtained is quite dependent on the choice of start-

11.6 Example: Simulated Data 401
ing weights. One must at least try a number of random starting conﬁgura-
tions, and choose the solution giving lowest (penalized) er ror. Probably a
better approach is to use the average predictions over the co llection of net-
works as the ﬁnal prediction (Ripley, 1996). This is prefera ble to averaging
the weights, since the nonlinearity of the model implies tha t this averaged
solution could be quite poor. Another approach is via bagging, which aver-
ages the predictions of networks training from randomly per turbed versions
of the training data. This is described in Section 8.7.
11.6 Example: Simulated Data
We generated data from two additive error models Y=f(X)+ε:
Sum of sigmoids: Y=σ(aT
1X)+σ(aT
2X)+ε1;
Radial:Y=10/productdisplay
m=1φ(Xm)+ε2.
HereXT= (X1,X2,...,X p), eachXjbeing a standard Gaussian variate,
withp= 2 in the ﬁrst model, and p= 10 in the second.
For the sigmoid model, a1= (3,3), a2= (3,−3); for the radial model,
φ(t) = (1/2π)1/2exp(−t2/2). Bothε1andε2are Gaussian errors, with
variance chosen so that the signal-to-noise ratio
Var(E(Y|X))
Var(Y−E(Y|X))=Var(f(X))
Var(ε)(11.18)
is 4 in both models. We took a training sample of size 100 and a t est sample
of size 10,000. We ﬁt neural networks with weight decay and various num-
bers of hidden units, and recorded the average test error E Test(Y−ˆf(X))2
for each of 10 random starting weights. Only one training set was gen-
erated, but the results are typical for an “average” trainin g set. The test
errors are shown in Figure 11.6. Note that the zero hidden uni t model refers
to linear least squares regression. The neural network is pe rfectly suited to
the sum of sigmoids model, and the two-unit model does perfor m the best,
achieving an error close to the Bayes rate. (Recall that the B ayes rate for
regression with squared error is the error variance; in the ﬁ gures, we report
test error relative to the Bayes error). Notice, however, th at with more hid-
den units, overﬁtting quickly creeps in, and with some start ing weights the
model does worse than the linear model (zero hidden unit) mod el. Even
with two hidden units, two of the ten starting weight conﬁgur ations pro-
duced results no better than the linear model, conﬁrming the importance
of multiple starting values.
A radial function is in a sense the most diﬃcult for the neural net, as it is
spherically symmetric and with no preferred directions. We see in the right

402 Neural Networks
1.0 1.5 2.0 2.5 3.0
0 1 2 3 4 5 6 7 8 9 10
Number of Hidden UnitsTest ErrorSum of Sigmoids
0 5 10 15 20 25 30
0 1 2 3 4 5 6 7 8 9 10
Number of Hidden UnitsTest ErrorRadial
FIGURE 11.6. Boxplots of test error, for simulated data example, relative to
the Bayes error (broken horizontal line). True function is a s um of two sigmoids
on the left, and a radial function is on the right. The test erro r is displayed for
10diﬀerent starting weights, for a single hidden layer neural ne twork with the
number of units as indicated.
panel of Figure 11.6 that it does poorly in this case, with the test error
staying well above the Bayes error (note the diﬀerent vertic al scale from
the left panel). In fact, since a constant ﬁt (such as the samp le average)
achieves a relative error of 5 (when the SNR is 4), we see that t he neural
networks perform increasingly worse than the mean.
In this example we used a ﬁxed weight decay parameter of 0 .0005, rep-
resenting a mild amount of regularization. The results in th e left panel of
Figure 11.6 suggest that more regularization is needed with greater num-
bers of hidden units.
InFigure11.7werepeatedtheexperimentforthesumofsigmo idsmodel,
with no weight decay in the left panel, and stronger weight de cay (λ= 0.1)
in the right panel. With no weight decay, overﬁtting becomes even more
severe for larger numbers of hidden units. The weight decay v alueλ= 0.1
produces good results for all numbers of hidden units, and th ere does not
appeartobeoverﬁttingasthenumberofunitsincrease.Fina lly,Figure11.8
shows the test error for a ten hidden unit network, varying th e weight decay
parameter over a wide range. The value 0 .1 is approximately optimal.
In summary, there are two free parameters to select: the weig ht decayλ
and number of hidden units M. As a learning strategy, one could ﬁx either
parameter at the value corresponding to the least constrain ed model, to
ensure that the model is rich enough, and use cross-validati on to choose
theotherparameter.Heretheleastconstrainedvaluesarez eroweightdecay
and ten hidden units. Comparing the left panel of Figure 11.7 to Figure
11.8, we see that the test error is less sensitive to the value of the weight

11.6 Example: Simulated Data 4031.0 1.5 2.0 2.5 3.0
0 1 2 3 4 5 6 7 8 9 10
Number of Hidden UnitsTest ErrorNo Weight Decay
1.0 1.5 2.0 2.5 3.0
0 1 2 3 4 5 6 7 8 9 10
Number of Hidden UnitsTest ErrorWeight Decay=0.1
FIGURE 11.7. Boxplots of test error, for simulated data example, relative to t he
Bayes error. True function is a sum of two sigmoids. The test e rror is displayed
for ten diﬀerent starting weights, for a single hidden layer ne ural network with
the number units as indicated. The two panels represent no wei ght decay (left)
and strong weight decay λ= 0.1(right).1.0 1.2 1.4 1.6 1.8 2.0 2.2
0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14
Weight Decay ParameterTest ErrorSum of Sigmoids, 10 Hidden Unit Model
FIGURE 11.8. Boxplots of test error, for simulated data example. True functi on
is a sum of two sigmoids. The test error is displayed for ten diﬀ erent starting
weights, for a single hidden layer neural network with ten hidd en units and weight
decay parameter value as indicated.

404 Neural Networks
FIGURE 11.9. Examples of training cases from ZIP code data. Each image is
a16×16 8-bit grayscale representation of a handwritten digit.
decay parameter, and hence cross-validation of this parame ter would be
preferred.
11.7 Example: ZIP Code Data
This example is a character recognition task: classiﬁcatio n of handwritten
numerals. This problem captured the attention of the machin e learning and
neural network community for many years, and has remained a b enchmark
problem in the ﬁeld. Figure 11.9 shows some examples of norma lized hand-
written digits, automatically scanned from envelopes by th e U.S. Postal
Service. The original scanned digits are binary and of diﬀer ent sizes and
orientations; the images shown here have been deslanted and size normal-
ized, resulting in 16 ×16 grayscale images (Le Cun et al., 1990). These 256
pixel values are used as inputs to the neural network classiﬁ er.
Ablack box neural network is not ideally suited to this pattern recogni -
tion task, partly because the pixel representation of the im ages lack certain
invariances (such as small rotations of the image). Consequ ently early at-
tempts with neural networks yielded misclassiﬁcation rate s around 4.5%
on various examples of the problem. In this section we show so me of the
pioneering eﬀorts to handcraft the neural network to overco me some these
deﬁciencies (Le Cun, 1989), which ultimately led to the stat e of the art in
neural network performance(Le Cun et al., 1998)1.
Although current digit datasets have tens of thousands of tr aining and
test examples, the sample size here is deliberately modest i n order to em-
1The ﬁgures and tables in this example were recreated from Le Cun (1989) .

11.7 Example: ZIP Code Data 405
16x168x8x2
16x1610
4x44x4
8x8x210
Shared WeightsNet-5Net-4Net-1
4x4x4Local Connectivity10
1010
Net-3Net-28x812
16x1616x1616x16
FIGURE 11.10. Architecture of the ﬁve networks used in the ZIP code example.
phasize the eﬀects. The examples were obtained by scanning s ome actual
hand-drawn digits, and then generating additional images b y random hor-
izontal shifts. Details may be found in Le Cun (1989). There a re 320 digits
in the training set, and 160 in the test set.
Five diﬀerent networks were ﬁt to the data:
Net-1:No hidden layer, equivalent to multinomial logistic regres sion.
Net-2:One hidden layer, 12 hidden units fully connected.
Net-3:Two hidden layers locally connected.
Net-4:Two hidden layers, locally connected with weight sharing.
Net-5:Two hidden layers, locally connected, two levels of weight s haring.
These are depicted in Figure 11.10. Net-1 for example has 256 inputs, one
each for the 16×16 input pixels, and ten output units for each of the digits
0–9. The predicted value ˆfk(x) represents the estimated probability that
an imagexhas digit class k, fork= 0,1,2,...,9.

406 Neural Networks
Training Epochs% Correct on Test Data
0 5 10 15 20 25 3060708090100
Net-1Net-2Net-3Net-4Net-5
FIGURE 11.11. Test performance curves, as a function of the number of train -
ing epochs, for the ﬁve networks of Table 11.1 applied to the ZIP code data.
(Le Cun, 1989)
The networks all have sigmoidal output units, and were all ﬁt with the
sum-of-squares error function. The ﬁrst network has no hidd en layer, and
hence is nearly equivalent to a linear multinomial regressi on model (Exer-
cise 11.4). Net-2 is a single hidden layer network with 12 hid den units, of
the kind described above.
The training set error for all of the networks was 0%, since in all cases
there are more parameters than training observations. The e volution of the
test error during the training epochs is shown in Figure 11.1 1. The linear
network (Net-1) starts to overﬁt fairly quickly, while test performance of
the others level oﬀ at successively superior values.
The other three networks have additional features which dem onstrate
the power and ﬂexibility of the neural network paradigm. The y introduce
constraints on the network, natural for the problem at hand, which allow
for more complex connectivity but fewer parameters.
Net-3 uses local connectivity: this means that each hidden u nit is con-
nected to only a small patch of units in the layer below. In the ﬁrst hidden
layer (an 8×8 array), each unit takes inputs from a 3 ×3 patch of the input
layer; for units in the ﬁrst hidden layer that are one unit apa rt, their recep-
tive ﬁelds overlap by one row or column, and hence are two pixe ls apart.
In the second hidden layer, inputs are from a 5 ×5 patch, and again units
that are one unit apart have receptive ﬁelds that are two unit s apart. The
weights for all other connections are set to zero. Local conn ectivity makes
each unit responsible for extracting local features from th e layer below, and

11.7 Example: ZIP Code Data 407
TABLE 11.1. Test set performance of ﬁve diﬀerent neural networks on a han d-
written digit classiﬁcation example (Le Cun, 1989).
Network Architecture LinksWeights % Correct
Net-1: Single layer network 2570 2570 80.0%
Net-2: Two layer network 3214 3214 87.0%
Net-3: Locally connected 1226 1226 88.5%
Net-4: Constrained network 1 2266 1132 94.0%
Net-5: Constrained network 2 5194 1060 98.4%
reduces considerably the total number of weights. With many more hidden
units than Net-2, Net-3 has fewer links and hence weights (12 26 vs. 3214),
and achieves similar performance.
Net-4 and Net-5 have local connectivity with shared weights . All units
in a local feature map perform the sameoperation on diﬀerent parts of the
image, achieved by sharing the same weights. The ﬁrst hidden layer of Net-
4 has two 8×8 arrays, and each unit takes input from a 3 ×3 patch just like
in Net-3. However, each of the units in a single 8 ×8 feature map share the
same set of nine weights (but have their own bias parameter). This forces
the extracted features in diﬀerent parts of the image to be co mputed by
the same linear functional, and consequently these network s are sometimes
known as convolutional networks . The second hidden layer of Net-4 has
no weight sharing, and is the same as in Net-3. The gradient of the error
functionRwith respect to a shared weight is the sum of the gradients of
Rwith respect to each connection controlled by the weights in question.
Table 11.1 gives the number of links, the number of weights an d the
optimal test performance for each of the networks. We see tha t Net-4 has
more links but fewer weights than Net-3, and superior test pe rformance.
Net-5 has four 4 ×4 feature maps in the second hidden layer, each unit
connected to a 5 ×5 local patch in the layer below. Weights are shared
in each of these feature maps. We see that Net-5 does the best, having
errors of only 1.6%, compared to 13% for the “vanilla” networ k Net-2.
The clever design of network Net-5, motivated by the fact tha t features of
handwriting style should appear in more than one part of a dig it, was the
result of many person years of experimentation. This and sim ilar networks
gave better performance on ZIP code problems than any other l earning
method at that time (early 1990s). This example also shows th at neural
networks are not a fully automatic tool, as they are sometime s advertised.
As with all statistical models, subject matter knowledge ca n and should be
used to improve their performance.
This network was later outperformed by the tangent distance approach
(Simard et al., 1993) described in Section 13.3.3, which exp licitly incorpo-
rates natural aﬃne invariances. At this point the digit reco gnition datasets
become test beds for every new learning procedure, and resea rchers worked

408 Neural Networks
hardtodrivedowntheerrorrates.Asofthiswriting,thebes terrorrateson
a large database (60 ,000 training, 10 ,000 test observations), derived from
standard NIST2databases, were reported to be the following: (Le Cun et
al., 1998):
•1.1% for tangent distance with a 1-nearest neighbor classiﬁer (Sec-
tion 13.3.3);
•0.8% for a degree-9 polynomial SVM (Section 12.3);
•0.8% forLeNet-5, a more complex version of the convolutional net-
work described here;
•0.7% for boosted LeNet-4. Boosting is described in Chapter 8. LeNet-
4is a predecessor of LeNet-5.
Le Cun et al. (1998) report a much larger table of performance results, and
it is evident that many groups have been working very hard to b ring these
test error rates down. They report a standard error of 0 .1% on the error
estimates, which is based on a binomial average with N= 10,000 and
p≈0.01. This implies that error rates within 0 .1—0.2% of one another
are statistically equivalent. Realistically the standard error is even higher,
since the test data has been implicitly used in the tuning of t he various
procedures.
11.8 Discussion
Bothprojectionpursuitregressionandneuralnetworkstak enonlinearfunc-
tions of linear combinations (“derived features”) of the in puts. This is a
powerful and very general approach for regression and class iﬁcation, and
has been shown to compete well with the best learning methods on many
problems.
Thesetoolsareespeciallyeﬀectiveinproblemswithahighs ignal-to-noise
ratio and settings where prediction without interpretatio n is the goal. They
are less eﬀective for problems where the goal is to describe t he physical pro-
cess that generated the data and the roles of individual inpu ts. Each input
enters into the model in many places, in a nonlinear fashion. Some authors
(Hinton, 1989) plot a diagram of the estimated weights into e ach hidden
unit, to try to understand the feature that each unit is extra cting. This
is limited however by the lack of identiﬁability of the param eter vectors
αm, m= 1,...,M. Often there are solutions with αmspanning the same
linear space as the ones found during training, giving predi cted values that
2The National Institute of Standards and Technology maintain l arge databases, in-
cluding handwritten character databases; http://www.nist.gov/srd/ .

11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 409
are roughly the same. Some authors suggest carrying out a pri ncipal com-
ponent analysis of these weights, to try to ﬁnd an interpreta ble solution. In
general, the diﬃculty of interpreting these models has limi ted their use in
ﬁelds like medicine, where interpretation of the model is ve ry important.
There has been a great deal of research on the training of neur al net-
works. Unlike methods like CART and MARS, neural networks ar e smooth
functions of real-valued parameters. This facilitates the development of
Bayesian inference for these models. The next sections disc usses a success-
ful Bayesian implementation of neural networks.
11.9 Bayesian Neural Nets and the NIPS 2003
Challenge
A classiﬁcation competition was held in 2003, in which ﬁve la beled train-
ing datasets were provided to participants. It was organize d for a Neural
Information Processing Systems (NIPS) workshop. Each of th e data sets
constitutedatwo-classclassiﬁcationproblems,withdiﬀe rentsizesandfrom
a variety of domains (see Table 11.2). Feature measurements for a valida-
tion dataset were also available.
Participants developed and applied statistical learning p rocedures to
make predictions on the datasets, and could submit predicti ons to a web-
site on the validation set for a period of 12 weeks. With this f eedback,
participants were then asked to submit predictions for a sep arate test set
and they received their results. Finally, the class labels f or the validation
set were released and participants had one week to train thei r algorithms
on the combined training and validation sets, and submit the ir ﬁnal pre-
dictions to the competition website. A total of 75 groups par ticipated, with
20 and 16 eventually making submissions on the validation an d test sets,
respectively.
There was an emphasis on feature extraction in the competiti on. Arti-
ﬁcial “probes” were added to the data: these are noise featur es with dis-
tributions resembling the real features but independent of the class labels.
The percentage of probes that were added to each dataset, rel ative to the
total set of features, is shown on Table 11.2. Thus each learn ing algorithm
had to ﬁgure out a way of identifying the probes and downweigh ting or
eliminating them.
A number of metrics were used to evaluate the entries, includ ing the
percentage correct on the test set, the area under the ROC cur ve, and a
combined score that compared each pair of classiﬁers head-t o-head. The
results of the competition are very interesting and are deta iled in Guyon et
al. (2006). The most notable result: the entries of Neal and Z hang (2006)
were the clear overall winners. In the ﬁnal competition they ﬁnished ﬁrst

410 Neural Networks
TABLE 11.2. NIPS 2003 challenge data sets. The column labeled pis the number
of features. For the Dorothea dataset the features are binary. Ntr,NvalandNte
are the number of training, validation and test cases, respec tively
Dataset Domain Feature p Percent NtrNvalNte
Type Probes
Arcene Mass spectrometry Dense 10,000 30 100 100 700
Dexter Text classiﬁcation Sparse 20,000 50 300 300 2000
Dorothea Drug discovery Sparse 100,000 50 800 350 800
Gisette Digit recognition Dense 5000 30 6000 1000 6500
Madelon Artiﬁcial Dense 500 96 2000 600 1800
in three of the ﬁve datasets, and were 5th and 7th on the remain ing two
datasets.
In their winning entries, Neal and Zhang (2006) used a series of pre-
processing feature-selection steps, followed by Bayesian neural networks,
Dirichlet diﬀusion trees, and combinations of these method s. Here we focus
only on the Bayesian neural network approach, and try to disc ern which
aspects of their approach were important for its success. We rerun their
programs and compare the results to boosted neural networks and boosted
trees, and other related methods.
11.9.1 Bayes, Boosting and Bagging
Let us ﬁrst review brieﬂy the Bayesian approach to inference and its appli-
cation to neural networks. Given training data Xtr,ytr, we assume a sam-
pling model with parameters θ; Neal and Zhang (2006) use a two-hidden-
layer neural network, with output nodes the class probabili ties Pr(Y|X,θ)
for the binary outcomes. Given a prior distribution Pr( θ), the posterior
distribution for the parameters is
Pr(θ|Xtr,ytr) =Pr(θ)Pr(ytr|Xtr,θ)/integraltext
Pr(θ)Pr(ytr|Xtr,θ)dθ(11.19)
For a test case with features Xnew, the predictive distribution for the
labelYnewis
Pr(Ynew|Xnew,Xtr,ytr) =/integraldisplay
Pr(Ynew|Xnew,θ)Pr(θ|Xtr,ytr)dθ(11.20)
(c.f.equation8.24).Sincetheintegralin(11.20)isintra ctable,sophisticated
Markov Chain Monte Carlo (MCMC) methods are used to sample fr om the
posterior distribution Pr( Ynew|Xnew,Xtr,ytr). A few hundred values θare
generated and then a simple average of these values estimate s the integral.
Neal and Zhang (2006) use diﬀuse Gaussian priors for all of th e parame-
ters. The particular MCMC approach that was used is called hybrid Monte
Carlo, and may be important for the success of the method. It includ es
an auxiliary momentum vector and implements Hamiltonian dy namics in
which the potential function is the target density. This is d one to avoid

11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 411
random walk behavior; the successive candidates move acros s the sample
space in larger steps. They tend to be less correlated and hen ce converge
to the target distribution more rapidly.
Neal and Zhang (2006) also tried diﬀerent forms of pre-proce ssing of the
features:
1. univariate screening using t-tests, and
2. automatic relevance determination.
In the latter method (ARD), the weights (coeﬃcients) for the jth feature
to each of the ﬁrst hidden layer units all share a common prior variance
σ2
j, and prior mean zero. The posterior distributions for each v arianceσ2
j
are computed, and the features whose posterior variance con centrates on
small values are discarded.
There are thus three main features of this approach that coul d be im-
portant for its success:
(a) the feature selection and pre-processing,
(b) the neural network model, and
(c) the Bayesian inference for the model using MCMC.
According to Neal and Zhang (2006), feature screening in (a) is carried
out purely for computational eﬃciency; the MCMC procedure i s slow with
alargenumberoffeatures.Thereisnoneedtousefeaturesel ectiontoavoid
overﬁtting. The posterior average (11.20) takes care of thi s automatically.
We would like to understand the reasons for the success of the Bayesian
method. In our view, power of modern Bayesian methods does no t lie in
their use as a formal inference procedure; most people would not believe
that the priors in a high-dimensional, complex neural netwo rk model are
actually correct. Rather the Bayesian/MCMC approach gives an eﬃcient
way of sampling the relevant parts of model space, and then av eraging the
predictions for the high-probability models.
Bagging and boosting are non-Bayesian procedures that have some simi-
larity toMCMCinaBayesian model. TheBayesian approach ﬁxe sthedata
and perturbs the parameters, according to current estimate of the poste-
rior distribution. Bagging perturbs the data in an i.i.d fas hion and then
re-estimates the model to give a new set of model parameters. At the end,
a simple average of the model predictions from diﬀerent bagg ed samples is
computed. Boosting is similar to bagging, but ﬁts a model tha t is additive
in the models of each individual base learner, which are lear ned using non
i.i.d. samples. We can write all of these models in the form
ˆf(xnew) =L/summationdisplay
ℓ=1wℓE(Ynew|xnew,ˆθℓ) (11.21)

412 Neural Networks
In all cases the ˆθℓare a large collection of model parameters. For the
Bayesian model the wℓ= 1/L, and the average estimates the posterior
mean (11.21) by sampling θℓfrom the posterior distribution. For bagging,
wℓ= 1/Las well, and the ˆθℓare the parameters reﬁt to bootstrap re-
samples of the training data. For boosting, the weights are a ll equal to
1, but the ˆθℓare typically chosen in a nonrandom sequential fashion to
constantly improve the ﬁt.
11.9.2 Performance Comparisons
Based on the similarities above, we decided to compare Bayes ian neural
networks to boosted trees, boosted neural networks, random forests and
bagged neural networks on the ﬁve datasets in Table 11.2. Bag ging and
boosting of neural networks are not methods that we have prev iously used
inourwork.Wedecidedtotrythemhere,becauseofthesucces sofBayesian
neural networks in this competition, and the good performan ce of bagging
and boosting with trees. We also felt that by bagging and boos ting neural
nets, we could assess both the choice of model as well as the mo del search
strategy.
Here are the details of the learning methods that were compar ed:
Bayesian neural nets. The results here are taken from Neal and Zhang
(2006), using their Bayesian approach to ﬁtting neural netw orks. The
models had two hidden layers of 20 and 8 units. We re-ran some
networks for timing purposes only.
Boosted trees. We used the gbmpackage (version 1.5-7) in the R language.
Tree depth and shrinkage factors varied from dataset to data set. We
consistently bagged 80% of the data at each boosting iterati on (the
default is 50%). Shrinkage was between 0.001 and 0.1. Tree de pth was
between 2 and 9.
Boosted neural networks. Since boosting is typically most eﬀective with
“weak”learners,weboostedasinglehiddenlayerneuralnet workwith
two or four units, ﬁt with the nnetpackage (version 7.2-36) in R.
Random forests. We used the R package randomForest (version 4.5-16)
with default settings for the parameters.
Bagged neural networks. Weusedthe same architecture as in the Bayesian
neural network above (two hidden layers of 20 and 8 units), ﬁt using
both Neal’s C language package “Flexible Bayesian Modeling ” (2004-
11-10 release), and Matlab neural-net toolbox (version 5.1 ).

11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 413Test Error (%)
Arcene Dexter Dorothea Gisette Madelon5 15 25Univariate Screened Features
Bayesian neural nets
boosted trees 
boosted neural nets
random forests
bagged neural networks 
Test Error (%)
Arcene Dexter Dorothea Gisette Madelon5 15 25ARD Reduced Features
FIGURE 11.12. Performance of diﬀerent learning methods on ﬁve problems,
using both univariate screening of features (top panel) and a reduced feature set
from automatic relevance determination. The error bars at th e top of each plot
have width equal to one standard error of the diﬀerence betwe en two error rates.
On most of the problems several competitors are within this er ror bound.
This analysis was carried out by Nicholas Johnson, and full d etails may
be found in Johnson (2008)3. The results are shown in Figure 11.12 and
Table 11.3.
TheﬁgureandtableshowBayesian,boostedandbaggedneural networks,
boosted trees, and random forests, using both the screened a nd reduced
features sets. The error bars at the top of each plot indicate one standard
error of the diﬀerence between two error rates. Bayesian neu ral networks
again emerge as the winner, although for some datasets the di ﬀerences
between the test error rates is not statistically signiﬁcan t. Random forests
performs the best among the competitors using the selected f eature set,
while the boosted neural networks perform best with the redu ced feature
set, and nearly match the Bayesian neural net.
The superiority of boosted neural networks over boosted tre es suggest
that the neural network model is better suited to these parti cular prob-
lems. Speciﬁcally, individual features might not be good pr edictors here
3We also thank Isabelle Guyon for help in preparing the results of this section.

414 Neural Networks
TABLE 11.3. Performance of diﬀerent methods. Values are average rank of t est
error across the ﬁve problems (low is good), and mean computati on time and
standard error of the mean, in minutes.
Screened Features ARD Reduced Features
Method Average Average Average Average
Rank Time Rank Time
Bayesian neural networks 1.5384(138) 1.6 600(186)
Boosted trees 3.43.03(2.5) 4.0 34.1(32.4)
Boosted neural networks 3.89.4(8.6) 2.2 35.6(33.5)
Random forests 2.71.9(1.7) 3.2 11.2(9.3)
Bagged neural networks 3.63.5(1.1) 4.0 6.4(4.4)
and linear combinations of features work better. However th e impressive
performance of random forests is at odds with this explanati on, and came
as a surprise to us.
Since the reduced feature sets come from the Bayesian neural network
approach, only the methods that use the screened features ar e legitimate,
self-contained procedures. However, this does suggest tha t better methods
for internal feature selection might help the overall perfo rmance of boosted
neural networks.
The table also shows the approximate training time required for each
method. Here the non-Bayesian methods show a clear advantag e.
Overall, the superior performance of Bayesian neural netwo rks here may
be due to the fact that
(a) the neural network model is well suited to these ﬁve probl ems, and
(b) the MCMC approach provides an eﬃcient way of exploring th e im-
portant part of the parameter space, and then averaging the r esulting
models according to their quality.
The Bayesian approach works well for smoothly parametrized models like
neural nets; it is not yet clear that it works as well for non-s mooth models
like trees.
11.10 Computational Considerations
WithNobservations, ppredictors,Mhiddenunitsand Ltrainingepochs,a
neuralnetwork ﬁttypically requires O(NpML)operations. Therearemany
packages available for ﬁtting neural networks, probably ma ny more than
exist for mainstream statistical methods. Because the avai lable software
varies widely in quality, and the learning problem for neura l networks is
sensitive to issues such as input scaling, such software sho uld be carefully
chosen and tested.

Exercises 415
Bibliographic Notes
Projection pursuit was proposed by Friedman and Tukey (1974 ), and spe-
cialized to regression by Friedman and Stuetzle (1981). Hub er (1985) gives
a scholarly overview, and Roosen and Hastie (1994) present a formulation
using smoothing splines. The motivation for neural network s dates back
to McCulloch and Pitts (1943), Widrow and Hoﬀ (1960) (reprin ted in An-
derson and Rosenfeld (1988)) and Rosenblatt (1962). Hebb (1 949) heavily
inﬂuencedthedevelopmentoflearningalgorithms.Theresu rgenceofneural
networks in the mid 1980s was due to Werbos (1974), Parker (19 85) and
Rumelhart et al. (1986), who proposed the back-propagation algorithm.
Today there are many books written on the topic, for a broad ra nge of
audiences. For readers of this book, Hertz et al. (1991), Bis hop (1995) and
Ripley (1996) may be the most informative. Bayesian learnin g for neural
networksisdescribedinNeal(1996). TheZIPcodeexamplewa stakenfrom
Le Cun (1989); see also Le Cun et al. (1990) and Le Cun et al. (19 98).
We do not discuss theoretical topics such as approximation p roperties of
neural networks, such as the work of Barron (1993), Girosi et al. (1995)
and Jones (1992). Some of these results are summarized by Rip ley (1996).
Exercises
Ex. 11.1 Establish the exact correspondence between the projection pur-
suit regression model (11.1) and the neural network (11.5). In particular,
show that the single-layer regression network is equivalen t to a PPR model
withgm(ωT
mx) =βmσ(α0m+sm(ωT
mx)), whereωmis themth unit vector.
Establish a similar equivalence for a classiﬁcation networ k.
Ex. 11.2 Consider a neural network for a quantitative outcome as in (1 1.5),
using squared-error loss and identity output function gk(t) =t. Suppose
that the weights αmfrom the input to hidden layer are nearly zero. Show
that the resulting model is nearly linear in the inputs.
Ex. 11.3 Derive the forward and backward propagation equations for t he
cross-entropy loss function.
Ex. 11.4 Consider a neural network for a Kclass outcome that uses cross-
entropy loss. If the network has no hidden layer, show that th e model is
equivalent to the multinomial logistic model described in C hapter 4.
Ex. 11.5
(a) Write a program to ﬁt a single hidden layer neural network (ten hidden
units) via back-propagation and weight decay.

416 Neural Networks
(b) Apply it to 100 observations from the model
Y=σ(aT
1X)+(aT
2X)2+0.30·Z,
whereσisthesigmoidfunction, Zisstandardnormal, XT= (X1,X2),
eachXjbeing independent standard normal, and a1= (3,3),a2=
(3,−3). Generate a test sample of size 1000, and plot the training and
test error curves as a function of the number of training epoc hs, for
diﬀerent values of the weight decay parameter. Discuss the o verﬁtting
behavior in each case.
(c) Vary the number of hidden units in the network, from 1 up to 10, and
determine the minimum number needed to perform well for this task.
Ex. 11.6 Write a program to carry out projection pursuit regression, using
cubic smoothing splines with ﬁxed degrees of freedom. Fit it to the data
from the previous exercise, for various values of the smooth ing parameter
and number of model terms. Find the minimum number of model te rms
necessary for the model to perform well and compare this to th e number
of hidden units from the previous exercise.
Ex.11.7 Fitaneuralnetworktothe spamdataofSection9.1.2,andcompare
the results to those for the additive model given in that chap ter. Compare
both the classiﬁcation performance and interpretability o f the ﬁnal model.

This is page 417
Printer: Opaque this
12
Support Vector Machines and
Flexible Discriminants
12.1 Introduction
In this chapter we describe generalizations of linear decis ion boundaries
for classiﬁcation. Optimal separating hyperplanes are int roduced in Chap-
ter 4 for the case when two classes are linearly separable. He re we cover
extensions to the nonseparable case, where the classes over lap. These tech-
niquesarethengeneralizedtowhatisknownasthe support vector machine ,
which produces nonlinear boundaries by constructing a line ar boundary in
alarge, transformed versionof the featurespace. Thesecon dsetof methods
generalize Fisher’s linear discriminant analysis (LDA). T he generalizations
includeﬂexible discriminant analysis which facilitates construction of non-
linear boundaries in a manner very similar to the support vec tor machines,
penalized discriminant analysis for problems such as signal and image clas-
siﬁcation where the large number of features are highly corr elated, and
mixture discriminant analysis for irregularly shaped classes.
12.2 The Support Vector Classiﬁer
In Chapter 4 we discussed a technique for constructing an optimalseparat-
ing hyperplane between two perfectly separated classes. We review this and
generalize to the nonseparable case, where the classes may n ot be separable
by a linear boundary.

418 12. Flexible Discriminants
••
•
••
•••
•
••
•••
••
••
•
•
marginM=1
/bardblβ/bardbl
M=1
/bardblβ/bardblxTβ+β0= 0
••
•
••
•••
•
••
••
•••
••
••
•
• •
marginξ∗
1ξ∗
1ξ∗
1
ξ∗
2ξ∗
2ξ∗
2ξ∗
3ξ∗
3ξ∗
4ξ∗
4ξ∗
4ξ∗
5
M=1
/bardblβ/bardbl
M=1
/bardblβ/bardblxTβ+β0= 0
FIGURE 12.1. Support vector classiﬁers. The left panel shows the separable
case. The decision boundary is the solid line, while broken lines bound the shaded
maximal margin of width 2M= 2//bardblβ/bardbl. The right panel shows the nonseparable
(overlap) case. The points labeled ξ∗
jare on the wrong side of their margin by
an amount ξ∗
j=Mξj; points on the correct side have ξ∗
j= 0. The margin is
maximized subject to a total budget/summationtextξi≤constant. Hence/summationtextξ∗
jis the total
distance of points on the wrong side of their margin.
Our training data consists of Npairs (x1,y1),(x2,y2),...,(xN,yN), with
xi∈IRpandyi∈ {−1,1}. Deﬁne a hyperplane by
{x:f(x) =xTβ+β0= 0}, (12.1)
whereβis a unit vector:∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l= 1. A classiﬁcation rule induced by f(x) is
G(x) = sign[xTβ+β0]. (12.2)
ThegeometryofhyperplanesisreviewedinSection4.5,wher eweshowthat
f(x) in (12.1) gives the signed distance from a point xto the hyperplane
f(x) =xTβ+β0= 0. Since the classes are separable, we can ﬁnd a function
f(x) =xTβ+β0withyif(xi)>0∀i. Hence we are able to ﬁnd the
hyperplane that creates the biggest marginbetween the training points for
class 1 and−1 (see Figure 12.1). The optimization problem
max
β,β0,/bardblβ/bardbl=1M
subject toyi(xT
iβ+β0)≥M, i= 1,...,N,(12.3)
captures this concept. The band in the ﬁgure is Munits away from the
hyperplane on either side, and hence 2 Munits wide. It is called the margin.
We showed that this problem can be more conveniently rephras ed as
min
β,β0∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l
subject toyi(xT
iβ+β0)≥1, i= 1,...,N,(12.4)

12.2 The Support Vector Classiﬁer 419
where we have dropped the norm constraint on β. Note that M= 1/∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l.
Expression (12.4) is the usual way of writing the support vec tor criterion
for separated data. This is a convex optimization problem (q uadratic cri-
terion, linear inequality constraints), and the solution i s characterized in
Section 4.5.2.
Suppose now that the classes overlap in feature space. One wa y to deal
with the overlap is to still maximize M, but allow for some points to be on
the wrong side of the margin. Deﬁne the slack variables ξ= (ξ1,ξ2,...,ξ N).
There are two natural ways to modify the constraint in (12.3) :
yi(xT
iβ+β0)≥M−ξi, (12.5)
or
yi(xT
iβ+β0)≥M(1−ξi), (12.6)
∀i, ξi≥0,/summationtextN
i=1ξi≤constant. The two choices lead to diﬀerent solutions.
The ﬁrst choice seems more natural, since it measures overla p in actual
distancefromthemargin;thesecondchoicemeasurestheove rlapinrelative
distance, which changes with the width of the margin M. However, the ﬁrst
choice results in a nonconvex optimization problem, while t he second is
convex; thus (12.6) leads to the “standard” support vector c lassiﬁer, which
we use from here on.
Hereistheideaoftheformulation.Thevalue ξiintheconstraint yi(xT
iβ+
β0)≥M(1−ξi) is the proportional amount by which the prediction
f(xi) =xT
iβ+β0is on the wrong side of its margin. Hence by bounding the
sum/summationtextξi, we bound the total proportional amount by which prediction s
fall on the wrong side of their margin. Misclassiﬁcations oc cur whenξi>1,
so bounding/summationtextξiat a valueKsay, bounds the total number of training
misclassiﬁcations at K.
As in (4.48) in Section 4.5.2, we can drop the norm constraint onβ,
deﬁneM= 1/∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l, and write (12.4) in the equivalent form
min∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊lsubject to/braceleftigg
yi(xT
iβ+β0)≥1−ξi∀i,
ξi≥0,/summationtextξi≤constant.(12.7)
This is the usual way the support vector classiﬁer is deﬁned f or the non-
separable case. However we ﬁnd confusing the presence of the ﬁxed scale
“1” in the constraint yi(xT
iβ+β0)≥1−ξi, and prefer to start with (12.6).
The right panel of Figure 12.1 illustrates this overlapping case.
By the nature of the criterion (12.7), we see that points well inside their
class boundary do not play a big role in shaping the boundary. This seems
like an attractive property, and one that diﬀerentiates it f rom linear dis-
criminant analysis (Section 4.3). In LDA, the decision boun dary is deter-
mined by the covariance of the class distributions and the po sitions of the
class centroids. We will see in Section 12.3.3 that logistic regression is more
similar to the support vector classiﬁer in this regard.

420 12. Flexible Discriminants
12.2.1 Computing the Support Vector Classiﬁer
The problem (12.7) is quadratic with linear inequality cons traints, hence it
is a convex optimization problem. We describe a quadratic pr ogramming
solution using Lagrange multipliers. Computationally it i s convenient to
re-express (12.7) in the equivalent form
min
β,β01
2∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l2+CN/summationdisplay
i=1ξi
subject to ξi≥0, yi(xT
iβ+β0)≥1−ξi∀i,(12.8)
where the “cost” parameter Creplaces the constant in (12.7); the separable
case corresponds to C=∞.
The Lagrange (primal) function is
LP=1
2∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l2+CN/summationdisplay
i=1ξi−N/summationdisplay
i=1αi[yi(xT
iβ+β0)−(1−ξi)]−N/summationdisplay
i=1µiξi,(12.9)
which we minimize w.r.t β,β0andξi. Setting the respective derivatives to
zero, we get
β=N/summationdisplay
i=1αiyixi, (12.10)
0 =N/summationdisplay
i=1αiyi, (12.11)
αi=C−µi,∀i, (12.12)
as well as the positivity constraints αi, µi, ξi≥0∀i. By substituting
(12.10)–(12.12) into (12.9), we obtain the Lagrangian (Wol fe) dual objec-
tive function
LD=N/summationdisplay
i=1αi−1
2N/summationdisplay
i=1N/summationdisplay
i′=1αiαi′yiyi′xT
ixi′, (12.13)
which gives a lower bound on the objective function (12.8) fo r any feasible
point. We maximize LDsubject to 0≤αi≤Cand/summationtextN
i=1αiyi= 0. In
addition to (12.10)–(12.12), the Karush–Kuhn–Tucker cond itions include
the constraints
αi[yi(xT
iβ+β0)−(1−ξi)] = 0, (12.14)
µiξi= 0, (12.15)
yi(xT
iβ+β0)−(1−ξi)≥0, (12.16)
fori= 1,...,N. Together these equations (12.10)–(12.16) uniquely char-
acterize the solution to the primal and dual problem.

12.2 The Support Vector Classiﬁer 421
From (12.10) we see that the solution for βhas the form
ˆβ=N/summationdisplay
i=1ˆαiyixi, (12.17)
with nonzero coeﬃcients ˆ αionly for those observations ifor which the
constraints in (12.16) are exactly met (due to (12.14)). The se observations
are called the support vectors , sinceˆβis represented in terms of them
alone. Among these support points, some will lie on the edge o f the margin
(ˆξi= 0), and hence from (12.15) and (12.12) will be characterize d by
0<ˆαi< C; the remainder ( ˆξi>0) have ˆαi=C. From (12.14) we can
see that any of these margin points (0 <ˆαi,ˆξi= 0) can be used to solve
forβ0, and we typically use an average of all the solutions for nume rical
stability.
Maximizing the dual (12.13) is a simpler convex quadratic pr ogramming
problemthantheprimal(12.9),andcanbesolvedwithstanda rdtechniques
(Murray et al., 1981, for example).
Given the solutions ˆβ0andˆβ, the decision function can be written as
ˆG(x) = sign[ ˆf(x)]
= sign[xTˆβ+ˆβ0]. (12.18)
The tuning parameter of this procedure is the cost parameter C.
12.2.2 Mixture Example (Continued)
Figure 12.2 shows the support vector boundary for the mixtur e example
of Figure 2.5 on page 21, with two overlapping classes, for tw o diﬀerent
values of the cost parameter C. The classiﬁers are rather similar in their
performance. Points on the wrong side of the boundary are sup port vectors.
In addition, points on the correct side of the boundary but cl ose to it (in
the margin), are also support vectors. The margin is larger f orC= 0.01
than it is for C= 10,000. Hence larger values of Cfocus attention more
on (correctly classiﬁed) points near the decision boundary , while smaller
values involve data further away. Either way, misclassiﬁed points are given
weight, no matter how far away. In this example the procedure is not very
sensitive to choices of C, because of the rigidity of a linear boundary.
The optimal value for Ccan be estimated by cross-validation, as dis-
cussed in Chapter 7. Interestingly, the leave-one-out cros s-validation error
can be bounded above by the proportion of support points in th e data. The
reason is that leaving out an observation that is not a suppor t vector will
not change the solution. Hence these observations, being cl assiﬁed correctly
by the original boundary, will be classiﬁed correctly in the cross-validation
process. However this bound tends to be too high, and not gene rally useful
for choosing C(62% and 85%, respectively, in our examples).

422 12. Flexible Discriminants
.. . . .. . . . . . .. . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . .. . . . ..
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
••
•
Training Error: 0.270
Test Error:       0.288
Bayes Error:    0.210
C= 10000
.. .. . . .. . . . .. . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .. . . . .. . . .. ..
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o•
Training Error: 0.26
Test Error:       0.30
Bayes Error:    0.21
C= 0.01
FIGURE 12.2. The linear support vector boundary for the mixture data exam-
ple with two overlapping classes, for two diﬀerent values of C. The broken lines
indicate the margins, where f(x) =±1. The support points ( αi>0) are all the
points on the wrong side of their margin. The black solid dots ar e those support
points falling exactly on the margin ( ξi= 0, αi>0). In the upper panel 62%of
the observations are support points, while in the lower panel 85%are. The broken
purple curve in the background is the Bayes decision boundary .

12.3 Support Vector Machines and Kernels 423
12.3 Support Vector Machines and Kernels
The support vector classiﬁer described so far ﬁnds linear bo undaries in the
input feature space. As with other linear methods, we can mak e the pro-
cedure more ﬂexible by enlarging the feature space using bas is expansions
such as polynomials or splines (Chapter 5). Generally linea r boundaries
in the enlarged space achieve better training-class separa tion, and trans-
late to nonlinear boundaries in the original space. Once the basis functions
hm(x), m= 1,...,Mare selected, the procedure is the same as before. We
ﬁttheSVclassiﬁerusinginputfeatures h(xi) = (h1(xi),h2(xi),...,h M(xi)),
i= 1,...,N, and produce the (nonlinear) function ˆf(x) =h(x)Tˆβ+ˆβ0.
The classiﬁer is ˆG(x) = sign( ˆf(x)) as before.
Thesupport vector machine classiﬁer is an extension of this idea, where
the dimension of the enlarged space is allowed to get very lar ge, inﬁnite
in some cases. It might seem that the computations would beco me pro-
hibitive. It would also seem that with suﬃcient basis functi ons, the data
would be separable, and overﬁtting would occur. We ﬁrst show how the
SVM technology deals with these issues. We then see that in fa ct the SVM
classiﬁer is solving a function-ﬁtting problem using a part icular criterion
and form of regularization, and is part of a much bigger class of problems
that includes the smoothing splines of Chapter 5. The reader may wish
to consult Section 5.8, which provides background material and overlaps
somewhat with the next two sections.
12.3.1 Computing the SVM for Classiﬁcation
We can represent the optimization problem (12.9) and its sol ution in a
special way that only involves the input features via inner p roducts. We do
this directly for the transformed feature vectors h(xi). We then see that for
particular choices of h, these inner products can be computed very cheaply.
The Lagrange dual function (12.13) has the form
LD=N/summationdisplay
i=1αi−1
2N/summationdisplay
i=1N/summationdisplay
i′=1αiαi′yiyi′∝an}⌊∇a⌋ketle{th(xi),h(xi′)∝an}⌊∇a⌋ket∇i}ht. (12.19)
From (12.10) we see that the solution function f(x) can be written
f(x) =h(x)Tβ+β0
=N/summationdisplay
i=1αiyi∝an}⌊∇a⌋ketle{th(x),h(xi)∝an}⌊∇a⌋ket∇i}ht+β0. (12.20)
As before, given αi,β0can be determined by solving yif(xi) = 1 in (12.20)
for any (or all) xifor which 0 <αi<C.

424 12. Flexible Discriminants
So both (12.19) and (12.20) involve h(x) only through inner products. In
fact, we need not specify the transformation h(x) at all, but require only
knowledge of the kernel function
K(x,x′) =∝an}⌊∇a⌋ketle{th(x),h(x′)∝an}⌊∇a⌋ket∇i}ht (12.21)
that computes inner products in the transformed space. Kshould be a
symmetric positive (semi-) deﬁnite function; see Section 5 .8.1.
Three popular choices for Kin the SVM literature are
dth-Degree polynomial: K(x,x′) = (1+∝an}⌊∇a⌋ketle{tx,x′∝an}⌊∇a⌋ket∇i}ht)d,
Radial basis: K(x,x′) = exp(−γ∝⌊a∇⌈⌊lx−x′∝⌊a∇⌈⌊l2),
Neural network: K(x,x′) = tanh(κ1∝an}⌊∇a⌋ketle{tx,x′∝an}⌊∇a⌋ket∇i}ht+κ2).(12.22)
Consider for example a feature space with two inputs X1andX2, and a
polynomial kernel of degree 2. Then
K(X,X′) = (1+∝an}⌊∇a⌋ketle{tX,X′∝an}⌊∇a⌋ket∇i}ht)2
= (1+X1X′
1+X2X′
2)2
= 1+2X1X′
1+2X2X′
2+(X1X′
1)2+(X2X′
2)2+2X1X′
1X2X′
2.
(12.23)
ThenM= 6, and if we choose h1(X) = 1,h2(X) =√
2X1,h3(X) =√
2X2,h4(X) =X2
1,h5(X) =X2
2,andh6(X) =√
2X1X2,thenK(X,X′) =
∝an}⌊∇a⌋ketle{th(X),h(X′)∝an}⌊∇a⌋ket∇i}ht. From (12.20) we see that the solution can be written
ˆf(x) =N/summationdisplay
i=1ˆαiyiK(x,xi)+ˆβ0. (12.24)
The role of the parameter Cis clearer in an enlarged feature space,
since perfect separation is often achievable there. A large value ofCwill
discourage any positive ξi, and lead to an overﬁt wiggly boundary in the
original feature space; a small value of Cwill encourage a small value of
∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l, which in turn causes f(x) and hence the boundary to be smoother.
Figure 12.3 show two nonlinear support vector machines appl ied to the
mixture example of Chapter 2. The regularization parameter was chosen
in both cases to achieve good test error. The radial basis ker nel produces
a boundary quite similar to the Bayes optimal boundary for th is example;
compare Figure 2.5.
In the early literature on support vectors, there were claim s that the
kernel property of the support vector machine is unique to it and allows
one to ﬁnesse the curse of dimensionality. Neither of these c laims is true,
and we go into both of these issues in the next three subsectio ns.

12.3 Support Vector Machines and Kernels 425
SVM - Degree-4 Polynomial in Feature Space
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . .. . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
•••••••
•
• ••
•
•••
Training Error: 0.180
Test Error:       0.245
Bayes Error:    0.210
SVM - Radial Kernel in Feature Space
. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . .. . . .. . . .. . . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
•
•••
•
••••
••••
••••
••
••
••••
••
•
••
•
Training Error: 0.160
Test Error:       0.218
Bayes Error:    0.210
FIGURE 12.3. Two nonlinear SVMs for the mixture data. The upper plot uses
a4th degree polynomial kernel, the lower a radial basis kernel (wi thγ= 1). In
each caseCwas tuned to approximately achieve the best test error perfor mance,
andC= 1worked well in both cases. The radial basis kernel performs t he best
(close to Bayes optimal), as might be expected given the data ar ise from mixtures
of Gaussians. The broken purple curve in the background is the B ayes decision
boundary.

426 12. Flexible Discriminants
−3 −2 −1 0 1 2 30.0 0.5 1.0 1.5 2.0 2.5 3.0Hinge Loss
Binomial Deviance
Squared Error
Class HuberLoss
yf
FIGURE 12.4. The support vector loss function (hinge loss), compared to the
negative log-likelihood loss (binomial deviance) for logistic r egression, squared-er-
ror loss, and a “Huberized” version of the squared hinge loss. A ll are shown as a
function of yfrather than f, because of the symmetry between the y= +1and
y=−1case. The deviance and Huber have the same asymptotes as the S VM
loss, but are rounded in the interior. All are scaled to have the limiting left-tail
slope of−1.
12.3.2 The SVM as a Penalization Method
Withf(x) =h(x)Tβ+β0, consider the optimization problem
min
β0, βN/summationdisplay
i=1[1−yif(xi)]++λ
2∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l2(12.25)
where the subscript “+” indicates positive part. This has th e formloss+
penalty, which is a familiar paradigm in function estimation. It is e asy to
show (Exercise 12.1) that the solution to (12.25), with λ= 1/C, is the
same as that for (12.8).
Examination of the “hinge” loss function L(y,f) = [1−yf]+shows that
it is reasonable for two-class classiﬁcation, when compare d to other more
traditional loss functions. Figure 12.4 compares it to the l og-likelihood loss
for logistic regression, as well as squared-error loss and a variant thereof.
The (negative) log-likelihood or binomial deviance has sim ilar tails as the
SVM loss, giving zero penalty to points well inside their mar gin, and a

12.3 Support Vector Machines and Kernels 427
TABLE 12.1. The population minimizers for the diﬀerent loss functions in F ig-
ure 12.4. Logistic regression uses the binomial log-likelihoo d or deviance. Linear
discriminant analysis (Exercise 4.2) uses squared-error los s. The SVM hinge loss
estimates the mode of the posterior class probabilities, wher eas the others estimate
a linear transformation of these probabilities.
Loss Function L[y,f(x)] Minimizing Function
Binomial
Deviance log[1+e−yf(x)]f(x) = logPr(Y= +1|x)
Pr(Y= -1|x)
SVM Hinge
Loss[1−yf(x)]+ f(x) = sign[Pr( Y= +1|x)−1
2]
Squared
Error[y−f(x)]2= [1−yf(x)]2f(x) = 2Pr(Y= +1|x)−1
“Huberised”
Square
Hinge Loss−4yf(x), yf (x)<-1
[1−yf(x)]2
+otherwisef(x) = 2Pr(Y= +1|x)−1
linear penalty to points on the wrong side and far away. Squar ed-error, on
the other hand gives a quadratic penalty, and points well ins ide their own
margin have a strong inﬂuence on the model as well. The square d hinge
lossL(y,f) = [1−yf]2
+is like the quadratic, except it is zero for points
inside their margin. It still rises quadratically in the lef t tail, and will be
less robust than hinge or deviance to misclassiﬁed observat ions. Recently
RossetandZhu(2007)proposeda“Huberized”versionofthes quaredhinge
loss, which converts smoothly to a linear loss at yf=−1.
We can characterize these loss functions in terms of what the y are es-
timating at the population level. We consider minimizing E L(Y,f(X)).
Table 12.1 summarizes the results. Whereas the hinge loss es timates the
classiﬁerG(x) itself, all the others estimate a transformation of the cla ss
posterior probabilities. The “Huberized” square hinge los s shares attractive
properties of logistic regression (smooth loss function, e stimates probabili-
ties), as well as the SVM hinge loss (support points).
Formulation (12.25) casts the SVM as a regularized function estimation
problem, where the coeﬃcients of the linear expansion f(x) =β0+h(x)Tβ
areshrunktowardzero(excludingtheconstant).If h(x)representsahierar-
chical basis having some ordered structure (such as ordered in roughness),

428 12. Flexible Discriminants
then the uniform shrinkage makes more sense if the rougher el ementshjin
the vectorhhave smaller norm.
All the loss-functions in Table 12.1 except squared-error a re so called
“margin maximizing loss-functions” (Rosset et al., 2004b) . This means that
if the data are separable, then the limit of ˆβλin (12.25) as λ→0 deﬁnes
the optimal separating hyperplane1.
12.3.3 Function Estimation and Reproducing Kernels
Here we describe SVMs in terms of function estimation in repr oducing
kernel Hilbert spaces, where the kernel property abounds. T his material is
discussed in some detail in Section 5.8. This provides anoth er view of the
support vector classiﬁer, and helps to clarify how it works.
Suppose the basis harises from the (possibly ﬁnite) eigen-expansion of
a positive deﬁnite kernel K,
K(x,x′) =∞/summationdisplay
m=1φm(x)φm(x′)δm (12.26)
andhm(x) =√δmφm(x). Then with θm=√δmβm, we can write (12.25)
as
min
β0, θN/summationdisplay
i=1/bracketleftigg
1−yi(β0+∞/summationdisplay
m=1θmφm(xi))/bracketrightigg
++λ
2∞/summationdisplay
m=1θ2
m
δm.(12.27)
Now (12.27) is identical in form to (5.49) on page 169 in Secti on 5.8, and
the theory of reproducing kernel Hilbert spaces described t here guarantees
a ﬁnite-dimensional solution of the form
f(x) =β0+N/summationdisplay
i=1αiK(x,xi). (12.28)
In particular we see there an equivalent version of the optim ization crite-
rion (12.19) [Equation (5.67) in Section 5.8.2; see also Wah ba et al. (2000)],
min
β0,αN/summationdisplay
i=1(1−yif(xi))++λ
2αTKα, (12.29)
whereKis theN×Nmatrix of kernel evaluations for all pairs of training
features (Exercise 12.2).
These models are quite general, and include, for example, th e entire fam-
ily of smoothing splines, additive and interaction spline m odels discussed
1For logistic regression with separable data, ˆβλdiverges, but ˆβλ//bardblˆβλ/bardblconverges to
the optimal separating direction.

12.3 Support Vector Machines and Kernels 429
in Chapters 5 and 9, and in more detail in Wahba (1990) and Hast ie and
Tibshirani (1990). They can be expressed more generally as
min
f∈HN/summationdisplay
i=1[1−yif(xi)]++λJ(f), (12.30)
whereHis the structured space of functions, and J(f) an appropriate reg-
ularizer on that space. For example, suppose His the space of additive
functionsf(x) =/summationtextp
j=1fj(xj), andJ(f) =/summationtext
j/integraltext
{f′′
j(xj)}2dxj. Then the
solution to (12.30) is an additive cubic spline, and has a ker nel representa-
tion (12.28) with K(x,x′) =/summationtextp
j=1Kj(xj,x′
j). Each of the Kjis the kernel
appropriate for the univariate smoothing spline in xj(Wahba, 1990).
Converselythisdiscussionalsoshowsthat,forexample, anyofthekernels
described in (12.22) above can be used with anyconvex loss function, and
will also lead to a ﬁnite-dimensional representation of the form (12.28).
Figure 12.5 uses the same kernel functions as in Figure 12.3, except using
the binomial log-likelihood as a loss function2. The ﬁtted function is hence
an estimate of the log-odds,
ˆf(x) = logˆPr(Y= +1|x)
ˆPr(Y=−1|x)
=ˆβ0+N/summationdisplay
i=1ˆαiK(x,xi), (12.31)
or conversely we get an estimate of the class probabilities
ˆPr(Y= +1|x) =1
1+e−ˆβ0−/summationtextN
i=1ˆαiK(x,xi). (12.32)
The ﬁtted models are quite similar in shape and performance. Examples
and more details are given in Section 5.8.
It does happen that for SVMs, a sizable fraction of the Nvalues ofαi
can be zero (the nonsupport points). In the two examples in Fi gure 12.3,
these fractions are 42% and 45%, respectively. This is a cons equence of the
piecewise linear nature of the ﬁrst part of the criterion (12 .25). The lower
the class overlap (on the training data), the greater this fr action will be.
Reducingλwill generally reduce the overlap (allowing a more ﬂexible f).
A small number of support points means that ˆf(x) can be evaluated more
quickly, which is important at lookup time. Of course, reduc ing the overlap
too much can lead to poor generalization.
2Ji Zhu assisted in the preparation of these examples.

430 12. Flexible Discriminants
LR - Degree-4 Polynomial in Feature Space
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . .. . . . . . .. . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.190
Test Error:       0.263
Bayes Error:    0.210
LR - Radial Kernel in Feature Space
.. . . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.150
Test Error:       0.221
Bayes Error:    0.210
FIGURE 12.5. The logistic regression versions of the SVM models in Fig-
ure 12.3, using the identical kernels and hence penalties, but the log-likelihood
loss instead of the SVM loss function. The two broken contours c orrespond to
posterior probabilities of 0.75and0.25for the +1 class (or vice versa). The bro-
ken purple curve in the background is the Bayes decision bound ary.

12.3 Support Vector Machines and Kernels 431
TABLE 12.2. Skin of the orange: Shown are mean (standard error of the mean )
of the test error over 50simulations. BRUTO ﬁts an additive spline model adap-
tively, while MARS ﬁts a low-order interaction model adaptively .
Test Error (SE)
Method No Noise Features Six Noise Features
1 SV Classiﬁer 0.450 (0.003) 0.472 (0.003)
2 SVM/poly 2 0.078 (0.003) 0.152 (0.004)
3 SVM/poly 5 0.180 (0.004) 0.370 (0.004)
4 SVM/poly 10 0.230 (0.003) 0.434 (0.002)
5 BRUTO 0.084 (0.003) 0.090 (0.003)
6 MARS 0.156 (0.004) 0.173 (0.005)
Bayes 0.029 0.029
12.3.4 SVMs and the Curse of Dimensionality
In this section, we address the question of whether SVMs have some edge
on the curse of dimensionality. Notice that in expression (1 2.23) we are not
allowed a fully general inner product in the space of powers a nd products.
For example, all terms of the form 2 XjX′
jare given equal weight, and the
kernel cannot adapt itself to concentrate on subspaces. If t he number of
featurespwere large, but the class separation occurred only in the lin ear
subspace spanned by say X1andX2, this kernel would not easily ﬁnd the
structure and would suﬀer from having many dimensions to sea rch over.
One would have to build knowledge about the subspace into the kernel;
that is, tell it to ignore all but the ﬁrst two inputs. If such k nowledge were
available a priori, much of statistical learning would be ma de much easier.
A major goal of adaptive methods is to discover such structur e.
We support these statements with an illustrative example. W e generated
100 observations in each of two classes. The ﬁrst class has fo ur standard
normal independent features X1,X2,X3,X4. The second class also has four
standard normal independent features, but conditioned on 9 ≤/summationtextX2
j≤16.
This is a relatively easy problem. As a second harder problem , we aug-
mented the features with an additional six standard Gaussia n noise fea-
tures. Hence the second class almost completely surrounds t he ﬁrst, like the
skin surrounding the orange, in a four-dimensional subspac e. The Bayes er-
ror rate for this problem is 0 .029 (irrespective of dimension). We generated
1000 test observations to compare diﬀerent procedures. The average test
errors over 50 simulations, with and without noise features , are shown in
Table 12.2.
Line 1 uses the support vector classiﬁer in the original feat ure space.
Lines2–4refertothesupportvectormachinewitha2-,5-and 10-dimension-
al polynomial kernel. For all support vector procedures, we chose the cost
parameterCto minimize the test error, to be as fair as possible to the

432 12. Flexible Discriminants
1e−01 1e+01 1e+030.20 0.25 0.30 0.35
1e−01 1e+01 1e+03 1e−01 1e+01 1e+03 1e−01 1e+01 1e+03Test Error
CTest Error Curves − SVM with Radial Kernel
γ= 5 γ= 1 γ= 0.5 γ= 0.1
FIGURE 12.6. Test-error curves as a function of the cost parameter Cfor
the radial-kernel SVM classiﬁer on the mixture data. At the top of each plot is
the scale parameter γfor the radial kernel: Kγ(x,y) = exp( −γ||x−y||2). The
optimal value for Cdepends quite strongly on the scale of the kernel. The Bayes
error rate is indicated by the broken horizontal lines.
method. Line 5 ﬁts an additive spline model to the ( −1,+1) response by
least squares, using the BRUTO algorithm for additive model s, described
in Hastie and Tibshirani (1990). Line 6 uses MARS (multivari ate adaptive
regression splines) allowing interaction of all orders, as described in Chap-
ter 9; as such it is comparable with the SVM/poly 10. Both BRUT O and
MARS have the ability to ignore redundant variables. Test er ror was not
used to choose the smoothing parameters in either of lines 5 o r 6.
In the original feature space, a hyperplane cannot separate the classes,
and the support vector classiﬁer (line 1) does poorly. The po lynomial sup-
port vector machine makes a substantial improvement in test error rate,
but is adversely aﬀected by the six noise features. It is also very sensitive to
the choice of kernel: the second degree polynomial kernel (l ine 2) does best,
since the true decision boundary is a second-degree polynom ial. However,
higher-degree polynomial kernels (lines 3 and 4) do much wor se. BRUTO
performs well, since the boundary is additive. BRUTO and MAR S adapt
well: their performance does not deteriorate much in the pre sence of noise.
12.3.5 A Path Algorithm for the SVM Classiﬁer
The regularization parameter for the SVM classiﬁer is the co st parameter
C, or its inverse λin (12.25). Common usage is to set Chigh, leading often
to somewhat overﬁt classiﬁers.
Figure 12.6 shows the test error on the mixture data as a funct ion of
C, using diﬀerent radial-kernel parameters γ. Whenγ= 5 (narrow peaked
kernels), the heaviest regularization (small C) is called for. With γ= 1

12.3 Support Vector Machines and Kernels 433
−0.5 0.0 0.5 1.0 1.5 2.0−1.0 −0.5 0.0 0.5 1.0 1.5789
1011
12
123
45
61/||β|| f(x) = 0f(x) = +1
f(x) =−1
0.0 0.2 0.4 0.6 0.8 1.00 2 4 6 8 101
2
34
56
789
1011
12
αi(λ)λ
FIGURE 12.7. A simple example illustrates the SVM path algorithm. (left
panel:) This plot illustrates the state of the model at λ= 1/2. The ‘‘+1” points
are orange, the “ −1” blue. The width of the soft margin is 2/||β||= 2×0.587.
Two blue points {3,5}are misclassiﬁed, while the two orange points {10,12}are
correctly classiﬁed, but on the wrong side of their margin f(x) = +1; each of
these hasyif(xi)<1. The three square shaped points {2,6,7}are exactly on
their margins. (right panel:) This plot shows the piecewise lin ear proﬁles αi(λ).
The horizontal broken line at λ= 1/2indicates the state of the αifor the model
in the left plot.
(the value used in Figure 12.3), an intermediate value of Cis required.
Clearly in situations such as these, we need to determine a go od choice
forC, perhaps by cross-validation. Here we describe a path algor ithm (in
the spirit of Section 3.8) for eﬃciently ﬁtting the entire se quence of SVM
models obtained by varying C.
It is convenient to use the loss+penalty formulation (12.25 ), along with
Figure 12.4. This leads to a solution for βat a given value of λ:
βλ=1
λN/summationdisplay
i=1αiyixi. (12.33)
Theαiare again Lagrange multipliers, but in this case they all lie in [0,1].
Figure 12.7 illustrates the setup. It can be shown that the KK T optimal-
ity conditions imply that the labeled points ( xi,yi) fall into three distinct
groups:

434 12. Flexible Discriminants
•Observationscorrectlyclassiﬁedandoutsidetheirmargin s.Theyhave
yif(xi)>1, and Lagrange multipliers αi= 0. Examples are the
orange points 8, 9 and 11, and the blue points 1 and 4.
•Observationssittingontheirmarginswith yif(xi) = 1,withLagrange
multipliers αi∈[0,1]. Examples are the orange 7 and the blue 2 and
6.
•Observations inside their margins have yif(xi)<1, withαi= 1.
Examples are the blue 3 and 5, and the orange 10 and 12.
The idea for the path algorithm is as follows. Initially λis large, the
margin 1/||βλ||is wide, and all points are inside their margin and have
αi= 1. Asλdecreases, 1 /||βλ||decreases, and the margin gets narrower.
Some points will move from inside their margins to outside th eir margins,
andtheirαiwillchangefrom1to0.Bycontinuity ofthe αi(λ),thesepoints
willlingeron the margin during this transition. From (12.33) we see tha t
the points with αi= 1 make ﬁxed contributions to β(λ), and those with
αi= 0 make no contribution. So all that changes as λdecreases are the
αi∈[0,1] of those (small number) of points on the margin. Since all t hese
points have yif(xi) = 1, this results in a small set of linear equations that
prescribe how αi(λ) and hence βλchanges during these transitions. This
results in piecewise linear paths for each of the αi(λ). The breaks occur
when points cross the margin. Figure 12.7 (right panel) show s theαi(λ)
proﬁles for the small example in the left panel.
Although we have described this for linear SVMs, exactly the same idea
works for nonlinear models, in which (12.33) is replaced by
fλ(x) =1
λN/summationdisplay
i=1αiyiK(x,xi). (12.34)
Details can be found in Hastie et al. (2004). An Rpackagesvmpath is
available on CRAN for ﬁtting these models.
12.3.6 Support Vector Machines for Regression
In this section we show how SVMs can be adapted for regression with a
quantitative response, in ways that inherit some of the prop erties of the
SVM classiﬁer. We ﬁrst discuss the linear regression model
f(x) =xTβ+β0, (12.35)
and then handle nonlinear generalizations. To estimate β, we consider min-
imization of
H(β,β0) =N/summationdisplay
i=1V(yi−f(xi))+λ
2∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l2, (12.36)

12.3 Support Vector Machines and Kernels 435
-4 -2 0 2 4-1 0 1 2 3 4
-4 -2 0 2 40 2 4 6 8 10 12ǫ−ǫ c −cVH(r)Vǫ(r)
r r
FIGURE 12.8. The left panel shows the ǫ-insensitive error function used by the
support vector regression machine. The right panel shows th e error function used
in Huber’s robust regression (blue curve). Beyond |c|, the function changes from
quadratic to linear.
where
Vǫ(r) =/braceleftigg
0 if|r|<ǫ,
|r|−ǫ,otherwise.(12.37)
This is an “ ǫ-insensitive” error measure, ignoring errors of size less t han
ǫ(left panel of Figure 12.8). There is a rough analogy with the support
vector classiﬁcation setup, where points on the correct sid e of the deci-
sion boundary and far away from it, are ignored in the optimiz ation. In
regression, these “low error” points are the ones with small residuals.
It is interesting to contrast this with error measures used i n robust re-
gression in statistics. The most popular, due to Huber (1964 ), has the form
VH(r) =/braceleftigg
r2/2 if|r|≤c,
c|r|−c2/2,|r|>c,(12.38)
shownintherightpanelofFigure12.8.Thisfunctionreduce sfromquadratic
to linear the contributions of observations with absolute r esidual greater
than a prechosen constant c. This makes the ﬁtting less sensitive to out-
liers. The support vector error measure (12.37) also has lin ear tails (beyond
ǫ), but in addition it ﬂattens the contributions of those case s with small
residuals.
Ifˆβ,ˆβ0are the minimizers of H, the solution function can be shown to
have the form
ˆβ=N/summationdisplay
i=1(ˆα∗
i−ˆαi)xi, (12.39)
ˆf(x) =N/summationdisplay
i=1(ˆα∗
i−ˆαi)∝an}⌊∇a⌋ketle{tx,xi∝an}⌊∇a⌋ket∇i}ht+β0, (12.40)

436 12. Flexible Discriminants
where ˆαi,ˆα∗
iare positive and solve the quadratic programming problem
min
αi,α∗
iǫN/summationdisplay
i=1(α∗
i+αi)−N/summationdisplay
i=1yi(α∗
i−αi)+1
2N/summationdisplay
i,i′=1(α∗
i−αi)(α∗
i′−αi′)∝an}⌊∇a⌋ketle{txi,xi′∝an}⌊∇a⌋ket∇i}ht
subject to the constraints
0≤αi, α∗
i≤1/λ,
N/summationdisplay
i=1(α∗
i−αi) = 0, (12.41)
αiα∗
i= 0.
Duetothenatureoftheseconstraints,typicallyonlyasubs etofthesolution
values (ˆα∗
i−ˆαi) are nonzero, and the associated data values are called the
support vectors. As was the case in the classiﬁcation settin g, the solution
depends on the input values only through the inner products ∝an}⌊∇a⌋ketle{txi,xi′∝an}⌊∇a⌋ket∇i}ht. Thus
we can generalize the methods to richer spaces by deﬁning an a ppropriate
inner product, for example, one of those deﬁned in (12.22).
Note that there are parameters, ǫandλ, associated with the criterion
(12.36). These seem to play diﬀerent roles. ǫis a parameter of the loss
functionVǫ, just likecis forVH. Note that both VǫandVHdepend on the
scale ofyand hencer. If we scale our response (and hence use VH(r/σ) and
Vǫ(r/σ)instead),thenwemightconsiderusingpresetvaluesfor candǫ(the
valuec= 1.345 achieves 95% eﬃciency for the Gaussian). The quantity λ
is a more traditional regularization parameter, and can be e stimated for
example by cross-validation.
12.3.7 Regression and Kernels
As discussed in Section 12.3.3, this kernel property is not u nique to sup-
port vector machines. Suppose we consider approximation of the regression
function in terms of a set of basis functions {hm(x)},m= 1,2,...,M:
f(x) =M/summationdisplay
m=1βmhm(x)+β0. (12.42)
To estimate βandβ0we minimize
H(β,β0) =N/summationdisplay
i=1V(yi−f(xi))+λ
2/summationdisplay
β2
m (12.43)
for some general error measure V(r). For any choice of V(r), the solution
ˆf(x) =/summationtextˆβmhm(x)+ˆβ0has the form
ˆf(x) =N/summationdisplay
i=1ˆaiK(x,xi) (12.44)

12.3 Support Vector Machines and Kernels 437
withK(x,y) =/summationtextM
m=1hm(x)hm(y). Notice that this has the same form
as both the radial basis function expansion and a regulariza tion estimate,
discussed in Chapters 5 and 6.
For concreteness, let’s work out the case V(r) =r2. LetHbe theN×M
basis matrix with imth element hm(xi), and suppose that M >Nis large.
For simplicity we assume that β0= 0, or that the constant is absorbed in
h; see Exercise 12.3 for an alternative.
We estimate βby minimizing the penalized least squares criterion
H(β) = (y−Hβ)T(y−Hβ)+λ∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l2. (12.45)
The solution is
ˆy=Hˆβ (12.46)
withˆβdetermined by
−HT(y−Hˆβ)+λˆβ= 0. (12.47)
From this it appears that we need to evaluate the M×Mmatrix of inner
products in the transformed space. However, we can premulti ply byHto
give
Hˆβ= (HHT+λI)−1HHTy. (12.48)
TheN×NmatrixHHTconsists of inner products between pairs of obser-
vationsi,i′; that is, the evaluation of an inner product kernel {HHT}i,i′=
K(xi,xi′). It is easy to show (12.44) directly in this case, that the pr edicted
values at an arbitrary xsatisfy
ˆf(x) =h(x)Tˆβ
=N/summationdisplay
i=1ˆαiK(x,xi), (12.49)
where ˆα= (HHT+λI)−1y. As in the support vector machine, we need not
specify or evaluate the large set of functions h1(x),h2(x),...,h M(x). Only
the inner product kernel K(xi,xi′) need be evaluated, at the Ntraining
points for each i,i′and at points xfor predictions there. Careful choice
ofhm(such as the eigenfunctions of particular, easy-to-evalua te kernels
K) means, for example, that HHTcan be computed at a cost of N2/2
evaluations of K, rather than the direct cost N2M.
Note, however, that this property depends on the choice of sq uared norm
∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l2in the penalty. It does not hold, for example, for the L1norm|β|,
which may lead to a superior model.

438 12. Flexible Discriminants
12.3.8 Discussion
The support vector machine can be extended to multiclass pro blems, es-
sentially by solving many two-class problems. A classiﬁer i s built for each
pair of classes, and the ﬁnal classiﬁer is the one that domina tes the most
(Kressel,1999;Friedman,1996;HastieandTibshirani,199 8).Alternatively,
one could use the multinomial loss function along with a suit able kernel,
as in Section 12.3.3. SVMs have applications in many other su pervised
and unsupervised learning problems. At the time of this writ ing, empirical
evidence suggests that it performs well in many real learnin g problems.
Finally, we mention the connection of the support vector mac hine and
structural risk minimization (7.9). Suppose the training p oints (or their
basis expansion) are contained in a sphere of radius R, and letG(x) =
sign[f(x)] = sign[βTx+β0] as in (12.2). Then one can show that the class
of functions{G(x),∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l≤A}has VC-dimension hsatisfying
h≤R2A2. (12.50)
Iff(x) separates the training data, optimally for ∝⌊a∇⌈⌊lβ∝⌊a∇⌈⌊l ≤A, then with
probability at least 1 −ηover training sets (Vapnik, 1996, page 139):
ErrorTest≤4h[log(2N/h)+1]−log(η/4)
N. (12.51)
The support vector classiﬁer was one of the ﬁrst practical le arning pro-
cedures for which useful bounds on the VC dimension could be o btained,
and hence the SRM program could be carried out. However in the deriva-
tion, balls are put around the data points—a process that depe nds on the
observed values of the features. Hence in a strict sense, the VC complexity
of the class is not ﬁxed a priori, before seeing the features.
The regularization parameter Ccontrols an upper bound on the VC
dimensionoftheclassiﬁer.FollowingtheSRMparadigm,wec ouldchoose C
byminimizingtheupperboundonthetesterror,givenin(12. 51). However,
it is not clear that this has any advantage over the use of cros s-validation
for choice of C.
12.4 Generalizing Linear Discriminant Analysis
In Section 4.3 we discussed linear discriminant analysis (L DA), a funda-
mental tool for classiﬁcation. For the remainder of this cha pter we discuss
a class of techniques that produce better classiﬁers than LD A by directly
generalizing LDA.
Some of the virtues of LDA are as follows:
•Itisasimpleprototypeclassiﬁer.Anewobservationisclas siﬁedtothe
class with closest centroid. A slight twist is that distance is measured
in the Mahalanobis metric, using a pooled covariance estima te.

12.4 Generalizing Linear Discriminant Analysis 439
•LDA is the estimated Bayes classiﬁer if the observations are multi-
variate Gaussian in each class, with a common covariance mat rix.
Since this assumption is unlikely to be true, this might not s eem to
be much of a virtue.
•The decision boundaries created by LDA are linear, leading t o deci-
sion rules that are simple to describe and implement.
•LDA provides natural low-dimensional views of the data. For exam-
ple, Figure 12.12 is an informative two-dimensional view of data in
256 dimensions with ten classes.
•Often LDA produces the best classiﬁcation results, because of its
simplicity and low variance. LDA was among the top three clas siﬁers
for 7 of the 22 datasets studied in the STATLOG project (Michi e et
al., 1994)3.
Unfortunately the simplicity of LDA causes it to fail in a num ber of situa-
tions as well:
•Oftenlineardecisionboundariesdonotadequatelyseparat etheclasses.
WhenNis large, it is possible to estimate more complex decision
boundaries. Quadratic discriminant analysis (QDA) is ofte n useful
here, and allows for quadratic decision boundaries. More ge nerally
we would like to be able to model irregular decision boundari es.
•The aforementioned shortcoming of LDA can often be paraphra sed
by saying that a single prototype per class is insuﬃcient. LD A uses
a single prototype (class centroid) plus a common covarianc e matrix
to describe the spread of the data in each class. In many situa tions,
several prototypes are more appropriate.
•At the other end of the spectrum, we may have way too many (corr e-
lated)predictors,forexample,inthecaseofdigitizedana loguesignals
and images. In this case LDA uses too many parameters, which a re
estimated with high variance, and its performance suﬀers. I n cases
such as this we need to restrict or regularize LDA even furthe r.
In the remainder of this chapter we describe a class of techni ques that
attend to all these issues by generalizing the LDA model. Thi s is achieved
largely by three diﬀerent ideas.
TheﬁrstideaistorecasttheLDAproblemasalinearregressi onproblem.
Many techniques exist for generalizing linear regression t o more ﬂexible,
nonparametric forms of regression. This in turn leads to mor e ﬂexible forms
of discriminant analysis, which we call FDA. In most cases of interest, the
3This study predated the emergence of SVMs.

440 12. Flexible Discriminants
regression procedures can be seen to identify an enlarged se t of predictors
via basis expansions. FDA amounts to LDA in this enlarged spa ce, the
same paradigm used in SVMs.
In the case of too many predictors, such as the pixels of a digi tized image,
we do not want to expand the set: it is already too large. The se cond idea is
to ﬁt an LDA model, but penalize its coeﬃcients to be smooth or otherwise
coherent in the spatial domain, that is, as an image. We call t his procedure
penalized discriminant analysis or PDA. With FDA itself, the expanded
basis set is often so large that regularization is also requi red (again as in
SVMs). Both of these can be achieved via a suitably regulariz ed regression
in the context of the FDA model.
The third idea is to model each class by a mixture of two or more Gaus-
sians with diﬀerent centroids, but with every component Gau ssian, both
withinandbetweenclasses,sharingthesamecovariancemat rix.Thisallows
for more complex decision boundaries, and allows for subspa ce reduction
as in LDA. We call this extension mixture discriminant analysis or MDA.
All three of these generalizations use a common framework by exploiting
their connection with LDA.
12.5 Flexible Discriminant Analysis
In this section we describe a method for performing LDA using linear re-
gression on derived responses. This in turn leads to nonpara metric and ﬂex-
ible alternatives to LDA. As in Chapter 4, we assume we have ob servations
with a quantitative response Gfalling into one of KclassesG={1,...,K},
each having measured features X. Supposeθ:G∝ma√sto→IR1is a function that
assigns scores to the classes, such that the transformed cla ss labels are op-
timally predicted by linear regression on X: If our training sample has the
form (gi,xi), i= 1,2,...,N, then we solve
min
β,θN/summationdisplay
i=1/parenleftbig
θ(gi)−xT
iβ/parenrightbig2, (12.52)
with restrictions on θto avoid a trivial solution (mean zero and unit vari-
ance over the training data). This produces a one-dimension al separation
between the classes.
More generally, we can ﬁnd up to L≤K−1 sets of independent scorings
fortheclasslabels, θ1,θ2,...,θ L,andLcorrespondinglinearmaps ηℓ(X) =
XTβℓ,ℓ= 1,...,L, chosen to be optimal for multiple regression in IRp. The
scoresθℓ(g) and the maps βℓare chosen to minimize the average squared
residual,
ASR=1
NL/summationdisplay
ℓ=1/bracketleftiggN/summationdisplay
i=1/parenleftbig
θℓ(gi)−xT
iβℓ/parenrightbig2/bracketrightigg
. (12.53)

12.5 Flexible Discriminant Analysis 441
The set of scores are assumed to be mutually orthogonal and no rmalized
with respect to an appropriate inner product to prevent triv ial zero
solutions.
Why are we going down this road? It can be shown that the sequen ce
of discriminant (canonical) vectors νℓderived in Section 4.3.3 are identical
to the sequence βℓup to a constant (Mardia et al., 1979; Hastie et al.,
1995). Moreover, the Mahalanobis distance of a test point xto thekth
class centroid ˆ µkis given by
δJ(x,ˆµk) =K−1/summationdisplay
ℓ=1wℓ(ˆηℓ(x)−¯ηk
ℓ)2+D(x), (12.54)
where ¯ηk
ℓis the mean of the ˆ ηℓ(xi) in thekth class, and D(x) does not
depend on k. Herewℓare coordinate weights that are deﬁned in terms of
the mean squared residual r2
ℓof theℓth optimally scored ﬁt
wℓ=1
r2
ℓ(1−r2
ℓ). (12.55)
In Section 4.3.2 we saw that these canonical distances are al l that is needed
forclassiﬁcationintheGaussiansetup,withequalcovaria ncesineachclass.
To summarize:
LDA can be performed by a sequence of linear regressions, fol -
lowed by classiﬁcation to the closest class centroid in the s pace
of ﬁts. The analogy applies both to the reduced rank version,
or the full rank case when L=K−1.
The real power of this result is in the generalizations that i t invites. We
can replace the linear regression ﬁts ηℓ(x) =xTβℓby far more ﬂexible,
nonparametric ﬁts, and by analogy achieve a more ﬂexible cla ssiﬁer than
LDA. We have in mind generalized additive ﬁts, spline functi ons, MARS
models and the like. In this more general form the regression problems are
deﬁned via the criterion
ASR({θℓ,ηℓ}L
ℓ=1) =1
NL/summationdisplay
ℓ=1/bracketleftiggN/summationdisplay
i=1(θℓ(gi)−ηℓ(xi))2+λJ(ηℓ)/bracketrightigg
,(12.56)
whereJisaregularizerappropriateforsomeformsofnonparametri cregres-
sion, such as smoothing splines, additive splines and lower -order ANOVA
spline models. Also included are the classes of functions an d associated
penalties generated by kernels, as in Section 12.3.3.
Before we describe the computations involved in this genera lization, let
us consider a very simple example. Suppose we use degree-2 po lynomial
regression for each ηℓ. The decision boundaries implied by the (12.54) will
be quadratic surfaces, since each of the ﬁtted functions is q uadratic, and as

442 12. Flexible Discriminants
-2 0 2-2 0 2o
ooo
oo
ooo
oo
o o
o
oooo
oo
oo oo
o
o
ooo
oooo
oo
ooo
oo
oo
oo
oo
ooo
o
ooo
oo
o
oo
oo
oo
o
oo
o
ooo
oo
o
oo
o
oooo
ooo
oo
oo
ooo
oo o
oo
ooo
oo
o
FIGURE 12.9. The data consist of 50points generated from each of N(0,I)and
N(0,9
4I). The solid black ellipse is the decision boundary found by FDA us ing
degree-two polynomial regression. The dashed purple circle is the Bayes decision
boundary.
in LDA their squares cancel out when comparing distances. We could have
achieved identical quadratic boundaries in a more conventional way, by
augmenting our original predictors with their squares and c ross-products.
In the enlarged space one performs an LDA, and the linear boun daries in
the enlarged space map down to quadratic boundaries in the or iginal space.
A classic example is a pair of multivariate Gaussians center ed at the origin,
one having covariance matrix I, and the other cIforc >1; Figure 12.9
illustrates. The Bayes decision boundary is the sphere ∝⌊a∇⌈⌊lx∝⌊a∇⌈⌊l=pclogc
2(c−1), which
is a linear boundary in the enlarged space.
Many nonparametric regression procedures operate by gener ating a basis
expansion of derived variables, and then performing a linea r regression in
the enlarged space. The MARS procedure (Chapter 9) is exactl y of this
form. Smoothing splines and additive spline models generat e an extremely
large basis set ( N×pbasis functions for additive splines), but then perform
a penalized regression ﬁt in the enlarged space. SVMs do as we ll; see also
the kernel-based regression example in Section 12.3.7. FDA in this case can
beshowntoperforma penalized linear discriminant analysis intheenlarged
space.WeelaborateinSection12.6.Linearboundariesinth eenlargedspace
map down to nonlinear boundaries in the reduced space. This i s exactly the
same paradigm that is used with support vector machines (Sec tion 12.3).
We illustrate FDA on the speech recognition example used in C hapter 4,
withK= 11 classes and p= 10 predictors. The classes correspond to

12.5 Flexible Discriminant Analysis 443
oooo oo
oooooooo
oooo
oo
o
o
o
o
oooooooooooo
ooo
o
oooooooo
ooooooo
o
oo
o
ooooo
o
ooooooo
oooooo
o
o
ooooooooooooo
ooooooooooo
oooo
oooo
o
ooooo
oooooo o
o
oooooooooooo
oo
o
oo
ooooooooooooo
oooooooooooooooooooooooooooooo
o
o
ooooooo
o
o
ooooooo
oooooooooooo
o
o
oooooooooooooooo
oo
o
o
oooooooo
ooooooooooooooooo
oooo
o
o
ooooo
o
ooooo
o
oo
o
o
o
o
ooooooooooooo
oo
oooooo
ooooooooo
ooo
ooooooooo
o
oo
o
ooo
ooooooooooooooo
ooo
oooooooo
oooo
ooooooooooooo
o
oooo
ooooo
oooooo
oo
o
o
o
o
oooooo
o
oooooo
oooo
o
ooooooooooooo
o ooooooooooooooooo
o
o
ooooo
ooooooooooo
o
o
ooo
ooooooo
oooooo
oooooo
oooooooooo
o
ooooooo
oooooooooooo
oo
o
ooo
Coordinate 1 for Training DataCoordinate 2 for Training DataLinear Discriminant Analysis
oo oooooo
oooo
oooooooooooo
oooooo
o
ooo
o
oooooo
oooo
ooo
ooooooo
ooooo
oooooo
ooooooooooo
ooooooo
ooooooo
o
oooo
oooooo
ooo ooooooooo
oooooooo
oooo
oooooo
o
oooo
ooooooo
oooooo
oooooooooooooooooo
oooooooooooooooooooooooooooo
ooooooooooooo
ooo
o
o oooooooooooooooooooo
oooo
o
ooooooo
oo
oooo
ooo
oo
ooooo
ooo
o
oooo ooooooooo
o
o
oooo
oooooooooo
ooo
oo
oo
ooo
ooo
o
o
o
oooooo
o
oooo
o
oooooo
ooo
ooooooo
ooo
ooooooooo
oooo
o
ooo
ooooo
o
ooooooo
o
oooooo
o
oooo
oooo
o
ooo
oooo
o
oooooo
o
oooooooooooo
oo
o
ooo
o
o
o
oooooo
o
ooooooooooooo
ooooo
oooooooo
oooo
oooooo
ooooooooo
o
oo
ooo
oooo
oooooo
o
oooo
ooooooooooo
ooooo oo
oooo
oooo
oooo
Coordinate 1 for Training DataCoordinate 2 for Training DataFlexible Discriminant Analysis -- Bruto
FIGURE 12.10. The left plot shows the ﬁrst two LDA canonical variates for
the vowel training data. The right plot shows the correspondi ng projection when
FDA/BRUTO is used to ﬁt the model; plotted are the ﬁtted regress ion functions
ˆη1(xi)andˆη2(xi). Notice the improved separation. The colors represent the ele ven
diﬀerent vowel sounds.
11 vowel sounds, each contained in 11 diﬀerent words. Here ar e the words,
preceded by the symbols that represent them:
Vowel Word Vowel Word Vowel Word Vowel Word
i: heed O hod I hid C: hoard
E head U hood A had u: who’d
a: hard 3: heard Y hud
Each of eight speakers spoke each word six times in the traini ng set, and
likewise seven speakers in the test set. The ten predictors a re derived from
thedigitizedspeechinarathercomplicatedway,butstanda rdinthespeech
recognition world. There are thus 528 training observation s, and 462 test
observations. Figure 12.10 shows two-dimensional project ions produced by
LDA and FDA. The FDA model used adaptive additive-spline reg ression
functions to model the ηℓ(x), and the points plotted in the right plot have
coordinates ˆ η1(xi) and ˆη2(xi). The routine used in S-PLUS is called bruto,
hence the heading on the plot and in Table 12.3. We see that ﬂex ible model-
ing has helped to separate the classes in this case. Table 12. 3 shows training
and test error rates for a number of classiﬁcation technique s. FDA/MARS
refers to Friedman’s multivariate adaptive regression spl ines; degree = 2
means pairwise products are permitted. Notice that for FDA/ MARS, the
best classiﬁcation results are obtained in a reduced-rank s ubspace.

444 12. Flexible Discriminants
TABLE 12.3. Vowel recognition data performance results. The results for n eural
networks are the best among a much larger set, taken from a neur al network
archive. The notation FDA/BRUTO refers to the regression me thod used with
FDA.
Technique Error Rates
Training Test
(1) LDA 0.32 0.56
Softmax 0.48 0.67
(2) QDA 0.01 0.53
(3) CART 0.05 0.56
(4) CART (linear combination splits) 0.05 0.54
(5) Single-layer perceptron 0.67
(6) Multi-layer perceptron (88 hidden units) 0.49
(7) Gaussian node network (528 hidden units) 0.45
(8) Nearest neighbor 0.44
(9) FDA/BRUTO 0.06 0.44
Softmax 0.11 0.50
(10) FDA/MARS (degree = 1) 0.09 0.45
Best reduced dimension (=2) 0.18 0.42
Softmax 0.14 0.48
(11) FDA/MARS (degree = 2) 0.02 0.42
Best reduced dimension (=6) 0.13 0.39
Softmax 0.10 0.50
12.5.1 Computing the FDA Estimates
The computations for the FDA coordinates can be simpliﬁed in many im-
portant cases, in particular when the nonparametric regres sion procedure
can be represented as a linear operator. We will denote this o perator by
Sλ; that is, ˆy=Sλy, whereyis the vector of responses and ˆythe vector
of ﬁts. Additive splines have this property, if the smoothin g parameters are
ﬁxed, as does MARS once the basis functions are selected. The subscriptλ
denotes the entire set of smoothing parameters. In this case optimal scoring
is equivalent to a canonical correlation problem, and the so lution can be
computedbyasingleeigen-decomposition.Thisispursuedi nExercise12.6,
and the resulting algorithm is presented here.
We create an N×Kindicator response matrix Yfrom the responses gi,
such thatyik= 1 ifgi=k, otherwise yik= 0. For a ﬁve-class problem Y
might look like the following:

12.5 Flexible Discriminant Analysis 445

C1C2C3C4C5
g1= 2 0 1 0 0 0
g2= 1 1 0 0 0 0
g3= 1 1 0 0 0 0
g4= 5 0 0 0 0 1
g5= 4 0 0 0 1 0
......
gN= 3 0 0 1 0 0

Here are the computational steps:
1.Multivariate nonparametric regression. Fit a multiresponse, adaptive
nonparametric regression of YonX, giving ﬁtted values ˆY. LetSλ
be the linear operator that ﬁts the ﬁnal chosen model, and η∗(x) be
the vector of ﬁtted regression functions.
2.Optimal scores. Compute the eigen-decomposition of YTˆY=YTSλY,
where the eigenvectors Θare normalized: ΘTDπΘ=I. HereDπ=
YTY/Nis a diagonal matrix of the estimated class prior
probabilities.
3.Updatethemodelfromstep1usingtheoptimalscores: η(x) =ΘTη∗(x).
The ﬁrst of the Kfunctions in η(x) is the constant function— a trivial
solution; the remaining K−1 functions are the discriminant functions. The
constant function, along with the normalization, causes al l the remaining
functions to be centered.
AgainSλcan correspond to any regression method. When Sλ=HX, the
linear regression projection operator, then FDA is linear d iscriminant anal-
ysis. The software that we reference in the Computational Considerations
section on page 455 makes good use of this modularity; the fdafunction
has amethod=argument that allows one to supply anyregression function,
as long as it follows some natural conventions. The regressi on functions
we provide allow for polynomial regression, adaptive addit ive models and
MARS. They all eﬃciently handle multiple responses, so step (1) is a single
call to a regression routine. The eigen-decomposition in st ep (2) simulta-
neously computes all the optimal scoring functions.
In Section 4.2 we discussed the pitfalls of using linear regr ession on an
indicator response matrix as a method for classiﬁcation. In particular, se-
vere masking can occur with three or more classes. FDA uses th e ﬁts from
such a regression in step (1), but then transforms them furth er to produce
useful discriminant functions that are devoid of these pitf alls. Exercise 12.9
takes another view of this phenomenon.

446 12. Flexible Discriminants
12.6 Penalized Discriminant Analysis
Although FDA is motivated by generalizing optimal scoring, it can also be
viewed directly as a form of regularized discriminant analy sis. Suppose the
regression procedure used in FDA amounts to a linear regress ion onto a
basis expansion h(X), with a quadratic penalty on the coeﬃcients:
ASR({θℓ,βℓ}L
ℓ=1) =1
NL/summationdisplay
ℓ=1/bracketleftiggN/summationdisplay
i=1(θℓ(gi)−hT(xi)βℓ)2+λβT
ℓΩβℓ/bracketrightigg
.(12.57)
The choice of Ωdepends on the problem. If ηℓ(x) =h(x)βℓis an expansion
on spline basis functions, Ωmight constrain ηℓto be smooth over IRp. In
the case of additive splines, there are Nspline basis functions for each
coordinate, resulting in a total of Npbasis functions in h(x);Ωin this case
isNp×Npand block diagonal.
The steps in FDA can then be viewed as a generalized form of LDA ,
which we call penalized discriminant analysis , or PDA:
•Enlarge the set of predictors Xvia a basis expansion h(X).
•Use (penalized) LDA in the enlarged space, where the penaliz ed
Mahalanobis distance is given by
D(x,µ) = (h(x)−h(µ))T(ΣW+λΩ)−1(h(x)−h(µ)),(12.58)
whereΣWis the within-class covariance matrix of the derived vari-
ablesh(xi).
•Decompose the classiﬁcation subspace using a penalized met ric:
maxuTΣBetusubject touT(ΣW+λΩ)u= 1.
Loosely speaking, the penalized Mahalanobis distance tend s to give less
weight to “rough” coordinates, and more weight to “smooth” o nes; since
the penalty is not diagonal, the same applies to linear combi nations that
are rough or smooth.
Forsomeclassesofproblems,theﬁrststep,involvingtheba sisexpansion,
is not needed; we already have far too many (correlated) pred ictors. A
leading example is when the objects to be classiﬁed are digit ized analog
signals:
•the log-periodogram of a fragment of spoken speech, sampled at a set
of 256 frequencies; see Figure 5.5 on page 149.
•the grayscale pixel values in a digitized image of a handwrit ten digit.

12.6 Penalized Discriminant Analysis 447
LDA: Coefficient 1 PDA: Coefficient 1 LDA: Coefficient 2 PDA: Coefficient 2 LDA: Coefficient 3 PDA: Coefficient 3
LDA: Coefficient 4 PDA: Coefficient 4 LDA: Coefficient 5 PDA: Coefficient 5 LDA: Coefficient 6 PDA: Coefficient 6
LDA: Coefficient 7 PDA: Coefficient 7 LDA: Coefficient 8 PDA: Coefficient 8 LDA: Coefficient 9 PDA: Coefficient 9
FIGURE 12.11. The images appear in pairs, and represent the nine discrim-
inant coeﬃcient functions for the digit recognition problem . The left member of
each pair is the LDA coeﬃcient, while the right member is the PD A coeﬃcient,
regularized to enforce spatial smoothness.
It is also intuitively clear in these cases why regularizati on is needed.
Take the digitized image as an example. Neighboring pixel va lues will tend
to be correlated, being often almost the same. This implies t hat the pair
of corresponding LDA coeﬃcients for these pixels can be wild ly diﬀerent
and opposite in sign, and thus cancel when applied to similar pixel values.
Positively correlated predictors lead to noisy, negativel y correlated coeﬃ-
cient estimates, and this noise results in unwanted samplin g variance. A
reasonable strategy is to regularize the coeﬃcients to be smooth over the
spatial domain, as with images. This is what PDA does. The com putations
proceed just as for FDA, except that an appropriate penalize d regression
method is used. Here hT(X)βℓ=Xβℓ, andΩis chosen so that βT
ℓΩβℓ
penalizes roughness in βℓwhen viewed as an image. Figure 1.2 on page 4
shows some examples of handwritten digits. Figure 12.11 sho ws the dis-
criminant variates using LDA and PDA. Those produced by LDA a ppear
assalt-and-pepper images, while those produced by PDA are smooth im-
ages. The ﬁrst smooth image can be seen as the coeﬃcients of a l inear
contrast functional for separating images with a dark centr al vertical strip
(ones, possibly sevens) from images that are hollow in the mi ddle (zeros,
some fours). Figure 12.12 supports this interpretation, an d with more dif-
ﬁculty allows an interpretation of the second coordinate. T his and other

448 12. Flexible Discriminants
-5 0 5-6 -4 -2 0 2 4 60
0
00
0000
0
0000
00
00
00000
00
00000
0
0000
000
00
0
00
00000
0
0000
0
0
0000
0
00000
0
000
00
00
000
000
0000
00
00
000
00
000
00
000
0
0000
0000
000
0
00
00
00
00
00
00
000
00000 000
0000
00
0
000 0
0000
000
000
00 0
0
0000
0
00
0 000
0
000
00
0
0
000
00 00
000
00
0
000
00
0
00
00 0 0
000
0
00
0
000000
0
0 0
000
00
0 00
000 0
00
000
00
000
000
0
000
000
000
000
000
00
000
00
00
00
000
0
000
0000
0
00 00
0000
00
0
000
00
00
0
00
0
0000
0
000000
00
000
0
000
000
00
000
000
00
000
00000
00
0
0000
00
0
0
011
11
11
11
1
1111
111
111
11
1
11
11
111
11
111
11
111
11
1
111
1
11
111
1
111
111
111
11
11
1111
111
11
1
1
111
1
11
1111
1
111
111
111
11
1
11111
11
1
11
111
11111
111
1
11111
1
111
1111
11
11
11
11111
1111
11
1
11
11
1
1
1 1
11
1
111
11
1
11
1
1
111
1
11
111
111
11111
111
1
11
111
111111
111
1
11
11
1111
111
11
11
1 1
111
11
1111111
1
11
111
111
1
11111111
1
111
111
1
22
22
2222
222
22
2
222
22
2
22
2
22
2222
222
2
22
2
2
2
2222
2222
22222
22
2
22
22
2
22
222
222
2
2
22
22
2
222
2
2
22
22
2
2
22
22
222222
222
2
2
2222222
22222
222
222
2
2222
2
2
2
2222
222
22
222
222
2
22
2
2222
2
222
2222
22
222
22
222
2
2
222 22
2
22
22
22
2222
22
222
22
22
2222
333
3
33
3
3 3
333
333
333
33
33
3
33
3
3
333
33333
33
33
3
33333
33
3
3
333
3333
33
3333
33
33
3
33
33
33
3
333
3
3
333
33
333
333
333
3
3
33 3
33333
3
3333
33
3
3
33
3
33
3333
33
3333
33
33
333
333
333
33
3
333333
3
3
33 3
333
333
33
33
33
4
4444
4
4 44444
44
4
44
444
4
444
44
44
4
44
444
44
4
4
44
44
44
44
444
44444
44
444
4
44
44
4
44
4
44444
4
444
44
44
4 44
4
4
444
4
444
44
4444
4
4444
4444
4
444
44
44
444
44
444
44
44
4
4
44444
44
4
4
444
44 44
44
444
4
44
4
44
4
444
444444
444
4444
44
44
4
44
4
444
444
44
444
44
444
4455
55
5
55
555
5555
5
5 5
555
5
555
55
55
55
5
5555
555
55
5555
5555
5
55
55
55
5
555
55
55
55555
555
5 55
5
5555
555
5
5
55
5
555
555
55
55 5
55
5
555
55
55
555
555
55
55
555
55
5 555
55
5
5
5
55
5
555555
5
55
55
55
55
55
55
555
5
55
6
6666
66
66
6
66
66
666
6
66
66
6
666
666
6
6
6
666
6
66
66666
66
6
66
6
666
66
666
66
66
6
66
6
66
666
6
6
66
666
6
6666
6
666
66
66
666
66 6
666
66
6
666
666
6
6
66
666
666
6666
6
6 66
6
66
6
66
66
6666
666
66
6666
666
6666
66
666
666
66
666
6
6
6
7
7777
77
777
77
77
7
777
77777
777
7
77 7
77
77
7
777
777777
7
777
77
777
7
77
77
77777
7
77
77 7
777
77
7
77
777
77777
77
77
7
77777
777
77
7
77
77
777
7
7
7
77
77
77
77
77
7777
7
77
77
777
77
7777
77777
7
7
788
8
8
88
8888
88
8
8
888
88
8
888
88
88
8
888
8
888
88
88
888
88
888
88 888
8888
88
8888
88 88
88
88
8888
88 8
8
8 8
88
888
888
88
8
888
88
888
8
8
88
8888
888
888
88
8
888
88
88
8
8
88888
88
8
8888888 8
8
8
8
88
888 888
888
88
8
888
8
8 888
9999
999
999
99999
999
99
999
999
9
9
99
999
9
9
999
999
999
9999
9
99
999
99
999
99
9
9
9999
99
999
99
9
99
9
9
99
99
9
9
9
9
999
999
9
9
99
9
99
9
99
999
99
9999
99999
9
999
99
999
99
9
999
9
99
999
9
99
99
99
9
99
99
99
9
99
9
999
99
999
9
9999
9
99
999
9901
2
3
456
78
9
PDA: Discriminant Coordinate 1PDA: Discriminant Coordinate 2
FIGURE 12.12. The ﬁrst two penalized canonical variates, evaluated for the
test data. The circles indicate the class centroids. The ﬁrst c oordinate contrasts
mainly0’s and1’s, while the second contrasts 6’s and7/9’s.

12.7 Mixture Discriminant Analysis 449
examples are discussed in more detail in Hastie et al. (1995) , who also show
that the regularization improves the classiﬁcation perfor mance of LDA on
independent test data by a factor of around 25% in the cases th ey tried.
12.7 Mixture Discriminant Analysis
Linear discriminant analysis can be viewed as a prototype classiﬁer. Each
class is represented by its centroid, and we classify to the c losest using an
appropriate metric. In many situations a single prototype i s not suﬃcient
to represent inhomogeneous classes, and mixture models are more appro-
priate. In this section we review Gaussian mixture models an d show how
they can be generalized via the FDA and PDA methods discussed earlier.
A Gaussian mixture model for the kth class has density
P(X|G=k) =Rk/summationdisplay
r=1πkrφ(X;µkr,Σ), (12.59)
where the mixing proportions πkrsum to one. This has Rkprototypes for
thekth class, and in our speciﬁcation, the same covariance matri xΣis
used as the metric throughout. Given such a model for each cla ss, the class
posterior probabilities are given by
P(G=k|X=x) =/summationtextRk
r=1πkrφ(X;µkr,Σ)Πk/summationtextK
ℓ=1/summationtextRℓ
r=1πℓrφ(X;µℓr,Σ)Πℓ, (12.60)
where Π krepresent the class prior probabilities.
We saw these calculations for the special case of two compone nts in
Chapter 8. As in LDA, we estimate the parameters by maximum li kelihood,
using the joint log-likelihood based on P(G,X):
K/summationdisplay
k=1/summationdisplay
gi=klog/bracketleftiggRk/summationdisplay
r=1πkrφ(xi;µkr,Σ)Πk/bracketrightigg
. (12.61)
The sum within the log makes this a rather messy optimization problem
if tackled directly. The classical and natural method for co mputing the
maximum-likelihood estimates (MLEs) for mixture distribu tions is the EM
algorithm (Dempster et al., 1977), which is known to possess good conver-
gence properties. EM alternates between the two steps:

450 12. Flexible Discriminants
E-step:Given the current parameters, compute the responsibility of sub-
classckrwithin class kfor each of the class- kobservations ( gi=k):
W(ckr|xi,gi) =πkrφ(xi;µkr,Σ)/summationtextRk
ℓ=1πkℓφ(xi;µkℓ,Σ).(12.62)
M-step:Compute the weighted MLEs for the parameters of each of the
component Gaussians within each of the classes, using the we ights
from the E-step.
In the E-step, the algorithm apportions the unit weight of an observation
in classkto the various subclasses assigned to that class. If it is clo se to the
centroid of a particular subclass, and far from the others, i t will receive a
massclosetooneforthatsubclass.Ontheotherhand,observ ationshalfway
between two subclasses will get approximately equal weight for both.
In the M-step, an observation in class kis usedRktimes, to estimate the
parameters in each of the Rkcomponent densities, with a diﬀerent weight
foreach.TheEMalgorithm isstudiedindetailinChapter8.T healgorithm
requires initialization, which can have an impact, since mi xture likelihoods
are generally multimodal. Our software (referenced in the Computational
Considerations on page 455) allows several strategies; here we describe the
default. The user supplies the number Rkof subclasses per class. Within
classk, ak-means clustering model, with multiple random starts, is ﬁt ted
to the data. This partitions the observations into Rkdisjoint groups, from
which an initial weight matrix, consisting of zeros and ones , is created.
Our assumption of an equal component covariance matrix Σthroughout
buys an additional simplicity; we can incorporate rank rest rictions in the
mixtureformulationjustlikeinLDA.Tounderstandthis,we reviewalittle-
known fact about LDA. The rank- LLDA ﬁt (Section 4.3.3) is equivalent to
the maximum-likelihood ﬁt of a Gaussian model,where the diﬀ erent mean
vectors in each class are conﬁned to a rank- Lsubspace of IRp(Exercise 4.8).
We can inherit this property for the mixture model, and maxim ize the log-
likelihood (12.61) subject to rank constraints on allthe/summationtext
kRkcentroids:
rank{µkℓ}=L.
Again the EM algorithm is available, and the M-step turns out to be
a weighted version of LDA, with R=/summationtextK
k=1Rk“classes.” Furthermore,
we can use optimal scoring as before to solve the weighted LDA problem,
which allows us to use a weighted version of FDA or PDA at this s tage.
One would expect, in addition to an increase in the number of “ classes,” a
similar increase in the number of “observations” in the kth class by a factor
ofRk. It turns out that this is not the case if linear operators are used for
the optimal scoring regression. The enlarged indicator Ymatrix collapses
in this case to a blurredresponse matrix Z, which is intuitively pleasing.
For example, suppose there are K= 3 classes, and Rk= 3 subclasses per
class. Then Zmight be

12.7 Mixture Discriminant Analysis 451

c11c12c13c21c22c23c31c32c33
g1= 2 0 0 0 0 .3 0.5 0.2 0 0 0
g2= 1 0.9 0.1 0.0 0 0 0 0 0 0
g3= 1 0.1 0.8 0.1 0 0 0 0 0 0
g4= 3 0 0 0 0 0 0 0 .5 0.4 0.1
g5= 2 0 0 0 0 .7 0.1 0.2 0 0 0
......
gN= 3 0 0 0 0 0 0 0 .1 0.1 0.8
,(12.63)
where the entries in a class- krow correspond to W(ckr|x,gi).
The remaining steps are the same:
ˆZ=SZ
ZTˆZ=ΘDΘT
Updateπs and Πs

M-step of MDA .
These simple modiﬁcations add considerable ﬂexibility to t he mixture
model:
•The dimension reduction step in LDA, FDA or PDA is limited by
the number of classes; in particular, for K= 2 classes no reduction is
possible. MDA substitutes subclasses for classes, and then allows us
to look at low-dimensional views of the subspace spanned by t hese
subclass centroids. This subspace will often be an importan t one for
discrimination.
•By using FDA or PDA in the M-step, we can adapt even more to par-
ticular situations. For example, we can ﬁt MDA models to digi tized
analog signals and images, with smoothness constraints bui lt in.
Figure 12.13 compares FDA and MDA on the mixture example.
12.7.1 Example: Waveform Data
We now illustrate some of these ideas on a popular simulated e xample,
taken from Breiman et al. (1984, pages 49–55), and used in Has tie and
Tibshirani (1996b) and elsewhere. It is a three-class probl em with 21 vari-
ables, and is considered to be a diﬃcult pattern recognition problem. The
predictors are deﬁned by
Xj=Uh1(j)+(1−U)h2(j)+ǫjClass 1,
Xj=Uh1(j)+(1−U)h3(j)+ǫjClass 2,(12.64)
Xj=Uh2(j)+(1−U)h3(j)+ǫjClass 3,
wherej= 1,2,...,21,Uis uniform on (0 ,1),ǫjare standard normal vari-
ates, and the hℓare the shifted triangular waveforms: h1(j) = max(6−

452 12. Flexible Discriminants
FDA / MARS - Degree 2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.185
Test Error:       0.235
Bayes Error:    0.210
MDA - 5 Subclasses per Class
.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
•••
••
•••
••••
••
•••
••
•
Training Error: 0.17
Test Error:       0.22
Bayes Error:    0.21
FIGURE 12.13. FDA and MDA on the mixture data. The upper plot uses
FDA with MARS as the regression procedure. The lower plot uses M DA with
ﬁve mixture centers per class (indicated). The MDA solution is close to Bayes
optimal, as might be expected given the data arise from mixtur es of Gaussians.
The broken purple curve in the background is the Bayes decisio n boundary.

12.7 Mixture Discriminant Analysis 453
1 1 1 1 1 1111111111
1
1
1
1
1
1 2 2 2 2 22222222222
2
2
2
2
2
2 3 3 3 3 3333333
3
3
3
3
3
3
3
3
3
3 4 4 4 4 4444444
4
4
4
4
4
44444 5 5 5 5 55555555555
5
5
5
5
5
5Class 1
11111111111
1
1
1
1
1
1 1 1 1 1 22222222222
2
2
2
2
2
2 2 2 2 2 33333333333
3
3
3
3
3
3 3 3 3 3 4444444
4
4
4
4
4
4
4
4
4
4 4 4 4 4 5555555
5
5
5
5
5
5
5
5
5
5 5 5 5 5Class 2
1111111
1
1
1
1
1
111111111 2222222
2
2
2
2
2
222222222 3333333
3
3333333
3
3
3
3
3
3 4444444
4
4
4
4
4
444444444 5555555
5
5
5
5
5
555555555Class 3
FIGURE 12.14. Some examples of the waveforms generated from model (12.64)
before the Gaussian noise is added.
|j−11|,0),h2(j) =h1(j−4) andh3(j) =h1(j+ 4). Figure 12.14 shows
some example waveforms from each class.
Table 12.4 shows the results of MDA applied to the waveform da ta, as
well as several other methods from this and other chapters. E ach train-
ing sample has 300 observations, and equal priors were used, so there are
roughly 100 observations in each class. We used test samples of size 500.
The two MDA models are described in the caption.
Figure 12.15 shows the leading canonical variates for the pe nalized MDA
model, evaluated at the test data. As we might have guessed, t he classes
appear to lie on the edges of a triangle. This is because the hj(i) are repre-
sented by three points in 21-space, thereby forming vertice s of a triangle,
and each class is represented as a convex combination of a pai r of vertices,
and hence lie on an edge. Also it is clear visually that all the information
lies in the ﬁrst two dimensions; the percentage of variance e xplained by the
ﬁrst two coordinates is 99 .8%, and we would lose nothing by truncating the
solution there. The Bayes risk for this problem has been esti mated to be
about 0.14 (Breiman et al., 1984). MDA comes close to the optimal rate ,
which is not surprising since the structure of the MDA model i s similar to
the generating model.

454 12. Flexible Discriminants
TABLE 12.4. Results for waveform data. The values are averages over ten sim -
ulations, with the standard error of the average in parenthes es. The ﬁve entries
above the line are taken from Hastie et al. (1994). The ﬁrst mode l below the line
is MDA with three subclasses per class. The next line is the same, except that the
discriminant coeﬃcients are penalized via a roughness penalt y to eﬀectively 4df.
The third is the corresponding penalized LDA or PDA model.
Technique Error Rates
Training Test
LDA 0.121(0.006) 0.191(0.006)
QDA 0.039(0.004) 0.205(0.006)
CART 0.072(0.003) 0.289(0.004)
FDA/MARS (degree = 1) 0.100(0.006) 0.191(0.006)
FDA/MARS (degree = 2) 0.068(0.004) 0.215(0.002)
MDA (3 subclasses) 0.087(0.005) 0.169(0.006)
MDA (3 subclasses, penalized 4 df) 0.137(0.006) 0.157(0.00 5)
PDA (penalized 4 df) 0.150(0.005) 0.171(0.005)
Bayes 0.140
Discriminant Var 1Discriminant Var 2
-6 -4 -2 0 2 4 6-6 -4 -2 0 2 411
111
11
111
11
11
111
111
1
1
11
1
111
1111
111
11
11
11
11
11
111
11
111
11
1
11
1
1
1
11
1
111
111111
111
11
11
11
11
11
11
11
11
1
1111
1
11
111
111
11
11
111
1
11
1
11111
11
1
11111
1
11
11
111
11
1111
111
111
1
1
1
11
11
1
11
111
11
1111
11
11111
11
22
22
2
22222
2 22
2
222
22
2
222
22
2
222
22
222
22
22222
2
2222
22
22
22
222
2
222
22
222
2
2
2
22
222
22
22
2
222
2
22
2
2
222
222
2
2
22
2
222
22
22
222
22
22
22
2
222222
22
2
22
2
2
22
222
2
22
22
22
2
2
2
22
2
22
22
2222
2
22
33
3 333
3333 33
33
3
3 3333
33
333
333
333
3
33
33
333
33
33
3333333
3
3
333
3333
333
33
333333
333
33
3
33
333
3
333 3
33
3
33
3
3
33
3
3
333
3
333 3
3 3
333
33
33
33
3
333
3
3
333
33
333
333
3
33
333
333
33
333
3
33
333
33 333
33
3
3 33
3
3
33
3
33333
333 Subclasses, Penalized 4 df
Discriminant Var 3Discriminant Var 4
-2 -1 0 1 2-1.0 0.0 0.5 1.01
1
11
111
11
11
1
11
11
11
1
111
11
11
11
11
11
11 111
111
11
1
11
11
11
11
1
111
11
11111
11111
1
1111
1
11
1 1
11
1 111
11
1
11
1111
11
11
11
1
11
1
11
11
11
11
111
111
1
11
11
1
11
11
11
1
11111
1
111
11
111
1
1
1
111
11
11
11
111
1
1
111
1111
11
111
11
1111
22
22
22
222
22
222
22 2
22
22
22
22
22
22
22
222
22
2
22222
2
222
22
2
22222
22
22
22
22
2
2 22
222
2
222
22
2
22222
222
22
222
222
2222
222
22
2222
2
22
2
2
22
2
222
2
222
2
22
22
22
2
22
22 2
22
22
2
2
2
22
2222
2
22
222
222
3
333
3
33
3 33
3
33
33
33
333
3
3
3
33
3333
3
333
3
33
333
33
3
3
33
33
33
3
333
3
33333
3
3
33
33
33
3
3
3333
33
3333
3 3
33
3333
3
3333
33
33
3333
33
33
333
3
33 3
33
3333
333
33
3
33
3
3
333
33
33
333
3333333
33
3333
3
33
333
3
33
333
3
33
3
33
3
333
33333 Subclasses, Penalized 4 df
FIGURE 12.15. Some two-dimensional views of the MDA model ﬁtted to a
sample of the waveform model. The points are independent test d ata, projected
on to the leading two canonical coordinates (left panel), and th e third and fourth
(right panel). The subclass centers are indicated.

Exercises 455
Computational Considerations
WithNtraining cases, ppredictors, and msupport vectors, the support
vector machine requires m3+mN+mpNoperations, assuming m≈N.
They do not scale well with N, although computational shortcuts are avail-
able (Platt, 1999). Since these are evolving rapidly, the re ader is urged to
search the web for the latest technology.
LDA requires Np2+p3operations, as does PDA. The complexity of
FDA depends on the regression method used. Many techniques a re linear
inN, such as additive models and MARS. General splines and kerne l-based
regression methods will typically require N3operations.
Software is available for ﬁtting FDA, PDA and MDA models in th eR
packagemda, which is also available in S-PLUS.
Bibliographic Notes
The theory behind support vector machines is due to Vapnik an d is de-
scribed in Vapnik (1996). There is a burgeoning literature o n SVMs; an
online bibliography, created and maintained by Alex Smola a nd Bernhard
Sch¨ olkopf, can be found at:
http://www.kernel-machines.org .
Our treatment is based on Wahba et al. (2000) and Evgeniou et a l. (2000),
and the tutorial by Burges (Burges, 1998).
Linear discriminant analysis is due to Fisher (1936) and Rao (1973). The
connection with optimal scoring dates back at least to Breim an and Ihaka
(1984), and in a simple form to Fisher (1936). There are stron g connections
withcorrespondenceanalysis(Greenacre,1984).Thedescr iptionofﬂexible,
penalized and mixture discriminant analysis is taken from H astie et al.
(1994), Hastie et al. (1995) and Hastie and Tibshirani (1996 b), and all
three are summarized in Hastie et al. (2000); see also Ripley (1996).
Exercises
Ex. 12.1 Show that the criteria (12.25) and (12.8) are equivalent.
Ex. 12.2 Show that the solution to (12.29) is the same as the solution t o
(12.25) for a particular kernel.
Ex. 12.3 Consider a modiﬁcation to (12.43) where you do not penalize t he
constant. Formulate the problem, and characterize its solu tion.
Ex. 12.4 Suppose you perform a reduced-subspace linear discriminan t anal-
ysis for aK-group problem. You compute the canonical variables of di-

456 12. Flexible Discriminants
mensionL≤K−1 given by z=UTx, whereUis thep×Lmatrix of
discriminant coeﬃcients, and p>Kis the dimension of x.
(a) IfL=K−1 show that
∝⌊a∇⌈⌊lz−¯zk∝⌊a∇⌈⌊l2−∝⌊a∇⌈⌊lz−¯zk′∝⌊a∇⌈⌊l2=∝⌊a∇⌈⌊lx−¯xk∝⌊a∇⌈⌊l2
W−∝⌊a∇⌈⌊lx−¯xk′∝⌊a∇⌈⌊l2
W,
where∝⌊a∇⌈⌊l·∝⌊a∇⌈⌊lWdenotesMahalanobis distance with respect to the covari-
anceW.
(b) IfL < K−1, show that the same expression on the left measures
the diﬀerence in Mahalanobis squared distances for the dist ributions
projected onto the subspace spanned by U.
Ex. 12.5 The data in phoneme.subset , available from this book’s website
http://www-stat.stanford.edu/ElemStatLearn
consists of digitized log-periodograms for phonemes utter ed by 60 speakers,
each speaker having produced phonemes from each of ﬁve class es. It is
appropriate to plot each vector of 256 “features” against th e frequencies
0–255.
(a) Produce a separate plot of all the phoneme curves against frequency
for each class.
(b) You plan to use a nearest prototype classiﬁcation scheme to classify
thecurvesintophonemeclasses.Inparticular,youwilluse aK-means
clustering algorithm in each class ( kmeans() inR), and then classify
observations to the class of the closest cluster center. The curves are
high-dimensionalandyouhavearathersmallsample-size-t o-variables
ratio. You decide to restrict all the prototypes to be smooth functions
of frequency. In particular, you decide to represent each pr ototypem
asm=BθwhereBis a 256×Jmatrix of natural spline basis
functions with Jknots uniformly chosen in (0 ,255) and boundary
knots at 0 and 255. Describe how to proceed analytically, and in
particular, how to avoid costly high-dimensional ﬁtting pr ocedures.
(Hint:It may help to restrict Bto be orthogonal.)
(c) Implement your procedure on the phoneme data, and try it o ut. Divide
the data into a training set and a test set (50-50), making sur e that
speakers are not split across sets (why?). Use K= 1,3,5,7 centers
per class, and for each use J= 5,10,15 knots (taking care to start
theK-means procedure at the same starting values for each value o f
J), and compare the results.
Ex.12.6SupposethattheregressionprocedureusedinFDA(Section1 2.5.1)
is a linear expansion of basis functions hm(x), m= 1,...,M. LetDπ=
YTY/Nbe the diagonal matrix of class proportions.

Exercises 457
(a)Showthattheoptimalscoringproblem(12.52)canbewrit teninvector
notation as
min
θ,β∝⌊a∇⌈⌊lYθ−Hβ∝⌊a∇⌈⌊l2, (12.65)
whereθis a vector of Kreal numbers, and His theN×Mmatrix
of evaluations hj(xi).
(b) Suppose that the normalization on θisθTDπ1 = 0 andθTDπθ= 1.
Interpret these normalizations in terms of the original sco redθ(gi).
(c) Show that, with this normalization, (12.65) can be parti ally optimized
w.r.t.β, and leads to
max
θθTYTSYθ, (12.66)
subject to the normalization constraints, where Sis the projection
operator corresponding to the basis matrix H.
(d) Suppose that the hjinclude the constant function. Show that the
largest eigenvalue of Sis 1.
(e) LetΘbe aK×Kmatrix of scores (in columns), and suppose the
normalization is ΘTDπΘ=I. Show that the solution to (12.53) is
given by the complete set of eigenvectors of S; the ﬁrst eigenvector is
trivial, and takes care of the centering of the scores. The re mainder
characterize the optimal scoring solution.
Ex. 12.7 Derive the solution to the penalized optimal scoring proble m
(12.57).
Ex.12.8Showthatcoeﬃcients βℓfoundbyoptimalscoringareproportional
to the discriminant directions νℓfound by linear discriminant analysis.
Ex. 12.9 LetˆY=XˆBbe the ﬁtted N×Kindicator response matrix after
linearregressiononthe N×pmatrixX,wherep>K.Considerthereduced
featuresx∗
i=ˆBTxi. Show that LDA using x∗
iis equivalent to LDA in the
original space.
Ex. 12.10 Kernels and linear discriminant analysis . Suppose you wish to
carry out a linear discriminant analysis (two classes) usin g a vector of
transformations of the input variables h(x). Sinceh(x) is high-dimensional,
you will use a regularized within-class covariance matrix Wh+γI. Show
that the model can be estimated using only the inner products K(xi,xi′) =
∝an}⌊∇a⌋ketle{th(xi),h(xi′)∝an}⌊∇a⌋ket∇i}ht. Hence the kernel property of support vector machines is als o
shared by regularized linear discriminant analysis.
Ex. 12.11 TheMDAproceduremodels eachclassas amixtureofGaussians .
Hence each mixture center belongs to one and only one class. A more
general model allows each mixture center to be shared by all c lasses. We
take the joint density of labels and features to be

458 12. Flexible Discriminants
P(G,X) =R/summationdisplay
r=1πrPr(G,X), (12.67)
a mixture of joint densities. Furthermore we assume
Pr(G,X) =Pr(G)φ(X;µr,Σ). (12.68)
This model consists of regions centered at µr, and for each there is a class
proﬁlePr(G). The posterior class distribution is given by
P(G=k|X=x) =/summationtextR
r=1πrPr(G=k)φ(x;µr,Σ)
/summationtextR
r=1πrφ(x;µr,Σ), (12.69)
where the denominator is the marginal distribution P(X).
(a) Show that this model (called MDA2) can be viewed as a gener alization
of MDA since
P(X|G=k) =/summationtextR
r=1πrPr(G=k)φ(x;µr,Σ)
/summationtextR
r=1πrPr(G=k),(12.70)
whereπrk=πrPr(G=k)//summationtextR
r=1πrPr(G=k) corresponds to the
mixing proportions for the kth class.
(b) Derive the EM algorithm for MDA2.
(c) Show that if the initial weight matrix is constructed as i n MDA, in-
volving separate k-means clustering in each class, then the algorithm
for MDA2 is identical to the original MDA procedure.

This is page 459
Printer: Opaque this
13
Prototype Methods and
Nearest-Neighbors
13.1 Introduction
In this chapter we discuss some simple and essentially model -free methods
for classiﬁcation and pattern recognition. Because they ar e highly unstruc-
tured, they typically are not useful for understanding the n ature of the
relationship between the features and class outcome. Howev er, asblack box
prediction engines, they can be very eﬀective, and are often among the best
performers in real data problems. The nearest-neighbor tec hnique can also
be used in regression; this was touched on in Chapter 2 and wor ks reason-
ably well for low-dimensional problems. However, with high -dimensional
features, the bias–variance tradeoﬀ does not work as favora bly for nearest-
neighbor regression as it does for classiﬁcation.
13.2 Prototype Methods
Throughout this chapter, our training data consists of the Npairs (x1,g1),
...,(xn,gN) wheregiis a class label taking values in {1,2,...,K}. Pro-
totype methods represent the training data by a set of points in feature
space. These prototypes are typically not examples from the training sam-
ple, except in the case of 1-nearest-neighbor classiﬁcatio n discussed later.
Each prototype has an associated class label, and classiﬁca tion of a query
pointxis made to the class of the closest prototype. “Closest” is us ually
deﬁned by Euclidean distance in the feature space, after eac h feature has

460 13. Prototypes and Nearest-Neighbors
been standardized to have overall mean 0 and variance 1 in the training
sample. Euclidean distance is appropriate for quantitativ e features. We
discuss distance measures between qualitative and other ki nds of feature
values in Chapter 14.
These methods can be very eﬀective if the prototypes are well positioned
to capture the distribution of each class. Irregular class b oundaries can be
represented, with enough prototypes in the right places in f eature space.
The main challenge is to ﬁgure out how many prototypes to use a nd where
to put them. Methods diﬀer according to the number and way in w hich
prototypes are selected.
13.2.1K-means Clustering
K-means clustering is a method for ﬁnding clusters and cluste r centers in a
setofunlabeleddata.Onechoosesthedesirednumberofclus tercenters,say
R, and theK-means procedure iteratively moves the centers to minimize
the total within cluster variance.1Given an initial set of centers, the K-
means algorithm alternates the two steps:
•for each center we identify the subset of training points (it s cluster)
that is closer to it than any other center;
•the means of each feature for the data points in each cluster a re
computed, and this mean vector becomes the new center for tha t
cluster.
These two steps are iterated until convergence. Typically t he initial centers
areRrandomly chosen observations from the training data. Detai ls of the
K-meansprocedure,aswellasgeneralizationsallowingford iﬀerentvariable
types and more general distance measures, are given in Chapt er 14.
To useK-means clustering for classiﬁcation of labeled data, the st eps
are:
•applyK-means clustering to the training data in each class sepa-
rately, using Rprototypes per class;
•assign a class label to each of the K×Rprototypes;
•classify a new feature xto the class of the closest prototype.
Figure 13.1 (upper panel) shows a simulated example with thr ee classes
and two features. We used R= 5 prototypes per class, and show the clas-
siﬁcation regions and the decision boundary. Notice that a n umber of the
1The “K” inK-means refers to the number of cluster centers. Since we have already
reserved Kto denote the number of classes, we denote the number of clusters by R.

13.2 Prototype Methods 461
K-means - 5 Prototypes  per Class
................................................................................................................................................................................................................................................................................................................................................................................................................................. ........................ ................................ ..................................... ............................................................................................................................................................................. .................................. ....................................... ......................................... ............................................ .............................................. .................................................. ......................... .............................. ........................ ................................... ......................... ..................................... .......................... ...................................... ............................. .................................. .................................. ....................... ....................................... ............. ............................................... ...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ........... ........................................ ............................................................. ........ ........................................... ........................................................ ..... ................................................................................................. .. .............................................................................................. . .......................................................................................... .... .................................................................................... ....... .............................................................................. .......... ............................................................................ ......... ............................................................................. ......... ............................................................................. ......... ............................................................................. ......... ............................................................................... ....... .................................................................................... ..... ........................................................................................ ... ............................................................................................ . ............................................................................................... . ................................................................................................ ... ................................................................................................ ..... ................................................................................................ ....... ............................................ ................................ .................... .... ....................................................................... ..................... .... ........................................................... ................. ............. .......................................... ............. ...................... ........................ .............. ....
................................................................................ ......................... ......................................... .................... .................................................... ................ ........................................................... ............... ............................................................. ............... ............................................................... ............... ............................................................... ............... ........................................................... ................ ....................................................... ................ .................................................... ................ ................................................... ............... ................................................. .............. .............................................. .............. ......................................... .................................................... ............................................... ............................................... ........... .......................................... .......... ................................................ .......... ...................................................... ......... .................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ .............................................................................................. ............................................................................................... ................................................................................................ ................................................................................................. ................................................
oo
oo
o
ooo
oo
o
oo
oo
oo
o
oo
o
ooo
ooo
ooo
oo
oooo
ooooo
oo
ooo
o
ooo
oo
ooo
o
oo
oo
oo
ooo
oo
o
oo
ooo
ooo
oo
oo
o
oo
o
o
oo
oo
ooo
ooo
oo
oo
o
ooo
ooo
o
oo
oo
o
oo
oooo
ooooo
oo
oooo
ooo
o
ooo
ooo
oo oo
oo
oo
ooo
oo
oooo
oo
oo
ooo
oo
o
oo
oo
o
oo
oo
oooo
o
o
oo
oo oo
o
ooo
ooo
oo
oooooo
o
ooo
oooo
oo
oooo
ooo
oo
o
oo
oo
o
oo
o
oooo
oo
o
oo
oo
o
oo
oooo
o
oo
ooo
oooo
o o o
ooo
oo
o
ooo
ooooo
oo
oo
o
o
oooo
o
oo
oo
oooo
ooooo
o
•
•••
••
•••
•
••
••
•• •
••
•••
•••
••
•••
LVQ - 5 Prototypes per Class
......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ................................... ........................................ .............................................. .................................................. .................... .................................. ..................... ...................................... ..................... ......................................... ....................... ..................................... .............................. ............................ ..................................... .............. ............................................ .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... .... ............................................... ............................................................ .... ............................................... ........................................................... .... ............................................... .......................................................... ... ................................................ ........................................................ ... ................................................ ...................................................... ... ................................................ ..................................................... .. .................................................................................................... .. .................................................................................................. . ................................................................................................. . ................................................................................................ . ............................................................................................... ............................................................................................. ........................................................................................... . ......................................................................................... . ............................................................ .................... ......... ........................................... ........................ ..............
.......................... ................................. ............................... ....................... ................................................. ................ .......................................................... ............... ............................................................ ............... ............................................................. ............... .............................................................. ............... .............................................................. ............... ................................................................ .............. .................................................................. .............. ................................................................... .............. ................................................................ .............. ........................................................... ............... ..................................................... ................ ................................................ ............... ............................................. .............. ......................................... ................................................... ................................................ .................................................. ........... .............................................. ........... ...................................................... .......... ....................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ ..........................................
oo
oo
o
ooo
oo
o
oo
oo
oo
o
oo
o
ooo
ooo
ooo
oo
oooo
ooooo
oo
ooo
o
ooo
oo
ooo
o
oo
oo
oo
ooo
oo
o
oo
ooo
ooo
oo
oo
o
oo
o
o
oo
oo
ooo
ooo
oo
oo
o
ooo
ooo
o
oo
oo
o
oo
oooo
ooooo
oo
oooo
ooo
o
ooo
ooo
oo oo
oo
oo
ooo
oo
oooo
oo
oo
ooo
oo
o
oo
oo
o
oo
oo
oooo
o
o
oo
oo oo
o
ooo
ooo
oo
oooooo
o
ooo
oooo
oo
oooo
ooo
oo
o
oo
oo
o
oo
o
oooo
oo
o
oo
oo
o
oo
oooo
o
oo
ooo
oooo
o o o
ooo
oo
o
ooo
ooooo
oo
oo
o
o
oooo
o
oo
oo
oooo
ooooo
o
••••
••
•••
•
• •
••
•• •
••
•••
•••
••
•••
FIGURE 13.1. Simulated example with three classes and ﬁve prototypes per
class. The data in each class are generated from a mixture of Gaus sians. In the
upper panel, the prototypes were found by applying the K-means clustering algo-
rithm separately in each class. In the lower panel, the LVQ algorit hm (starting
from theK-means solution) moves the prototypes away from the decision bound-
ary. The broken purple curve in the background is the Bayes dec ision boundary.

462 13. Prototypes and Nearest-Neighbors
Algorithm 13.1 Learning Vector Quantization—LVQ.
1. ChooseRinitial prototypes for each class: m1(k),m2(k),...,m R(k),
k= 1,2,...,K,forexample,bysampling Rtrainingpointsatrandom
from each class.
2. Sampleatrainingpoint xirandomly(withreplacement),andlet( j,k)
index the closest prototype mj(k) toxi.
(a) Ifgi=k(i.e., they are in the same class), move the prototype
towards the training point:
mj(k)←mj(k)+ǫ(xi−mj(k)),
whereǫis thelearning rate .
(b) Ifgi∝ne}ationslash=k(i.e., they are in diﬀerent classes), move the prototype
away from the training point:
mj(k)←mj(k)−ǫ(xi−mj(k)).
3. Repeat step 2, decreasing the learning rate ǫwith each iteration to-
wards zero.
prototypes are near the class boundaries, leading to potent ial misclassiﬁca-
tion errors for points near these boundaries. This results f rom an obvious
shortcoming with this method: for each class, the other clas ses do not have
a say in the positioning of the prototypes for that class. A be tter approach,
discussed next, uses all of the data to position all prototyp es.
13.2.2 Learning Vector Quantization
InthistechniqueduetoKohonen(1989),prototypesareplac edstrategically
with respect to the decision boundaries in an ad-hoc way. LVQ is anonline
algorithm—observations are processed one at a time.
Theideaisthatthetrainingpointsattractprototypesofth ecorrectclass,
and repel other prototypes. When the iterations settle down , prototypes
should be close to the training points in their class. The lea rning rateǫis
decreased to zero with each iteration, following the guidel ines for stochastic
approximation learning rates (Section 11.4.)
Figure 13.1 (lower panel) shows the result of LVQ, using the K-means
solution as starting values. The prototypes have tended to m ove away from
the decision boundaries, and away from prototypes of compet ing classes.
The procedure just described is actually called LVQ1. Modiﬁ cations
(LVQ2, LVQ3, etc.) have been proposed, that can sometimes im prove per-
formance. A drawback of learning vector quantization metho ds is the fact

13.3k-Nearest-Neighbor Classiﬁers 463
that they are deﬁned by algorithms, rather than optimizatio n of some ﬁxed
criteria; this makes it diﬃcult to understand their propert ies.
13.2.3 Gaussian Mixtures
The Gaussian mixture model can also bethought of as aprototy pe method,
similar in spirit to K-means and LVQ. We discuss Gaussian mixtures in
some detail in Sections 6.8, 8.5 and 12.7. Each cluster is des cribed in terms
of a Gaussian density, which has a centroid (as in K-means), and a covari-
ance matrix. The comparison becomes crisper if we restrict t he component
Gaussians to have a scalar covariance matrix (Exercise 13.1 ). The two steps
of the alternating EM algorithm are very similar to the two st eps inK-
means:
•In the E-step, each observation is assigned a responsibility or weight
for each cluster, based on the likelihood of each of the corre spond-
ing Gaussians. Observations close to the center of a cluster will most
likely get weight 1 for that cluster, and weight 0 for every ot her clus-
ter. Observations half-way between two clusters divide the ir weight
accordingly.
•In the M-step, each observation contributes to the weighted means
(and covariances) for everycluster.
As a consequence, the Gaussian mixture model is often referr ed to as a soft
clustering method, while K-means is hard.
Similarly, when Gaussian mixture models are used to represe nt the fea-
ture density in each class, it produces smooth posterior pro babilities ˆp(x) =
{ˆp1(x),...,ˆpK(x)}for classifying x(see (12.60) on page 449.) Often this
is interpreted as a soft classiﬁcation, while in fact the cla ssiﬁcation rule is
ˆG(x) = argmax kˆpk(x). Figure 13.2 compares the results of K-means and
Gaussian mixtures on the simulated mixture problem of Chapt er 2. We
see that although the decision boundaries are roughly simil ar, those for the
mixture model are smoother (although the prototypes are in a pproximately
the same positions.) We also see that while both procedures d evote a blue
prototype (incorrectly) to a region in the northwest, the Ga ussian mixture
classiﬁer can ultimately ignore this region, while K-means cannot. LVQ
gave very similar results to K-means on this example, and is not shown.
13.3k-Nearest-Neighbor Classiﬁers
These classiﬁers are memory-based , and require no model to be ﬁt. Given
a query point x0, we ﬁnd the ktraining points x(r),r= 1,...,kclosest in
distanceto x0,andthenclassifyusingmajorityvoteamongthe kneighbors.

464 13. Prototypes and Nearest-Neighbors
K-means - 5 Prototypes per Class
... .. . . .. . . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o••
••
•••
••
•
•••
••
•••
••
Training Error: 0.170
Test Error:       0.243
Bayes Error:    0.210
Gaussian Mixtures - 5 Subclasses per Class
.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
ooooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
•••
••
•••
••••
••
•••
••
•
Training Error: 0.17
Test Error:       0.22
Bayes Error:    0.21
FIGURE 13.2. The upper panel shows the K-means classiﬁer applied to the
mixture data example. The decision boundary is piecewise line ar. The lower panel
shows a Gaussian mixture model with a common covariance for al l component
Gaussians. The EM algorithm for the mixture model was started a t theK-means
solution. The broken purple curve in the background is the Baye s decision
boundary.

13.3k-Nearest-Neighbor Classiﬁers 465
Ties are broken at random. For simplicity we will assume that the features
are real-valued, and we use Euclidean distance in feature sp ace:
d(i)=||x(i)−x0||. (13.1)
Typically we ﬁrst standardize each of the features to have me an zero and
variance 1, since it is possible that they are measured in diﬀ erent units. In
Chapter 14 we discuss distance measures appropriate for qua litative and
ordinal features, and how to combine them for mixed data. Ada ptively
chosen distance metrics are discussed later in this chapter .
Despite its simplicity, k-nearest-neighbors has been successful in a large
number of classiﬁcation problems, including handwritten d igits, satellite
image scenes and EKG patterns. It is often successful where e ach class
has many possible prototypes, and the decision boundary is v ery irregular.
Figure 13.3 (upper panel) shows the decision boundary of a 15 -nearest-
neighbor classiﬁer applied to the three-class simulated da ta. The decision
boundary is fairly smooth compared to the lower panel, where a 1-nearest-
neighbor classiﬁer was used. There is a close relationship b etween nearest-
neighbor and prototype methods: in 1-nearest-neighbor cla ssiﬁcation, each
training point is a prototype.
Figure 13.4 shows the training, test and tenfold cross-vali dation errors
as a function of the neighborhood size, for the two-class mix ture problem.
Since the tenfold CV errors are averages of ten numbers, we ca n estimate
a standard error.
Becauseitusesonly thetrainingpoint closest tothequeryp oint, thebias
of the 1-nearest-neighbor estimate is often low, but the var iance is high.
A famous result of Cover and Hart (1967) shows that asymptoti cally the
error rate of the 1-nearest-neighbor classiﬁer is never mor e than twice the
Bayes rate. The rough idea of the proof is as follows (using sq uared-error
loss). We assume that the query point coincides with one of th e training
points, so that the bias is zero. This is true asymptotically if the dimension
of the feature space is ﬁxed and the training data ﬁlls up the s pace in a
dense fashion. Then the error of the Bayes rule is just the var iance of a
Bernoulli random variate (the target at the query point), wh ile the error of
1-nearest-neighbor rule is twicethe variance of a Bernoulli random variate,
one contribution each for the training and query targets.
We now give more detail for misclassiﬁcation loss. At xletk∗be the
dominant class, and pk(x) the true conditional probability for class k. Then
Bayes error = 1 −pk∗(x), (13.2)
1-nearest-neighbor error =K/summationdisplay
k=1pk(x)(1−pk(x)),(13.3)
≥1−pk∗(x). (13.4)
The asymptotic 1-nearest-neighbor error rate is that of a ra ndom rule; we
pick both the classiﬁcation and the test point at random with probabili-

466 13. Prototypes and Nearest-Neighbors
15-Nearest Neighbors
. ........ ......... ..... ...... ......... ......................................................................................... ............................................................................................................................................................................................................................................................................ .............................................................................................................. .. .. . ...................................... .. . .... .... ......................................... ........... ....................................................... ......... ............................................................ ...... .......................................................................... ............................................................................... ...................... .................................................. ................................................................... ........................... ...................................... ........ .. .................................................................................... . .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................. . . ................................................... ............................................................. . .................................................. ............................................................ ................................................... ............................................................... .. ................................................. .................................................... ...... ... ................................................... ............................................... ..... ... .. ................................................. .............................................. .... .... ................................................... ............................................... ...... ................................................... ..................................................... .... ............................................... ........................................................ .... ............................................... ........................ ............................... ... .............................................................. .... . . ........................ ....... ...................................................... ........................ ................. ................................... ................. .............................. ............... ....... ... ........................................ ................................................................ .............................. . ...................... ......
.... .. . ..................... .............................. .................................................................................................. .......................................... .. ......... ............................................... ........... ..................................................... .............. ............................................................. .............. .................................................................. ........... .................................................................. . ............. .............................................................. ................ .......................................................... .. . . ............ .................... .................................... . . ............... .. ......... .. ................................... .................................................... ............................................... ................................................................................................................................ .................................................................................................... . ....................................................... . ........ ............................................................. ......................................................................... ................ ... ..................................................... ........................................................................... ............................................................................ .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................. ...........................................
oo
oo
o
ooo
oo
o
oo
oo
oo
o
oo
o
ooo
ooo
ooo
oo
oooo
ooooo
oo
ooo
o
ooo
oo
ooo
o
oo
oo
oo
ooo
oo
o
oo
ooo
ooo
oo
oo
o
oo
o
o
oo
oo
ooo
ooo
oo
oo
o
ooo
ooo
o
oo
oo
o
oo
oooo
ooooo
oo
oooo
ooo
o
ooo
ooo
oo oo
oo
oo
ooo
oo
oooo
oo
oo
ooo
oo
o
oo
oo
o
oo
oo
oooo
o
o
oo
oo oo
o
ooo
ooo
oo
oooooo
o
ooo
oooo
oo
oooo
ooo
oo
o
oo
oo
o
oo
o
oooo
oo
o
oo
oo
o
oo
oooo
o
oo
ooo
oooo
o o o
ooo
oo
o
ooo
ooooo
oo
oo
o
o
oooo
o
oo
oo
oooo
ooooo
o
1-Nearest Neighbor
................. ................. ............ .............. .................. ....................... ... ................ ..... ............... ..... ............. .... ................ ......... ..... ..... ...... ................... ..... ...... ...... .... ....... ... .......... ... .... .. .... . ........ ............ ............ ...... ............ .......... ........ .............. ........ ........ ............. .......... ...... ............... ....... ............. ............. ......... .......................... ............. ......................... . ..... .. .. ............................. ....... .. . ...... .......................... ....... ............ .............. ................ ............... .............................. ............. ........... ............ ....................... ............. ........... ....................... .......................................... ...... ..................... ..................... ..... ......................... .......... ............... ........ ....... ... .... ....................... .... ... ................ ...................... . ........................ . ......... ..... .... .............. ............... .............. ...... ... .................................. ......... .......... ... .............................. ........... ...... .......... ............................. ......... .......... ...................................... ... ................................................... ...................................... ..................................... .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................ ................................... .............................................................. ....... ............................................ .............................................. ...... .... ......................................................................................... ........ ... ................................................ ............................................ ........ ............... .............................................................................. ........ .......... ............................................................................... ....... ........ ......................................................... .............. ......... ... .. .......................................................... ............ ....... ... ........ ...................................................... .......... ...... .... .......... .................................................. ...... .. ....... ................ .................................................... . .. .............. .............. ..................................................... ... ............ .... ... ........... ............................................. .. ............. ... .................. ....................................... .............. ..... ... ..... ..... ....... .......................................... ............. ...... . ....... ... ......... ....................................... ............... .. .......... ...... ..... ... .............................................. .................. .......... ........ . ............................................ ..... .......... ......... ............. .................................. ... ..... ......... ..................... ........................... . ... ... ........ ........................ ....................... ..... ... ....... ............ ............. ................ .. .... ..... ............. ... .... .................... .... ..... . ........... ..... ...... ...........
................................................. ............ ................. ................................................................. .................. ..................... ...................... ............................ ......................................... ................................................ .. ... .......... .......................... ...... . ...... ...................................... ...... . ..... ............ .................................... .. .............. .................................... .. . ................ ................................................ ... ....... .................................................. ...... .......... ........................................... ..... ........ ......... ........................................... ... . .. .. ... ................................... .. .. .. .. .................................. ........... .... . ...... . .... ............................................... ..... ..... ................................................... ...... . .................................................. .... .. . ......................................................... ..... ..... .................................................... ...... ............................................................ .................. ........................................................ ................... ......................................................... ................... .......................................................... ............................................................................... .............................................................................. .............................................................................. ................................................................................ .................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ........................................................................................... .........................................
oo
oo
o
ooo
oo
o
oo
oo
oo
o
oo
o
ooo
ooo
ooo
oo
oooo
ooooo
oo
ooo
o
ooo
oo
ooo
o
oo
oo
oo
ooo
oo
o
oo
ooo
ooo
oo
oo
o
oo
o
o
oo
oo
ooo
ooo
oo
oo
o
ooo
ooo
o
oo
oo
o
oo
oooo
ooooo
oo
oooo
ooo
o
ooo
ooo
oo oo
oo
oo
ooo
oo
oooo
oo
oo
ooo
oo
o
oo
oo
o
oo
oo
oooo
o
o
oo
oo oo
o
ooo
ooo
oo
oooooo
o
ooo
oooo
oo
oooo
ooo
oo
o
oo
oo
o
oo
o
oooo
oo
o
oo
oo
o
oo
oooo
o
oo
ooo
oooo
o o o
ooo
oo
o
ooo
ooooo
oo
oo
o
o
oooo
o
oo
oo
oooo
ooooo
o
FIGURE 13.3. k-nearest-neighbor classiﬁers applied to the simulation data o f
Figure 13.1. The broken purple curve in the background is the B ayes decision
boundary.

13.3k-Nearest-Neighbor Classiﬁers 467
Number of NeighborsMisclassification Errors
0 5 10 15 20 25 300.0 0.05 0.10 0.15 0.20 0.25 0.30•
••••••• • ••• •••
•• ••••
••••• •• • ••
•••••• •••• ••••
Test Error
10-fold CV
Training Error
Bayes Error
7-Nearest Neighbors
.. .. . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
oo
o
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
o
oooo
oo
oo
oo
ooo
ooooooo
oo o
ooo
oo
oo
oooo
o
oo
oo
o
oooo
ooo
o
o
ooo
o
ooooo
oo
o
oo
ooo
o
Training Error: 0.145
Test Error:       0.225
Bayes Error:    0.210
FIGURE 13.4. k-nearest-neighbors on the two-class mixture data. The upper
panel shows the misclassiﬁcation errors as a function of neig hborhood size. Stan-
dard error bars are included for 10-fold cross validation. The lo wer panel shows
the decision boundary for 7-nearest-neighbors, which appe ars to be optimal for
minimizing test error. The broken purple curve in the backgro und is the Bayes
decision boundary.

468 13. Prototypes and Nearest-Neighbors
tiespk(x), k= 1,...,K. ForK= 2 the 1-nearest-neighbor error rate is
2pk∗(x)(1−pk∗(x))≤2(1−pk∗(x)) (twice the Bayes error rate). More
generally, one can show (Exercise 13.3)
K/summationdisplay
k=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K
K−1(1−pk∗(x))2.(13.5)
Many additional results of this kind have been derived; Ripl ey (1996) sum-
marizes a number of them.
This result can provide a rough idea about the best performan ce that
is possible in a given problem. For example, if the 1-nearest -neighbor rule
has a 10% error rate, then asymptotically the Bayes error rat e is at least
5%. The kicker here is the asymptotic part, which assumes the bias of the
nearest-neighbor rule is zero. In real problems the bias can be substantial.
The adaptive nearest-neighbor rules, described later in th is chapter, are an
attempt to alleviate this bias. For simple nearest-neighbo rs, the bias and
variance characteristics can dictate the optimal number of near neighbors
for a given problem. This is illustrated in the next example.
13.3.1 Example: A Comparative Study
We tested the nearest-neighbors, K-means and LVQ classiﬁers on two sim-
ulated problems. There are 10 independent features Xj, each uniformly
distributed on [0 ,1]. The two-class 0-1 target variable is deﬁned as follows:
Y=I/parenleftbigg
X1>1
2/parenrightbigg
; problem 1: “easy”,
Y=I
sign

3/productdisplay
j=1/parenleftbigg
Xj−1
2/parenrightbigg

>0
; problem 2: “diﬃcult.”(13.6)
Hence in the ﬁrst problem the two classes are separated by the hyperplane
X1= 1/2; in the second problem, the two classes form a checkerboard
pattern in the hypercube deﬁned by the ﬁrst three features. T he Bayes
error rate is zero in both problems. There were 100 training a nd 1000 test
observations.
Figure 13.5 shows the mean and standard error of the misclass iﬁcation
error for nearest-neighbors, K-means and LVQ over ten realizations, as
the tuning parameters are varied. We see that K-means and LVQ give
nearly identical results. For the best choices of their tuni ng parameters,
K-means and LVQ outperform nearest-neighbors for the ﬁrst pr oblem, and
they perform similarly for the second problem. Notice that t he best value
of each tuning parameter is clearly situation dependent. Fo r example 25-
nearest-neighbors outperforms 1-nearest-neighbor by a fa ctor of 70% in the

13.3k-Nearest-Neighbor Classiﬁers 469
Number of NeighborsMisclassification Error
0 20 40 600.1 0.2 0.3 0.4 0.5Nearest Neighbors / Easy
Number of Prototypes per ClassMisclassification Error
0 5 10 15 20 25 300.1 0.2 0.3 0.4 0.5K-means & LVQ / Easy
Number of NeighborsMisclassification Error
0 20 40 600.40 0.45 0.50 0.55 0.60Nearest Neighbors / Difficult
Number of Prototypes per ClassMisclassification Error
0 5 10 15 20 25 300.40 0.45 0.50 0.55 0.60K-means & LVQ / Difficult
FIGURE 13.5. Mean±one standard error of misclassiﬁcation error for near-
est-neighbors, K-means (blue) and LVQ (red) over ten realizations for two sim-
ulated problems: “easy” and “diﬃcult,” described in the text.

470 13. Prototypes and Nearest-Neighbors
Spectral Band 1 Spectral Band 2 Spectral Band 3
Spectral Band 4 Land Usage Predicted Land Usage
FIGURE 13.6. The ﬁrst four panels are LANDSAT images for an agricultural
area in four spectral bands, depicted by heatmap shading. Th e remaining two
panels give the actual land usage (color coded) and the predicte d land usage using
a ﬁve-nearest-neighbor rule described in the text.
ﬁrst problem, while 1-nearest-neighbor is best in the secon d problem by a
factor of 18%. These results underline the importance of usi ng an objective,
data-based method like cross-validation to estimate the be st value of a
tuning parameter (see Figure 13.4 and Chapter 7).
13.3.2 Example: k-Nearest-Neighbors and Image Scene
Classiﬁcation
The STATLOG project (Michie et al., 1994) used part of a LANDS AT
image as a benchmark for classiﬁcation (82 ×100 pixels). Figure 13.6 shows
four heat-map images, two in the visible spectrum and two in t he infrared,
for an area of agricultural land in Australia. Each pixel has a class label
from the 7-element set G={red soil, cotton, vegetation stubble, mixture,
gray soil, damp gray soil, very damp gray soil }, determined manually by
research assistants surveying the area. The lower middle pa nel shows the
actual land usage, shaded by diﬀerent colors to indicate the classes. The
objective is to classify the land usage at a pixel, based on th e information
in the four spectral bands.
Five-nearest-neighbors produced the predicted map shown i n the bot-
tom right panel, and was computed as follows. For each pixel w e extracted
an 8-neighbor feature map—the pixel itself and its 8 immediat e neighbors

13.3k-Nearest-Neighbor Classiﬁers 471
N
N
N
 N
X
N
N
N
N
FIGURE 13.7. A pixel and its 8-neighbor feature map.
(see Figure 13.7). This is done separately in the four spectr al bands, giving
(1+8)×4 = 36 input features per pixel. Then ﬁve-nearest-neighbors classi-
ﬁcation was carried out in this 36-dimensional feature spac e. The resulting
test error rate was about 9 .5% (see Figure 13.8). Of all the methods used
in the STATLOG project, including LVQ, CART, neural network s, linear
discriminant analysis and many others, k-nearest-neighbors performed best
on this task. Hence it is likely that the decision boundaries in IR36are quite
irregular.
13.3.3 Invariant Metrics and Tangent Distance
In some problems, the training features are invariant under certain natural
transformations. The nearest-neighbor classiﬁer can expl oit such invari-
ances by incorporating them into the metric used to measure t he distances
between objects. Here we give an example where this idea was u sed with
great success, and the resulting classiﬁer outperformed al l others at the
time of its development (Simard et al., 1993).
The problem is handwritten digit recognition, as discussed is Chapter 1
and Section 11.7. The inputs are grayscale images with 16 ×16 = 256
pixels; some examples are shown in Figure 13.9. At the top of F igure 13.10,
a “3” is shown, in its actual orientation (middle) and rotate d 7.5◦and 15◦
in either direction. Such rotations can often occur in real h andwriting, and
it is obvious to our eye that this “3” is still a “3” after small rotations.
Hence we want our nearest-neighbor classiﬁer to consider th ese two “3”s
to be close together (similar). However the 256 grayscale pi xel values for a
rotated “3” will look quite diﬀerent from those in the origin al image, and
hence the two objects can be far apart in Euclidean distance i n IR256.
We wish to remove the eﬀect of rotation in measuring distance s between
two digits of the same class. Consider the set of pixel values consisting of
the original “3” and its rotated versions. This is a one-dime nsional curve in
IR256, depicted by the green curve passing through the “3” in Figur e 13.10.
Figure 13.11 shows a stylized version of IR256, with two images indicated by
xiandxi′. These might be two diﬀerent “3”s, for example. Through each
image we have drawn the curve of rotated versions of that imag e, called

472 13. Prototypes and Nearest-Neighbors
STATLOG results
MethodTest Misclassification Error
2 4 6 8 10 12 140.0 0.05 0.10 0.15LVQRBFALLOC80CART NeuralNewID C4.5QDASMARTLogisticLDA
DANNK-NN
FIGURE 13.8. Test-error performance for a number of classiﬁers, as report ed
by the STATLOG project. The entry DANN is a variant of k-nearest neighbors,
using an adaptive metric (Section 13.4.2).
FIGURE 13.9. Examples of grayscale images of handwritten digits.

13.3k-Nearest-Neighbor Classiﬁers 473
Tangent
+ α.Transformations of 30 7.5 −15 −7.5
3
α=0 α=0.1 α=− 0.2 α=− 0.1 α=0.2
Linear equation for 
images above15
Pixel space
FIGURE 13.10. The top row shows a “ 3” in its original orientation (middle)
and rotated versions of it. The green curve in the middle of the ﬁgure depicts
this set of rotated “ 3” in256-dimensional space. The red line is the tangent line
to the curve at the original image, with some “ 3”s on this tangent line, and its
equation shown at the bottom of the ﬁgure.
invariance manifolds in this context. Now, rather than using the usual
Euclidean distance between the two images, we use the shorte st distance
between the two curves. In other words, the distance between the two
images is taken to be the shortest Euclidean distance betwee n any rotated
version of ﬁrst image, and any rotated version of the second i mage. This
distance is called an invariant metric .
In principle one could carry out 1-nearest-neighbor classi ﬁcation using
this invariant metric. However there are two problems with i t. First, it is
very diﬃcult to calculate for real images. Second, it allows large trans-
formations that can lead to poor performance. For example a “ 6” would
be considered close to a “9” after a rotation of 180◦. We need to restrict
attention to small rotations.
The use of tangent distance solves both of these problems. As shown in
Figure 13.10, we can approximate the invariance manifold of the image
“3” by its tangent at the original image. This tangent can be c omputed
by estimating the direction vector from small rotations of t he image, or by
more sophisticated spatial smoothing methods (Exercise 13 .4.) For large
rotations, the tangent image no longer looks like a “3,” so th e problem
with large transformations is alleviated.

474 13. Prototypes and Nearest-Neighbors
TransformationsTransformations
xixi′ofxi
ofxi′Tangent distance
Euclidean distance
between xiandxi′Distance between
transformed
xiandxi′
FIGURE 13.11. Tangent distance computation for two images xiandxi′.
Rather than using the Euclidean distance between xiandxi′, or the shortest
distance between the two curves, we use the shortest distanc e between the two
tangent lines.
The idea then is to compute the invariant tangent line for eac h training
image. For a query image to be classiﬁed, we compute its invar iant tangent
line, and ﬁnd the closest line to it among the lines in the trai ning set. The
class (digit) corresponding to this closest line is our pred icted class for the
query image. In Figure 13.11 the two tangent lines intersect , but this is only
because we have been forced to draw a two-dimensional repres entation of
the actual 256-dimensional situation. In IR256the probability of two such
lines intersecting is eﬀectively zero.
Now a simpler way to achieve this invariance would be to add in to the
training set a number of rotated versions of each training im age, and then
just use a standard nearest-neighbor classiﬁer. This idea i s called “hints” in
Abu-Mostafa (1995), and works well when the space of invaria nces is small.
So far we have presented a simpliﬁed version of the problem. I n addition to
rotation, there are six other types of transformations unde r which we would
like our classiﬁer to be invariant. There are translation (t wo directions),
scaling (two directions), sheer, and character thickness. Hence the curves
and tangent lines in Figures 13.10 and 13.11 are actually 7-d imensional
manifolds and hyperplanes. It is infeasible to add transfor med versions
of each training image to capture all of these possibilities . The tangent
manifolds provide an elegant way of capturing the invarianc es.
Table 13.1 shows the test misclassiﬁcation error for a probl em with 7291
training images and 2007 test digits (the U.S. Postal Servic es database), for
a carefully constructed neural network, and simple 1-neare st-neighbor and

13.4 Adaptive Nearest-Neighbor Methods 475
TABLE 13.1. Test error rates for the handwritten ZIP code problem.
Method Error rate
Neural-net 0.049
1-nearest-neighbor/Euclidean distance 0.055
1-nearest-neighbor/tangent distance 0.026
tangent distance 1-nearest-neighbor rules. The tangent di stance nearest-
neighbor classiﬁer works remarkably well, with test error r ates near those
for the human eye (this is a notoriously diﬃcult test set). In practice,
it turned out that nearest-neighbors are too slow for online classiﬁcation
in this application (see Section 13.5), and neural network c lassiﬁers were
subsequently developed to mimic it.
13.4 Adaptive Nearest-Neighbor Methods
When nearest-neighbor classiﬁcation is carried out in a hig h-dimensional
feature space, the nearest neighbors of a point can be very fa r away, causing
bias and degrading the performance of the rule.
Toquantifythis,consider Ndatapointsuniformlydistributedintheunit
cube [−1
2,1
2]p. LetRbe the radius of a 1-nearest-neighborhood centered at
the origin. Then
median(R) =v−1/p
p/parenleftig
1−1
21/N/parenrightig1/p
, (13.7)
wherevprpis the volume of the sphere of radius rinpdimensions. Fig-
ure 13.12 shows the median radius for various training sampl e sizes and
dimensions. We see that median radius quickly approaches 0 .5, the dis-
tance to the edge of the cube.
What can be done about this problem? Consider the two-class s ituation
in Figure 13.13. There are two features, and a nearest-neigh borhood at
a query point is depicted by the circular region. Implicit in near-neighbor
classiﬁcation is the assumption that the class probabiliti es are roughly con-
stant in the neighborhood, and hence simple averages give go od estimates.
However, in this example the class probabilities vary only i n the horizontal
direction. If we knew this, we would stretch the neighborhoo d in the verti-
cal direction, as shown by the tall rectangular region. This will reduce the
bias of our estimate and leave the variance the same.
In general, this calls for adapting the metric used in neares t-neighbor
classiﬁcation, so that the resulting neighborhoods stretc h out in directions
for which the class probabilities don’t change much. In high -dimensional
feature space, the class probabilities might change only a l ow-dimensional
subspace and hence there can be considerable advantage to ad apting the
metric.

476 13. Prototypes and Nearest-Neighbors
DimensionMedian Radius
0 5 10 150.0 0.1 0.2 0.3 0.4 0.5 0.6N=100N=1,000
N=10,000
FIGURE 13.12. Median radius of a 1-nearest-neighborhood, for uniform dat a
withNobservations in pdimensions.
o
oo
oo
oo
ooo
o
oo
o
ooo
oo
o
oo
oo
o
•5-Nearest Neighborhoods
FIGURE 13.13. The points are uniform in the cube, with the vertical line sepa -
rating class red and green. The vertical strip denotes the 5-nearest-neighbor region
using only the horizontal coordinate to ﬁnd the nearest-neig hbors for the target
point (solid dot). The sphere shows the 5-nearest-neighbor region using both co-
ordinates, and we see in this case it has extended into the clas s-red region (and
is dominated by the wrong class in this instance).

13.4 Adaptive Nearest-Neighbor Methods 477
Friedman (1994a) proposed a method in which rectangular nei ghbor-
hoods are found adaptively by successively carving away edg es of a box
containing the training data. Here we describe the discriminant adaptive
nearest-neighbor (DANN) rule of Hastie and Tibshirani (1996a). Earlier,
related proposals appear in Short and Fukunaga (1981) and My les and
Hand (1990).
At each query point a neighborhood of say 50 points is formed, and the
class distribution among the points is used to decide how to d eform the
neighborhood—that is, to adapt the metric. The adapted metri c is then
used in a nearest-neighbor rule at the query point. Thus at ea ch query
point a potentially diﬀerent metric is used.
In Figure 13.13 it is clear that the neighborhood should be st retched in
the direction orthogonal to line joining the class centroid s. This direction
also coincides with the linear discriminant boundary, and i s the direction
in which the class probabilities change the least. In genera l this direction
of maximum change will not be orthogonal to the line joining t he class cen-
troids (see Figure 4.9 on page 116.) Assuming a local discrim inant model,
the information contained in the local within- and between- class covari-
ance matrices is all that is needed to determine the optimal s hape of the
neighborhood.
Thediscriminant adaptive nearest-neighbor (DANN) metric at a query
pointx0is deﬁned by
D(x,x0) = (x−x0)TΣ(x−x0), (13.8)
where
Σ=W−1/2[W−1/2BW−1/2+ǫI]W−1/2
=W−1/2[B∗+ǫI]W−1/2. (13.9)
HereWis the pooled within-class covariance matrix/summationtextK
k=1πkWkandB
is the between class covariance matrix/summationtextK
k=1πk(¯xk−¯x)(¯xk−¯x)T, with
WandBcomputed using only the 50 nearest neighbors around x0. After
computation of the metric, it is used in a nearest-neighbor r ule atx0.
This complicated formula is actually quite simple in its ope ration. It ﬁrst
spheres the data with respect to W, and then stretches the neighborhood
in the zero-eigenvalue directions of B∗(the between-matrix for the sphered
data ). This makes sense, since locally the observed class me ans do not dif-
fer in these directions. The ǫparameter rounds the neighborhood, from an
inﬁnite strip to an ellipsoid, to avoid using points far away from the query
point. The value of ǫ= 1 seems to work well in general. Figure 13.14 shows
the resulting neighborhoods for a problem where the classes form two con-
centric circles. Notice how the neighborhoods stretch out o rthogonally to
the decision boundaries when both classes are present in the neighborhood.
In the pure regions with only one class, the neighborhoods re main circular;

478 13. Prototypes and Nearest-Neighbors
o
oo ooo
oo
oo
oo
oo
o
oooo
o
o
ooo
o
oo
ooo
ooooo
oo
o
ooo
oo ooo
oo
o
ooo
ooo
oo
o oo
ooo
o
ooo
ooo
oooooo
o
o
oo
oo
ooooo
oo
o
oooo
oo
o
o
o
oo
oo
o
ooo
oooo
ooo
oo
oo
oo
o
ooo
oo
ooo
ooo
o
oo
oooo
oo o
oo
oooo
oo
oooo
oo
o
o
oo
o
ooo
ooo
oo
o
o
oo
oo
ooo
oo
oo
ooo
oo
oooo
oo
o
oo
o
ooo
o
oooo
oo
ooo
oooo
ooo
o
ooo
ooo
oo
oo
oo
oo
oooo
oooo
o
oo
ooooo
oo
o
oo
ooo
oo
o o
ooo
ooo
o
oooo
ooo
oo
o
ooo
oo
oo
oo
oo
oo
o
oo
o o
oooo
oo
oo
oo
oooo
o
ooo
ooo
oo
oo oo
o
oo
oooo
oo
ooo
o
oooo
o
oo
ooo o
o ooo
ooooo
oo
oo
o
o
ooo
oo
ooo
o
oo
ooo
oo
ooo
oo
oooo ooo
oo
oo
o
o
oo
ooo
FIGURE 13.14. Neighborhoods found by the DANN procedure, at various query
points (centers of the crosses). There are two classes in the d ata, with one class
surrounding the other. 50nearest-neighbors were used to estimate the local met-
rics. Shown are the resulting metrics used to form 15-nearest-neighborhoods.
in these cases the between matrix B= 0, and the Σin (13.8) is the identity
matrix.
13.4.1 Example
Here we generate two-class data in ten dimensions, analogou s to the two-
dimensional example of Figure 13.14. All ten predictors in c lass 1 are inde-
pendent standard normal, conditioned on the squared radius being greater
than 22.4 and less than 40, while the predictors in class 2 are independent
standardnormalwithouttherestriction.Thereare250obse rvationsineach
class. Hence the ﬁrst class almost completely surrounds the second class in
the full ten-dimensional space.
In this example there are no pure noise variables, the kind th at a nearest-
neighbor subset selection rule might be able to weed out. At a ny given
point in the feature space, the class discrimination occurs along only one
direction. However, this direction changes as we move acros s the feature
space and all variables are important somewhere in the space .
Figure 13.15 shows boxplots of the test error rates over ten r ealiza-
tions, for standard 5-nearest-neighbors, LVQ, and discrim inant adaptive
5-nearest-neighbors. We used 50 prototypes per class for LV Q, to make
it comparable to 5 nearest-neighbors (since 250 /5 = 50). The adaptive
metric signiﬁcantly reduces the error rate, compared to LVQ or standard
nearest-neighbors.

13.4 Adaptive Nearest-Neighbor Methods 4790.0 0.1 0.2 0.3 0.4
5NN LVQ DANNTest Error
FIGURE 13.15. Ten-dimensional simulated example: boxplots of the test error
rates over ten realizations, for standard 5-nearest-neighbors, LVQ with 50centers,
and discriminant-adaptive 5-nearest-neighbors
13.4.2 Global Dimension Reduction for Nearest-Neighbors
The discriminant-adaptive nearest-neighbor method carri es out local di-
mension reduction—that is, dimension reduction separately at each query
point. In many problems we can also beneﬁt from global dimens ion re-
duction, that is, apply a nearest-neighbor rule in some opti mally chosen
subspace of the original feature space. For example, suppos e that the two
classes form two nested spheres in four dimensions of featur e space, and
there are an additional six noise features whose distributi on is independent
of class. Then we would like to discover the important four-d imensional
subspace, and carry out nearest-neighbor classiﬁcation in that reduced sub-
space.HastieandTibshirani(1996a)discussavariationof thediscriminant-
adaptive nearest-neighbor method for this purpose. At each training point
xi, the between-centroids sum of squares matrix Biis computed, and then
these matrices are averaged over all training points:
¯B=1
NN/summationdisplay
i=1Bi. (13.10)
Lete1,e2,...,e pbe the eigenvectors of the matrix ¯B, ordered from largest
to smallest eigenvalue θk. Then these eigenvectors span the optimal sub-
spaces for global subspace reduction. The derivation is bas ed on the fact
that the best rank- Lapproximation to ¯B,¯B[L]=/summationtextL
ℓ=1θℓeℓeT
ℓ, solves the
least squares problem
min
rank(M)=LN/summationdisplay
i=1trace[(Bi−M)2]. (13.11)
Since each Bicontains information on (a) the local discriminant subspac e,
and (b) the strength of discrimination in that subspace, (13 .11) can be seen

480 13. Prototypes and Nearest-Neighbors
as a way of ﬁnding the best approximating subspace of dimensi onLto a
series ofNsubspaces by weighted least squares (Exercise 13.5.)
In the four-dimensional sphere example mentioned above and examined
in Hastie and Tibshirani (1996a), four of the eigenvalues θℓturn out to be
large (having eigenvectors nearly spanning the interestin g subspace), and
the remaining six are near zero. Operationally, we project t he data into
the leading four-dimensional subspace, and then carry out n earest neighbor
classiﬁcation. In the satellite image classiﬁcation examp le in Section 13.3.2,
the technique labeled DANNin Figure 13.8 used 5-nearest-neighbors in a
globally reduced subspace. There are also connections of th is technique
with the sliced inverse regression proposal of Duan and Li (1991). These
authors use similar ideas in the regression setting, but do g lobal rather
than local computations. They assume and exploit spherical symmetry of
the feature distribution to estimate interesting subspace s.
13.5 Computational Considerations
One drawback of nearest-neighbor rules in general is the com putational
load, both in ﬁnding the neighbors and storing the entire tra ining set. With
Nobservationsand ppredictors,nearest-neighborclassiﬁcationrequires Np
operations to ﬁnd the neighbors per query point. There are fa st algorithms
for ﬁnding nearest-neighbors (Friedman et al., 1975; Fried man et al., 1977)
which can reduce this load somewhat. Hastie and Simard (1998 ) reduce
the computations for tangent distance by developing analog s ofK-means
clustering in the context of this invariant metric.
Reducing the storage requirements is more diﬃcult, and vari ousediting
andcondensing procedures have been proposed. The idea is to isolate a
subset of the training set that suﬃces for nearest-neighbor predictions, and
throw away the remaining training data. Intuitively, it see ms important to
keep the training points that are near the decision boundari es and on the
correct side of those boundaries, while some points far from the boundaries
could be discarded.
Themulti-edit algorithm of Devijver and Kittler (1982) divides the data
cyclically into training and test sets, computing a nearest neighbor rule on
the training set and deleting test points that are misclassi ﬁed. The idea is
to keep homogeneous clusters of training observations.
Thecondensing procedure of Hart (1968) goes further, trying to keep
only important exterior points of these clusters. Starting with a single ran-
domly chosen observation as the training set, each addition al data item is
processed one at a time, adding it to the training set only if i t is misclas-
siﬁed by a nearest-neighbor rule computed on the current tra ining set.
These procedures are surveyed in Dasarathy (1991) and Riple y (1996).
They can also be applied to other learning procedures beside s nearest-

Exercises 481
neighbors. While such methods are sometimes useful, we have not had
much practical experience with them, nor have we found any sy stematic
comparison of their performance in the literature.
Bibliographic Notes
The nearest-neighbor method goes back at least to Fix and Hod ges (1951).
The extensive literature on the topic is reviewed by Dasarat hy (1991);
Chapter 6 of Ripley (1996) contains a good summary. K-means cluster-
ing is due to Lloyd (1957) and MacQueen (1967). Kohonen (1989 ) intro-
duced learning vector quantization. The tangent distance m ethod is due to
Simard et al. (1993). Hastie and Tibshirani (1996a) propose d the discrim-
inant adaptive nearest-neighbor technique.
Exercises
Ex. 13.1 Consider a Gaussian mixture model where the covariance matr ices
are assumed to be scalar: Σr=σI∀r= 1,...,R, andσis a ﬁxed param-
eter. Discuss the analogy between the K-means clustering algorithm and
the EM algorithm for ﬁtting this mixture model in detail. Sho w that in the
limitσ→0 the two methods coincide.
Ex. 13.2 Derive formula (13.7) for the median radius of the 1-nearest -
neighborhood.
Ex. 13.3 LetE∗be the error rate of the Bayes rule in a K-class problem,
where the true class probabilities are given by pk(x), k= 1,...,K. As-
suming the test point and training point have identical feat uresx, prove
(13.5)
K/summationdisplay
k=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K
K−1(1−pk∗(x))2.
wherek∗= argmax kpk(x). Hence argue that the error rate of the 1-
nearest-neighbor rule converges in L1, as the size of the training set in-
creases, to a value E1, bounded above by
E∗/parenleftig
2−E∗K
K−1/parenrightig
. (13.12)
[This statement of the theorem of Cover and Hart (1967) is tak en from
Chapter 6 of Ripley (1996), where a short proof is also given] .

482 13. Prototypes and Nearest-Neighbors
Ex. 13.4 Consider an image to be a function F(x) : IR2∝ma√sto→IR1over the two-
dimensionalspatialdomain(papercoordinates).Then F(c+x0+A(x−x0))
represents an aﬃne transformation of the image F, whereAis a 2×2
matrix.
1. Decompose A(via Q-R) in such a way that parameters identifying
the four aﬃne transformations (two scale, shear and rotatio n) are
clearly identiﬁed.
2. Using the chain rule, show that the derivative of F(c+x0+A(x−x0))
w.r.t. each of these parameters can be represented in terms o f the two
spatial derivatives of F.
3. Using a two-dimensional kernel smoother (Chapter 6), des cribe how
to implement this procedure when the images are quantized to 16×16
pixels.
Ex. 13.5 LetBi,i= 1,2,...,Nbe squarep×ppositive semi-deﬁnite ma-
trices and let ¯B= (1/N)/summationtextBi. Write the eigen-decomposition of ¯Bas/summationtextp
ℓ=1θℓeℓeT
ℓwithθℓ≥θℓ−1≥···≥θ1. Show that the best rank- Lapprox-
imation for the Bi,
min
rank(M)=LN/summationdisplay
i=1trace[(Bi−M)2],
is given by ¯B[L]=/summationtextL
ℓ=1θℓeℓeT
ℓ. (Hint:Write/summationtextN
i=1trace[(Bi−M)2] as
N/summationdisplay
i=1trace[(Bi−¯B)2]+N/summationdisplay
i=1trace[(M−¯B)2]).
Ex. 13.6 Here we consider the problem of shape averaging . In particular,
Li, i= 1,...,Mare eachN×2 matrices of points in IR2, each sampled
from corresponding positions of handwritten (cursive) let ters. We seek an
aﬃne invariant average V, alsoN×2,VTV=I, of theMlettersLiwith
the following property: Vminimizes
M/summationdisplay
j=1min
Aj∝⌊a∇⌈⌊lLj−VAj∝⌊a∇⌈⌊l2.
Characterize the solution.
This solution can suﬀer if some of the letters are bigand dominate the
average. An alternative approach is to minimize instead:
M/summationdisplay
j=1min
Aj/vextenddouble/vextenddoubleLjA∗
j−V/vextenddouble/vextenddouble2.
Derive the solution to this problem. How do the criteria diﬀe r? Use the
SVD of the Ljto simplify the comparison of the two approaches.

Exercises 483
Ex. 13.7 Consider the application of nearest-neighbors to the “easy ” and
“hard” problems in the left panel of Figure 13.5.
1. Replicate the results in the left panel of Figure 13.5.
2. Estimate the misclassiﬁcation errors using ﬁvefold cros s-validation,
and compare the error rate curves to those in 1.
3. Consider an “AIC-like” penalization of the training set m isclassiﬁca-
tion error. Speciﬁcally, add 2 t/Nto the training set misclassiﬁcation
error, where tis the approximate number of parameters N/r,rbe-
ing the number of nearest-neighbors. Compare plots of the re sulting
penalized misclassiﬁcation error to those in 1 and 2. Which m ethod
gives a better estimate of the optimal number of nearest-nei ghbors:
cross-validation or AIC?
Ex. 13.8 Generate data in two classes, with two features. These featu res
are all independent Gaussian variates with standard deviat ion 1. Their
mean vectors are ( −1,−1) in class 1 and (1 ,1) in class 2. To each feature
vector apply a random rotation of angle θ,θchosen uniformly from 0 to
2π. Generate 50 observations from each class to form the traini ng set, and
500 in each class as the test set. Apply four diﬀerent classiﬁ ers:
1. Nearest-neighbors.
2. Nearest-neighbors with hints: ten randomly rotated vers ions of each
data point are added to the training set before applying near est-
neighbors.
3. Invariant metric nearest-neighbors, using Euclidean di stance invari-
ant to rotations about the origin.
4. Tangent distance nearest-neighbors.
In each case choose the number of neighbors by tenfold cross- validation.
Compare the results.

484 13. Prototypes and Nearest-Neighbors

This is page 485
Printer: Opaque this
14
Unsupervised Learning
14.1 Introduction
The previous chapters have been concerned with predicting t he values
of one or more outputs or response variables Y= (Y1,...,Y m) for a
given set of input or predictor variables XT= (X1,...,X p). Denote by
xT
i= (xi1,...,x ip) the inputs for the ith training case, and let yibe a
response measurement. The predictions are based on the trai ning sample
(x1,y1),...,(xN,yN) of previously solved cases, where the joint values of
all of the variables are known. This is called supervised learning or “learn-
ing with a teacher.” Under this metaphor the “student” prese nts an an-
swer ˆyifor eachxiin the training sample, and the supervisor or “teacher”
provides either the correct answer and/or an error associat ed with the stu-
dent’s answer. This is usually characterized by some loss fu nctionL(y,ˆy),
for example, L(y,ˆy) = (y−ˆy)2.
If one supposes that ( X,Y) are random variables represented by some
jointprobabilitydensityPr( X,Y),thensupervisedlearningcanbeformally
characterized as a density estimation problem where one is c oncerned with
determining properties of the conditional density Pr( Y|X). Usually the
properties of interest are the “location” parameters µthat minimize the
expected error at each x,
µ(x) = argmin
θEY|XL(Y,θ). (14.1)

486 14. Unsupervised Learning
Conditioning one has
Pr(X,Y) = Pr(Y|X)·Pr(X),
where Pr(X) is the joint marginal density of the Xvalues alone. In su-
pervised learning Pr( X) is typically of no direct concern. One is interested
mainly in the properties of the conditional density Pr( Y|X). SinceYis of-
ten of low dimension (usually one), and only its location µ(x) is of interest,
the problem is greatly simpliﬁed. As discussed in the previo us chapters,
there are many approaches for successfully addressing supe rvised learning
in a variety of contexts.
In this chapter we address unsupervised learning or “learning without a
teacher.” In this case one has a set of Nobservations ( x1,x2,...,x N) of a
randomp-vectorXhaving joint density Pr( X). The goal is to directly infer
the properties of this probability density without the help of a supervisor or
teacher providing correct answers or degree-of-error for e ach observation.
The dimension of Xis sometimes much higher than in supervised learn-
ing, and the properties of interest are often more complicat ed than simple
location estimates. These factors are somewhat mitigated b y the fact that
Xrepresents all of the variables under consideration; one is not required
to infer how the properties of Pr( X) change, conditioned on the changing
values of another set of variables.
In low-dimensional problems (say p≤3), there are a variety of eﬀective
nonparametric methods for directly estimating the density Pr(X) itself at
allX-values, and representing it graphically (Silverman, 1986 , e.g.). Owing
to the curse of dimensionality, these methods fail in high di mensions. One
must settle for estimating rather crude global models, such as Gaussian
mixtures or various simple descriptive statistics that cha racterize Pr( X).
Generally, these descriptive statistics attempt to charac terizeX-values,
or collections of such values, where Pr( X) is relatively large. Principal
components, multidimensional scaling, self-organizing m aps, and principal
curves, for example, attempt to identify low-dimensional m anifolds within
theX-space that represent high data density. This provides info rmation
about the associations among the variables and whether or no t they can be
considered as functions of a smaller set of “latent” variabl es. Cluster anal-
ysis attempts to ﬁnd multiple convex regions of the X-space that contain
modes of Pr( X). This can tell whether or not Pr( X) can be represented by
a mixture of simpler densities representing distinct types or classes of ob-
servations. Mixture modeling has a similar goal. Associati on rules attempt
to construct simple descriptions (conjunctive rules) that describe regions
of high density in the special case of very high dimensional b inary-valued
data.
With supervised learning there is a clear measure of success , or lack
thereof, that can be used to judge adequacy in particular sit uations and
to compare the eﬀectiveness of diﬀerent methods over variou s situations.

14.2 Association Rules 487
Lack of success is directly measured by expected loss over th e joint dis-
tribution Pr( X,Y). This can be estimated in a variety of ways including
cross-validation. In the context of unsupervised learning , there is no such
directmeasureofsuccess.Itisdiﬃculttoascertaintheval idityofinferences
drawnfromtheoutputofmostunsupervisedlearningalgorit hms.Onemust
resort to heuristic arguments not only for motivating the al gorithms, as is
often the case in supervised learning as well, but also for ju dgments as to
the quality of the results. This uncomfortable situation ha s led to heavy
proliferation of proposed methods, since eﬀectiveness is a matter of opinion
and cannot be veriﬁed directly.
In this chapter we present those unsupervised learning tech niques that
are among the most commonly used in practice, and additional ly, a few
others that are favored by the authors.
14.2 Association Rules
Association rule analysis has emerged as a popular tool for m ining com-
mercial data bases. The goal is to ﬁnd joint values of the vari ablesX=
(X1,X2,...,X p) that appear most frequently in the data base. It is most
often applied to binary-valued data Xj∈{0,1}, where it is referred to as
“market basket” analysis. In this context the observations are sales trans-
actions, such as those occurring at the checkout counter of a store. The
variables represent all of the items sold in the store. For ob servationi, each
variableXjis assigned one of two values; xij= 1 if thejth item is pur-
chased as part of the transaction, whereas xij= 0 if it was not purchased.
Thosevariablesthatfrequentlyhavejointvaluesofonerep resentitemsthat
are frequently purchased together. This information can be quite useful for
stocking shelves, cross-marketing in sales promotions, ca talog design, and
consumer segmentation based on buying patterns.
More generally, the basic goal of association rule analysis is to ﬁnd a
collection of prototype X-valuesv1,...,v Lfor the feature vector X, such
that the probability density Pr( vl) evaluated at each of those values is rela-
tivelylarge.Inthisgeneralframework,theproblemcanbev iewedas“mode
ﬁnding” or “bump hunting.” As formulated, this problem is im possibly dif-
ﬁcult. A natural estimator for each Pr( vl) is the fraction of observations
for whichX=vl. For problems that involve more than a small number
of variables, each of which can assume more than a small numbe r of val-
ues, the number of observations for which X=vlwill nearly always be too
small for reliable estimation. In order to have a tractable p roblem, both the
goals of the analysis and the generality of the data to which i t is applied
must be greatly simpliﬁed.
The ﬁrst simpliﬁcation modiﬁes the goal. Instead of seeking valuesx
where Pr(x) is large, one seeks regionsof theX-space with high probability

488 14. Unsupervised Learning
content relative to their size or support. Let Sjrepresent the set of all
possible values of the jth variable (its support), and letsj⊆Sjbe a subset
of these values. The modiﬁed goal can be stated as attempting to ﬁnd
subsets of variable values s1,...,s psuch that the probability of each of the
variables simultaneously assuming a value within its respe ctive subset,
Pr
p/intersectiondisplay
j=1(Xj∈sj)
, (14.2)
is relatively large. The intersection of subsets ∩p
j=1(Xj∈sj) is called a
conjunctive rule . For quantitative variables the subsets sjare contiguous
intervals;forcategoricalvariablesthesubsetsaredelin eatedexplicitly.Note
that if the subset sjis in fact the entire set of values sj=Sj, as is often
the case, the variable Xjis saidnotto appear in the rule (14.2).
14.2.1 Market Basket Analysis
General approaches to solving (14.2) are discussed in Secti on 14.2.5. These
can be quite useful in many applications. However, they are n ot feasible
for the very large ( p≈104,N≈108) commercial data bases to which
market basket analysis is often applied. Several further si mpliﬁcations of
(14.2) are required. First, only two types of subsets are con sidered; either
sjconsists of a singlevalue ofXj,sj=v0j, or it consists of the entire set
of values that Xjcan assume, sj=Sj. This simpliﬁes the problem (14.2)
to ﬁnding subsets of the integers J⊂{1,...,p}, and corresponding values
v0j, j∈J, such that
Pr
/intersectiondisplay
j∈J(Xj=v0j)
 (14.3)
is large. Figure 14.1 illustrates this assumption.
One can apply the technique of dummy variables to turn (14.3) into
a problem involving only binary-valued variables. Here we a ssume that
the supportSjis ﬁnite for each variable Xj. Speciﬁcally, a new set of
variablesZ1,...,Z Kis created, one such variable for each of the values
vljattainable by each of the original variables X1,...,X p. The number of
dummy variables Kis
K=p/summationdisplay
j=1|Sj|,
where|Sj|is the number of distinct values attainable by Xj. Each dummy
variable is assigned the value Zk= 1 if the variable with which it is as-
sociated takes on the corresponding value to which Zkis assigned, and
Zk= 0 otherwise. This transforms (14.3) to ﬁnding a subset of th e integers
K⊂{1,...,K}such that

14.2 Association Rules 489
X1 X1 X1
X2X2X2
FIGURE 14.1. Simpliﬁcations for association rules. Here there are two inpu ts
X1andX2, taking four and six distinct values, respectively. The red sq uares
indicate areas of high density. To simplify the computations , we assume that the
derived subset corresponds to either a single value of an input or all values. With
this assumption we could ﬁnd either the middle or right pattern , but not the left
one.
Pr/bracketleftigg/intersectiondisplay
k∈K(Zk= 1)/bracketrightigg
= Pr/bracketleftigg/productdisplay
k∈KZk= 1/bracketrightigg
(14.4)
is large. This is the standard formulation of the market bask et problem.
The setKis called an “item set.” The number of variables Zkin the item
set is called its “size” (note that the size is no bigger than p). The estimated
value of (14.4) is taken to be the fraction of observations in the data base
for which the conjunction in (14.4) is true:
/hatwiderPr/bracketleftigg/productdisplay
k∈K(Zk= 1)/bracketrightigg
=1
NN/summationdisplay
i=1/productdisplay
k∈Kzik. (14.5)
Herezikis the value of Zkfor thisith case. This is called the “support” or
“prevalence” T(K)oftheitemsetK.Anobservation iforwhich/producttext
k∈Kzik=
1 is said to “contain” the item set K.
In association rule mining a lower support bound tis speciﬁed, and one
seeksallitem setsKlthat can be formed from the variables Z1,...,Z K
with support in the data base greater than this lower bound t
{Kl|T(Kl)>t}. (14.6)
14.2.2 The Apriori Algorithm
The solution to this problem (14.6) can be obtained with feas ible compu-
tation for very large data bases provided the threshold tis adjusted so that
(14.6) consists of only a small fraction of all 2Kpossible item sets. The
“Apriori” algorithm (Agrawal et al., 1995) exploits severa l aspects of the

490 14. Unsupervised Learning
curse of dimensionality to solve (14.6) with a small number o f passes over
the data. Speciﬁcally, for a given support threshold t:
•The cardinality|{K|T(K)>t}|is relatively small.
•Any item setLconsisting of a subset of the items in Kmust have
support greater than or equal to that of K,L⊆K⇒T(L)≥T(K).
The ﬁrst pass over the data computes the support of all single -item sets.
Those whose support is less than the threshold are discarded . The second
pass computes the support of all item sets of size two that can be formed
from pairs of the single items surviving the ﬁrst pass. In oth er words, to
generate all frequent itemsets with |K|=m, we need to consider only
candidates such that allof theirmancestral item sets of size m−1 are
frequent. Those size-two item sets with support less than th e threshold are
discarded. Each successive pass over the data considers onl y those item
sets that can be formed by combining those that survived the p revious
pass with those retained from the ﬁrst pass. Passes over the d ata continue
until all candidate rules from the previous pass have suppor t less than the
speciﬁed threshold. The Apriori algorithm requires only on e pass over the
data for each value of |K|, which is crucial since we assume the data cannot
be ﬁtted into a computer’s main memory. If the data are suﬃcie ntly sparse
(or if the threshold tis high enough), then the process will terminate in
reasonable time even for huge data sets.
There are many additional tricks that can be used as part of th is strat-
egy to increase speed and convergence (Agrawal et al., 1995) . The Apriori
algorithm represents one of the major advances in data minin g technology.
Each high support item set K(14.6) returned by the Apriori algorithm is
cast into a set of “association rules.” The items Zk,k∈K, are partitioned
into two disjoint subsets, A∪B=K, and written
A⇒B. (14.7)
The ﬁrst item subset Ais called the “antecedent” and the second Bthe
“consequent.”Associationrulesaredeﬁnedtohaveseveral propertiesbased
on the prevalence of the antecedent and consequent item sets in the data
base. The “support” of the rule T(A⇒B) is the fraction of observations
in the union of the antecedent and consequent, which is just t he support
of the item setKfrom which they were derived. It can be viewed as an
estimate (14.5) of the probability of simultaneously obser ving both item
sets Pr(AandB) in a randomly selected market basket. The “conﬁdence”
or “predictability” C(A⇒B) of the rule is its support divided by the
support of the antecedent
C(A⇒B) =T(A⇒B)
T(A), (14.8)
which can be viewed as an estimate of Pr( B|A). The notation Pr( A), the
probability of an item set Aoccurring in a basket, is an abbreviation for

14.2 Association Rules 491
Pr(/producttext
k∈AZk= 1). The “expected conﬁdence” is deﬁned as the support of
the consequent T(B), which is an estimate of the unconditional probability
Pr(B). Finally, the “lift” of the rule is deﬁned as the conﬁdence d ivided by
the expected conﬁdence
L(A⇒B) =C(A⇒B)
T(B).
This is an estimate of the association measure Pr( AandB)/Pr(A)Pr(B).
Asanexample,supposetheitemset K={peanut butter, jelly, bread }
and consider the rule {peanut butter, jelly }⇒{bread}. A support value
of 0.03 for this rule means that peanut butter ,jelly, andbreadappeared
together in 3% of the market baskets. A conﬁdence of 0.82 for t his rule im-
plies that when peanut butter andjellywere purchased, 82% of the time
breadwas also purchased. If bread appeared in 43% of all market bas kets
then the rule{peanut butter, jelly }⇒{bread}would have a lift of 1 .95.
The goal of this analysis is to produce association rules (14 .7) with both
high values of support and conﬁdence (14.8). The Apriori alg orithm returns
all item sets with high support as deﬁned by the support thres holdt(14.6).
A conﬁdence threshold cis set, and all rules that can be formed from those
item sets (14.6) with conﬁdence greater than this value
{A⇒B|C(A⇒B)>c} (14.9)
are reported. For each item set Kof size|K|there are 2|K|−1−1 rules of
the formA⇒(K−A),A⊂K. Agrawal et al. (1995) present a variant of
the Apriori algorithm that can rapidly determine which rule s survive the
conﬁdence threshold (14.9) from all possible rules that can be formed from
the solution item sets (14.6).
The output of the entire analysis is a collection of associat ion rules (14.7)
that satisfy the constraints
T(A⇒B)>tandC(A⇒B)>c.
These are generally stored in a data base that can be queried b y the user.
Typical requests might be to display the rules in sorted orde r of conﬁdence,
lift or support. More speciﬁcally, one might request such a l ist conditioned
on particular items in the antecedent or especially the cons equent. For
example, a request might be the following:
Display all transactions in which ice skates are the consequ ent
that have conﬁdence over 80%and support of more than 2%.
This could provide information on those items (antecedent) that predicate
sales of ice skates. Focusing on a particular consequent cas ts the problem
into the framework of supervised learning.
Association rules have become a popular tool for analyzing v ery large
commercial data bases in settings where market basket is rel evant. That is

492 14. Unsupervised Learning
when the data can be cast in the form of a multidimensional con tingency
table. The output is in the form of conjunctive rules (14.4) t hat are easily
understood and interpreted. The Apriori algorithm allows t his analysis to
beappliedtohugedatabases,muchlargerthatareamenablet oothertypes
of analyses. Association rules are among data mining’s bigg est successes.
Besides the restrictive form of the data to which they can be a pplied, as-
sociation rules have other limitations. Critical to comput ational feasibility
is the support threshold (14.6). The number of solution item sets, their size,
and the number of passes required over the data can grow expon entially
with decreasing size of this lower bound. Thus, rules with hi gh conﬁdence
or lift, but low support, will not be discovered. For example , a high conﬁ-
dence rule such as vodka⇒caviarwill not be uncovered owing to the low
sales volume of the consequent caviar.
14.2.3 Example: Market Basket Analysis
We illustrate the use of Apriori on a moderately sized demogr aphics data
base. This data set consists of N= 9409 questionnaires ﬁlled out by shop-
pingmallcustomersintheSanFranciscoBayArea(ImpactRes ources,Inc.,
Columbus OH, 1987). Here we use answers to the ﬁrst 14 questio ns, relat-
ing to demographics, for illustration. These questions are listed in Table
14.1. The data are seen to consist of a mixture of ordinal and ( unordered)
categorical variables, many of the latter having more than a few values.
There are many missing values.
WeusedafreewareimplementationoftheApriorialgorithmd uetoChris-
tianBorgelt1.Afterremovingobservationswithmissingvalues,eachord inal
predictor was cut at its median and coded by two dummy variabl es; each
categorical predictor with kcategories was coded by kdummy variables.
This resulted in a 6876 ×50 matrix of 6876 observations on 50 dummy
variables.
The algorithm found a total of 6288 association rules, invol ving≤5
predictors, with support of at least 10%. Understanding thi s large set of
rulesisitselfachallengingdataanalysistask.Wewillnot attemptthishere,
but only illustrate in Figure 14.2 the relative frequency of each dummy
variable in the data (top) and the association rules (bottom ). Prevalent
categories tend to appear more often in the rules, for exampl e, the ﬁrst
category in language (English). However, others such as occ upation are
under-represented, with the exception of the ﬁrst and ﬁfth l evel.
Here are three examples of association rules found by the Apr iori algo-
rithm:
Association rule 1: Support 25%, conﬁdence 99.7% and lift 1.03.
1Seehttp://fuzzy.cs.uni-magdeburg.de/ ∼borgelt.

14.2 Association Rules 493
0 10 20 30 40 500.0 0.02 0.04 0.06
AttributeRelative Frequency in Data
incomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic language
0 10 20 30 40 500.0 0.04 0.08 0.12
AttributeRelative Frequency in Association Rules
incomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic languageFIGURE 14.2. Market basket analysis: relative frequency of each dummy vari -
able (coding an input category) in the data (top), and the asso ciation rules found
by the Apriori algorithm (bottom).

494 14. Unsupervised Learning
TABLE 14.1. Inputs for the demographic data.
Feature Demographic # Values Type
1 Sex 2 Categorical
2 Marital status 5 Categorical
3 Age 7 Ordinal
4 Education 6 Ordinal
5 Occupation 9 Categorical
6 Income 9 Ordinal
7 Years in Bay Area 5 Ordinal
8 Dual incomes 3 Categorical
9 Number in household 9 Ordinal
10 Number of children 9 Ordinal
11 Householder status 3 Categorical
12 Type of home 5 Categorical
13 Ethnic classiﬁcation 8 Categorical
14 Language in home 3 Categorical
/bracketleftbiggnumber in household = 1
number of children = 0/bracketrightbigg
⇓
language in home = English
Association rule 2: Support 13.4%, conﬁdence 80.8%, and lift 2.13.

language in home = English
householder status = own
occupation = {professional/managerial }

⇓
income≥$40,000
Association rule 3: Support 26.5%, conﬁdence 82.8% and lift 2.15.

language in home = English
income<$40,000
marital status = not married
number of children = 0

⇓
education/∈{college graduate, graduate study }

14.2 Association Rules 495
We chose the ﬁrst and third rules based on their high support. The second
rule is an association rule with a high-income consequent, a nd could be
used to try to target high-income individuals.
As stated above, we created dummy variables for each categor y of the
input predictors, for example, Z1=I(income<$40,000) andZ2=
I(income≥$40,000) for below and above the median income. If we were
interested only in ﬁnding associations with the high-incom e category, we
would include Z2but notZ1. This is often the case in actual market basket
problems, where we are interested in ﬁnding associations wi th the presence
of a relatively rare item, but not associations with its abse nce.
14.2.4 Unsupervised as Supervised Learning
Here we discuss a technique for transforming the density est imation prob-
lem into one of supervised function approximation. This for ms the basis
for the generalized association rules described in the next section.
Letg(x) be the unknown data probability density to be estimated, an d
g0(x) be a speciﬁed probability density function used for refere nce. For ex-
ample,g0(x) might be the uniform density over the range of the variables .
Other possibilities are discussed below. The data set x1,x2,...,x Nis pre-
sumed to be an i.i.d.random sample drawn from g(x). A sample of size N0
can be drawn from g0(x) using Monte Carlo methods. Pooling these two
data sets, and assigning mass w=N0/(N+N0) to those drawn from g(x),
andw0=N/(N+N0) to those drawn from g0(x), results in a random
sample drawn from the mixture density ( g(x)+g0(x))/2. If one assigns
the valueY= 1 to each sample point drawn from g(x) andY= 0 those
drawn from g0(x), then
µ(x) =E(Y|x) =g(x)
g(x)+g0(x)
=g(x)/g0(x)
1+g(x)/g0(x)(14.10)
can be estimated by supervised learning using the combined s ample
(y1,x1),(y2,x2),...,(yN+N0,xN+N0) (14.11)
as training data. The resulting estimate ˆ µ(x) can be inverted to provide an
estimate for g(x)
ˆg(x) =g0(x)ˆµ(x)
1−ˆµ(x). (14.12)
Generalizedversionsoflogistic regression(Section 4.4) areespeciallywell
suited for this application since the log-odds,
f(x) = logg(x)
g0(x), (14.13)
are estimated directly. In this case one has

496 14. Unsupervised Learning
-1 0 1 2-2 0 2 4 6••
••••
••
•
•••
••
••
••
••
•
••••
••••
••
•
••
••
••
••
•
••••
•••
•
••
••••••
••••
• •
••
••••
••••
•
•••
•••••
•
• • •
••
••
••
•
••
•••
••
••
••
•••
•••
••
••
••
••
•••
••
••
••
•••
••••
•
••••
••••
•••••
•••
••••••••
•
••
•••
••
••
•••
•••
• •
••
••
•
•••
•
••
•••
•
••
•••••
•
-1 0 1 2-2 0 2 4 6••
••••
••
•
•••
••
••
••
••
•
••••
••••
••
•
••
••
••
••
•
••••
•••
•
••
••••••
••••
• •
••
••••
••••
•
•••
•••••
•
• • •
••
••
••
•
••
•••
••
••
••
•••
•••
••
••
••
••
•••
••
••
••
•••
••••
•
••••
••••
•••••
•••
••••••••
•
••
•••
••
••
•••
•••
• •
••
••
•
•••
•
••
•••
•
••
•••••
••
• ••
•
•••
••
•
••••
••
••••
•
••
••••
•
••
••
••
••
•••••• •
••••
•
•••
•
••
•••
••
•
••
•••••
•
•••••
•
•• ••
••
••
••
•••
••
•
•
••
•
••
•••
•
••••
•
••
••
••
•••
•
•• •••
•••••
••
•••
•••
•
••
••
•••
••
•
••
••
•
••
••
••
••••
•••
•••
•
••
••
•
••
••
•••
••
•
••
•••
•
• •
••
••••
••
• •
X1 X1
X2X2
FIGURE 14.3. Density estimation via classiﬁcation. (Left panel:) Trainin g set
of200data points. (Right panel:) Training set plus 200reference data points,
generated uniformly over the rectangle containing the traini ng data. The training
sample was labeled as class 1, and the reference sample class 0, and a semipara-
metric logistic regression model was ﬁt to the data. Some cont ours for ˆg(x)are
shown.
ˆg(x) =g0(x)eˆf(x). (14.14)
An example is shown in Figure 14.3. We generated a training se t of size
200 shown in the left panel. The right panel shows the referen ce data (blue)
generated uniformly over the rectangle containing the trai ning data. The
training sample was labeled as class 1, and the reference sam ple class 0,
and a logistic regression model, using a tensor product of na tural splines
(Section 5.2.1), was ﬁt to the data. Some probability contou rs of ˆµ(x) are
shown in the right panel; these are also the contours of the de nsity estimate
ˆg(x), since ˆg(x) = ˆµ(x)/(1−ˆµ(x)), is a monotone function. The contours
roughly capture the data density.
In principle any reference density can be used for g0(x) in (14.14). In
practice the accuracy of the estimate ˆ g(x) can depend greatly on partic-
ular choices. Good choices will depend on the data density g(x) and the
procedure used to estimate (14.10) or (14.13). If accuracy i s the goal,g0(x)
should be chosen so that the resulting functions µ(x) orf(x) are approx-
imated easily by the method being used. However, accuracy is not always
the primary goal. Both µ(x) andf(x) are monotonic functions of the den-
sity ratiog(x)/g0(x). They can thus be viewed as “contrast” statistics that
provide information concerning departures of the data dens ityg(x) from
the chosen reference density g0(x). Therefore, in data analytic settings, a
choice forg0(x) is dictated by types of departures that are deemed most
interesting in the context of the speciﬁc problem at hand. Fo r example, if
departures from uniformity are of interest, g0(x) might be the a uniform
density over the range of the variables. If departures from j oint normality

14.2 Association Rules 497
are of interest, a good choice for g0(x) would be a Gaussian distribution
with the same mean vector and covariance matrix as the data. D epartures
from independence could be investigated by using
g0(x) =p/productdisplay
j=1gj(xj), (14.15)
wheregj(xj) is the marginal data density of Xj, thejth coordinate of X.
A sample from this independent density (14.15) is easily gen erated from the
data itself by applying a diﬀerent random permutation to the data values
of each of the variables.
As discussed above, unsupervised learning is concerned wit h revealing
properties of the data density g(x). Each technique focuses on a particu-
lar property or set of properties. Although this approach of transforming
the problem to one of supervised learning (14.10)–(14.14) s eems to have
been part of the statistics folklore for some time, it does no t appear to
have had much impact despite its potential to bring well-dev eloped su-
pervised learning methodology to bear on unsupervised lear ning problems.
One reason may be that the problem must be enlarged with a simu lated
data set generated by Monte Carlo techniques. Since the size of this data
set should be at least as large as the data sample N0≥N, the compu-
tation and memory requirements of the estimation procedure are at least
doubled. Also, substantial computation may be required to g enerate the
Monte Carlo sample itself. Although perhaps a deterrent in t he past, these
increased computational requirements are becoming much le ss of a burden
as increased resources become routinely available. We illu strate the use of
supervised learning methods for unsupervised learning in t he next section.
14.2.5 Generalized Association Rules
The more general problem (14.2) of ﬁnding high-density regi ons in the data
space can be addressed using the supervised learning approa ch described
above. Although not applicable to the huge data bases for whi ch market
basket analysis is feasible, useful information can be obta ined from mod-
erately sized data sets. The problem (14.2) can be formulate d as ﬁnding
subsets of the integers J ⊂{1,2,...,p}and corresponding value subsets
sj, j∈Jfor the corresponding variables Xj, such that
/hatwiderPr
/intersectiondisplay
j∈J(Xj∈sj)
=1
NN/summationdisplay
i=1I
/intersectiondisplay
j∈J(xij∈sj)
 (14.16)
is large. Following the nomenclature of association rule an alysis,{(Xj∈
sj)}j∈Jwill be called a “generalized” item set. The subsets sjcorrespond-
ing to quantitative variables are taken to be contiguous int ervals within

498 14. Unsupervised Learning
their range of values, and subsets for categorical variable s can involve more
than a single value. The ambitious nature of this formulatio n precludes a
thorough search for all generalized item sets with support ( 14.16) greater
than a speciﬁed minimum threshold, as was possible in the mor e restric-
tive setting of market basket analysis. Heuristic search me thods must be
employed, and the most one can hope for is to ﬁnd a useful colle ction of
such generalized item sets.
Bothmarketbasketanalysis(14.5)andthegeneralizedform ulation(14.16)
implicitly reference the uniform probability distributio n. One seeks item
sets that are more frequent than would be expected if all join t data values
(x1,x2,...,x N) were uniformly distributed. This favors the discovery of
item sets whose marginal constituents ( Xj∈sj) areindividually frequent,
that is, the quantity
1
NN/summationdisplay
i=1I(xij∈sj) (14.17)
is large. Conjunctions of frequent subsets (14.17) will ten d to appear more
often among item sets of high support (14.16) than conjuncti ons of margin-
ally less frequent subsets. This is why the rule vodka⇒caviaris not likely
to be discovered in spite of a high association (lift); neith er item has high
marginal support, so that their joint support is especially small. Reference
to the uniform distribution can cause highly frequent item s ets with low
associations among their constituents to dominate the coll ection of highest
support item sets.
Highly frequent subsets sjare formed as disjunctions of the most fre-
quentXj-values. Using the product of the variable marginal data den sities
(14.15) as a reference distribution removes the preference for highly fre-
quent values of the individual variables in the discovered i tem sets. This is
because the density ratio g(x)/g0(x) is uniform if there are no associations
among the variables (complete independence), regardless o f the frequency
distribution of the individual variable values. Rules like vodka⇒caviar
would have a chance to emerge. It is not clear however, how to i ncorporate
reference distributions other than the uniform into the Apr iori algorithm.
As explained in Section 14.2.4, it is straightforward to gen erate a sample
from the product density (14.15), given the original data se t.
After choosing a reference distribution, and drawing a samp le from it
as in (14.11), one has a supervised learning problem with a bi nary-valued
output variable Y∈{0,1}. The goal is to use this training data to ﬁnd
regions
R=/intersectiondisplay
j∈J(Xj∈sj) (14.18)
forwhichthetargetfunction µ(x) =E(Y|x)isrelativelylarge.Inaddition,
one might wish to require that the datasupport of these regions

14.2 Association Rules 499
T(R) =/integraldisplay
x∈Rg(x)dx (14.19)
not be too small.
14.2.6 Choice of Supervised Learning Method
The regions (14.18) are deﬁned by conjunctive rules. Hence s upervised
methods that learn such rules would be most appropriate in th is context.
The terminal nodes of a CART decision tree are deﬁned by rules precisely
of the form (14.18). Applying CART to the pooled data (14.11) will pro-
duce a decision tree that attempts to model the target (14.10 ) over the
entire data space by a disjoint set of regions (terminal node s). Each region
is deﬁned by a rule of the form (14.18). Those terminal nodes twith high
averagey-values
¯yt= ave(yi|xi∈t)
are candidates for high-support generalized item sets (14. 16). The actual
(data) support is given by
T(R) = ¯yt·Nt
N+N0,
whereNtis the number of (pooled) observations within the region rep re-
sented by the terminal node. By examining the resulting deci sion tree, one
might discover interesting generalized item sets of relati vely high-support.
These can then be partitioned into antecedents and conseque nts in a search
for generalized association rules of high conﬁdence and/or lift.
Another natural learning method for this purpose is the pati ent rule
induction method PRIM described in Section 9.3. PRIM also pr oduces
rules precisely of the form (14.18), but it is especially des igned for ﬁnding
high-support regions that maximize the average target (14. 10) value within
them, rather than trying to model the target function over th e entire data
space. It also provides more control over the support/avera ge-target-value
tradeoﬀ.
Exercise 14.3 addresses an issue that arises with either of t hese methods
when we generate random data from the product of the marginal distribu-
tions.
14.2.7 Example: Market Basket Analysis (Continued)
We illustrate the use of PRIM on the demographics data of Tabl e 14.1.
Three of the high-support generalized item sets emerging fr om the PRIM
analysis were the following:
Item set 1: Support= 24%.

500 14. Unsupervised Learning

marital status = married
householder status = own
type of home∝ne}ationslash=apartment

Item set 2: Support= 24%.

age≤24
marital status∈ {living together-not married, single }
occupation /∈ {professional, homemaker, retired }
householder status ∈ {rent, live with family }

Item set 3: Support= 15%.

householder status = rent
type of home∝ne}ationslash=house
number in household ≤2
number of children = 0
occupation /∈ {homemaker, student, unemployed }
income∈[$20,000,$150,000]

Generalized association rules derived from these item sets with conﬁdence
(14.8) greater than 95% are the following:
Association rule 1: Support 25%, conﬁdence 99.7% and lift 1.35.
/bracketleftbigg
marital status = married
householder status = own/bracketrightbigg
⇓
type of home∝ne}ationslash=apartment
Association rule 2: Support 25%, conﬁdence 98.7% and lift 1.97.

age≤24
occupation /∈ {professional, homemaker, retired }
householder status ∈ {rent, live with family }

⇓
marital status∈{single, living together-not married }
Association rule 3: Support 25%, conﬁdence 95.9% and lift 2.61.
/bracketleftbigghouseholder status = own
type of home∝ne}ationslash=apartment/bracketrightbigg
⇓
marital status = married

14.3 Cluster Analysis 501
Association rule 4: Support 15%, conﬁdence 95.4% and lift 1.50.

householder status = rent
type of home∝ne}ationslash=house
number in household ≤2
occupation /∈ {homemaker, student, unemployed }
income∈[$20,000,$150,000]

⇓
number of children = 0
There are no great surprises among these particular rules. F or the most
part they verify intuition. In other contexts where there is less prior in-
formation available, unexpected results have a greater cha nce to emerge.
These results do illustrate the type of information general ized association
rules can provide, and that the supervised learning approac h, coupled with
a ruled induction method such as CART or PRIM, can uncover ite m sets
exhibiting high associations among their constituents.
Howdothesegeneralizedassociationrulescomparetothose foundearlier
by the Apriori algorithm? Since the Apriori procedure gives thousands of
rules, it is diﬃcult to compare them. However some general po ints can be
made. The Apriori algorithm is exhaustive—it ﬁnds allrules with support
greater than a speciﬁed amount. In contrast, PRIM is a greedy algorithm
and is not guaranteed to give an “optimal” set of rules. On the other hand,
the Apriori algorithm can deal only with dummy variables and hence could
not ﬁnd some of the above rules. For example, since type of home is a
categorical input, with a dummy variable for each level, Apr iori could not
ﬁnd a rule involving the set
type of home∝ne}ationslash=apartment.
To ﬁnd this set, we would have to code a dummy variable for apartment
versus the other categories of type of home. It will not gener ally be feasible
to precode all such potentially interesting comparisons.
14.3 Cluster Analysis
Cluster analysis, also called data segmentation, has a vari ety of goals. All
relate to grouping or segmenting a collection of objects int o subsets or
“clusters,” such that those within each cluster are more clo sely related to
one another than objects assigned to diﬀerent clusters. An o bject can be
described by a set of measurements, or by its relation to othe r objects.
In addition, the goal is sometimes to arrange the clusters in to a natural
hierarchy. This involves successively grouping the cluste rs themselves so

502 14. Unsupervised Learning
• •••
••••
• •••
••
•••••• • ••
•••
••
••
••
•
•••••
••••
••
•••
••
••••
••
••••
•••
••
••
••
•••
••
••• ••
•
•••
•••••
••••••
•
••••
•••••
•••••
•••••
••••
•• •
•••••
•••
••
•••
••
•••
••••
•••
••• ••
X1X2
FIGURE 14.4. Simulated data in the plane, clustered into three classes (repre -
sented by orange, blue and green) by the K-means clustering algorithm
that at each level of the hierarchy, clusters within the same group are more
similar to each other than those in diﬀerent groups.
Cluster analysis is also used to form descriptive statistic s to ascertain
whether or not the data consists of a set distinct subgroups, each group
representing objects with substantially diﬀerent propert ies. This latter goal
requires an assessment of the degree of diﬀerence between th e objects as-
signed to the respective clusters.
Central to all of the goals of cluster analysis is the notion o f the degree of
similarity (or dissimilarity) between the individual obje cts being clustered.
A clustering method attempts to group the objects based on th e deﬁnition
of similarity supplied to it. This can only come from subject matter consid-
erations. The situation is somewhat similar to the speciﬁca tion of a loss or
cost function in prediction problems (supervised learning ). There the cost
associated with an inaccurate prediction depends on consid erations outside
the data.
Figure 14.4 shows some simulated data clustered into three g roups via
the popular K-means algorithm. In this case two of the clusters are not
well separated, so that “segmentation” more accurately des cribes the part
of this process than “clustering.” K-means clustering starts with guesses
for the three cluster centers. Then it alternates the follow ing steps until
convergence:
•for each data point, the closest cluster center (in Euclidea n distance)
is identiﬁed;

14.3 Cluster Analysis 503
•each cluster center is replaced by the coordinate-wise aver age of all
data points that are closest to it.
We describe K-means clustering in more detail later, including the prob-
lem of how to choose the number of clusters (three in this exam ple).K-
means clustering is a top-down procedure, while other cluster approaches
that we discuss are bottom-up . Fundamental to all clustering techniques is
the choice of distance or dissimilarity measure between two objects. We
ﬁrst discuss distance measures before describing a variety of algorithms for
clustering.
14.3.1 Proximity Matrices
Sometimes the data is represented directly in terms of the pr oximity (alike-
ness or aﬃnity) between pairs of objects. These can be either similarities or
dissimilarities (diﬀerence or lack of aﬃnity). For example, in social scienc e
experiments, participants are asked to judge by how much cer tain objects
diﬀer from one another. Dissimilarities can then be compute d by averaging
over the collection of such judgments. This type of data can b e represented
by anN×NmatrixD, whereNis the number of objects, and each element
dii′records the proximity between the ith andi′th objects. This matrix is
then provided as input to the clustering algorithm.
Most algorithms presume a matrix of dissimilarities with no nnegative
entries and zero diagonal elements: dii= 0, i= 1,2,...,N.If the original
data were collected as similarities, a suitable monotone-d ecreasing function
can be used to convert them to dissimilarities. Also, most al gorithms as-
sume symmetric dissimilarity matrices, so if the original m atrixDis not
symmetric it must be replaced by ( D+DT)/2. Subjectively judged dissimi-
larities are seldom distances in the strict sense, since the triangle inequality
dii′≤dik+di′k,forallk∈{1,...,N}doesnothold.Thus,somealgorithms
that assume distances cannot be used with such data.
14.3.2 Dissimilarities Based on Attributes
Most often we have measurements xijfori= 1,2,...,N, on variables
j= 1,2,...,p(also called attributes ). Since most of the popular clustering
algorithmstakeadissimilaritymatrixastheirinput,wemu stﬁrstconstruct
pairwisedissimilaritiesbetweentheobservations.Inthe mostcommoncase,
we deﬁne a dissimilarity dj(xij,xi′j) between values of the jth attribute,
and then deﬁne
D(xi,xi′) =p/summationdisplay
j=1dj(xij,xi′j) (14.20)
as the dissimilarity between objects iandi′. By far the most common
choice is squared distance

504 14. Unsupervised Learning
dj(xij,xi′j) = (xij−xi′j)2. (14.21)
However, other choices are possible, and can lead to potenti ally diﬀerent
results. For nonquantitative attributes (e.g., categoric al data), squared dis-
tance may not be appropriate. In addition, it is sometimes de sirable to
weigh attributes diﬀerently rather than giving them equal w eight as in
(14.20).
We ﬁrst discuss alternatives in terms of the attribute type:
Quantitative variables. Measurements of this type of variable or attribute
are represented by continuous real-valued numbers. It is na tural to
deﬁne the “error” between them as a monotone-increasing fun ction
of their absolute diﬀerence
d(xi,xi′) =l(|xi−xi′|).
Besides squared-error loss ( xi−xi′)2, a common choice is the identity
(absolute error). The former places more emphasis on larger diﬀer-
ences than smaller ones. Alternatively, clustering can be b ased on the
correlation
ρ(xi,xi′) =/summationtext
j(xij−¯xi)(xi′j−¯xi′)/radicalig/summationtext
j(xij−¯xi)2/summationtext
j(xi′j−¯xi′)2, (14.22)
with ¯xi=/summationtext
jxij/p. Note that this is averaged over variables , not ob-
servations. If the observations are ﬁrst standardized, then/summationtext
j(xij−
xi′j)2∝2(1−ρ(xi,xi′)). Hence clustering based on correlation (simi-
larity) is equivalent to that based on squared distance (dis similarity).
Ordinal variables. The values of this type of variable are often represented
as contiguous integers, and the realizable values are consi dered to be
an ordered set. Examples are academic grades (A, B, C, D, F), d egree
of preference (can’t stand, dislike, OK, like, terriﬁc). Ra nk data are a
special kind of ordinal data. Error measures for ordinal var iables are
generally deﬁned by replacing their Moriginal values with
i−1/2
M, i= 1,...,M (14.23)
in the prescribed order of their original values. They are th en treated
as quantitative variables on this scale.
Categorical variables. With unordered categorical (also called nominal)
variables, the degree-of-diﬀerence between pairs of value s must be
delineated explicitly. If the variable assumes Mdistinct values, these
can be arranged in a symmetric M×Mmatrix with elements Lrr′=
Lr′r,Lrr= 0,Lrr′≥0. The most common choice is Lrr′= 1 for all
r∝ne}ationslash=r′, while unequal losses can be used to emphasize some errors
more than others.

14.3 Cluster Analysis 505
14.3.3 Object Dissimilarity
Next we deﬁne a procedure for combining the p-individual attribute dissim-
ilaritiesdj(xij,xi′j), j= 1,2,...,pinto a single overall measure of dissim-
ilarityD(xi,xi′) between two objects or observations ( xi,xi′) possessing
the respective attribute values. This is nearly always done by means of a
weighted average (convex combination)
D(xi,xi′) =p/summationdisplay
j=1wj·dj(xij,xi′j);p/summationdisplay
j=1wj= 1. (14.24)
Herewjis a weight assigned to the jth attribute regulating the relative
inﬂuence of that variable in determining the overall dissim ilarity between
objects. This choice should be based on subject matter consi derations.
It is important to realize that setting the weight wjto the same value
for each variable (say, wj= 1∀j) doesnotnecessarily give all attributes
equalinﬂuence.Theinﬂuenceofthe jthattribute Xjonobjectdissimilarity
D(xi,xi′) (14.24) depends upon its relative contribution to the aver age
object dissimilarity measure over all pairs of observation s in the data set
¯D=1
N2N/summationdisplay
i=1N/summationdisplay
i′=1D(xi,xi′) =p/summationdisplay
j=1wj·¯dj,
with
¯dj=1
N2N/summationdisplay
i=1N/summationdisplay
i′=1dj(xij,xi′j) (14.25)
being the average dissimilarity on the jth attribute. Thus, the relative in-
ﬂuence of the jth variable is wj·¯dj, and setting wj∼1/¯djwould give all
attributesequalinﬂuenceincharacterizingoveralldissi milaritybetweenob-
jects. For example, with pquantitative variables and squared-error distance
used for each coordinate, then (14.24) becomes the (weighte d) squared Eu-
clidean distance
DI(xi,xi′) =p/summationdisplay
j=1wj·(xij−xi′j)2(14.26)
between pairs of points in an IRp, with the quantitative variables as axes.
In this case (14.25) becomes
¯dj=1
N2N/summationdisplay
i=1N/summationdisplay
i′=1(xij−xi′j)2= 2·varj, (14.27)
where var jis the sample estimate of Var( Xj). Thus, the relative impor-
tance of each such variable is proportional to its variance o ver the data

506 14. Unsupervised Learning
-6 -4 -2 0 2 4-6 -4 -2 0 2 4•••
••
•••
••••
••
••••••
•
••
••
••
•
••••••••
••••••••
••
•••• •••
••••
•••
•••• •••
••
••••
••••
••
•••••
•••
•••
•••••
•••
••
-2 -1 0 1 2-2 -1 0 1 2••
••••
•
••••
•
••
•
•
••••
•
•••
•••
••
•••
•••
•••••
••
•••
•••
••
•
•
•••
•
•••
••
•••
••
••
•••
•
•••••
•••
•
•••
••
••••••••
••
••
••
X1 X1
X2X2
FIGURE 14.5. Simulated data: on the left, K-means clustering (with K=2) has
been applied to the raw data. The two colors indicate the cluster memberships. On
the right, the features were ﬁrst standardized before cluste ring. This is equivalent
to using feature weights 1/[2·var(Xj)]. The standardization has obscured the two
well-separated groups. Note that each plot uses the same units in the horizontal
and vertical axes.
set. In general, setting wj= 1/¯djfor all attributes, irrespective of type,
will cause each one of them to equally inﬂuence the overall di ssimilarity
between pairs of objects ( xi,xi′). Although this may seem reasonable, and
is often recommended, it can be highly counterproductive. I f the goal is to
segment the data into groups of similar objects, all attribu tes may not con-
tribute equally to the (problem-dependent) notion of dissi milarity between
objects. Some attribute value diﬀerences may reﬂect greate r actual object
dissimilarity in the context of the problem domain.
If the goal is to discover natural groupings in the data, some attributes
may exhibit more of a grouping tendency than others. Variabl es that are
more relevant in separating the groups should be assigned a h igher inﬂu-
ence in deﬁning object dissimilarity. Giving all attribute s equal inﬂuence
in this case will tend to obscure the groups to the point where a clustering
algorithm cannot uncover them. Figure 14.5 shows an example .
Although simple generic prescriptions for choosing the ind ividual at-
tribute dissimilarities dj(xij,xi′j) and their weights wjcan be comforting,
there is no substitute for careful thought in the context of e ach individ-
ual problem. Specifying an appropriate dissimilarity meas ure is far more
important in obtaining success with clustering than choice of clustering
algorithm. This aspect of the problem is emphasized less in t he cluster-
ing literature than the algorithms themselves, since it dep ends on domain
knowledge speciﬁcs and is less amenable to general research .

14.3 Cluster Analysis 507
Finally, often observations have missing values in one or more of the
attributes. The most common method of incorporating missin g values in
dissimilarity calculations (14.24) is to omit each observa tion pairxij,xi′j
having at least one value missing, when computing the dissim ilarity be-
tween observations xiandx′
i. This method can fail in the circumstance
when both observations have no measured values in common. In this case
both observations could be deleted from the analysis. Alter natively, the
missingvaluescouldbeimputedusingthemeanormedianofea chattribute
over the nonmissing data. For categorical variables, one co uld consider the
value “missing” as just another categorical value, if it wer e reasonable to
consider two objects as being similar if they both have missi ng values on
the same variables.
14.3.4 Clustering Algorithms
The goal of cluster analysis is to partition the observation s into groups
(“clusters”) so that the pairwise dissimilarities between those assigned to
the same cluster tend to be smaller than those in diﬀerent clu sters. Clus-
tering algorithms fall into three distinct types: combinat orial algorithms,
mixture modeling, and mode seeking.
Combinatorial algorithms work directly on the observed data with no
direct reference to an underlying probability model. Mixture modeling sup-
poses that the data is an i.i.dsample from some population described by a
probability density function. This density function is cha racterized by a pa-
rameterized model taken to be a mixture of component density functions;
each component density describes one of the clusters. This m odel is then ﬁt
to the data by maximum likelihood or corresponding Bayesian approaches.
Mode seekers (“bumphunters”)takeanonparametricperspective,attemp t-
ing to directly estimate distinct modes of the probability d ensity function.
Observations “closest” to each respective mode then deﬁne t he individual
clusters.
Mixture modeling is described in Section 6.8. The PRIM algor ithm, dis-
cussed in Sections 9.3 and 14.2.5, is an example of mode seeki ng or “bump
hunting.” We discuss combinatorial algorithms next.
14.3.5 Combinatorial Algorithms
The most popular clustering algorithms directly assign eac h observation
to a group or cluster without regard to a probability model de scribing the
data. Each observation is uniquely labeled by an integer i∈{1,···,N}.
A prespeciﬁed number of clusters K < N is postulated, and each one is
labeled by an integer k∈{1,...,K}. Each observation is assigned to one
and only one cluster. These assignments can be characterize d by a many-
to-one mapping, or encoderk=C(i), that assigns the ith observation to
thekth cluster. One seeks the particular encoder C∗(i) that achieves the

508 14. Unsupervised Learning
required goal (details below), based on the dissimilaritie sd(xi,xi′) between
every pair of observations. These are speciﬁed by the user as described
above. Generally, the encoder C(i) is explicitly delineated by giving its
value (cluster assignment) for each observation i. Thus, the “parameters”
of the procedure are the individual cluster assignments for each of the N
observations. These are adjusted so as to minimize a “loss” function that
characterizes the degree to which the clustering goal is notmet.
One approach is to directly specify a mathematical loss func tion and
attempttominimizeitthroughsomecombinatorialoptimiza tionalgorithm.
Since the goal is to assign close points to the same cluster, a natural loss
(or “energy”) function would be
W(C) =1
2K/summationdisplay
k=1/summationdisplay
C(i)=k/summationdisplay
C(i′)=kd(xi,xi′). (14.28)
This criterion characterizes the extent to which observati ons assigned to
the same cluster tend to be close to one another. It is sometim es referred
to as the “within cluster” point scatter since
T=1
2N/summationdisplay
i=1N/summationdisplay
i′=1dii′=1
2K/summationdisplay
k=1/summationdisplay
C(i)=k
/summationdisplay
C(i′)=kdii′+/summationdisplay
C(i′)/ne}ationslash=kdii′
,
or
T=W(C)+B(C),
wheredii′=d(xi,xi′). HereTis thetotalpoint scatter, which is a constant
given the data, independent of cluster assignment. The quan tity
B(C) =1
2K/summationdisplay
k=1/summationdisplay
C(i)=k/summationdisplay
C(i′)/ne}ationslash=kdii′ (14.29)
is thebetween-cluster point scatter. This will tend to be large when obser-
vations assigned to diﬀerent clusters are far apart. Thus on e has
W(C) =T−B(C)
and minimizing W(C) is equivalent to maximizing B(C).
Clusteranalysisbycombinatorialoptimizationisstraigh tforwardinprin-
ciple. One simply minimizes Wor equivalently maximizes Bover all pos-
sible assignments of the Ndata points to Kclusters. Unfortunately, such
optimization by complete enumeration is feasible only for v ery small data
sets. The number of distinct assignments is (Jain and Dubes, 1988)
S(N,K) =1
K!K/summationdisplay
k=1(−1)K−k/parenleftbiggK
k/parenrightbigg
kN. (14.30)
For example, S(10,4) = 34,105 which is quite feasible. But, S(N,K) grows
very rapidly with increasing values of its arguments. Alrea dyS(19,4)≃

14.3 Cluster Analysis 509
1010, and most clustering problems involve much larger data sets than
N= 19. For this reason, practical clustering algorithms are a ble to examine
only a very small fraction of all possible encoders k=C(i). The goal is to
identify a small subset that is likely to contain the optimal one, or at least
a good suboptimal partition.
Such feasible strategies are based on iterative greedy desc ent. An initial
partition is speciﬁed. At each iterative step, the cluster a ssignments are
changed in such a way that the value of the criterion is improv ed from
its previous value. Clustering algorithms of this type diﬀe r in their pre-
scriptions for modifying the cluster assignments at each it eration. When
the prescription is unable to provide an improvement, the al gorithm ter-
minates with the current assignments as its solution. Since the assignment
of observations to clusters at any iteration is a perturbati on of that for the
previous iteration, only a very small fraction of all possib le assignments
(14.30) are examined. However, these algorithms converge t olocaloptima
which may be highly suboptimal when compared to the global op timum.
14.3.6K-means
TheK-means algorithm is one of the most popular iterative descen t clus-
tering methods. It is intended for situations in which all va riables are of
the quantitative type, and squared Euclidean distance
d(xi,xi′) =p/summationdisplay
j=1(xij−xi′j)2=||xi−xi′||2
is chosen as the dissimilarity measure. Note that weighted E uclidean dis-
tance can be used by redeﬁning the xijvalues (Exercise 14.1).
The within-point scatter (14.28) can be written as
W(C) =1
2K/summationdisplay
k=1/summationdisplay
C(i)=k/summationdisplay
C(i′)=k||xi−xi′||2
=K/summationdisplay
k=1Nk/summationdisplay
C(i)=k||xi−¯xk||2, (14.31)
where ¯xk= (¯x1k,...,¯xpk) is the mean vector associated with the kth clus-
ter, andNk=/summationtextN
i=1I(C(i) =k). Thus, the criterion is minimized by
assigning the Nobservations to the Kclusters in such a way that within
each cluster the average dissimilarity of the observations from the cluster
mean, as deﬁned by the points in that cluster, is minimized.
An iterative descent algorithm for solving

510 14. Unsupervised Learning
Algorithm 14.1 K-means Clustering.
1. For a given cluster assignment C, the total cluster variance (14.33) is
minimized with respect to {m1,...,m K}yielding the means of the
currently assigned clusters (14.32).
2. Given a current set of means {m1,...,m K}, (14.33) is minimized by
assigning each observation to the closest (current) cluste r mean. That
is,
C(i) = argmin
1≤k≤K||xi−mk||2. (14.34)
3. Steps 1 and 2 are iterated until the assignments do not chan ge.
C∗= min
CK/summationdisplay
k=1Nk/summationdisplay
C(i)=k||xi−¯xk||2
can be obtained by noting that for any set of observations S
¯xS= argmin
m/summationdisplay
i∈S||xi−m||2. (14.32)
Hence we can obtain C∗by solving the enlarged optimization problem
min
C,{mk}K
1K/summationdisplay
k=1Nk/summationdisplay
C(i)=k||xi−mk||2. (14.33)
This can be minimized by an alternating optimization proced ure given in
Algorithm 14.1.
Each of steps 1 and 2 reduces the value of the criterion (14.33 ), so that
convergence is assured. However, the result may represent a suboptimal
local minimum. The algorithm of Hartigan and Wong (1979) goe s further,
and ensures that there is no single switch of an observation f rom one group
to another group that will decrease the objective. In additi on, one should
start the algorithm with many diﬀerent random choices for th e starting
means, and choose the solution having smallest value of the o bjective func-
tion.
Figure 14.6 shows some of the K-means iterations for the simulated data
of Figure 14.4. The centroids are depicted by “O”s. The strai ght lines show
the partitioning of points, each sector being the set of poin ts closest to
each centroid. This partitioning is called the Voronoi tessellation . After 20
iterations the procedure has converged.
14.3.7 Gaussian Mixtures as Soft K-means Clustering
TheK-means clustering procedure is closely related to the EM alg orithm
for estimating a certain Gaussian mixture model. (Sections 6.8 and 8.5.1).

14.3 Cluster Analysis 511
-4 -2 0 2 4 6-2 0 2 4 6Initial Centroids
••••
••••••••
••
•••••••••
••••
••
••
••
•
•••••
•••
••
•••
••
••••
••••
••
•••
••
•••
••
••• ••
•
••••••••
••••••
•
••••
••••••••
•••••
••••••••
•••
••••••
•••
••
•••
••
•••
•••
••
•••
•••••
•••
•••
••••
••••••••
••
•••••••••
••••
••
••
••
•
•••••
•••
••
•••
••
••••
••••
••
•••
••
•••
••
••• ••
•
••••••••
••••••
•
••••
••••••••
•••••
••••••••
•••
••••••
•••
••
•••
••
•••
•••
••
•••
•••••
•••
•••Initial Partition
••••
••••••••
••
•••••••••
•••
••
••
••
•
•••••
••••
••
•••
••
•••
••
••••
••
••
••
••
•••
••
••• ••
•
••••••••
••••••
•
••••
••••••
••
••••••
•••
••••
••
••
••••
••
•••
••
•••
••
••
•••
•••
•••••Iteration Number  2
•••
•••
••••
••••••••
••
•••••••••
•••
••
••
••
•
•••••
••••
••
•••
••
••••
••
••••
•••
••
••
••
•••
••
••• ••
•
••••••••
••••••
•
••••
•••••
•••••
•••••••••
•••
•••••
•••
••
•••
••
•••
••••
•••
•••••Iteration Number  20
•••
•••
FIGURE 14.6. Successive iterations of the K-means clustering algorithm for
the simulated data of Figure 14.4.

512 14. Unsupervised Learning
• •Responsibilities
0.0 0.2 0.4 0.6 0.8 1.0
• •Responsibilities
0.0 0.2 0.4 0.6 0.8 1.0σ= 1.0 σ= 1.0
σ= 0.2 σ= 0.2
FIGURE 14.7. (Left panels:) two Gaussian densities g0(x) andg1(x)(blue and
orange) on the real line, and a single data point (green dot) at x= 0.5. The colored
squares are plotted at x=−1.0andx= 1.0, the means of each density. (Right
panels:) the relative densities g0(x)/(g0(x) +g1(x))andg1(x)/(g0(x) +g1(x)),
called the “responsibilities” of each cluster, for this data po int. In the top panels,
the Gaussian standard deviation σ= 1.0; in the bottom panels σ= 0.2. The
EM algorithm uses these responsibilities to make a “soft” assi gnment of each
data point to each of the two clusters. When σis fairly large, the responsibilities
can be near 0.5(they are 0.36and0.64 in the top right panel). As σ→0, the
responsibilities →1, for the cluster center closest to the target point, and 0for
all other clusters. This “hard” assignment is seen in the bott om right panel.
The E-step of the EM algorithm assigns “responsibilities” f or each data
point based in its relative density under each mixture compo nent, while
the M-step recomputes the component density parameters bas ed on the
current responsibilities. Suppose we specify Kmixture components, each
with a Gaussian density having scalar covariance matrix σ2I. Then the
relative density under each mixture component is a monotone function of
the Euclidean distance between the data point and the mixtur e center.
Hence in this setup EM is a “soft” version of K-means clustering, making
probabilistic (rather than deterministic) assignments of points to cluster
centers. As the variance σ2→0, these probabilities become 0 and 1, and
the two methods coincide. Details are given in Exercise 14.2 . Figure 14.7
illustrates this result for two clusters on the real line.
14.3.8 Example: Human Tumor Microarray Data
We applyK-means clustering to the human tumor microarray data de-
scribed in Chapter 1. This is an example of high-dimensional clustering.

14.3 Cluster Analysis 513
Number of Clusters KSum of Squares
2 4 6 8 10160000 200000 240000•
•
•
•••••••
FIGURE 14.8. Total within-cluster sum of squares for K-means clustering ap-
plied to the human tumor microarray data.
TABLE 14.2. Human tumor data: number of cancer cases of each type, in each
of the three clusters from K-means clustering.
Cluster Breast CNS Colon K562 Leukemia MCF7
1 3 5 0 0 0 0
2 2 0 0 2 6 2
3 2 0 7 0 0 0
Cluster Melanoma NSCLC Ovarian Prostate Renal Unknown
1 1 7 6 2 9 1
2 7 2 0 0 0 0
3 0 0 0 0 0 0
The data are a 6830 ×64 matrix of real numbers, each representing an
expression measurement for a gene (row) and sample (column) . Here we
cluster the samples, each of which is a vector of length 6830, correspond-
ing to expression values for the 6830 genes. Each sample has a label such
asbreast(for breast cancer), melanoma , and so on; we don’t use these la-
bels in the clustering, but will examine posthocwhich labels fall into which
clusters.
We applied K-means clustering with Krunning from 1 to 10, and com-
puted the total within-sum of squares for each clustering, s hown in Fig-
ure 14.8. Typically one looks for a kink in the sum of squares c urve (or its
logarithm) to locate the optimal number of clusters (see Sec tion 14.3.11).
Here there is no clear indication: for illustration we chose K= 3 giving the
three clusters shown in Table 14.2.

514 14. Unsupervised Learning
FIGURE 14.9. Sir Ronald A. Fisher ( 1890−1962) was one of the founders
of modern day statistics, to whom we owe maximum-likelihood, s uﬃciency, and
many other fundamental concepts. The image on the left is a 1024×1024grayscale
image at 8bits per pixel. The center image is the result of 2×2block VQ, using
200code vectors, with a compression rate of 1.9bits/pixel. The right image uses
only four code vectors, with a compression rate of 0.50bits/pixel
We see that the procedure is successful at grouping together samples of
the same cancer. In fact, the two breast cancers in the second cluster were
later found to be misdiagnosed and were melanomas that had me tastasized.
However,K-meansclusteringhasshortcomingsinthisapplication. Fo rone,
it does not give a linear ordering of objects within a cluster : we have simply
listed them in alphabetic order above. Secondly, as the numb er of clusters
Kis changed, the cluster memberships can change in arbitrary ways. That
is, with say four clusters, the clusters need not be nested wi thin the three
clusters above. For these reasons, hierarchical clusterin g (described later),
is probably preferable for this application.
14.3.9 Vector Quantization
TheK-means clustering algorithm represents a key tool in the app arently
unrelatedareaofimageandsignalcompression,particular lyinvector quan-
tizationor VQ (Gersho and Gray, 1992). The left image in Figure 14.92is a
digitized photograph of a famous statistician, Sir Ronald F isher. It consists
of 1024×1024 pixels, where each pixel is a grayscale value ranging fr om 0
to 255, and hence requires 8 bits of storage per pixel. The ent ire image oc-
cupies 1 megabyte of storage. The center image is a VQ-compre ssed version
of the left panel, and requires 0 .239 of the storage (at some loss in quality).
The right image is compressed even more, and requires only 0 .0625 of the
storage (at a considerable loss in quality).
The version of VQ implemented here ﬁrst breaks the image into small
blocks, in this case 2 ×2 blocks of pixels. Each of the 512 ×512 blocks of four
2This example was prepared by Maya Gupta.

14.3 Cluster Analysis 515
numbers is regarded as a vector in IR4. AK-means clustering algorithm
(also known as Lloyd’s algorithm in this context) is run in th is space.
The center image uses K= 200, while the right image K= 4. Each of
the 512×512 pixel blocks (or points) is approximated by its closest c luster
centroid,knownasacodeword.Theclusteringprocessiscal ledtheencoding
step, and the collection of centroids is called the codebook.
To represent the approximated image, we need to supply for ea ch block
the identity of the codebook entry that approximates it. Thi s will require
log2(K) bits per block. We also need to supply the codebook itself, w hich
isK×4 real numbers (typically negligible). Overall, the storag e for the
compressed image amounts to log2(K)/(4·8) of the original (0 .239 for
K= 200, 0.063 forK= 4). This is typically expressed as a ratein bits
per pixel: log2(K)/4, which are 1 .91 and 0.50, respectively. The process
of constructing the approximate image from the centroids is called the
decoding step.
Why do we expect VQ to work at all? The reason is that for typica l
everyday images like photographs, many of the blocks look th e same. In
this case there are many almost pure white blocks, and simila rly pure gray
blocks of various shades. These require only one block each t o represent
them, and then multiple pointers to that block.
What we have described is known as lossycompression, since our im-
ages are degraded versions of the original. The degradation ordistortion is
usually measured in terms of mean squared error. In this case D= 0.89
forK= 200 and D= 16.95 forK= 4. More generally a rate/distortion
curve would be used to assess the tradeoﬀ. One can also perfor mlossless
compression using block clustering, and still capitalize o n the repeated pat-
terns. If you took the original image and losslessly compres sed it, the best
you would do is 4.48 bits per pixel.
Weclaimedabovethatlog2(K)bitswereneededtoidentifyeachofthe K
codewords in the codebook. This uses a ﬁxed-length code, and is ineﬃcient
if some codewords occur many more times than others in the ima ge. Using
Shannon coding theory, we know that in general a variable len gth code
will do better, and the rate then becomes −/summationtextK
ℓ=1pℓlog2(pℓ)/4. The term
in the numerator is the entropy of the distribution pℓof the codewords
in the image. Using variable length coding our rates come dow n to 1.42
and 0.39, respectively. Finally, there are many generalizations of VQ that
have been developed: for example, tree-structured VQ ﬁnds t he centroids
with a top-down, 2-means style algorithm, as alluded to in Se ction 14.3.12.
This allows successive reﬁnement of the compression. Furth er details may
be found in Gersho and Gray (1992).
14.3.10 K-medoids
As discussed above, the K-means algorithm is appropriate when the dis-
similarity measure is taken to be squared Euclidean distanc eD(xi,xi′)

516 14. Unsupervised Learning
Algorithm 14.2 K-medoids Clustering.
1. For a given cluster assignment Cﬁnd the observation in the cluster
minimizing total distance to other points in that cluster:
i∗
k= argmin
{i:C(i)=k}/summationdisplay
C(i′)=kD(xi,xi′). (14.35)
Thenmk=xi∗
k, k= 1,2,...,Kare the current estimates of the
cluster centers.
2. Given a current set of cluster centers {m1,...,m K}, minimize the to-
tal error by assigning each observation to the closest (curr ent) cluster
center:
C(i) = argmin
1≤k≤KD(xi,mk). (14.36)
3. Iterate steps 1 and 2 until the assignments do not change.
(14.112). This requires all of the variables to be of the quan titative type. In
addition, using squaredEuclidean distance places the highest inﬂuence on
the largest distances. This causes the procedure to lack rob ustness against
outliers that produce very large distances. These restrict ions can be re-
moved at the expense of computation.
The only part of the K-means algorithm that assumes squared Eu-
clideandistanceistheminimizationstep(14.32);theclus terrepresentatives
{m1,...,m K}in(14.33)aretakentobethemeansofthecurrentlyassigned
clusters. The algorithm can be generalized for use with arbi trarily deﬁned
dissimilarities D(xi,xi′) by replacing this step by an explicit optimization
with respect to{m1,...,m K}in (14.33). In the most common form, cen-
ters for each cluster are restricted to be one of the observat ions assigned
to the cluster, as summarized in Algorithm 14.2. This algori thm assumes
attribute data, but the approach can also be applied to data d escribed
onlyby proximity matrices (Section 14.3.1). There is no need to e xplicitly
compute cluster centers; rather we just keep track of the ind icesi∗
k.
Solving (14.32) for each provisional cluster krequires an amount of com-
putation proportional to the number of observations assign ed to it, whereas
for solving (14.35) the computation increases to O(N2
k). Given a set of clus-
ter “centers,”{i1,...,iK}, obtaining the new assignments
C(i) = argmin
1≤k≤Kdii∗
k(14.37)
requires computation proportional to K·Nas before. Thus, K-medoids is
far more computationally intensive than K-means.
Alternating between (14.35) and (14.37) represents a parti cular heuristic
search strategy for trying to solve

14.3 Cluster Analysis 517
TABLE 14.3. Data from a political science survey: values are average pairw ise
dissimilarities of countries from a questionnaire given to p olitical science students.
BEL BRA CHI CUB EGY FRA IND ISR USA USS YUG
BRA5.58
CHI7.00 6.50
CUB7.08 7.00 3.83
EGY4.83 5.08 8.17 5.83
FRA2.17 5.75 6.67 6.92 4.92
IND6.42 5.00 5.58 6.00 4.67 6.42
ISR3.42 5.50 6.42 6.42 5.00 3.92 6.17
USA2.50 4.92 6.25 7.33 4.50 2.25 6.33 2.75
USS6.08 6.67 4.25 2.67 6.00 6.17 6.17 6.92 6.17
YUG5.25 6.83 4.50 3.75 5.75 5.42 6.08 5.83 6.67 3.67
ZAI4.75 3.00 6.08 6.67 5.00 5.58 4.83 6.17 5.67 6.50 6.92
min
C,{ik}K
1K/summationdisplay
k=1/summationdisplay
C(i)=kdiik. (14.38)
KaufmanandRousseeuw(1990)proposeanalternativestrate gyfordirectly
solving (14.38) that provisionally exchanges each center ikwith an obser-
vation that is not currently a center, selecting the exchang e that produces
the greatest reduction in the value of the criterion (14.38) . This is repeated
until no advantageous exchanges can be found. Massart et al. (1983) derive
a branch-and-bound combinatorial method that ﬁnds the glob al minimum
of (14.38) that is practical only for very small data sets.
Example: Country Dissimilarities
This example, taken from Kaufman and Rousseeuw (1990), come s from a
study in which political science students were asked to prov ide pairwise dis-
similarity measures for 12 countries: Belgium, Brazil, Chi le, Cuba, Egypt,
France, India, Israel, United States, Union of Soviet Socia list Republics,
Yugoslavia and Zaire. The average dissimilarity scores are given in Ta-
ble 14.3. We applied 3-medoid clustering to these dissimila rities. Note that
K-means clustering could not be applied because we have only d istances
rather than raw observations. The left panel of Figure 14.10 shows the
dissimilarities reordered and blocked according to the 3-m edoid clustering.
The right panel is a two-dimensional multidimensional scal ing plot, with
the 3-medoid clusters assignments indicated by colors (mul tidimensional
scaling is discussed in Section 14.8.) Both plots show three well-separated
clusters, but the MDS display indicates that “Egypt” falls a bout halfway
between two clusters.

518 14. Unsupervised Learning
CHICUBUSSYUGBRAINDZAIBELEGYFRAISR
CUBUSSYUGBRAINDZAIBELEGYFRAISRUSA
Reordered Dissimilarity Matrix First MDS CoordinateSecond MDS Coordinate
-2 0 2 4-2 -1 0 1 2 3CHI
CUB
USS
YUGBRAIND ZAI
BELEGY
FRAISRUSA
FIGURE 14.10. Survey of country dissimilarities. (Left panel:) dissimilari ties
reordered and blocked according to 3-medoid clustering. Heat map is coded from
most similar (dark red) to least similar (bright red). (Right pa nel:) two-dimen-
sional multidimensional scaling plot, with 3-medoid clusters indicated by diﬀerent
colors.
14.3.11 Practical Issues
In order to apply K-means orK-medoids one must select the number of
clustersK∗and an initialization. The latter can be deﬁned by specifyin g
an initial set of centers {m1,...,m K}or{i1,...,iK}or an initial encoder
C(i). Usually specifying the centers is more convenient. Sugge stions range
from simple random selection to a deliberate strategy based on forward
stepwise assignment. At each step a new center ikis chosen to minimize
the criterion (14.33) or (14.38), given the centers i1,...,ik−1chosen at the
previous steps. This continues for Ksteps, thereby producing Kinitial
centers with which to begin the optimization algorithm.
A choice for the number of clusters Kdepends on the goal. For data
segmentation Kis usually deﬁned as part of the problem. For example,
a company may employ Ksales people, and the goal is to partition a
customerdatabaseinto Ksegments,oneforeachsalesperson,suchthatthe
customers assigned to each one are as similar as possible. Of ten, however,
cluster analysis is used to provide a descriptive statistic for ascertaining the
extent to which the observations comprising the data base fa ll into natural
distinct groupings. Here the number of such groups K∗is unknown and
one requires that it, as well as the groupings themselves, be estimated from
the data.
Data-based methods for estimating K∗typically examine the within-
clusterdissimilarity WKasafunctionofthenumberofclusters K.Separate
solutions are obtained for K∈{1,2,...,K max}. The corresponding values

14.3 Cluster Analysis 519
{W1,W2,...,W Kmax}generally decrease with increasing K. This will be
the case even when the criterion is evaluated on an independe nt test set,
since a large number of cluster centers will tend to ﬁll the fe ature space
densely and thus will be close to all data points. Thus cross- validation
techniques, so useful for model selection in supervised lea rning, cannot be
utilized in this context.
The intuition underlying the approach is that if there are ac tuallyK∗
distinct groupings of the observations (as deﬁned by the dis similarity mea-
sure), then for K < K∗the clusters returned by the algorithm will each
contain a subset of the true underlying groups. That is, the s olution will
not assign observations in the same naturally occurring gro up to diﬀerent
estimated clusters. To the extent that this is the case, the s olution criterion
value will tend todecreasesubstantially with each success iveincrease inthe
number of speciﬁed clusters, WK+1≪WK, as the natural groups are suc-
cessively assigned to separate clusters. For K >K∗, one of the estimated
clusters must partition at least one of the natural groups in to two sub-
groups. This will tend to provide a smaller decrease in the cr iterion asKis
further increased. Splitting a natural group, within which the observations
are all quite close to each other, reduces the criterion less than partitioning
the union of two well-separated groups into their proper con stituents.
To the extent this scenario is realized, there will be a sharp decrease in
successive diﬀerences in criterion value, WK−WK+1, atK=K∗. That
is,{WK−WK+1|K < K∗}≫{WK−WK+1|K≥K∗}. An estimate
ˆK∗forK∗is then obtained by identifying a “kink” in the plot of WKas a
functionofK.Aswithotheraspectsofclusteringprocedures,thisappro ach
is somewhat heuristic.
The recently proposed Gap statistic (Tibshirani et al., 2001b) compares
the curve log WKto the curve obtained from data uniformly distributed
over a rectangle containing the data. It estimates the optim al number of
clusters to be the place where the gap between the two curves i s largest.
Essentially this is an automatic way of locating the aforeme ntioned “kink.”
It also works reasonably well when the data fall into a single cluster, and
in that case will tend to estimate the optimal number of clust ers to be one.
This is the scenario where most other competing methods fail .
Figure 14.11 shows the result of the Gap statistic applied to simulated
dataofFigure14.4.Theleftpanelshowslog WKforK= 1,2,...,8clusters
(green curve) and the expected value of log WKover 20 simulations from
uniformdata(bluecurve).Therightpanelshowsthegapcurv e,whichisthe
expected curve minus the observed curve. Shown also are erro r bars of half-
widths′
K=sK/radicalbig
1+1/20, wheresKis the standard deviation of log WK
over the 20 simulations. The Gap curve is maximized at K= 2 clusters. If
G(K) is the Gap curve at Kclusters, the formal rule for estimating K∗is
K∗= argmin
K{K|G(K)≥G(K+1)−s′
K+1}.(14.39)

520 14. Unsupervised Learning
Number of Clusters2 4 6 8-3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0•
••
••
••••
•
•
•
•
•••
Number of ClustersGap
2 4 6 8-0.5 0.0 0.5 1.0••
•••••
•logWK−logW1
FIGURE 14.11. (Left panel): observed (green) and expected (blue) values of
logWKfor the simulated data of Figure 14.4. Both curves have been tr anslated
to equal zero at one cluster. (Right panel): Gap curve, equal to t he diﬀerence
between the observed and expected values of logWK. The Gap estimate K∗is the
smallestKproducing a gap within one standard deviation of the gap at K+ 1;
hereK∗= 2.
This givesK∗= 2, which looks reasonable from Figure 14.4.
14.3.12 Hierarchical Clustering
The results of applying K-means orK-medoids clustering algorithms de-
pend on the choice for the number of clusters to be searched an d a starting
conﬁguration assignment. In contrast, hierarchical clust ering methods do
not require such speciﬁcations. Instead, they require the u ser to specify a
measure of dissimilarity between (disjoint) groupsof observations, based
on the pairwise dissimilarities among the observations in t he two groups.
As the name suggests, they produce hierarchical representa tions in which
the clusters at each level of the hierarchy are created by mer ging clusters
at the next lower level. At the lowest level, each cluster con tains a single
observation. At the highest level there is only one cluster c ontaining all of
the data.
Strategies for hierarchical clustering divide into two bas ic paradigms: ag-
glomerative (bottom-up) and divisive(top-down). Agglomerative strategies
start at the bottom and at each level recursively merge a sele cted pair of
clusters into a single cluster. This produces a grouping at t he next higher
level with one less cluster. The pair chosen for merging cons ist of the two
groups with the smallest intergroup dissimilarity. Divisi ve methods start
at the top and at each level recursively split one of the exist ing clusters at

14.3 Cluster Analysis 521
that level into two new clusters. The split is chosen to produ ce two new
groups with the largest between-group dissimilarity. With both paradigms
there areN−1 levels in the hierarchy.
Each level of the hierarchy represents a particular groupin g of the data
into disjoint clusters of observations. The entire hierarc hy represents an
ordered sequence of such groupings. It is up to the user to dec ide which
level (if any) actually represents a “natural” clustering i n the sense that
observations within each of its groups are suﬃciently more s imilar to each
other than to observations assigned to diﬀerent groups at th at level. The
Gap statistic described earlier can be used for this purpose .
Recursive binary splitting/agglomeration can be represen ted by a rooted
binary tree. The nodes of the trees represent groups. The roo t node repre-
sents the entire data set. The Nterminal nodes each represent one of the
individual observations (singleton clusters). Each nonte rminal node (“par-
ent”) has two daughter nodes. For divisive clustering the tw o daughters
represent the two groups resulting from the split of the pare nt; for agglom-
erative clustering the daughters represent the two groups t hat were merged
to form the parent.
Most agglomerative and some divisive methods (when viewed b ottom-
up) possess a monotonicity property. That is, the dissimila rity between
merged clusters is monotone increasing with the level of the merger. Thus
thebinarytreecanbeplottedsothattheheightofeachnodei sproportional
to the value of the intergroup dissimilarity between its two daughters. The
terminal nodes representing individual observations are a ll plotted at zero
height. This type of graphical display is called a dendrogram .
A dendrogram provides a highly interpretable complete desc ription of
the hierarchical clustering in a graphical format. This is o ne of the main
reasons for the popularity of hierarchical clustering meth ods.
For the microarray data, Figure 14.12 shows the dendrogram r esulting
from agglomerative clustering with average linkage; agglo merative cluster-
ing and this example are discussed in more detail later in thi s chapter.
Cutting the dendrogram horizontally at a particular height partitions the
data into disjoint clusters represented by the vertical lin es that intersect
it. These are the clusters that would be produced by terminat ing the pro-
cedure when the optimal intergroup dissimilarity exceeds t hat threshold
cut value. Groups that merge at high values, relative to the m erger values
of the subgroups contained within them lower in the tree, are candidates
for natural clusters. Note that this may occur at several diﬀ erent levels,
indicating a clustering hierarchy: that is, clusters neste d within clusters.
Such a dendrogram is often viewed as a graphical summary of th e data
itself, rather than a description of the results of the algor ithm. However,
such interpretations should be treated with caution. First , diﬀerent hierar-
chical methods (see below), as well as small changes in the da ta, can lead
to quite diﬀerent dendrograms. Also, such a summary will be v alid only to
the extent that the pairwise observation dissimilarities possess the hierar-

522 14. Unsupervised Learning
CNSCNSCNSRENAL
BREASTCNSCNS
BREASTNSCLC
NSCLCRENAL
RENALRENALRENAL
RENALRENALRENALBREAST
NSCLCRENAL
UNKNOWN
OVARIAN
MELANOMA
PROSTATEOVARIANOVARIAN
OVARIANOVARIAN
OVARIAN
PROSTATENSCLCNSCLCNSCLCLEUKEMIAK562B-reproK562A-reproLEUKEMIA
LEUKEMIALEUKEMIALEUKEMIALEUKEMIA
COLONCOLON
COLON
COLONCOLONCOLON
COLONMCF7A-repro
BREAST
MCF7D-reproBREASTNSCLC
NSCLCNSCLCMELANOMA
BREASTBREAST
MELANOMA
MELANOMA
MELANOMAMELANOMAMELANOMA
MELANOMA
FIGURE 14.12. Dendrogram from agglomerative hierarchical clustering with
average linkage to the human tumor microarray data.
chical structure produced by the algorithm. Hierarchical m ethods impose
hierarchical structure whether or not such structure actua lly exists in the
data.
The extent to which the hierarchical structure produced by a dendro-
gram actually represents the data itself can be judged by the cophenetic
correlation coeﬃcient . This is the correlation between the N(N−1)/2 pair-
wise observation dissimilarities dii′input to the algorithm and their corre-
sponding cophenetic dissimilarities Cii′derived from the dendrogram. The
cophenetic dissimilarity Cii′between two observations ( i,i′) is the inter-
group dissimilarity at which observations iandi′are ﬁrst joined together
in the same cluster.
The cophenetic dissimilarity is a very restrictive dissimi larity measure.
First,theCii′overtheobservationsmustcontainmanyties,sinceonly N−1
of the total N(N−1)/2 values can be distinct. Also these dissimilarities
obey the ultrametric inequality
Cii′≤max{Cik,Ci′k} (14.40)

14.3 Cluster Analysis 523
for any three observations ( i,i′,k). As a geometric example, suppose the
data were represented as points in a Euclidean coordinate sy stem. In order
for the set of interpoint distances over the data to conform t o (14.40), the
triangles formed by all triples of points must be isosceles t riangles with the
unequal length no longer than the length of the two equal side s (Jain and
Dubes, 1988). Therefore it is unrealistic to expect general dissimilarities
over arbitrary data sets to closely resemble their correspo nding cophenetic
dissimilarities as calculated from a dendrogram, especial ly if there are not
many tied values. Thus the dendrogram should be viewed mainl y as a de-
scriptionofthe clustering structureofthedataasimposedbytheparticular
algorithm employed.
Agglomerative Clustering
Agglomerative clustering algorithms begin with every obse rvation repre-
senting a singleton cluster. At each of the N−1 steps the closest two (least
dissimilar) clusters are merged into a single cluster, prod ucing one less clus-
ter at the next higher level. Therefore, a measure of dissimi larity between
two clusters (groups of observations) must be deﬁned.
LetGandHrepresent two such groups. The dissimilarity d(G,H) be-
tweenGandHis computed from the set of pairwise observation dissim-
ilaritiesdii′where one member of the pair iis inGand the other i′is
inH.Single linkage (SL) agglomerative clustering takes the intergroup
dissimilarity to be that of the closest (least dissimilar) p air
dSL(G,H) = min
i∈G
i′∈Hdii′. (14.41)
This is also often called the nearest-neighbor technique. Complete linkage
(CL) agglomerative clustering ( furthest-neighbor technique) takes the in-
tergroup dissimilarity to be that of the furthest (most diss imilar) pair
dCL(G,H) = max
i∈G
i′∈Hdii′. (14.42)
Group average (GA) clustering uses the average dissimilarity between the
groups
dGA(G,H) =1
NGNH/summationdisplay
i∈G/summationdisplay
i′∈Hdii′ (14.43)
whereNGandNHare the respective number of observations in each group.
Although there have been many other proposals for deﬁning in tergroup
dissimilarity in the context of agglomerative clustering, the above three are
the ones most commonly used. Figure 14.13 shows examples of a ll three.
Ifthedatadissimilarities {dii′}exhibit astrongclusteringtendency,with
each of the clusters being compact and well separated from ot hers, then all
three methods produce similar results. Clusters are compac t if all of the

524 14. Unsupervised Learning
Average Linkage Complete Linkage Single Linkage
FIGURE 14.13. Dendrograms from agglomerative hierarchical clustering of h u-
man tumor microarray data.
observationswithinthemarerelativelyclosetogether(sm alldissimilarities)
as compared with observations in diﬀerent clusters. To the e xtent this is
not the case, results will diﬀer.
Single linkage (14.41) only requires that a single dissimil aritydii′,i∈G
andi′∈H, be small for two groups GandHto be considered close
together, irrespective of the other observation dissimila rities between the
groups. It will therefore have a tendency to combine, at rela tively low
thresholds, observations linked by a series of close interm ediate observa-
tions. This phenomenon, referred to as chaining, is often considered a de-
fect of the method. The clusters produced by single linkage c an violate the
“compactness” property that all observations within each c luster tend to
be similar to one another, based on the supplied observation dissimilari-
ties{dii′}. If we deﬁne the diameterDGof a group of observations as the
largest dissimilarity among its members
DG= max
i∈G
i′∈Gdii′, (14.44)
then single linkage can produce clusters with very large dia meters.
Complete linkage (14.42) represents the opposite extreme. Two groups
GandHare considered close only if all of the observations in their union
are relatively similar. It will tend to produce compact clus ters with small
diameters (14.44). However, it can produce clusters that vi olate the “close-
ness” property. That is, observations assigned to a cluster can be much

14.3 Cluster Analysis 525
closer to members of other clusters than they are to some memb ers of their
own cluster.
Group average clustering (14.43) represents a compromise b etween the
two extremes of single and complete linkage. It attempts to p roducerel-
ativelycompact clusters that are relatively far apart. However, its results
depend on the numerical scale on which the observation dissi milaritiesdii′
are measured. Applying a monotone strictly increasing tran sformation h(·)
to thedii′,hii′=h(dii′), can change the result produced by (14.43). In
contrast, (14.41) and (14.42) depend only on the ordering of thedii′and
are thus invariant to such monotone transformations. This i nvariance is
often used as an argument in favor of single or complete linka ge over group
average methods.
One can argue that group average clustering has a statistica l consis-
tency property violated by single and complete linkage. Ass ume we have
attribute-value data XT= (X1,...,X p) and that each cluster kis a ran-
dom sample from some population joint density pk(x). The complete data
set is a random sample from a mixture of Ksuch densities. The group
average dissimilarity dGA(G,H) (14.43) is an estimate of
/integraldisplay /integraldisplay
d(x,x′)pG(x)pH(x′)dxdx′, (14.45)
whered(x,x′) is the dissimilarity between points xandx′in the space
of attribute values. As the sample size Napproaches inﬁnity dGA(G,H)
(14.43) approaches (14.45), which is a characteristic of th e relationship
between the two densities pG(x) andpH(x). For single linkage, dSL(G,H)
(14.41) approaches zero as N→∞independent of pG(x) andpH(x). For
complete linkage, dCL(G,H) (14.42) becomes inﬁnite as N→∞, again
independent of the two densities. Thus, it is not clear what a spects of the
population distribution are being estimated by dSL(G,H) anddCL(G,H).
Example: Human Cancer Microarray Data (Continued)
The left panel of Figure 14.13 shows the dendrogram resultin g from average
linkageagglomerative clusteringofthesamples(columns) ofthemicroarray
data.Themiddleandrightpanelsshowtheresultusingcompl eteandsingle
linkage. Average and complete linkage gave similar results , while single
linkage produced unbalanced groups with long thin clusters . We focus on
the average linkage clustering.
LikeK-meansclustering,hierarchicalclusteringissuccessful atclustering
simple cancers together. However it has other nice features . By cutting oﬀ
the dendrogram at various heights, diﬀerent numbers of clus ters emerge,
and the sets of clusters are nested within one another. Secon dly, it gives
some partial ordering information about the samples. In Fig ure 14.14, we
have arranged the genes (rows) and samples (columns) of the e xpression
matrix in orderings derived from hierarchical clustering.

526 14. Unsupervised Learning
Notethatifweﬂiptheorientationofthebranchesofadendro gramatany
merge, the resulting dendrogram is still consistent with th e series of hierar-
chical clustering operations. Hence to determine an orderi ng of the leaves,
we must add a constraint. To produce the row ordering of Figur e 14.14,
we have used the default rule in S-PLUS: at each merge, the sub tree with
the tighter cluster is placed to the left (toward the bottom i n the rotated
dendrogram in the ﬁgure.) Individual genes are the tightest clusters possi-
ble, and merges involving two individual genes place them in order by their
observation number. The same rule was used for the columns. M any other
rules are possible—for example, ordering by a multidimensio nal scaling of
the genes; see Section 14.8.
The two-way rearrangement of Figure 14.14 produces an infor mative pic-
ture of the genes and samples. This picture is more informati ve than the
randomly ordered rows and columns of Figure 1.3 of Chapter 1. Further-
more,thedendrogramsthemselvesareuseful,asbiologists can,forexample,
interpret the gene clusters in terms of biological processe s.
Divisive Clustering
Divisive clustering algorithms begin with the entire data s et as a single
cluster, and recursively divide one of the existing cluster s into two daugh-
ter clusters at each iteration in a top-down fashion. This ap proach has not
been studied nearly as extensively as agglomerative method s in the cluster-
ing literature. It has been explored somewhat in the enginee ring literature
(Gersho and Gray, 1992) in the context of compression. In the clustering
setting, a potential advantage of divisive over agglomerat ive methods can
occur when interest is focused on partitioning the data into a relatively
smallnumber of clusters.
The divisive paradigm can be employed by recursively applyi ng any of
the combinatorial methods such as K-means (Section 14.3.6) or K-medoids
(Section 14.3.10), with K= 2, to perform the splits at each iteration. How-
ever,suchanapproachwoulddependonthestartingconﬁgura tionspeciﬁed
at each step. In addition, it would not necessarily produce a splitting se-
quence that possesses the monotonicity property required f or dendrogram
representation.
A divisive algorithm that avoids these problems was propose d by Mac-
naughton Smith et al. (1965). It begins by placing all observ ations in a
single cluster G. It then chooses that observation whose average dissimi-
larity from all the other observations is largest. This obse rvation forms the
ﬁrstmemberofasecondcluster H.Ateachsuccessivestepthatobservation
inGwhose average distance from those in H, minus that for the remaining
observations in Gis largest, is transferred to H. This continues until the
corresponding diﬀerence in averages becomes negative. Tha t is, there are
no longer any observations in Gthat are, on average, closer to those in
H. The result is a split of the original cluster into two daught er clusters,

14.3 Cluster Analysis 527
FIGURE 14.14. DNA microarray data: average linkage hierarchical clusterin g
has been applied independently to the rows (genes) and columns ( samples), de-
termining the ordering of the rows and columns (see text). The colors range from
bright green (negative, under-expressed) to bright red (po sitive, over-expressed).

528 14. Unsupervised Learning
the observations transferred to H, and those remaining in G. These two
clusters represent the second level of the hierarchy. Each s uccessive level
is produced by applying this splitting procedure to one of th e clusters at
the previous level. Kaufman and Rousseeuw (1990) suggest ch oosing the
cluster at each level with the largest diameter (14.44) for s plitting. An al-
ternative would be to choose the one with the largest average dissimilarity
among its members
¯dG=1
NG2/summationdisplay
i∈G/summationdisplay
i′∈Gdii′.
The recursive splitting continues until all clusters eithe r become singletons
or all members of each one have zero dissimilarity from one an other.
14.4 Self-Organizing Maps
This method can be viewed as a constrained version of K-means clustering,
in which the prototypes are encouraged to lie in a one- or two- dimensional
manifold in the feature space. The resulting manifold is als o referred to
as aconstrained topological map , since the original high-dimensional obser-
vations can be mapped down onto the two-dimensional coordin ate system.
The original SOM algorithm was online—observations are proc essed one at
a time—and later a batch version was proposed. The technique a lso bears
a close relationship to principal curves and surfaces , which are discussed in
the next section.
We consider a SOM with a two-dimensional rectangular grid of Kproto-
typesmj∈IRp(other choices, such as hexagonal grids, can also be used).
Each of the Kprototypes are parametrized with respect to an integer
coordinate pair ℓj∈Q1×Q2. HereQ1={1,2,...,q 1}, similarlyQ2, and
K=q1·q2.Themjareinitialized,forexample,tolieinthetwo-dimensional
principal component plane of the data (next section). We can think of the
prototypes as “buttons,” “sewn” on the principal component plane in a
regular pattern. The SOM procedure tries to bend the plane so that the
buttons approximate the data points as well as possible. Onc e the model is
ﬁt, the observations can be mapped down onto the two-dimensi onal grid.
The observations xiare processed one at a time. We ﬁnd the closest
prototypemjtoxiin Euclidean distance in IRp, and then for all neighbors
mkofmj, movemktowardxivia the update
mk←mk+α(xi−mk). (14.46)
The “neighbors” of mjare deﬁned to be all mksuch that the distance
betweenℓjandℓkis small. The simplest approach uses Euclidean distance,
and “small” is determined by a threshold r. This neighborhood always
includes the closest prototype mjitself.

14.4 Self-Organizing Maps 529
Notice that distance is deﬁned in the space Q1×Q2of integer topological
coordinates of the prototypes, rather than in the feature sp ace IRp. The
eﬀect of the update (14.46) is to move the prototypes closer t o the data,
butalsotomaintainasmoothtwo-dimensionalspatialrelat ionshipbetween
the prototypes.
The performance of the SOM algorithm depends on the learning rate
αand the distance threshold r. Typically αis decreased from say 1 .0 to
0.0 over a few thousand iterations (one per observation). Simi larlyris
decreasedlinearlyfromstartingvalue Rto1overafewthousanditerations.
We illustrate a method for choosing Rin the example below.
We have described the simplest version of the SOM. More sophi sticated
versions modify the update step according to distance:
mk←mk+αh(∝⌊a∇⌈⌊lℓj−ℓk∝⌊a∇⌈⌊l)(xi−mk), (14.47)
wherethe neighborhood function hgivesmoreweighttoprototypes mkwith
indicesℓkcloser toℓjthan to those further away.
Ifwetakethedistance rsmallenoughsothateachneighborhoodcontains
only one point, then the spatial connection between prototy pes is lost. In
that case one can show that the SOM algorithm is an online vers ion of
K-means clustering, and eventually stabilizes at one of the l ocal minima
found byK-means. Since the SOM is a constrained version of K-means
clustering, it is important to check whether the constraint is reasonable
in any given problem. One can do this by computing the reconst ruction
error∝⌊a∇⌈⌊lx−mj∝⌊a∇⌈⌊l2, summed over observations, for both methods. This will
necessarily be smaller for K-means, but should not be much smaller if the
SOM is a reasonable approximation.
As an illustrative example, we generated 90 data points in th ree dimen-
sions, near the surface of a half sphere of radius 1. The point s were in each
of three clusters—red, green, and blue—located near (0 ,1,0), (0,0,1) and
(1,0,0). The data are shown in Figure 14.15
By design, the red cluster was much tighter than the green or b lue ones.
(Full details of the data generation are given in Exercise 14 .5.) A 5×5 grid
of prototypes was used, with initial grid size R= 2; this meant that about
a third of the prototypes were initially in each neighborhoo d. We did a
total of 40 passes through the dataset of 90 observations, an d letrandα
decrease linearly over the 3600 iterations.
In Figure 14.16 the prototypes are indicated by circles, and the points
that project to each prototype are plotted randomly within t he correspond-
ing circle. The left panel shows the initial conﬁguration, w hile the right
panel shows the ﬁnal one. The algorithm has succeeded in sepa rating the
clusters; however, the separation of the red cluster indica tes that the man-
ifold has folded back on itself (see Figure 14.17). Since the distances in the
two-dimensional display are not used, there is little indic ation in the SOM
projection that the red cluster is tighter than the others.

530 14. Unsupervised Learning
−1−0.500.511.5
−1−0.500.511.5−1−0.500.511.5
FIGURE 14.15. Simulated data in three classes, near the surface of a half–
sphere.
•••
••
•••
••
•••
•••••
••••••••
•••
•••
••
•
••
••••
••••
•• ••• ••
••
••• •
••••
•••••
••
••••
••• •
••
•
•••
••
•••
••
1 2 3 4 512345
••••
••
•
••
••••••
•
••
••••
•
•••••
••
• • • •••
••
•
••••
•••••
•
•••
•• •• ••
• •
•••
••••
•••
•••
••
••
•• •
•••
• • ••
•••
1 2 3 4 512345
FIGURE 14.16. Self-organizing map applied to half-sphere data example. Left
panel is the initial conﬁguration, right panel the ﬁnal one. The5×5grid of
prototypes are indicated by circles, and the points that proj ect to each prototype
are plotted randomly within the corresponding circle.

14.4 Self-Organizing Maps 531
FIGURE 14.17. Wiremesh representation of the ﬁtted SOM model in IR3. The
lines represent the horizontal and vertical edges of the topo logical lattice. The
double lines indicate that the surface was folded diagonally bac k on itself in order
to model the red points. The cluster members have been jittere d to indicate their
color, and the purple points are the node centers.
Figure 14.18 shows the reconstruction error, equal to the to tal sum of
squares of each data point around its prototype. For compari son we carried
out aK-means clustering with 25 centroids, and indicate its recon struction
error by the horizontal line on the graph. We see that the SOM s igniﬁcantly
decreases the error, nearly to the level of the K-means solution. This pro-
vides evidence that the two-dimensional constraint used by the SOM is
reasonable for this particular dataset.
In the batch version of the SOM, we update each mjvia
mj=/summationtextwkxk/summationtextwk. (14.48)
The sum is over points xkthat mapped (i.e., were closest to) neighbors mk
ofmj. The weight function may be rectangular, that is, equal to 1 f or the
neighborsof mk,ormaydecreasesmoothlywithdistance ∝⌊a∇⌈⌊lℓk−ℓj∝⌊a∇⌈⌊lasbefore.
If the neighborhood size is chosen small enough so that it con sists only
ofmk, with rectangular weights, this reduces to the K-means clustering
procedure described earlier. It can also be thought of as a di screte version
of principal curves and surfaces, described in Section 14.5 .

532 14. Unsupervised Learning
IterationReconstruction Error
0 500 1000 1500 2000 25000 10 20 30 40 50••
•
•••
••
••
••••
•••••••••••••••••••••••••••••••••••••
FIGURE 14.18. Half-sphere data: reconstruction error for the SOM as a func-
tion of iteration. Error for k-means clustering is indicated by the horizontal line.
Example: Document Organization and Retrieval
Document retrieval has gained importance with the rapid dev elopment of
the Internet and the Web, and SOMs have proved to be useful for organiz-
ing and indexing large corpora. This example is taken from th e WEBSOM
homepage http://websom.hut.fi/ (Kohonen et al., 2000). Figure 14.19 rep-
resents a SOM ﬁt to 12,088 newsgroup comp.ai.neural-nets articles. The
labels are generated automatically by the WEBSOM software a nd provide
a guide as to the typical content of a node.
In applications such as this, the documents have to be prepro cessed in
order to create a feature vector. A term-document matrix is created, where
each row represents a single document. The entries in each ro w are the
relative frequency of each of a predeﬁned set of terms. These terms could
be a large set of dictionary entries (50,000 words), or an eve n larger set
of bigrams (word pairs), or subsets of these. These matrices are typically
very sparse, and so often some preprocessing is done to reduc e the number
of features (columns). Sometimes the SVD (next section) is u sed to reduce
the matrix; Kohonen et al. (2000) use a randomized variant th ereof. These
reduced vectors are then the input to the SOM.

14.4 Self-Organizing Maps 533
FIGURE 14.19. Heatmap representation of the SOM model ﬁt to a corpus
of12,088newsgroup comp.ai.neural-nets contributions (courtesy WEBSOM
homepage). The lighter areas indicate higher-density areas . Populated nodes are
automatically labeled according to typical content.

534 14. Unsupervised Learning
•••
•
••
••••
•••
••
• ••
••
•••
•
••
••••
•••
••
• ••
•v1v1v1v1v1v1v1v1
ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1
xixixixixixixixi
FIGURE 14.20. The ﬁrst linear principal component of a set of data. The line
minimizes the total squared distance from each point to its o rthogonal projection
onto the line.
In this application the authors have developed a “zoom” feat ure, which
allows one to interact with the map in order to get more detail . The ﬁnal
level of zooming retrieves the actual news articles, which c an then be read.
14.5 Principal Components, Curves and Surfaces
Principal components are discussed in Sections 3.4.1, wher e they shed light
on the shrinkage mechanism of ridge regression. Principal c omponents are
a sequence of projections of the data, mutually uncorrelate d and ordered
in variance. In the next section we present principal compon ents as linear
manifolds approximating a set of Npointsxi∈IRp. We then present
some nonlinear generalizations in Section 14.5.2. Other re cent proposals
for nonlinear approximating manifolds are discussed in Sec tion 14.9.
14.5.1 Principal Components
The principal components of a set of data in IRpprovide a sequence of best
linear approximations to that data, of all ranks q≤p.
Denote the observations by x1,x2,...,x N, and consider the rank- qlinear
model for representing them

14.5 Principal Components, Curves and Surfaces 535
f(λ) =µ+Vqλ, (14.49)
whereµis a location vector in IRp,Vqis ap×qmatrix with qorthogonal
unit vectors as columns, and λis aqvector of parameters. This is the
parametric representation of an aﬃne hyperplane of rank q. Figures 14.20
and 14.21 illustrate for q= 1 andq= 2, respectively. Fitting such a model
to the data by least squares amounts to minimizing the reconstruction error
min
µ,{λi},VqN/summationdisplay
i=1∝⌊a∇⌈⌊lxi−µ−Vqλi∝⌊a∇⌈⌊l2. (14.50)
We can partially optimize for µand theλi(Exercise 14.7) to obtain
ˆµ= ¯x, (14.51)
ˆλi=VT
q(xi−¯x). (14.52)
This leaves us to ﬁnd the orthogonal matrix Vq:
min
VqN/summationdisplay
i=1||(xi−¯x)−VqVT
q(xi−¯x)||2. (14.53)
For convenience we assume that ¯ x= 0 (otherwise we simply replace the
observations by their centered versions ˜ xi=xi−¯x). Thep×pmatrix
Hq=VqVT
qis aprojection matrix , and maps each point xionto its rank-
qreconstruction Hqxi, the orthogonal projection of xionto the subspace
spanned by the columns of Vq. The solution can be expressed as follows.
Stack the (centered) observations into the rows of an N×pmatrixX. We
construct the singular value decomposition ofX:
X=UDVT. (14.54)
This is a standard decomposition in numerical analysis, and many algo-
rithms exist for its computation (Golub and Van Loan, 1983, f or example).
HereUis anN×porthogonal matrix ( UTU=Ip) whose columns ujare
called the left singular vectors ;Vis ap×porthogonal matrix ( VTV=Ip)
with columns vjcalled the right singular vectors , andDis ap×pdiagonal
matrix, with diagonal elements d1≥d2≥···≥dp≥0 known as the sin-
gular values . For each rank q, the solution Vqto (14.53) consists of the ﬁrst
qcolumns of V. The columns of UDare called the principal components
ofX(see Section 3.5.1). The Noptimalˆλiin (14.52) are given by the ﬁrst
qprincipal components (the Nrows of the N×qmatrixUqDq).
The one-dimensional principal component line in IR2is illustrated in Fig-
ure 14.20. For each data point xi, there is a closest point on the line, given
byui1d1v1. Herev1is the direction of the line and ˆλi=ui1d1measures
distance along the line from the origin. Similarly Figure 14 .21 shows the

536 14. Unsupervised Learning
First principal componentSecond principal component
−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0•
•••
•••
•
••
•••
••••
•
•••
••••••
••
•••
••
•
••
•••
•
••••••
•••
••
••
•••
•
••••
•••
••
••
••
•
••
•••
•
•••
••••
•••
••
FIGURE 14.21. The best rank-two linear approximation to the half-sphere dat a.
The right panel shows the projected points with coordinates given by U2D2, the
ﬁrst two principal components of the data.
two-dimensional principal component surface ﬁt to the half -sphere data
(left panel). The right panel shows the projection of the dat a onto the
ﬁrst two principal components. This projection was the basi s for the initial
conﬁguration for the SOM method shown earlier. The procedur e is quite
successful at separating the clusters. Since the half-sphe re is nonlinear, a
nonlinear projection will do a better job, and this is the top ic of the next
section.
Principal components have many other nice properties, for e xample, the
linear combination Xv1has the highest variance among all linear com-
binations of the features; Xv2has the highest variance among all linear
combinations satisfying v2orthogonal to v1, and so on.
Example: Handwritten Digits
Principal components are a useful tool for dimension reduct ion and com-
pression.Weillustratethisfeatureonthehandwrittendig itsdatadescribed
in Chapter 1. Figure 14.22 shows a sample of 130 handwritten 3 ’s, each a
digitized 16×16 grayscale image, from a total of 658 such 3’s. We see
considerable variation in writing styles, character thick ness and orienta-
tion. We consider these images as points xiin IR256, and compute their
principal components via the SVD (14.54).
Figure 14.23 shows the ﬁrst two principal components of thes e data. For
each of these ﬁrst two principal components ui1d1andui2d2, we computed
the 5%, 25%, 50%, 75% and 95% quantile points, and used them to deﬁne
the rectangular grid superimposed on the plot. The circled p oints indicate

14.5 Principal Components, Curves and Surfaces 537
FIGURE 14.22. A sample of 130handwritten 3’s shows a variety of writing
styles.
those images close to the vertices of the grid, where the dist ance measure
focuses mainly on these projected coordinates, but gives so me weight to the
components in the orthogonal subspace. The right plot shows the images
corresponding to these circled points. This allows us to vis ualize the nature
of the ﬁrst two principal components. We see that the v1(horizontal move-
ment) mainly accounts for the lengthening of the lower tail o f the three,
whilev2(vertical movement) accounts for character thickness. In t erms of
the parametrized model (14.49), this two-component model h as the form
ˆf(λ) = ¯x+λ1v1+λ2v2
= +λ1·+λ2·. (14.55)
Here we have displayed the ﬁrst two principal component dire ctions,v1
andv2, as images. Although there are a possible 256 principal comp onents,
approximately 50 account for 90% of the variation in the thre es, 12 ac-
count for 63%. Figure 14.24 compares the singular values to t hose obtained
for equivalent uncorrelated data, obtained by randomly scr ambling each
column of X. The pixels in a digitized image are inherently correlated,
and since these are all the same digit the correlations are ev en stronger.

538 14. Unsupervised Learning
First Principal ComponentSecond Principal Component
-6 -4 -2 0 2 4 6 8-5 0 5••
••
••
••
• ••••
••
•••
••
••••
••
•
•••
•
••
••••
•••
••
•
••
••
•
• •
••
•••
••
••••
•••
•••••
••
•
••
••
•
•••
•
••
••••
•
•
•
••••••
•
•••
•••
•
•••••
•
•••
•
••
••
••
••
•••
•••
•
•••
••
••••
•
•
••
••
••
••
•
•••
•
••
•
•• •
••
••
•••
••
•••
•
•••••
••
•
••
••
•
•••
•
••
•••
•••••
•
••
••
•
••
••
•
•• ••
••••
••
••
•••
••
•
•••
•••
•
•
•
•• •
•
•••
• •• ••
••••
•
•
••
•• •
•
•
••
•
••
•••
••
••
••
• •••
•
•
•••
••••
•••
••
••••
••
•
• ••
•••
•••
••
••••
••••
••
• ••
••
•••••
•
•
••••
•
••••
•
••
••
•
• •••
••
••••
•
•••
•
•
••
••••
••••
•••
•
•••
•
•••
••
•• •
•
•
••
•••
••
••
•
••
•
••
•••
•••
•
•••
•
•••
•••••
••••
•• •
•
•••
•
••
••
•••
•
••
•• ••
••
••
••••
••
•
•••
••
•
•
•••
•
•
••
••
••
••
••
••
•
••
•
•
••
•
•
••••
•
••
•••
•••
•••• ••
••
••••
••
•••
••••
•
••
•••
••
••
••
•
•••
••
••
••••
•
•
•••••
•••
••
••••
•••••
••
•••
••
••
••••
•
••
•• •••
•••
••
•
••••
••• ••
•
••
•••
•••
•••
•
•
•••
•••••
••
•
••
••
••
•
••••••
••
•
••
•
••
O O O OOO O OO
OOO
OO OO O O OOO O OOO
FIGURE 14.23. (Left panel:) the ﬁrst two principal components of the hand-
written threes. The circled points are the closest projected i mages to the vertices
of a grid, deﬁned by the marginal quantiles of the principal co mponents. (Right
panel:) The images corresponding to the circled points. These show the nature of
the ﬁrst two principal components.
DimensionSingular Values
0 50 100 150 200 2500 20 40 60 80•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••
•••
•
••
••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Real Trace
•Randomized Trace
FIGURE 14.24. The256singular values for the digitized threes, compared to
those for a randomized version of the data (each column of Xwas scrambled).

14.5 Principal Components, Curves and Surfaces 539
A relatively small subset of the principal components serve as excellent
lower-dimensional features for representing the high-dim ensional data.
Example: Procrustes Transformations and Shape Averaging
FIGURE 14.25. (Left panel:) Two diﬀerent digitized handwritten Ss, each re p-
resented by 96 corresponding points in IR2. The green S has been deliberately
rotated and translated for visual eﬀect. (Right panel:) A Proc rustes transforma-
tion applies a translation and rotation to best match up the two set of points.
Figure 14.25 represents two sets of points, the orange and gr een, in the
same plot. In this instance these points represent two digit ized versions
of a handwritten S, extracted from the signature of a subject “Suresh.”
Figure 14.26 shows the entire signatures from which these we re extracted
(third and fourth panels). The signatures are recorded dyna mically using
touch-screen devices, familiar sights in modern supermark ets. There are
N= 96 points representing each S, which we denote by the N×2 matrices
X1andX2. There is a correspondence between the points—the ith rows
ofX1andX2are meant to represent the same positions along the two S’s.
In the language of morphometrics, these points represent landmarks on
the two objects. How one ﬁnds such corresponding landmarks i s in general
diﬃcult and subject speciﬁc. In this particular case we used dynamic time
warping of the speed signal along each signature (Hastie et al., 1992 ), but
will not go into details here.
In the right panel we have applied a translation and rotation to the green
points so as best to match the orange—a so-called Procrustes3transforma-
tion (Mardia et al., 1979, for example).
Consider the problem
min
µ,R||X2−(X1R+1µT)||F, (14.56)
3Procrustes was an African bandit in Greek mythology, who stretched or sq uashed
his visitors to ﬁt his iron bed (eventually killing them).

540 14. Unsupervised Learning
withX1andX2bothN×pmatrices of corresponding points, Ran or-
thonormal p×pmatrix4, andµap-vector of location coordinates. Here
||X||2
F= trace(XTX) is the squared Frobenius matrix norm.
Let ¯x1and ¯x2be the column mean vectors of the matrices, and ˜X1and
˜X2be the versions of these matrices with the means removed. Con sider
the SVD ˜XT
1˜X2=UDVT. Then the solution to (14.56) is given by (Exer-
cise 14.8)
ˆR=UVT
ˆµ= ¯x2−ˆR¯x1,(14.57)
and the minimal distances is referred to as the Procrustes distance . From
the form of the solution, we can center each matrix at its colu mn centroid,
and then ignore location completely. Hereafter we assume th is is the case.
TheProcrustes distance with scaling solves a slightly more general
problem,
min
β,R||X2−βX1R||F, (14.58)
whereβ >0 is a positive scalar. The solution for Ris as before, with
ˆβ= trace(D)/||X1||2
F.
Related to Procrustes distance is the Procrustes average of a collection
ofLshapes, which solves the problem
min
{Rℓ}L
1,ML/summationdisplay
ℓ=1||XℓRℓ−M||2
F; (14.59)
that is, ﬁnd the shape Mclosest in average squared Procrustes distance to
all the shapes. This is solved by a simple alternating algori thm:
0. Initialize M=X1(for example).
1. Solve the LProcrustes rotation problems with Mﬁxed, yielding
X′
ℓ←XˆRℓ.
2. LetM←1
L/summationtextL
ℓ=1X′
ℓ.
Steps 1. and 2. are repeated until the criterion (14.59) conv erges.
Figure 14.26 shows a simple example with three shapes. Note t hat we can
only expect a solution up to a rotation; alternatively, we ca n impose a
constraint, such as that Mbe upper-triangular, to force uniqueness. One
can easily incorporate scaling in the deﬁnition (14.59); se e Exercise 14.9.
Most generally we can deﬁne the aﬃne-invariant average of a set of
shapes via
4To simplify matters, we consider only orthogonal matrices which inclu de reﬂections
aswellasrotations[the O(p)group];althoughreﬂectionsareunlikelyhere,thesemethods
can be restricted further to allow only rotations [ SO(p) group].

14.5 Principal Components, Curves and Surfaces 541
FIGURE 14.26. The Procrustes average of three versions of the leading S in
Suresh’s signatures. The left panel shows the preshape average, with each of the
shapesX′
ℓin preshape space superimposed. The right three panels map th e pre-
shapeMseparately to match each of the original S’s.
min
{Aℓ}L
1,ML/summationdisplay
ℓ=1||XℓAℓ−M||2
F, (14.60)
where the Aℓare anyp×pnonsingular matrices. Here we require a stan-
dardization, such as MTM=I, to avoid a trivial solution. The solution is
attractive, and can be computed without iteration (Exercis e 14.10):
1. LetHℓ=Xℓ(XT
ℓXℓ)−1XT
ℓbe the rank- pprojection matrix deﬁned
byXℓ.
2.Mis theN×p matrix formed from the plargest eigenvectors of ¯H=
1
L/summationtextL
ℓ=1Hℓ.
14.5.2 Principal Curves and Surfaces
Principalcurvesgeneralizetheprincipalcomponentline, providingasmooth
one-dimensionalcurvedapproximationtoasetofdatapoint sinIRp.Aprin-
cipal surface is more general, providing a curved manifold a pproximation
of dimension 2 or more.
We will ﬁrst deﬁne principal curves for random variables X∈IRp, and
then move to the ﬁnite data case. Let f(λ) be a parameterized smooth
curve in IRp. Hencef(λ) is a vector function with pcoordinates, each a
smooth function of the single parameter λ. The parameter λcan be chosen,
for example, to be arc-length along the curve from some ﬁxed o rigin. For
each data value x, letλf(x) deﬁne the closest point on the curve to x. Then
f(λ) is called a principal curve for the distribution of the rand om vector
Xif
f(λ) = E(X|λf(X) =λ). (14.61)
This saysf(λ)is theaverage ofall data points that project toit, that is, the
points for which it is “responsible.” This is also known as a self-consistency
property. Although in practice, continuous multivariate d istributes have
inﬁnitely many principal curves (Duchamp and Stuetzle, 199 6), we are

542 14. Unsupervised Learning
....
•••
•••
••••
•• ••
•
••
•••
•
•••
•
•
••••
••••
•••
••••
••••
•
••
•••
•
•••
•
•
••••
•.....
f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)]
FIGURE 14.27. The principal curve of a set of data. Each point on the curve
is the average of all data points that project there.
interested mainly in the smooth ones. A principal curve is il lustrated in
Figure 14.27.
Principal points are an interesting related concept. Consider a set of k
prototypes and for each point xin the support of a distribution, identify
the closest prototype, that is, the prototype that is respon sible for it. This
induces a partition of the feature space into so-called Voro noi regions. The
set ofkpoints that minimize the expected distance from Xto its prototype
are called the principal points of the distribution. Each pr incipal point is
self-consistent, in that it equals the mean of Xin its Voronoi region. For
example, with k= 1, the principal point of a circular normal distribution is
the mean vector; with k= 2 they are a pair of points symmetrically placed
on a ray through the mean vector. Principal points are the dis tributional
analogs of centroids found by K-means clustering. Principal curves can be
viewed ask=∞principal points, but constrained to lie on a smooth curve,
in a similar way that a SOM constrains K-means cluster centers to fall on
a smooth manifold.
Toﬁndaprincipalcurve f(λ)ofadistribution,weconsideritscoordinate
functionsf(λ) = [f1(λ),f2(λ),...,f p(λ)] and letXT= (X1,X2,...,X p).
Consider the following alternating steps:
(a)ˆfj(λ)←E(Xj|λ(X) =λ);j= 1,2,...,p,
(b)ˆλf(x)←argminλ′||x−ˆf(λ′)||2.(14.62)
The ﬁrst equation ﬁxes λand enforces the self-consistency requirement
(14.61). The second equation ﬁxes the curve and ﬁnds the clos est point on

14.5 Principal Components, Curves and Surfaces 543
-0.1 0.0 0.1 0.2-0.2 -0.1 0.0 0.1 0.2••••
•••
•
••
•••
•••••
•••
••••••
••
• ••
••
•••
•••
•
••••••
•••
••
••
•••
• •••
•
•••
•••
•
••
•
•••••
••••
••••
•••
••
λ1λ2
FIGURE 14.28. Principal surface ﬁt to half-sphere data. (Left panel:) ﬁtted
two-dimensional surface. (Right panel:) projections of dat a points onto the sur-
face, resulting in coordinates ˆλ1,ˆλ2.
thecurvetoeachdatapoint.Withﬁnitedata,theprincipalc urvealgorithm
starts with the linear principal component, and iterates th e two steps in
(14.62) until convergence. A scatterplot smoother is used t o estimate the
conditional expectations in step (a) by smoothing each Xjas a function of
the arc-length ˆλ(X), and the projection in (b) is done for each of the ob-
served data points. Proving convergence in general is diﬃcu lt, but one can
show that if a linear least squares ﬁt is used for the scatterp lot smoothing,
then the procedure converges to the ﬁrst linear principal co mponent, and
is equivalent to the power method for ﬁnding the largest eigenvector of a
matrix.
Principal surfaces have exactly the same form as principal c urves, but
are of higher dimension. The mostly commonly used is the two- dimensional
principal surface, with coordinate functions
f(λ1,λ2) = [f1(λ1,λ2),...,f p(λ1,λ2)].
The estimates in step (a) above are obtained from two-dimens ional surface
smoothers.Principalsurfacesofdimensiongreaterthantw oarerarelyused,
since the visualization aspect is less attractive, as is smo othing in high
dimensions.
Figure 14.28 shows the result of a principal surface ﬁt to the half-sphere
data. Plotted are the data points as a function of the estimat ed nonlinear
coordinates ˆλ1(xi),ˆλ2(xi). The class separation is evident.
Principal surfaces are very similar to self-organizing map s. If we use a
kernel surface smoother to estimate each coordinate functi onfj(λ1,λ2),
this has the same form as the batch version of SOMs (14.48). Th e SOM
weightswkare just the weights in the kernel. There is a diﬀerence, howe ver:

544 14. Unsupervised Learning
the principal surface estimates a separate prototype f(λ1(xi),λ2(xi)) for
each data point xi, while the SOM shares a smaller number of prototypes
for all data points. As a result, the SOM and principal surfac e will agree
only as the number of SOM prototypes grows very large.
There also is a conceptual diﬀerence between the two. Princi pal sur-
faces provide a smooth parameterization of the entire manif old in terms
of its coordinate functions, while SOMs are discrete and pro duce only the
estimated prototypes for approximating the data. The smoot h parameter-
ization in principal surfaces preserves distance locally: in Figure 14.28 it
reveals that the red cluster is tighter than the green or blue clusters. In
simple examples the estimates coordinate functions themse lves can be in-
formative: see Exercise 14.13.
14.5.3 Spectral Clustering
Traditional clustering methods like K-means use a spherical or elliptical
metric to group data points. Hence they will not work well whe n the clus-
ters are non-convex, such as the concentric circles in the to p left panel of
Figure 14.29. Spectral clustering is a generalization of st andard clustering
methods, and is designed for these situations. It has close c onnections with
the local multidimensional-scaling techniques (Section 1 4.9) that generalize
MDS.
The starting point is a N×Nmatrix of pairwise similarities sii′≥0 be-
tween all observation pairs. We represent the observations in an undirected
similarity graph G=∝an}⌊∇a⌋ketle{tV, E∝an}⌊∇a⌋ket∇i}ht. TheNverticesvirepresent the observations,
and pairs of vertices are connected by an edge if their simila rity is positive
(or exceeds some threshold). The edges are weighted by the sii′. Clustering
is now rephrased as a graph-partition problem, where we iden tify connected
components with clusters. We wish to partition the graph, su ch that edges
between diﬀerent groups have low weight, and within a group h ave high
weight. The idea in spectral clustering is to construct simi larity graphs that
represent the local neighborhood relationships between ob servations.
Tomakethingsmoreconcrete,considerasetof Npointsxi∈IRp,andlet
dii′be the Euclidean distance between xiandxi′. We will use as similarity
matrix the radial-kernel gram matrix; that is, sii′= exp(−d2
ii′/c), where
c>0 is a scale parameter.
There are many ways to deﬁne a similarity matrix and its assoc iated
similarity graph that reﬂect local behavior. The most popul ar is the mutual
K-nearest-neighbor graph . DeﬁneNKto be the symmetric set of nearby
pairs of points; speciﬁcally a pair ( i,i′) is inNKif pointiis among the
K-nearest neighbors of i′, or vice-versa. Then we connect all symmetric
nearest neighbors, and give them edge weight wii′=sii′; otherwise the
edge weight is zero. Equivalently we set to zero all the pairw ise similarities
not inNK, and draw the graph for this modiﬁed similarity matrix.

14.5 Principal Components, Curves and Surfaces 545
Alternatively, a fully connected graph includes all pairwi se edges with
weightswii′=sii′, and the local behavior is controlled by the scale param-
eterc.
The matrix of edge weights W={wii′}from a similarity graph is called
theadjacency matrix . Thedegreeof vertexiisgi=/summationtext
i′wii′, the sum of
the weights of the edges connected to it. Let Gbe a diagonal matrix with
diagonal elements gi.
Finally, the graph Laplacian is deﬁned by
L=G−W (14.63)
This is called the unnormalized graph Laplacian ; a number of normalized
versions have been proposed—these standardize the Laplacia n with respect
to the node degrees gi, for example, ˜L=I−G−1W.
Spectral clustering ﬁnds the meigenvectors ZN×mcorresponding to the
msmallest eigenvalues of L(ignoring the trivial constant eigenvector).
Using a standard method like K-means, we then cluster the rows of Zto
yield a clustering of the original data points.
An example is presented in Figure 14.29. The top left panel sh ows 450
simulated data points in three circular clusters indicated by the colors. K-
means clustering would clearly have diﬃculty identifying t he outer clusters.
We applied spectral clustering using a 10-nearest neighbor similarity graph,
and display the eigenvector corresponding to the second and third smallest
eigenvalue of the graph Laplacian in the lower left. The 15 sm allest eigen-
values are shown in the top right panel. The two eigenvectors shown have
identiﬁed the three clusters, and a scatterplot of the rows o f the eigenvector
matrixYin the bottom right clearly separates the clusters. A proced ure
such asK-means clustering applied to these transformed points woul d eas-
ily identify the three groups.
Why does spectral clustering work? For any vector fwe have
fTLf=N/summationdisplay
i=1gif2
i−N/summationdisplay
i=1N/summationdisplay
i′=1fifi′wii′
=1
2N/summationdisplay
i=1N/summationdisplay
i′=1wii′(fi−fi′)2. (14.64)
Formula 14.64 suggests that a small value of fTLfwill be achieved if pairs
of points with large adjacencies have coordinates fiandfi′close together.
Since1TL1= 0 for any graph, the constant vector is a trivial eigenvecto r
with eigenvalue zero. Not so obvious is the fact that if the gr aph is con-
nected5, it is the onlyzero eigenvector (Exercise 14.21). Generalizing this
argument,itiseasytoshowthatforagraphwith mconnectedcomponents,
5A graph is connected if any two nodes can be reached via a path of connected no des.

546 14. Unsupervised Learning
−4 −2 0 2 4−4 −2 0 2 4
x1x2
0.0 0.1 0.2 0.3 0.4 0.5
NumberEigenvalue
1 3 5 10 15
0 100 200 300 400Eigenvectors
Index2nd Smallest 3rd Smallest
−0.05  0.05 −0.05  0.05
−0.04 −0.02 0.00 0.02−0.06 −0.02 0.02 0.06
Second Smallest EigenvectorThird Smallest EigenvectorSpectral Clustering
FIGURE 14.29. Toy example illustrating spectral clustering. Data in top left a re
450points falling in three concentric clusters of 150points each. The points are
uniformly distributed in angle, with radius 1,2.8and5in the three groups, and
Gaussian noise with standard deviation 0.25added to each point. Using a k= 10
nearest-neighbor similarity graph, the eigenvector corres ponding to the second and
third smallest eigenvalues of Lare shown in the bottom left; the smallest eigen-
vector is constant. The data points are colored in the same way as in the top left.
The 15 smallest eigenvalues are shown in the top right panel. The coordinates of
the 2nd and 3rd eigenvectors (the 450rows ofZ) are plotted in the bottom right
panel. Spectral clustering does standard (e.g., K-means) clustering of these points
and will easily recover the three original clusters.

14.5 Principal Components, Curves and Surfaces 547
the nodes can be reordered so that Lis block diagonal with a block for each
connected component. Then Lhasmeigenvectors of eigenvalue zero, and
the eigenspace of eigenvalue zero is spanned by the indicato r vectors of the
connected components. In practice one has strong and weak co nnections,
so zero eigenvalues are approximated by small eigenvalues.
Spectralclusteringisaninterestingapproachforﬁndingn on-convexclus-
ters. When a normalized graph Laplacian is used, there is ano ther way to
view this method. Deﬁning P=G−1W, we consider a random walk on
the graph with transition probability matrix P. Then spectral clustering
yields groups of nodes such that the random walk seldom trans itions from
one group to another.
There are a number of issues that one must deal with in applyin g spec-
tralclusteringinpractice.Wemustchoosethetypeofsimil arity graph—eg.
fully connected or nearest neighbors, and associated param eters such as the
number of nearest of neighbors kor the scale parameter of the kernel c. We
must also choose the number of eigenvectors to extract from Land ﬁnally,
as with all clustering methods, the number of clusters. In th e toy example
of Figure 14.29 we obtained good results for k∈[5,200], the value 200 cor-
responding to a fully connected graph. With k<5 the results deteriorated.
Looking at the top-right panel of Figure 14.29, we see no stro ng separation
between the smallest three eigenvalues and the rest. Hence i t is not clear
how many eigenvectors to select.
14.5.4 Kernel Principal Components
Spectral clustering is related to kernel principal components , a non-linear
version of linear principal components. Standard linear pr incipal compo-
nents (PCA) are obtained from the eigenvectors of the covari ance matrix,
and give directions in which the data have maximal variance. Kernel PCA
(Sch¨ olkopfetal.,1999)expandthescopeofPCA,mimicking whatwewould
obtain if we were to expand the features by non-linear transf ormations, and
then apply PCA in this transformed feature space.
We show in Section 18.5.2 that the principal components vari ablesZof
a data matrix Xcan be computed from the inner-product (gram) matrix
K=XXT. In detail, we compute the eigen-decomposition of the doubl e-
centered version of the gram matrix
/tildewideK= (I−M)K(I−M) =UD2UT, (14.65)
withM=11T/N, and then Z=UD. Exercise 18.15 shows how to com-
pute the projections of new observations in this space.
Kernel PCA simply mimics this procedure, interpreting the k ernel ma-
trixK={K(xi,xi′)}as an inner-product matrix of the implicit fea-
tures∝an}⌊∇a⌋ketle{tφ(xi),φ(xi′)∝an}⌊∇a⌋ket∇i}htand ﬁnding its eigenvectors. The elements of the mth
component zm(mth column of Z) can be written (up to centering) as
zim=/summationtextN
j=1αjmK(xi,xj), whereαjm=ujm/dm(Exercise 14.16).

548 14. Unsupervised Learning
We can gain more insight into kernel PCA by viewing the zmas sam-
ple evaluations of principal component functionsgm∈HK, withHKthe
reproducing kernel Hilbert space generated by K(see Section 5.8.1). The
ﬁrst principal component function g1solves
max
g1∈HKVarTg1(X) subject to||g1||HK= 1 (14.66)
Here Var Trefers to the sample variance over training data T. The norm
constraint||g1||HK= 1 controls the size and roughness of the function g1,
as dictated by the kernel K. As in the regression case it can be shown that
the solution to (14.66) is ﬁnite dimensional with represent ationg1(x) =/summationtextN
j=1cjK(x,xj). Exercise 14.17 shows that the solution is deﬁned by ˆ cj=
αj1, j= 1,...,Nabove. The second principal component function is de-
ﬁned in a similar way, with the additional constraint that ∝an}⌊∇a⌋ketle{tg1,g2∝an}⌊∇a⌋ket∇i}htHK= 0,
and so on.6
Sch¨ olkopf et al. (1999) demonstrate the use of kernel princ ipal compo-
nents as features for handwritten-digit classiﬁcation, an d show that they
can improve the performance of a classiﬁer when these are use d instead of
linear principal components.
Note that if we use the radial kernel
K(x,x′) = exp(−∝⌊a∇⌈⌊lx−x′∝⌊a∇⌈⌊l2/c), (14.67)
then the kernel matrix Khas the same form as the similarity matrix Sin
spectral clustering. The matrix of edge weights Wis a localized version of
K, setting to zero all similarities for pairs of points that ar e not nearest
neighbors.
Kernel PCA ﬁnds the eigenvectors corresponding to the large st eigenval-
ues of/tildewideK; this is equivalent to ﬁnding the eigenvectors correspondi ng to the
smallest eigenvalues of
I−/tildewideK. (14.68)
This is almost the same as the Laplacian (14.63), the diﬀeren ces being the
centering of/tildewideKand the fact that Ghas the degrees of the nodes along the
diagonal.
Figure 14.30 examines the performance of kernel principal c omponents
in the toy example of Figure 14.29. In the upper left panel we u sed the ra-
dial kernel with c= 2, the same value that was used in spectral clustering.
This does not separate the groups, but with c= 10 (upper right panel), the
ﬁrst component separates the groups well. In the lower-left panel we ap-
plied kernel PCA using the nearest-neighbor radial kernel Wfrom spectral
clustering. In the lower right panel we use the kernel matrix itself as the
6This section beneﬁted from helpful discussions with Jonathan Taylor.

14.5 Principal Components, Curves and Surfaces 549
−0.10 −0.06 −0.02 0.02−0.10 −0.05 0.00 0.05 0.10
First Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=2)
−0.06 −0.02 0.02 0.06−0.05 0.00 0.05
First Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=10)
0.00 0.05 0.10 0.15−0.2 −0.1 0.0 0.1 0.2
First Largest EigenvectorSecond Largest EigenvectorNN Radial Kernel (c=2)
−0.05 0.00 0.05 0.10 0.15−0.10 0.00 0.05 0.10 0.15
Second Smallest EigenvectorThird Smallest EigenvectorRadial Kernel Laplacian (c=2)
FIGURE 14.30. Kernel principal components applied to the toy example of Fig-
ure 14.29, using diﬀerent kernels. (Top left:) Radial kernel ( 14.67) with c= 2.
(Top right:) Radial kernel with c= 10. (Bottom left): Nearest neighbor radial ker-
nelWfrom spectral clustering. (Bottom right:) Spectral clusteri ng with Laplacian
constructed from the radial kernel.

550 14. Unsupervised Learning
similarity matrix for constructing the Laplacian (14.63) i n spectral cluster-
ing. In neither case do the projections separate the two grou ps. Adjusting
cdid not help either.
In this toy example, we see that kernel PCA is quite sensitive to the scale
and nature of the kernel. We also see that the nearest-neighb or truncation
of the kernel is important for the success of spectral cluste ring.
14.5.5 Sparse Principal Components
Weofteninterpretprincipalcomponentsbyexaminingthedi rectionvectors
vj, also known as loadings, to see which variables play a role. We did this
with the image loadings in (14.55). Often this interpretati on is made easier
if the loadings are sparse. In this section we brieﬂy discuss some methods
for deriving principal components with sparse loadings. Th ey are all based
on lasso (L1) penalties.
We start with an N×pdata matrix X, with centered columns. The
proposed methods focus on either the maximum-variance prop erty of prin-
cipal components, or the minimum reconstruction error. The SCoTLASS
procedure of Joliﬀe et al. (2003) takes the ﬁrst approach, by solving
maxvT(XTX)v,subject to/summationtextp
j=1|vj|≤t,vTv= 1.(14.69)
The absolute-value constraint encourages some of the loadi ngs to be zero
and hencevto be sparse. Further sparse principal components are found
in the same way, by forcing the kth component to be orthogonal to the
ﬁrstk−1 components. Unfortunately this problem is not convex and t he
computations are diﬃcult.
Zou et al. (2006) start instead with the regression/reconst ruction prop-
erty of PCA, similar to theapproach in Section 14.5.1. Let xibetheith row
ofX. For a single component, their sparse principal component technique
solves
min
θ,vN/summationdisplay
i=1||xi−θvTxi||2
2+λ||v||2
2+λ1||v||1 (14.70)
subject to||θ||2= 1.
Let’s examine this formulation in more detail.
•If bothλandλ1are zero and N > p, it is easy to show that v=θ
and is the largest principal component direction.
•Whenp≫Nthe solution is not necessarily unique unless λ>0. For
anyλ>0 andλ1= 0 the solution for vis proportional to the largest
principal component direction.
•The second penalty on vencourages sparseness of the loadings.

14.5 Principal Components, Curves and Surfaces 551
Walking Speed
Verbal Fluency
Principal Components Sparse Principal Components
FIGURE 14.31. Standard and sparse principal components from a study of
the corpus callosum variation. The shape variations corresp onding to signiﬁcant
principal components (red curves) are overlaid on the mean CC shape (black
curves).
For multiple components, the sparse principal components p rocedures
minimizes
N/summationdisplay
i=1||xi−ΘVTxi||2+λK/summationdisplay
k=1||vk||2
2+K/summationdisplay
k=1λ1k||vk||1, (14.71)
subject to ΘTΘ=IK. HereVis ap×Kmatrix with columns vkandΘ
is alsop×K.
Criterion (14.71) is not jointly convex in VandΘ, but it is convex in
each parameter with the other parameter ﬁxed7. Minimization over Vwith
Θﬁxed is equivalent to Kelastic net problems (Section 18.4) and can be
done eﬃciently. On the other hand, minimization over ΘwithVﬁxed is a
version of the Procrustes problem (14.56), and is solved by a simple SVD
calculation (Exercise 14.12). These steps are alternated u ntil convergence.
Figure 14.31 shows an example of sparse principal component s analysis
using (14.71), taken from Sj¨ ostrand et al. (2007). Here the shape of the
mid-sagittal cross-section of the corpus callosum (CC) is r elated to various
clinical parameters in a study involving 569 elderly person s8. In this exam-
7Note that the usual principal component criterion, for example (14. 50), is not jointly
convex in the parameters either. Nevertheless, the solution is well deﬁned and an eﬃcient
algorithm is available.
8We thank Rasmus Larsen and Karl Sj¨ ostrand for suggesting this appli cation, and
supplying us with the postscript ﬁgures reproduced here.

552 14. Unsupervised Learning
FIGURE 14.32. An example of a mid-saggital brain slice, with the corpus col-
losum annotated with landmarks.
ple PCA is applied to shapedata, and is a popular tool in morphometrics.
For such applications, a number of landmarks are identiﬁed a long the cir-
cumference of the shape; an example is given in Figure 14.32. These are
aligned by Procrustes analysis to allow for rotations, and i n this case scal-
ing as well (see Section 14.5.1). The features used for PCA ar e the sequence
of coordinate pairs for each landmark, unpacked into a singl e vector.
In this analysis, both standard and sparse principal compon ents were
computed, and components that were signiﬁcantly associate d with various
clinical parameters were identiﬁed. In the ﬁgure, the shape variations cor-
responding to signiﬁcant principal components (red curves ) are overlaid on
the mean CC shape (black curves). Low walking speed relates t o CCs that
are thinner (displaying atrophy) in regions connecting the motor control
and cognitive centers of the brain. Low verbal ﬂuency relate s to CCs that
are thinner in regions connecting auditory/visual/cognit ive centers. The
sparse principal components procedure gives a more parsimo nious, and po-
tentially more informative picture of the important diﬀere nces.

14.6 Non-negative Matrix Factorization 553
14.6 Non-negative Matrix Factorization
Non-negative matrix factorization (Lee and Seung, 1999) is a recent al-
ternative approach to principal components analysis, in wh ich the data
and components are assumed to be non-negative. It is useful f or modeling
non-negative data such as images.
TheN×pdata matrix Xis approximated by
X≈WH (14.72)
whereWisN×randHisr×p,r≤max(N,p). We assume that
xij,wik,hkj≥0.
The matrices WandHare found by maximizing
L(W,H) =N/summationdisplay
i=1p/summationdisplay
j=1[xijlog(WH)ij−(WH)ij]. (14.73)
This is the log-likelihood from a model in which xijhas a Poisson dis-
tribution with mean ( WH)ij—quite reasonable for positive data.
The following alternating algorithm (Lee and Seung, 2001) c onverges to
a local maximum of L(W,H):
wik←wik/summationtextp
j=1hkjxij/(WH)ij/summationtextp
j=1hkj
hkj←hkj/summationtextN
i=1wikxij/(WH)ij/summationtextN
i=1wik(14.74)
This algorithm can be derived as a minorization procedure fo r maximizing
L(W,H) (Exercise 14.23) and is also related to the iterative-prop ortional-
scaling algorithm for log-linear models (Exercise 14.24).
Figure 14.33 shows an example taken from Lee and Seung (1999)9, com-
paring non-negative matrix factorization (NMF), vector qu antization (VQ,
equivalentto k-meansclustering)andprincipalcomponentsanalysis(PCA ).
The three learning methods were applied to a database of N= 2,429 fa-
cial images, each consisting of 19 ×19 pixels, resulting in a 2 ,429×381
matrixX. As shown in the 7 ×7 array of montages (each a 19 ×19 image),
each method has learned a set of r= 49 basis images. Positive values are
illustrated with black pixels and negative values with red p ixels. A par-
ticular instance of a face, shown at top right, is approximat ed by a linear
superposition of basis images. The coeﬃcients of the linear superposition
are shown next to each montage, in a 7 ×7 array10, and the resulting su-
perpositions are shown to the right of the equality sign. The authors point
9We thank Sebastian Seung for providing this image.
10These 7 ×7 arrangements allow for a compact display, and have no structural
signiﬁcance.

554 14. Unsupervised Learning
out that unlike VQ and PCA, NMF learns to represent faces with a set of
basis images resembling parts of faces.
Donoho and Stodden (2004) point out a potentially serious pr oblem with
non-negativematrixfactorization.Eveninsituationswhe reX=WHholds
exactly, the decomposition may not be unique. Figure 14.34 i llustrates the
problem. The data points lie in p= 2 dimensions, and there is “open space”
between the data and the coordinate axes. We can choose the ba sis vectors
h1andh2anywhere in this open space, and represent each data point
exactly with a nonnegative linear combination of these vect ors. This non-
uniqueness means that the solution found by the above algori thm depends
on the starting values, and it would seem to hamper the interp retability of
the factorization. Despite this interpretational drawbac k, the non-negative
matrix factorization and its applications has attracted a l ot of interest.
14.6.1 Archetypal Analysis
This method, due to Cutler and Breiman (1994), approximates data points
by prototypes that are themselves linear combinations of da ta points. In
this sense it has a similar ﬂavor to K-means clustering. However, rather
than approximating each data point by a single nearby protot ype, archety-
pal analysis approximates each data point by a convex combin ation of a
collection of prototypes. The use of a convex combination fo rces the proto-
typestolieontheconvexhullofthedatacloud.Inthissense ,theprototypes
are “pure,”, or “archetypal.”
As in (14.72), the N×pdata matrix Xis modeled as
X≈WH (14.75)
whereWisN×randHisr×p. We assume that wik≥0 and/summationtextr
k=1wik=
1∀i. Hence the Ndata points (rows of X) inp-dimensional space are
represented by convex combinations of the rarchetypes (rows of H). We
also assume that
H=BX (14.76)
whereBisr×Nwithbki≥0 and/summationtextN
i=1bki= 1∀k. Thus the archetypes
themselves are convex combinations of the data points. Usin g both (14.75)
and (14.76) we minimize
J(W,B) =||X−WH||2
=||X−WBX||2(14.77)
over the weights WandB. This function is minimized in an alternating
fashion, with each separate minimization involving a conve x optimization.
The overall problem is not convex however, and so the algorit hm converges
to a local minimum of the criterion.

14.6 Non-negative Matrix Factorization 555
VQ
× =NMF
= ×
PCA
= ×
Original
FIGURE 14.33. Non-negative matrix factorization (NMF), vector quantiza tion
(VQ, equivalent to k-means clustering) and principal components analysis (PCA)
applied to a database of facial images. Details are given in the text. Unlike VQ
and PCA, NMF learns to represent faces with a set of basis image s resembling
parts of faces.

556 14. Unsupervised Learning
h1
h2
FIGURE 14.34. Non-uniqueness of the non-negative matrix factorization.
There are 11 data points in two dimensions. Any choice of the b asis vectors h1
andh2in the open space between the coordinate axes and data, gives an exact
reconstruction of the data.
Figure 14.35 shows an example with simulated data in two dime nsions.
The top panel displays the results of archetypal analysis, w hile the bottom
panel shows the results from K-means clustering. In order to best recon-
struct the data from convexcombinations of the prototypes, it pays to
locate the prototypes on the convex hull of the data. This is s een in the top
panels of Figure 14.35 and is the case in general, as proven by Cutler and
Breiman (1994). K-means clustering, shown in the bottom panels, chooses
prototypes in the middle of the data cloud.
We can think of K-means clustering as a special case of the archetypal
model, in which each row of Whas a single one and the rest of the entries
are zero.
Notice also that the archetypal model (14.75) has the same ge neral form
as the non-negative matrix factorization model (14.72). Ho wever, the two
models are applied in diﬀerent settings, and have somewhat d iﬀerent goals.
Non-negative matrix factorization aims to approximate the columns of the
data matrix X, and the main output of interest are the columns of W
representing the primary non-negative components in the da ta. Archetypal
analysis focuses instead on the approximation of the rows of Xusing the
rowsofH,whichrepresentthearchetypaldatapoints.Non-negative matrix
factorization also assumes that r≤p. Withr=p, we can get an exact
reconstruction simply choosing Wto be the data Xwith columns scaled
so that they sum to 1. In contrast, archetypal analysis requi resr≤N,
but allows r > p. In Figure 14.35, for example, p= 2,N= 50 while
r= 2,4 or 8. The additional constraint (14.76) implies that the ar chetypal
approximation will not be perfect, even if r>p.
Figure 14.36 shows the results of archetypal analysis appli ed to the
database of 3’s displayed in Figure 14.22. The three rows in F igure 14.36
are the resulting archetypes from three runs, specifying tw o, three and four

14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 557
2 Prototypes 4 Prototypes 8 Prototypes
FIGURE 14.35. Archetypal analysis (top panels) and K-means clustering (bot-
tom panels) applied to 50data points drawn from a bivariate Gaussian distribu-
tion. The colored points show the positions of the prototypes in each case.
archetypes, respectively. As expected, the algorithm has p roduced extreme
3’s both in size and shape.
14.7 Independent Component Analysis and
Exploratory Projection Pursuit
Multivariate data are often viewed as multiple indirect mea surements aris-
ingfromanunderlyingsource,whichtypicallycannotbedir ectlymeasured.
Examples include the following:
•Educational and psychological tests use the answers to ques tionnaires
to measure the underlying intelligence and other mental abi lities of
subjects.
•EEG brain scans measure the neuronal activity in various par ts of
the brain indirectly via electromagnetic signals recorded at sensors
placed at various positions on the head.
•The trading prices of stocks change constantly over time, an d reﬂect
various unmeasured factors such as market conﬁdence, exter nal in-

558 14. Unsupervised Learning
FIGURE 14.36. Archetypal analysis applied to the database of digitized 3’s. The
rows in the ﬁgure show the resulting archetypes from three run s, specifying two,
three and four archetypes, respectively.
ﬂuences, and other driving forces that may be hard to identif y or
measure.
Factor analysis is a classical technique developed in the st atistical liter-
ature that aims to identify these latent sources. Factor ana lysis models
are typically wed to Gaussian distributions, which has to so me extent hin-
dered their usefulness. More recently, independent compon ent analysis has
emerged as a strong competitor to factor analysis, and as we w ill see, relies
on the non-Gaussian nature of the underlying sources for its success.
14.7.1 Latent Variables and Factor Analysis
Thesingular-valuedecomposition X=UDVT(14.54) hasalatentvariable
representation. Writing S=√
NUandAT=DVT/√
N, we have X=
SAT, and hence each of the columns of Xis a linear combination of the
columns of S. Now since Uis orthogonal, and assuming as before that the
columns of X(and hence U) each have mean zero, this implies that the
columns of Shave zero mean, are uncorrelated and have unit variance. In
terms of random variables, we can interpret the SVD, or the co rresponding
principal component analysis (PCA) as an estimate of a laten t variable
model

14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 559
X1=a11S1+a12S2+···+a1pSp
X2=a21S1+a22S2+···+a2pSp
......
Xp=ap1S1+ap2S2+···+appSp,(14.78)
or simplyX=AS. The correlated Xjare each represented as a linear
expansion in the uncorrelated, unit variance variables Sℓ. This is not too
satisfactory, though, because given any orthogonal p×pmatrixR, we can
write
X=AS
=ARTRS
=A∗S∗, (14.79)
and Cov(S∗) =RCov(S)RT=I. Hence there are many such decom-
positions, and it is therefore impossible to identify any pa rticular latent
variables as unique underlying sources. The SVD decomposit ion does have
the property that any rank q < ptruncated decomposition approximates
Xin an optimal way.
Theclassical factor analysis model,developedprimarilybyresearchersin
psychometrics, alleviates these problems to some extent; s ee, for example,
Mardia et al. (1979). With q<p, a factor analysis model has the form
X1=a11S1+···+a1qSq+ε1
X2=a21S1+···+a2qSq+ε2
......
Xp=ap1S1+···+apqSq+εp,(14.80)
orX=AS+ε. HereSis a vector of q <punderlying latent variables or
factors,Ais ap×qmatrix of factor loadings, and theεjare uncorrelated
zero-mean disturbances. The idea is that the latent variabl esSℓare com-
mon sources of variation amongst the Xj, and account for their correlation
structure, while the uncorrelated εjare unique to each Xjand pick up the
remaining unaccounted variation. Typically the Sℓand theεjare modeled
as Gaussian random variables, and the model is ﬁt by maximum l ikelihood.
The parameters all reside in the covariance matrix
Σ=AAT+Dε, (14.81)
whereDε= diag[Var( ε1),...,Var(εp)]. TheSℓbeing Gaussian and un-
correlated makes them statistically independent random va riables. Thus a
battery of educational test scores would be thought to be dri ven by the
independent underlying factors such as intelligence ,driveand so on. The
columns of Aare referred to as the factor loadings , and are used to name
and interpret the factors.

560 14. Unsupervised Learning
Unfortunately the identiﬁability issue (14.79) remains, s inceAandART
are equivalent in (14.81) for any q×qorthogonal R. This leaves a certain
subjectivity in the use of factor analysis, since the user ca n search for ro-
tated versions of the factors that are more easily interpret able. This aspect
has left many analysts skeptical of factor analysis, and may account for its
lack of popularity in contemporary statistics. Although we will not go into
details here, the SVD plays a key role in the estimation of (14 .81). For ex-
ample, if the Var( εj) are all assumed to be equal, the leading qcomponents
of the SVD identify the subspace determined by A.
Because of the separate disturbances εjfor eachXj, factor analysis can
be seen to be modeling the correlation structure of the Xjrather than the
covariance structure. This can be easily seen by standardiz ing the covari-
ance structure in (14.81) (Exercise 14.14). This is an impor tant distinction
between factor analysis and PCA, although not central to the discussion
here. Exercise 14.15 discusses a simple example where the so lutions from
factor analysis and PCA diﬀer dramatically because of this d istinction.
14.7.2 Independent Component Analysis
The independent component analysis (ICA) model has exactly the same
form as (14.78), except the Sℓare assumed to be statistically indepen-
dentrather than uncorrelated. Intuitively, lack of correlatio n determines
the second-degree cross-moments (covariances) of a multiv ariate distribu-
tion, while in general statistical independence determine s all of the cross-
moments. These extra moment conditions allow us to identify the elements
ofAuniquely. Since the multivariate Gaussian distribution is determined
by its second moments alone, it is the exception, and any Gaus sian inde-
pendent components can be determined only up to a rotation, a s before.
Hence identiﬁability problems in (14.78) and (14.80) can be avoided if we
assume that the Sℓare independent and non-Gaussian .
Here we will discuss the full p-component model as in (14.78), where the
Sℓare independent with unit variance; ICA versions of the fact or analysis
model (14.80) exist as well. Our treatment is based on the sur vey article
by Hyv¨ arinen and Oja (2000).
We wish to recover the mixing matrix AinX=AS. Without loss
of generality, we can assume that Xhas already been whitened to have
Cov(X) =I; this is typically achieved via the SVD described above. Thi s
in turn implies that Ais orthogonal, since Salso has covariance I. So
solving the ICA problem amounts to ﬁnding an orthogonal Asuch that
the components of the vector random variable S=ATXare independent
(and non-Gaussian).
Figure 14.37 shows the power of ICA in separating two mixed si gnals.
This is an example of the classical cocktail party problem , where diﬀer-
ent microphones Xjpick up mixtures of diﬀerent independent sources Sℓ
(music, speech from diﬀerent speakers, etc.). ICA is able to perform blind

14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 561
Source Signals Measured Signals
PCA Solution ICA Solution
FIGURE 14.37. Illustration of ICA vs. PCA on artiﬁcial time-series data. Th e
upper left panel shows the two source signals, measured at 1000uniformly spaced
time points. The upper right panel shows the observed mixed s ignals. The lower
two panels show the principal components and independent com ponent solutions.
source separation , by exploiting the independence and non-Gaussianity of
the original sources.
Many of the popular approaches to ICA are based on entropy. Th e dif-
ferential entropy Hof a random variable Ywith density g(y) is given by
H(Y) =−/integraldisplay
g(y)logg(y)dy. (14.82)
A well-known result in information theory says that among al l random
variables with equal variance, Gaussian variables have the maximum en-
tropy. Finally, the mutual information I(Y) between the components of the
random vector Yis a natural measure of dependence:
I(Y) =p/summationdisplay
j=1H(Yj)−H(Y). (14.83)
The quantity I(Y) is called the Kullback–Leibler distance between the
densityg(y) ofYand its independence version/producttextp
j=1gj(yj), wheregj(yj)
is the marginal density of Yj. Now ifXhas covariance I, andY=ATX
withAorthogonal, then it is easy to show that
I(Y) =p/summationdisplay
j=1H(Yj)−H(X)−log|detA| (14.84)
=p/summationdisplay
j=1H(Yj)−H(X). (14.85)
Finding an Ato minimize I(Y) =I(ATX) looks for the orthogonal trans-
formation that leads to the most independence between its co mponents. In

562 14. Unsupervised Learning
*
**
**
**
**
**
**
*
*
**
**
**
*
**
**
****
***
*
**
*
**
*
*
**
** ***
**
**
**
**
**
**
***
*
***
*****
**
**
*
**
**
*
**
*
***
*****
**
* **
*
***
* **
***
** **
**
****
*
******
***
*
**
*
* ***
***
**
**
**
**
**
***
*
***
** *
**
***
****
** *
**
*
**
**
*
**
**
*
* **
***
***
******
***
**
**
******
*
*****
***
**
*
*****
**
***
*
***
*
***
**
***
*
***
*
**
*
*
**
*
***
**
*
**
****
***
*
*
**
***
**
**
***
***
*
****
**
***
*
**
**
*
*
*
****
**
***
**
**
***
*
***
****
**
**
** *
***
**
* ***
***
***
*
**
**
****
**
*
*
***
*
***
**
**
**
***
* **
**
**
*
***
*
*
***
*
***
**
****
*
***
*
**
*
**
****
**
*
***
*****
**
**
** **
**
***
*** **
***
***
***
***
**
**
**
*
** **
*
**
**
**
**
*
**
*
***
***
***
**Source S
**
*
**
**
****
**
*
*****
****
*
**
***
***
**
***
*
**
*
**
****
*
**
*
***
*
***
**
***
*
***
***
****
****
*
**
*
**
*****
*
*
****
***
*
*
***
**
****
**
***
***
*
*
***
*
**
*
**
**
***
***
***
****
****
**
***
**
**
**
***
***
****
***
**
**
* **
*
****
***
***
*
***
*
**
***
***
**
**
**
****
**
*
****
*
*
**
**
**
***
*
***
**
**
****
*
*
***
****
*
***
****
*
***
**
**
*
**
*
***
**
**
*****
*
***
*
****
******
*
**
**
**
*
*
**
****
***
**
****
***
**
**
****
*
***
***
***
**
*
***
***
***
*
***
****
*
***
**
******
*
*****
**
*
*
**
*
*
**
*
***
**
***
***
*
**
**
**
***
*
******
****
*
**
***
****
*
**
**
***
* *
**
***
****
******
**
***
*
**
**
***
****
*
**
**
**
**
***
****
*
**
*
**
**Data X
*
***
***
*
*
***
***
**
**
***
**
*** ***
*
**
* **
***
*
***
**
***
****
**
**
***
***
**
***
*
****
****
*
** *
***
**
**
*****
*
***
***
***
***
***
**
**
*
*
*****
*
*
****
**
*
*
**
*
**
****
***
***
*
***
*
**
**
****
**
**
**
**
*** **
*
***
**
***
**
***
*
**
*
**
* *
*
**
**
**
**
**
***
***
****
****
*
***
***
**
**
*
**
*
***
**
**
*
*
***
*
** *
***
***
**
**
*
***
**
**
*
**
**
* *
**
** *
***
**
**
*
***
**
*
****
**
**
*
*
**
**
**
*******
** *
**
***
***
***
**
**
**
*
* **
**
***
*
***
*
*******
**
**
**
**
**
*
*
** *
*
*****
**
**
*
**
***
**
***
**
**
** ***
****
*
****
*
*
***
**
*
****
*
***
**
***
****
*
*
*
***
**
*****
***
*****
*
***
**
*
**
*
*****
**
*
*****
**
*
*
***
***
***
***
***
**PCA Solution
*
**
**
**
**
**
* *
*
*
**
**
**
*
**
**
****
***
*
**
*
**
*
*
**
*****
**
**
**
**
**
**
***
*
***
*** **
**
**
*
**
**
*
**
*
***
**** *
**
*
**
*
***
* **
***
** **
**
****
*
** ****
***
*
**
*
** **
***
**
**
*
*
**
**
**
*
*
***
** *
**
***
****
***
**
*
**
**
*
**
**
*
* **
***
***
*** ***
***
**
**
******
*
** ***
***
***
*****
**
***
*
***
*
***
**
***
*
***
*
**
*
*
**
*
***
**
*
**
* ***
***
*
*
**
***
**
**
***
***
*
****
**
***
*
**
**
*
*
*
****
**
***
**
**
***
*
***
****
**
**
** *
***
* *
****
***
***
*
**
**
*** *
**
*
*
***
*
***
**
**
**
***
* **
**
**
*
***
*
* ***
*
*****
****
*
***
*
**
*
**
****
**
*
***
*
****
**
**
****
**
***
* ** **
***
***
** *
*
**
**
**
**
*
****
*
**
**
**
**
*
**
*
***
***
***
**ICA Solution
FIGURE 14.38. Mixtures of independent uniform random variables. The upper
left panel shows 500realizations from the two independent uniform sources, the
upper right panel their mixed versions. The lower two panels sh ow the PCA and
ICA solutions, respectively.
light of (14.84) this is equivalent to minimizing the sum of t he entropies of
the separate components of Y, which in turn amounts to maximizing their
departures from Gaussianity.
For convenience, rather than using the entropy H(Yj), Hyv¨ arinen and
Oja (2000) use the negentropy measureJ(Yj) deﬁned by
J(Yj) =H(Zj)−H(Yj), (14.86)
whereZjis a Gaussian random variable with the same variance as Yj. Ne-
gentropy is non-negative, and measures the departure of Yjfrom Gaussian-
ity. They propose simple approximations to negentropy whic h can be com-
puted and optimized on data. The ICA solutions shown in Figur es 14.37–
14.39 use the approximation
J(Yj)≈[EG(Yj)−EG(Zj)]2, (14.87)
whereG(u) =1
alogcosh(au) for 1≤a≤2. When applied to a sample
ofxi, the expectations are replaced by data averages. This is one of the
options in the FastICA software provided by these authors. More classical
(andlessrobust)measuresarebasedonfourthmoments,andh encelookfor
departures from the Gaussian via kurtosis. See Hyv¨ arinen a nd Oja (2000)
for more details. In Section 14.7.4 we describe their approx imate Newton
algorithm for ﬁnding the optimal directions.
In summary then, ICA applied to multivariate data looks for a sequence
of orthogonal projections such that the projected data look as far from

14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 563
Component
 1oooooo o
ooo
oo
oo
oo
o
oooooo
oo o
oooo ooo
ooooooo
oo
o
oo
oooooo
oo
ooo ooooo
ooooooooo
ooooooooooo
oooo
oo o
oo
oooooooooo
oo
o
oooooooo
ooo
oooooooooo
oooooo
oooooo
oooooo
oo
ooo
ooo ooo
ooooo
o
oo
ooooooooooo
oooo
ooooo
oooooooo
ooo
oo
ooooo
oo
ooo
ooooo
ooooooo
ooo
oooo
oo
oooo
oooooo
oo
ooo
oooo
oooooooooooooo
oooooooooooo
oooooo
ooo
ooo
oo
oo
ooooo
ooooooo
ooo
oooo
ooooo
oo
oooo
ooo ooo
ooooo
oooooooo
ooooooooooo
ooo
ooo
oooo
oo
oo
ooooo
ooooo
ooo
oo
oo
oo
ooooooooo
ooooooo
ooooo
oo
oooo ooooo
oo
o
oooo
ooo
oo
oooooooo
ooo
oo
ooo
ooo
oooo
ooo
oooo
ooo
oooooooooo
ooooo
ooo
ooooo
o
oo
ooo
oo
o
oooooooo
ooo
oooooo oooooooo
ooooooo
ooooo oo
oo
oooo
oooo
ooooo o
oooooooo
ooooo
oo
ooooooo
ooo
oooo
oooo
o
ooo
oo ooo o
ooo
ooooo
ooo
ooooo
oooooooo
oo
o
ooo
oo ooooooooo
ooooooo
oo
oo
oo
ooo oo
o
ooooo
oo
oooo
oooooooo
ooo
oo
oo
oo
o
oooooo
ooo
ooooooo
ooooooo
oo
o
oo
oooooo
oo
ooo ooooo
ooooooooo
ooooooooooo
oooo
oo o
oo
oooooooooo
oo
o
oooooooo
ooo
oooooooooo
oooooo
oooooo
oooooo
oo
ooo
ooo ooo
ooooo
o
oo
ooooooooooo
oooo
oooo o
oooooooo
ooo
oo
ooooo
oo
ooo
ooooo
ooooooo
ooo
oooo
oo
oooo
oooooo
oo
ooo
oooo
oooooo oooooooo
oooooooooooo
oooooo
ooo
ooo
oo
oo
ooooo
ooooooo
ooo
oooo
ooooo
oo
oooo
oooooo
ooooo
oooooooo
ooooooooooo
ooo
ooo
oooo
oo
oo
ooooo
ooooo
ooo
oo
oo
oo
ooooooooo
ooooooo
ooooo
oo
ooooooooo
oo
o
oooo
ooo
oo
oooooooo
ooo
oo
ooo
ooo
oooo
ooo
oooo
oo o
oooooooooo
ooooo
ooo
ooooo
o
oo
ooo
oo
o
oooooooo
ooo
oooo o o oooooooo
ooooooo
ooooo oo
oo
oooo
oooo
ooooo o
oooooooo
ooooo
oo
ooooooo
ooo
oooo
oooo
o
ooo
oo oooo
o oo
ooooo
ooo
ooooo
oooooooo
oo
o
ooo
oo ooooooooo
ooooooo
oo
oo
oo
ooooo
o
ooooo
oo
oooo
ooooooo o
ooo
oo
oo
oo
o
oooooo
oo o
ooooooo
ooooooo
oo
o
oo
oooooo
oo
oooooooo
ooooooooo
ooooooooooo
oooo
oo o
oo
ooooo o oooo
oo
o
oooooooo
ooo
oooo oooooo
oooooo
oooooo
oooooo
oo
ooo
ooo ooo
ooooo
o
oo
oooooooo ooo
oooo
ooooo
oooooooo
ooo
oo
ooooo
oo
ooo
ooooo
ooooooo
ooo
oooo
oo
oooo
oooooo
oo
ooo
oooo
oooooo oooooooo
oooooooooooo
oooooo
ooo
ooo
oo
oo
ooooo
ooooooo
ooo
oooo
ooooo
oo
oooo
ooo ooo
ooooo
oooooooo
ooo oooooooo
ooo
ooo
oooo
oo
oo
ooooo
ooooo
ooo
oo
oo
oo
ooooooooo
ooooooo
ooooo
oo
oooo ooooo
oo
o
oooo
ooo
oo
oooooooo
ooo
oo
ooo
ooo
oooo
ooo
oooo
ooo
oooooooooo
ooooo
ooo
ooooo
o
oo
ooo
oo
o
oooooooo
ooo
ooooo o oooooooo
ooooooo
ooooo oo
oo
oooo
oooo
oooooo
oooooooo
ooooo
oo
ooooooo
ooo
oooo
o ooo
o
ooo
oo ooo o
ooo
ooooo
ooo
ooooo
oooooooo
oo
o
ooo
oo ooooooooo
ooooooo
oo
oo
oo
ooooo
o
ooooo
oo
oooo
ooooooo o
ooo
oo
oo
oo
o
oooooo
oo o
oooo ooo
ooooooo
oo
o
oo
oooooo
oo
ooo ooooo
ooooooooo
ooooooooooo
oooo
oo o
oo
oooooooooo
oo
o
oooooooo
ooo
oooo oooooo
oooo oo
oooooo
ooo ooo
oo
ooo
ooo ooo
ooooo
o
oo
ooooooooooo
oooo
ooooo
oooooooo
ooo
oo
ooooo
oo
ooo
ooooo
ooooooo
ooo
oooo
oo
oooo
oooooo
oo
ooo
oooo
ooo ooo oooooooo
oooooooooooo
oooooo
ooo
ooo
oo
oo
ooooo
ooooooo
ooo
oooo
ooooo
oo
oooo
ooo ooo
ooooo
oooooooo
ooooooooooo
ooo
ooo
oooo
oo
oo
ooooo
ooooo
ooo
oo
oo
oo
ooooooooo
ooooooo
ooooo
oo
oooo ooooo
oo
o
oooo
ooo
oo
oooooooo
ooo
oo
ooo
ooo
o ooo
oo o
oooo
ooo
oooooooooo
ooooo
ooo
ooooo
o
oo
ooo
oo
o
oooooooo
ooo
oooooooooooooo
ooooooo
ooooo oo
oo
oooo
o ooo
oooooo
oooooooo
ooooo
oo
ooooooo
ooo
oooo
oooo
o
ooo
oo oooo
ooo
ooooo
ooo
ooooo
oooooooo
oo
o
oo o
oo ooooooooo
ooooooo
oo
oo
oo
ooooo
o
ooooo
oo
oooo
o
oo
oo
oo
oo
ooooo
oo
ooooo
oooo
ooo
oooo
oo
oooo
ooo
ooo
oo
oo
ooooo
ooo
oo
oooo
ooo
ooooo
oo
o
oo
oooooo
o
oo
oo oo
o
oo
oooooo
o
ooo
oooo
ooooo
o
ooo
o
oo
oo
oo
oo
ooo
oooo
ooo
oo
ooooo
o
oooo
oo
ooo
ooo
o
ooo
ooo
oo
oo
ooo
oo
ooooo oooo
oo
ooo
ooo
ooo
ooo
ooo
ooooo
o
oo
ooo
oo
oo
o
oo oo
oooo
oooo
ooo
oo
o
ooo
ooo
o
o
o
ooo
o
ooo
ooooo
oooo
oo
oooo o
oo
oo
oooooo
oo
oo
oo
ooooo
oooo
oooo
ooo
oo
oooo
oo
o
ooo
ooo
ooo
oo
oooo
oooo
ooooo
oo
oooooo
o
ooooo
ooooo
ooooo
oooo
oo
oooo
oooo
o
o
oooo oooooo
ooo
o
oooo
ooo
oo
oo o
oooo
ooo
oo
oo
o
oo
o
oo
ooo
ooo
oooo
oooo
ooooo
oooo
ooo
o
ooo
o
oo
oo
ooo
o
oo
oooo
oo
oo
oooo
oo
o
ooo
oo
o
oooo
oo
oo
oo
oo
oo
oo
oo
oooo
o
oooo
oooo
ooo
ooo
ooo
oooooo
oooooo
oo
ooo
oooo
o
ooooooo
oo
oo
o
ooo
oo
oooooo
o
o
ooooo
ooo
oo
oooo
ooooo
oo
ooooooo
oooo
ooo
oo ooo
ooo
ooo
oooo
ooooo
ooo
ooo
ooo
ooo
oo
ooo
ooooo
oo
o
oo
oo
ooo
oooooo
oo
o
oo
ooo
Component
 2
oo oo
ooooo
ooo
oooo
ooooooo
oo
oooo
oooo
oooo
oooo
oooo ooo ooooooo
ooooooooooo
ooo
oo
oooo
oo
oooo
oo oooo o
oo
oooooooooo
ooo
o
oo
ooooooooooooooo
o o
oo o
ooooo
oooo
ooo
oooooo
o
oo
oo
ooo
o
o oooooooooooo
oooo
o
ooo
oo
ooooo oo
oo
o
ooo
oooo
ooo
oo
oo
o
oooo
oooooo
ooooo
oooo
oooooo
ooooo
ooo
oooo
ooooo
ooooo
oooo
o oooo
ooo
ooooooo
oo
o
oooo
ooooo
oo
oo
ooo
oo
oooo
oo
oo
ooo ooooo
oooooo
oo
ooooo
ooooo
ooo
oo
ooo
ooooo
oooooo
ooooo
ooooo
oooooo
oooo
oooooooo
o
oo oooooo
o
ooooooooo
oooooooo
oooo
ooo
oo
oo
oooo
ooo
ooo
o
ooo
oo
ooo
ooo
oo
oooooo
oo
ooooo
ooooo
oooo oooo
oo
oo
ooooo
oo
oooooooo
oooo
oo
oooo
o
oo
o oooooo
oooooooo
ooooooooooo
oo
oooo
o
ooooo
ooo
o
oooooo
oooooo
ooo
ooo
oo oooooo ooo
ooo
o
ooo
ooo
oooooo
oo
oooo
ooo
oo oooo
ooo
oooooo
oooo
ooooo
oooooo
o oo
ooooo
oooo
o
oooo
oo
oo
ooooo
oo
ooooo oooo
o
o
oo
oooo oo
ooooo
ooo
oooo
ooooooo
oo
oooo
oooo
oooo
oooo
oooo ooo ooooooo
ooooooooooo
ooo
oo
oooo
oo
oooo
oo ooooo
oo
ooooo ooooo
ooo
o
oo
oo ooooooooooooo
o o
oo o
ooooo
oooo
ooo
oooooo
o
oo
oo
ooo
o
o oooooooooooo
oooo
o
ooo
oo
ooooo oo
oo
o
ooo
oooo
ooo
oo
oo
o
oooo
oooooo
ooooo
oooo
oooooo
ooooo
ooo
oooo
ooooo
ooo oo
oooo
ooooo
ooo
ooooooo
oo
o
oooo
ooooo
oo
oo
ooo
oo
oooo
oo
oo
ooo ooooo
oooooo
oo
ooooo
ooooo
ooo
oo
ooo
ooooo
oo o ooo
ooo oo
ooo oo
oooooo
oooo
oooooooo
o
oooooooo
o
ooooooooo
oooooooo
oooo
ooo
oo
oo
oooo
ooo
ooo
o
ooo
oo
ooo
ooo
oo
oooooo
oo
ooooo
o oooo
oooooooo
oo
oo
ooooo
oo
oooooooo
oooo
oo
oooo
o
oo
o oooooo
oooooooo
ooooooooooo
oo
oooo
o
ooooo
ooo
o
oooooo
oooooo
ooo
ooo
oooooooo ooo
ooo
o
ooo
ooo
oooooo
oo
oooo
ooo
oooooo
ooo
oooooo
oooo
ooooo
oooooo
o oo
ooooo
oooo
o
oooo
oo
oo
ooooo
oo
ooooooooo
o
o
oo
oooo oo
ooooo
ooo
oooo
ooooooo
o o
oooo
oooo
oooo
oooo
oooo ooo ooooooo
ooooooooooo
ooo
oo
oooo
oo
oooo
oo oo ooo
oo
oooooooooo
ooo
o
oo
oo ooooooooooooo
o o
oo o
ooooo
oooo
ooo
oo oooo
o
oo
oo
ooo
o
o oooo oooooooo
oooo
o
ooo
oo
ooooo oo
oo
o
ooo
oooo
ooo
oo
oo
o
oooo
oooooo
ooooo
oooo
oooooo
ooooo
ooo
oooo
ooooo
ooooo
oooo
ooooo
ooo
ooooooo
oo
o
oooo
ooooo
oo
oo
ooo
oo
oooo
oo
oo
ooo ooooo
oooooo
oo
ooooo
ooooo
ooo
oo
ooo
ooooo
oooooo
ooo oo
ooo oo
oooooo
oooo
oooooooo
o
oooooooo
o
ooooooooo
oooooooo
oooo
ooo
oo
oo
oooo
ooo
ooo
o
ooo
oo
ooo
ooo
oo
oooooo
oo
ooooo
oo oo o
oooooooo
oo
oo
ooooo
oo
oooooooo
oooo
oo
oooo
o
oo
o oooooo
oooooooo
ooooooooooo
oo
oooo
o
ooooo
ooo
o
ooo ooo
oooooo
ooo
ooo
oooooooo ooo
ooo
o
ooo
ooo
oooooo
oo
oooo
ooo
oooooo
ooo
oooooo
oooo
ooooo
oooooo
o oo
ooooo
oooo
o
oooo
oo
oo
ooooo
oo
ooooooooo
o
o
oo
oo
o
oooooo
ooo
ooo
o
o oo
ooo
o
ooo oooo
oo
oo o
ooooo
oo
oo
o
oooooo
o
ooo
ooo
ooo
oooo
ooo
ooo
ooooo
oo
ooo
oo
ooooooo
o o
oo
o
ooo
oo
oo
oo
o
ooo
ooooo
oo
oo
ooo
o
ooo
ooo o
o oooo
oo
o
oo
oo
o
ooo
o
ooo
oo
oooo
ooo ooo
ooo
o
oo
oo
ooo
ooo o
oo
oo
o
ooo
o
oo
ooooo o
ooo
oo
ooo
o
oo
ooo
ooo
oo
oo
oo
oo
oo
ooooo
o
o
oooo
ooo
oooo
oo
ooo
oo
o
ooooo
ooo
oo
o
o
oo
oo
o
oo
oo
oooooooo
ooooo
ooooo
o
oooo
oo
ooooo
oo
ooooo
oooo
ooo
oo
ooo
o
oo
o
oooo
ooo
oo
oo
oooo
oo
ooo
o
ooo
oo
oooooo
oo
oooo
oo
ooo
oooo
ooooo
oooo
o
oooo
ooooo
oo
o
oooo
oo
oo
o
o
oooo
oo
ooo
o
oo
oo
o
oooooo
o
o
ooooo
ooo
oooo
o
o
ooo
ooo
oooooooooo
oo
ooo
oo
oooooo
ooo
oooo
oo
ooo
ooo
oooooo
oo
o
oooooo
o
oo
o
ooo
o
ooo
o
oo oo
oo
ooo
ooo
o
oo
ooo
ooooo
oo
ooo
o
oooo
o
oooo
oo
o
oo
ooo
ooo
o
oooo o
oo
oo
ooo
oo
ooo
ooo
ooooo
o
ooooooo
oooo
ooo
ooooo
oo
oo
ooooooo
oo o
oo
o
ooo
oo o
oo
o
oo
oooooo
oo
o
ooo
oo
o
ooo
o
oo
oo
o
ooo
o
oooo
oooooo
ooo
ooo
o
ooo
ooo
o
ooooooo
oo
oo o
ooooo
oo
oo
o
ooooo oo
ooo
ooo
ooo
oooo
ooo
ooo
ooooo
oo
ooo
oo
ooooooo
oo
oo
o
ooo
oo
oo
oo
o
ooo
oo ooo
oo
oo
ooo
o
ooo
ooo o
o oooo
oo
o
oo
oo
o
ooo
o
ooo
oo
oooo
oooooo
ooo
o
oo
oo
ooo
oooo
oo
oo
o
ooo
o
oo
ooooo o
ooo
oo
ooo
o
oo
ooo
ooo
oo
oo
oo
oo
oo
ooooo
o
o
oooo
ooo
oooo
oo
ooo
oo
o
ooooo
ooo
oo
o
o
oo
oo
o
oo
oo
oooooo oo
ooooo
ooooo
o
oooo
oo
ooooo
oo
ooooo
oooo
ooo
oo
ooo
o
oo
o
oooo
ooo
oo
oo
oooo
oo
ooo
o
ooo
oo
oooooo
oo
oooo
oo
ooo
oooo
ooooo
oooo
o
o ooo
ooooo
oo
o
oooo
oo
oo
oo
oooo
oo
ooo
o
oo
oo
o
oooooo
o
o
ooo oo
ooo
oooo
o
o
ooo
ooo
oooooooooo
oo
ooo
oo
oooooo
ooo
oooo
oo
ooo
ooo
oooooo
oo
o
oooooo
o
oo
o
ooo
o
ooo
o
oooo
oo
ooo
ooo
o
oo
ooo
ooooo
oo
ooo
o
oooo
o
oooo
oo
o
oo
ooo
ooo
o
ooooo
oo
oo
ooo
o o
ooo
ooo
ooooo
o
ooooooo
oooo
ooo
ooooo
oo
oo
ooooooo
ooo
oo
o
ooo
ooo
oo
o
oo
oooooo
oo
o
ooo
oo
o
ooo
o
oo
oo
o
ooo
o
oooComponent
 3oo
o oooooooo
oooo
oo
oo
oo
ooo
ooo
oo
ooooooo
ooooo
oooo
ooooooo
ooo
ooooo
ooo
ooooo
o ooooooo
ooo
o
oo
o o
ooo
oo
ooooo
o ooo
ooo ooo
ooo
oooooooo
oo
ooo
ooooo
oooooooooo
oo
oooooooo
oo
ooooo o ooooooooo
ooooo
ooooo
ooo
ooo
oo
ooooooo
ooooooo
o
o
oo
oooo
oo
oooo
oo
oo
ooooooo
ooo
oooooo
ooo
ooo
ooo
oooooo
ooo oo
oooo
oo
ooooo
o
oo
oooooooo
oo
oo
oo
oo
oo
oo
ooo
ooooooooo
oo
oooo
oooo
ooo
oooo
ooo
oo
oooooo
o
oooo
oooo
oooooo o
ooo
oooooo
ooo
oooo
o
oo
oo
oo
oooooooooo
o
oooo
ooo
ooooo
oo oo
oo
ooo
ooo
oo
ooooooooo
ooo
ooo
oooooo
ooo
oooooooo
oooooo
o
oooo
oooooooooooo
oo
oo
ooooo ooo o
oo
ooo
oooooooo
oooooooo
oooo
oo
oooooooo
oo
oo
oooooo
oooooooo
oooo
ooo
oooooooo
oooooo
o
oo
o ooo
oooo
o
ooooo
oooo
oooo
ooooo
o ooooooo
oo
ooo ooo
ooooooo
ooooooo
ooooo
oooo
ooooooooo
oooo oo
oooo
oooo
oooooooo
ooo o
ooo
o
ooo
oo
ooooooo
o oooooooo
oooo
oo
oo
oo
ooo
ooo
oo
ooooooo
ooooo
oooo
ooooooo
ooo
ooooo
ooo
ooooo
oooooooo
ooo
o
oo
o o
ooo
oo
ooooo
oooo
ooo ooo
ooo
oooooooo
oo
ooo
ooooo
oooooooooo
oo
oooooooo
oo
ooooooooooo o ooo
ooooo
ooo oo
ooo
ooo
oo
ooooo oo
ooooooo
o
o
oo
oooo
oo
oooo
oo
oo
ooooooo
ooo
oooooo
ooo
ooo
ooo
oooooo
ooooo
oooo
oo
ooooo
o
oo
oooooooo
oo
oo
oo
oo
oo
oo
ooo
ooooooooo
oo
oooo
oooo
ooo
oooo
ooo
oo
oooooo
o
oooo
oooo
ooooooo
ooo
oooooo
ooo
oooo
o
oo
oo
oo
oooooooooo
o
oooo
ooo
ooooo
oooo
oo
ooo
ooo
oo
ooooooooo
ooo
ooo
oooooo
ooo
oooooooo
oooooo
o
oooo
ooooo o oooooo
oo
oo
ooooo ooo o
oo
ooo
oooooooo
oooooooo
oooo
oo
oooooooo
oo
oo
oooooo
oooooooo
oooo
ooo
oooooooo
oooooo
o
oo
o ooo
oooo
o
ooooo
oooo
oooo
ooooo
o ooooooo
oo
ooo ooo
ooooooo
ooooooo
ooooo
oooo
oooo oo ooo
oooo oo
oooo
oooo
oooooooo
oo o o
ooo
o
ooo
oo
ooooo
oooo
oooooooo
oo
oo
oo
o
oooo
oo
oo
o
oooo
oo
ooo
o
oooo
oo
oo
ooo
oo
oo
oooo
o
ooo
oo
o
oo
ooo
oooo
oooo
o
o
ooo
o
oo
ooo
oo
o
ooooo
oo
ooo
ooo
oo
ooo
oooo
oooooo
oooo
oo
oo o
ooo
oo
oo
oo
oo
oo
ooo
oo
ooo
oo
oo
oo
ooo
ooooo
o
oo
oooo
ooo
oo
o
ooo
ooooooooooo
ooo
o
ooo
oooo
o
ooo
ooo
o
ooo
o
oo
ooo
ooo
oooo
ooo
oo o
ooooo
o
oo
ooo
ooo
ooo
ooo
oooooo
oo
o
ooo
o
ooooooooooo
ooo
o
oo
o
oooo
o
ooooooo
oo
oo
oo
ooooo
oo
ooooo
o
oooo
o
oo
oo
ooo
o
ooo oo
oo
ooo
oo
o
oooo
oo
oo
ooo
o
oo
ooooo
oooo
o
oo
o
oooooooo
oo
oooooo
ooo
ooooooo
oooo
o
oo
o
ooo
o
oooo
oooo
oo
oo
oo
o
o
oooo
oo
o
oooo
o oooooo
o
oooo
oo
oo
oooo
ooo
ooo
oo
oooo
ooo
o
ooo
oooo
oo
ooo
ooo
o
oooo
oo
o
oo
o
oooo
ooo
oooo
ooo
ooooooooo
o
ooooo
o
ooo
o
oo
ooo
oooo
ooo
oooo
o
ooooo
o
ooo
ooo
ooo
oo
oooooo
ooo
oo
oo o
ooooooooo
oo
oooo
ooo
oo
ooo
oo
ooo
oo
o
ooo
o
oo
ooooo
oo
oooooo
oooooo
oo
o
oooo
ooo
ooo
ooo
oo
o
ooo
oo
oo
o oooo
oooooooo
oo
oo
oo
o
oooo
oo
oo
o
oooo
oo
ooo
o
oooo
oo
oo
ooo
oo
oo
oooo
o
ooo
oo
o
oo
ooo
oooo
oooo
o
o
ooo
o
oo
ooo
oo
o
ooooo
oo
ooo
ooo
oo
oo
ooooo
oooo oo
oooo
oo
oo o
ooo
oo
oo
oo
oo
oo
ooo
oo
ooo
oo
oo
oo
ooo
o
oooo
o
oo
oooo
ooo
oo
o
o oo
oooo ooooo oo
ooo
o
ooo
oooo
o
ooo
ooo
o
ooo
o
oo
ooo
ooo
oooo
ooo
ooo
ooooo
o
oo
ooo
ooo
ooo
ooo
oooooo
oo
o
ooo
o
ooooooooooo
ooo
o
oo
o
oooo
o
oooo
ooo
oo
oo
oo
ooooo
oo
ooooo
o
oooo
o
oo
oo
ooo
o
ooooo
oo
ooo
oo
o
oooo
oo
oo
ooo
o
oo
ooooo
oooo
o
oo
o
oooooooo
oo
oooooo
ooo
ooooooo
oooo
o
oo
o
ooo
o
oooo
ooo o
oo
oo
oo
o
o
oooo
oo
o
oooo
ooooooo
o
oooo
oo
oo
oooo
ooo
ooo
oo
oooo
ooo
o
ooo
oooo
oo
ooo
ooo
o
oooo
oo
o
oo
o
oooo
ooo
oooo
ooo
ooooooooo
o
ooooo
o
ooo
o
oo
ooo
oooo
ooo
oooo
o
ooooo
o
ooo
ooo
ooo
oo
oooooo
ooo
oo
ooo
ooooooooo
oo
oooo
ooo
oo
ooo
oo
ooo
oo
o
ooo
o
oo
ooooo
oo
ooo ooo
oooooo
oo
o
oooo
ooo
ooo
ooo
oo
o
ooo
oo
oo
ooooo
oooooooo
oo
oo
oo
o
oooo
oo
oo
o
oooo
oo
ooo
o
oooo
oo
oo
ooo
oo
oo
oooo
o
ooo
oo
o
oo
ooo
oooo
oooo
o
o
oo o
o
oo
ooo
oo
o
ooooo
oo
ooo
ooo
oo
ooo
oooo
oooooo
o ooo
oo
oo o
ooo
oo
oo
oo
oo
oo
ooo
oo
ooo
oo
oo
oo
ooo
ooooo
o
oo
oooo
ooo
oo
o
ooo
ooooooooooo
ooo
o
ooo
oooo
o
ooo
ooo
o
ooo
o
oo
ooo
ooo
oooo
ooo
ooo
ooooo
o
oo
ooo
ooo
ooo
ooo
oooooo
oo
o
ooo
o
oooo ooooooo
ooo
o
oo
o
oooo
o
ooooooo
oo
oo
oo
ooooo
oo
ooooo
o
oooo
o
oo
oo
ooo
o
ooooo
oo
ooo
oo
o
oooo
oo
oo
ooo
o
oo
ooooo
oooo
o
oo
o
oooooooo
oo
oooooo
ooo
ooooooo
oooo
o
oo
o
ooo
o
oooo
oooo
oo
oo
oo
o
o
oooo
oo
o
oooo
o oooooo
o
oooo
oo
oo
oooo
ooo
ooo
oo
oooo
ooo
o
ooo
oooo
oo
ooo
ooo
o
oooo
oo
o
oo
o
oooo
ooo
oooo
ooo
ooooooooo
o
ooooo
o
ooo
o
oo
ooo
oooo
ooo
oooo
o
ooooo
o
ooo
ooo
ooo
oo
oo oooo
ooo
oo
oo o
ooooooooo
oo
oooo
ooo
oo
ooo
oo
ooo
oo
o
ooo
o
oo
ooooo
oo
ooo ooo
oooooo
oo
o
oooo
ooo
ooo
ooo
oo
o
ooo
oo
oo
oComponent
 4ooo
ooo
ooo
o
oooo oooooo
oooooo
oooooo
oo
oo
ooo
oo
oo ooo
o
ooo
oo
ooooo
o
ooo
o
oooo
o
o
oooooo ooo
ooooooo
o oooo
ooooo
o
oooooo
oo
oooooooo
o
ooo
oo
oooo
ooo
oo
ooo
oooooo
o
oooo
ooo
oo
oo
o
oo
oo
oooooo o
ooo
ooooo
oo
o ooo
o
oo
ooooooo
ooo
ooo
o
ooo
ooo
oo
oooo
ooooooo oo
ooooooo
o
ooooo
oo
oooo
oooo
oo
o
ooooooo
oo
ooooooooooo
o
oooooo
oo
oooo
ooo o
ooooooo
ooo
ooo
o
oooo
oooo
ooo
oooooooooooo
ooooooooooo
oooo
ooooo
ooo
o
o
o
ooooo
oooooooooo
oooooo
oo o
o
oooo
oooooo
o
oooooooooo
oo
oooooooooo
oo ooooo
ooo oo
oo
o
oooo
ooo
ooo oo
ooooooo
o
ooo
oo
o
o
ooooo
oo
oo
ooo
o
oo
oooo
ooo
ooo
oo
ooo
oo
oo
oo
oo
ooo
ooooooo
oo
oo
oooooooo
ooo
oo
oo
oo
oooooooo
ooo
ooooooo
ooo
ooooooooooo
oooo
oooo
o
oooooooo
oo
oooooo
o
oo
ooooooo
ooo
ooooooooo
ooo
oo
o
oooooo
oooo
oooooo
oo
oo
oo oo o
o
o
oo oooooo
ooo
o
oo
oo
o
oooo
o
oooo
ooooo
oo
o
oo
oo
oo
o
o
oo
oooo
o
o
oo
oo
ooo
ooo
ooo
o ooo
ooooo
o
o
oo
oo
o
oo
oo
o
ooooo
ooooo
oo
oo
ooo
oo
o
ooo
oooo
oooo
oooo
oooo
o
oo
o
o
oooooo
ooooooooo
ooo
oo
oo
oo
o
oo
oo
oo
o
oo
o
oo
o
oooo
ooo
oo
oo
o
oo
ooooo
oo
ooo
oo
ooo
o
oooooooooo
oo
o
oooo
oooo
oo
o
ooo
oo
oo
oooooo
ooo
oo
ooo
oooo
ooo
oo
ooooooo
oo
ooo
o
oo
ooooo
o
ooooo
o
oooo
oo
ooo
oooo
oooo
oo
o
o
oo
ooo
ooooo
oo
oo
o
o
ooo
ooo
oo
ooooooooo
o
oooo
o
oo
ooo
oo
oooo
oo oo
oo
oo
oo
oo
oo
oooo
ooo
ooooo
oo
oo
ooo
oo
oooooo
oo
o
ooooo
oo
oooooooooo
oooooo
oo
oooo
o
ooo
oo
o
oooooo
ooo
ooo
ooo
ooo
oo
ooo
ooo
ooooo
oooooo
ooooo
ooo
ooo
oo
ooo
o
ooo
ooo
ooooooo
o
ooooo
oo
o
oo
oo
ooo
oo
oo
ooo
o
oooo
o
ooo
oooooo
o
o
ooooooo
o
oooo
oo
oooo
ooo
ooo
o
ooooo
oooo
oooooo oooooo
oo
oooo
oo
ooooo
ooo
oo
ooo
oo
oooo
ooo
oo
o
oooo
ooooo
o
oo
o
oooo
oo
o
o
ooo
oo
o
ooo
oo
oooo
oo
o
oo
oo
oo
oooooo
oo
oo
oo
ooooo
ooo
oooo
oo
o
oo
o
ooooo
ooooo
o
oo
oooo
o
o
oo
oo
ooo
ooo
ooo
oooo
ooooo
o
o
oo
oo
o
oo
oo
o
ooooo
ooooo
oo
oo
ooo
oo
o
ooo
oooo
oooo
oooo
oooo
o
oo
o
o
oooooo
ooooooooo
ooooo
oo
oo
o
oo
oo
oo
o
oo
o
oo
o
oooo
ooo
oo
oo
o
oo
ooooo
oo
ooo
oo
ooo
o
o
ooooooooo
oo
o
oooo
oooo
oo
o
ooo
oo
oo
oooooo
ooo
oo
ooo
oooo
ooo
oo
ooooooo
oo
ooo
o
oo
ooooo
o
ooooo
o
oooo
oo
ooo
oooo
oooo
oo
o
o
oo
ooo
ooooo
oo
oo
o
o
ooo
ooo
oo
ooooooooo
o
oooo
o
oo
ooo
oo
oooo
oooo
oo
oo
oo
oo
oo
o ooo
ooo
ooooo
oo
oo
ooo
oo
oooooo
oo
o
ooooo
oo
oooooooo oo
oooooo
oo
oooo
o
ooo
oo
o
oooooo
ooo
ooo
ooo
ooo
oo
ooo
ooo
ooooo
oooooo
ooooo
ooo
ooo
oo
ooo
o
ooo
ooo
ooooooo
o
ooooo
oo
o
oo
oo
ooo
oo
oo
ooo
o
oooo
o
ooo
ooo ooo
o
o
ooooooo
o
oooo
oo
oooo
ooo
ooo
o
ooooo
oooo
oooooooooooo
oo
oooo
oo
ooooo
ooo
oo
ooo
oo
ooo o
ooo
oo
o
oooo
ooooo
o
oo
o
oooo
oo
o
o
ooo
oo
o
ooo
oo
oooo
oo
o
oo
oo
oo
oooooo
oo
oo
oo
ooooo
o oo
oooo
oo
o
oo
o
ooooo
ooooo
o
oo
oooo
o
o
oo
oo
ooo
ooo
ooo
oooo
ooooo
o
o
oo
oo
o
oo
oo
o
ooooo
ooooo
oo
oo
ooo
oo
o
ooo
oooo
oooo
o ooo
oooo
o
oo
o
o
oooooo
ooooooo oo
ooo
oo
oo
oo
o
oo
oo
oo
o
oo
o
oo
o
oooo
ooo
oo
oo
o
oo
ooooo
oo
ooo
oo
ooo
o
oooooooooo
oo
o
oooo
oooo
oo
o
ooo
oo
oo
oooooo
ooo
oo
ooo
oooo
ooo
oo
oooo ooo
oo
ooo
o
oo
ooooo
o
ooooo
o
oooo
oo
ooo
oooo
oooo
oo
o
o
oo
ooo
ooooo
oo
oo
o
o
ooo
ooo
oo
ooooooooo
o
oooo
o
oo
ooo
oo
oooo
oooo
oo
oo
oo
oo
oo
oooo
ooo
ooooo
oo
oo
ooo
oo
oooooo
oo
o
ooooo
oo
ooooo ooooo
oooooo
oo
oooo
o
ooo
oo
o
oooooo
ooo
ooo
ooo
ooo
oo
ooo
ooo
ooooo
oooooo
ooooo
ooo
ooo
oo
ooo
o
ooo
ooo
ooooooo
o
ooooo
oo
o
oo
oo
ooo
oo
oo
ooo
o
oooo
o
ooo
oooooo
o
o
ooooooo
o
oooo
oo
oooo
ooo
ooo
o
ooooo
oooo
oooooo oooo oo
oo
oooo
oo
ooooo
ooo
oo
ooo
oo
oooo
ooo
oo
o
oooo
ooooo
o
oo
o
oooo
oo
o
o
ooo
oo
o
ooo
oo
oooo
oo
o
oo
oo
oo
oooooo
oo
oo
oo
ooooo
ooo
oooo
oo
o
oo
o
ooooo
ooooo
o
oo
oooo
o
o
oo
oo
ooo
ooo
ooo
oooo
ooooo
o
o
oo
oo
o
oo
oo
o
ooooo
ooooo
oo
oo
ooo
oo
o
ooo
oooo
oooo
oooo
oooo
o
oo
o
o
oooooo
ooooooooo
ooo
oo
oo
oo
o
oo
oo
oo
o
oo
o
oo
o
oooo
ooo
oo
oo
o
oo
ooooo
oo
ooo
oo
ooo
o
o
ooooooooo
oo
o
oooo
oooo
oo
o
ooo
oo
oo
oooooo
ooo
oo
ooo
oooo
ooo
oo
oooo ooo
oo
ooo
o
oo
ooooo
o
ooooo
o
oooo
oo
ooo
oooo
oooo
oo
o
o
oo
ooo
ooooo
oo
oo
o
o
ooo
ooo
oo
ooooooooo
o
oooo
o
oo
ooo
oo
oooo
oooo
oo
oo
oo
oo
oo
oooo
ooo
ooooo
oo
oo
ooo
oo
oooooo
oo
o
ooooo
oo
oooooooo oo
oooooo
oo
oooo
o
ooo
oo
o
oooooo
ooo
ooo
ooo
ooo
oo
ooo
ooo
ooooo
oooooo
ooooo
ooo
ooo
oo
ooo
o
ooo
ooo
ooooo oo
o
ooooo
oo
o
oo
oo
ooo
oo
oo
ooo
o
oooo
o
ooo
oooooo
o
o
ooooooo
o
oooo
oooooo
ooo
ooo
o
ooooo
oooo
oooo
oo oooooo
oo
oooo
oo
ooooo
ooo
oo
ooo
oo
oooo
ooo
oo
o
oooo
ooooo
o
oo
o
oo
oo
oo
o
o
ooo
oo
o
ooo
oo
oooo
oo
o
oo
oo
oo
oooooo
oooo
oo
ooooo
ooo
oooo
oo
o
oo
o
ooooo
oooo Component
 5PCA ComponentsICA Components
FIGURE 14.39. A comparison of the ﬁrst ﬁve ICA components computed using
FastICA (above diagonal) with the ﬁrst ﬁve PCA components(below diago nal).
Each component is standardized to have unit variance.
Gaussian as possible. With pre-whitened data, this amounts to looking for
components that are as independent as possible.
ICA starts from essentially a factor analysis solution, and looks for rota-
tions that lead to independent components. From this point o f view, ICA is
just another factor rotation method, along with the traditi onal “varimax”
and “quartimax” methods used in psychometrics.
Example: Handwritten Digits
We revisit the handwritten threes analyzed by PCA in Section 14.5.1. Fig-
ure 14.39 compares the ﬁrst ﬁve (standardized) principal co mponents with
the ﬁrst ﬁve ICA components, all shown in the same standardiz ed units.
Note that each plot is a two-dimensional projection from a 25 6-dimensional

564 14. Unsupervised Learning
Mean ICA 1 ICA 2 ICA 3 ICA 4 ICA 5
FIGURE 14.40. The highlighted digits from Figure 14.39. By comparing with
the mean digits, we see the nature of the ICA component.
space. While the PCA components all appear to have joint Gaus sian distri-
butions, the ICA components have long-tailed distribution s. This is not too
surprising, since PCA focuses on variance, while ICA speciﬁ cally looks for
non-Gaussian distributions. All the components have been s tandardized,
so we do not see the decreasing variances of the principal com ponents.
For each ICA component we have highlighted two of the extreme digits,
as well as a pair of central digits and displayed them in Figur e 14.40.
This illustrates the nature of each of the components. For ex ample, ICA
component ﬁve picks up the long sweeping tailed threes.
Example: EEG Time Courses
ICA has become an important tool in the study of brain dynamic s—the
example we present here uses ICA to untangle the components o f signals
in multi-channel electroencephalographic (EEG) data (Ont on and Makeig,
2006).
Subjects wear a cap embedded with a lattice of 100 EEG electro des,
whichrecordbrainactivityatdiﬀerentlocationsonthesca lp.Figure14.4111
(top panel) shows 15 seconds of output from a subset of nine of these elec-
trodes from a subject performing a standard “two-back” lear ning task over
a 30 minute period. The subject is presented with a letter (B, H, J, C, F, or
K) at roughly 1500-ms intervals, and responds by pressing on e of two but-
tons to indicate whether the letter presented is the same or d iﬀerent from
that presented two steps back. Depending on the answer, the s ubject earns
or loses points, and occasionally earns bonus or loses penal ty points. The
time-course data show spatial correlation in the EEG signal s—the signals
of nearby sensors look very similar.
The key assumption here is that signals recorded at each scal p electrode
are a mixture of independent potentials arising from diﬀere nt cortical ac-
11Reprinted from Progress in Brain Research , Vol. 159, Julie Onton and Scott Makeig,
“Information based modeling of event-related brain dynamics,” Page 1 06 , Copyright
(2006), with permission from Elsevier. We thank Julie Onton and Scot t Makeig for
supplying an electronic version of the image.

14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 565
tivities, as well as non-cortical artifact domains; see the reference for a
detailed overview of ICA in this domain.
The lower part of Figure 14.41 shows a selection of ICA compon ents.
The colored images represent the estimated unmixing coeﬃci ent vectors ˆ aj
as heatmap images superimposed on the scalp, indicating the location of
activity. The corresponding time-courses show the activit y of the learned
ICA components.
For example, the subject blinked after each performance fee dback signal
(colored vertical lines), which accounts for the location a nd artifact signal
in IC1 and IC3. IC12 is an artifact associated with the cardia c pulse. IC4
andIC7accountforfrontaltheta-bandactivities,andappe arafterastretch
of correct performance. See Onton and Makeig (2006) for a mor e detailed
discussion of this example, and the use of ICA in EEG modeling .
14.7.3 Exploratory Projection Pursuit
Friedman and Tukey (1974) proposed exploratory projection pursuit, a
graphicalexplorationtechniqueforvisualizinghigh-dim ensionaldata.Their
view was that most low (one- or two-dimensional) projection s of high-
dimensional data look Gaussian. Interesting structure, su ch as clusters or
long tails, would be revealed by non-Gaussian projections. They proposed
a number of projection indices for optimization, each focusing on a diﬀer-
ent departure from Gaussianity. Since their initial propos al, a variety of
improvements have been suggested (Huber, 1985; Friedman, 1 987), and a
variety of indices, including entropy, are implemented in t he interactive
graphics package Xgobi (Swayne et al., 1991, now called GGob i). These
projection indices are exactly of the same form as J(Yj) above, where
Yj=aT
jX, a normalized linear combination of the components of X. In
fact, some of the approximations and substitutions for cros s-entropy coin-
cide with indices proposed for projection pursuit. Typical ly with projection
pursuit, the directions ajare not constrained to be orthogonal. Friedman
(1987) transforms the data to look Gaussian in the chosen pro jection, and
then searches for subsequent directions. Despite their diﬀ erent origins, ICA
and exploratory projection pursuit are quite similar, at le ast in the repre-
sentation described here.
14.7.4 A Direct Approach to ICA
Independent components have by deﬁnition a joint product de nsity
fS(s) =p/productdisplay
j=1fj(sj), (14.88)
so here we present an approach that estimates this density di rectly us-
ing generalized additive models (Section 9.1). Full detail s can be found in

566 14. Unsupervised Learning
FIGURE 14.41. Fifteen seconds of EEG data (of 1917seconds) at nine (of
100) scalp channels (top panel), as well as nine ICA components (lowe r panel).
While nearby electrodes record nearly identical mixtures of br ain and non-brain
activity, ICA components are temporally distinct. The colore d scalps represent the
ICA unmixing coeﬃcients ˆajas a heatmap, showing brain or scalp location of the
source.

14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 567
Hastie and Tibshirani (2003), and the method is implemented in the R
packageProDenICA , available from CRAN.
In the spirit of representing departures from Gaussianity, we represent
eachfjas
fj(sj) =φ(sj)egj(sj), (14.89)
atiltedGaussian density. Here φis the standard Gaussian density, and
gjsatisﬁes the normalization conditions required of a densit y. Assuming
as before that Xis pre-whitened, the log-likelihood for the observed data
X=ASis
ℓ(A,{gj}p
1;X) =N/summationdisplay
i=1p/summationdisplay
j=1/bracketleftbig
logφj(aT
jxi)+gj(aT
jxi/parenrightbig
], (14.90)
which we wish to maximize subject to the constraints that Ais orthogonal
and that the gjresult in densities in (14.89). Without imposing any furthe r
restrictions on gj, the model (14.90) is over-parametrized, so we instead
maximize a regularized version
p/summationdisplay
j=1/bracketleftigg
1
NN/summationdisplay
i=1/bracketleftbig
logφ(aT
jxi)+gj(aT
jxi)/bracketrightbig
−/integraldisplay
φ(t)egj(t)dt−λj/integraldisplay
{g′′′
j(t)}2(t)dt/bracketrightigg
.
(14.91)
We have subtracted two penalty terms (for each j) in (14.91), inspired by
Silverman (1986, Section 5.4.4):
•The ﬁrst enforces the density constraint/integraltext
φ(t)eˆgj(t)dt= 1 on any
solution ˆgj.
•The second is a roughness penalty, which guarantees that the solution
ˆgjis a quartic-spline with knots at the observed values of sij=aT
jxi.
It can further be shown that the solution densities ˆfj=φeˆgjeach have
mean zero and variance one (Exercise 14.18). As we increase λj, these
solutions approach the standard Gaussian φ.
Algorithm 14.3 Product Density ICA Algorithm: ProDenICA
1. Initialize A(random Gaussian matrix followed by orthogonalization).
2. Alternate until convergence of A:
(a) Given A, optimize (14.91) w.r.t. gj(separately for each j).
(b) Givengj, j= 1,...,p, perform one step of a ﬁxed point algo-
rithm towards ﬁnding the optimal A.
We ﬁt the functions gjand directions ajby optimizing (14.91) in an
alternating fashion, as described in Algorithm 14.3.

568 14. Unsupervised Learning
Step 2(a) amounts to a semi-parametric density estimation, which can
be solved using a novel application of generalized additive models. For
convenience we extract one of the pseparate problems,
1
NN/summationdisplay
i=1[logφ(si)+g(si)]−/integraldisplay
φ(t)eg(t)dt−λ/integraldisplay
{g′′′(t)}2(t)dt.(14.92)
Although the second integral in (14.92) leads to a smoothing spline, the
ﬁrst integral is problematic, and requires an approximatio n. We construct
a ﬁne grid of Lvaluess∗
ℓin increments ∆ covering the observed values si,
and count the number of siin the resulting bins:
y∗
ℓ=#si∈(s∗
ℓ−∆/2,s∗
ℓ+∆/2)
N. (14.93)
Typically we pick Lto be 1000, which is more than adequate. We can then
approximate (14.92) by
L/summationdisplay
ℓ=1/braceleftig
y∗
i[log(φ(s∗
ℓ))+g(s∗
ℓ)]−∆φ(s∗
ℓ)eg(s∗
ℓ)/bracerightig
−λ/integraldisplay
g′′′2(s)ds.(14.94)
This last expression can be seen to be proportional to a penal ized Poisson
log-likelihood with response y∗
ℓ/∆ and penalty parameter λ/∆, and mean
µ(s) =φ(s)eg(s). This is a generalized additive spline model (Hastie and
Tibshirani, 1990; Efron and Tibshirani, 1996), with an oﬀsetterm logφ(s),
and can be ﬁt using a Newton algorithm in O(L) operations. Although
a quartic spline is called for, we ﬁnd in practice that a cubic spline is
adequate. We have ptuning parameters λjto set; in practice we make
them all the same, and specify the amount of smoothing via the eﬀective
degrees-of-freedom df( λ). Our software uses 5df as a default value.
Step 2(b) in Algorithm 14.3 requires optimizing (14.91) wit h respect to
A, holding the ˆ gjﬁxed. Only the ﬁrst terms in the sum involve A, and
sinceAis orthogonal, the collection of terms involving φdo not depend on
A(Exercise 14.19). Hence we need to maximize
C(A) =1
Np/summationdisplay
j=1N/summationdisplay
i=1ˆgj(aT
jxi) (14.95)
=p/summationdisplay
j=1Cj(aj)
C(A) is a log-likelihood ratio between the ﬁtted density and a Ga ussian,
and can be seen as an estimate of negentropy (14.86), with eac h ˆgja con-
trastfunctionasin(14.87).Theﬁxedpointupdateinstep2( b)isamodiﬁed
Newton step (Exercise 14.20)

14.7IndependentComponentAnalysisandExploratoryProject ionPursuit 569
1. For each jupdate
aj←E/braceleftbig
Xˆg′
j(aT
jX)−E[ˆg′′
j(aT
jX)]aj/bracerightbig
,(14.96)
whereErepresentsexpectationw.r.tthesample xi.Since ˆgjisaﬁtted
quartic (or cubic) spline, the ﬁrst and second derivatives a re readily
available.
2. Orthogonalize Ausing the symmetric square-root transformation
(AAT)−1
2A. IfA=UDVTis the SVD of A, it is easy to show that
this leads to the update A←UVT.
OurProDenICA algorithm works as well as FastICA on the artiﬁcial time
series data of Figure 14.37, the mixture of uniforms data of F igure 14.38,
and the digit data in Figure 14.39.
Example: Simulations
a b c
d e f
g h i
j k l
m n o
p q r
DistributionAmari Distance from True A
a b c d e f g h i j k l m n o p q r0.01 0.02 0.05 0.10 0.20 0.50FastICA
KernelICA
ProdDenICA
FIGURE 14.42. The left panel shows 18distributions used for comparisons.
These include the “t”, uniform, exponential, mixtures of expo nentials, symmetric
and asymmetric Gaussian mixtures. The right panel shows (on t he log scale)
the average Amari metric for each method and each distributi on, based on 30
simulations in IR2for each distribution.
Figure 14.42 shows the results of a simulation comparing ProDenICA to
FastICA, and another semi-parametric competitor KernelICA (Bach and
Jordan, 2002). The left panel shows the 18 distributions use d as a basis
of comparison. For each distribution, we generated a pair of independent
components( N= 1024),andarandommixingmatrixinIR2withcondition
numberbetween1and2.WeusedourRimplementationsof FastICA,using
the negentropy criterion (14.87), and ProDenICA . ForKernelICA we used

570 14. Unsupervised Learning
the authors MATLAB code.12Since the search criteria are nonconvex, we
used ﬁve random starts for each method. Each of the algorithm s delivers
an orthogonal mixing matrix A(the data were pre-whitened ), which is
available for comparison with the generating orthogonaliz ed mixing matrix
A0. We used the Amari metric (Bach and Jordan, 2002) as a measure of
the closeness of the two frames:
d(A0,A) =1
2pp/summationdisplay
i=1/parenleftigg/summationtextp
j=1|rij|
maxj|rij|−1/parenrightigg
+1
2pp/summationdisplay
j=1/parenleftbigg/summationtextp
i=1|rij|
maxi|rij|−1/parenrightbigg
,(14.97)
whererij= (AoA−1)ij. The right panel in Figure 14.42 compares the
averages (on the log scale) of the Amari metric between the tr uth and the
estimated mixing matrices. ProDenICA is competitive with FastICA and
KernelICA inallsituations,anddominatesmostofthemixturesimulat ions.
14.8 Multidimensional Scaling
Both self-organizing maps and principal curves and surface s map data
points in IRpto a lower-dimensional manifold. Multidimensional scalin g
(MDS) has a similar goal, but approaches the problem in a some what dif-
ferent way.
We start with observations x1,x2,...,x N∈IRp, and letdijbe the dis-
tance between observations iandj. Often we choose Euclidean distance
dij=||xi−xj||, but other distances may be used. Further, in some ap-
plications we may not even have available the data points xi, but only
have some dissimilarity measuredij(see Section 14.3.10). For example, in
a wine tasting experiment, dijmight be a measure of how diﬀerent a sub-
ject judged wines iandj, and the subject provides such a measure for all
pairs of wines i,j. MDS requires only the dissimilarities dij, in contrast to
the SOM and principal curves and surfaces which need the data pointsxi.
Multidimensional scaling seeks values z1,z2,...,z N∈IRkto minimize
the so-called stress function13
SM(z1,z2,...,z N) =/summationdisplay
i/ne}ationslash=i′(dii′−||zi−zi′||)2. (14.98)
This is known as least squares orKruskal–Shephard scaling. The idea is
to ﬁnd a lower-dimensional representation of the data that p reserves the
pairwise distances as well as possible. Notice that the appr oximation is
12Francis Bach kindly supplied this code, and helped us set up the simulatio ns.
13Some authors deﬁne stress as the square-root of SM; since it does not aﬀect the
optimization, we leave it squared to make comparisons with other crit eria simpler.

14.8 Multidimensional Scaling 571
in terms of the distances rather than squared distances (whi ch results in
slightly messier algebra). A gradient descent algorithm is used to minimize
SM.
A variation on least squares scaling is the so-called Sammon mapping
which minimizes
SSm(z1,z2,...,z N) =/summationdisplay
i/ne}ationslash=i′(dii′−||zi−zi′||)2
dii′.(14.99)
Here more emphasis is put on preserving smaller pairwise dis tances.
Inclassical scaling , we instead start with similarities sii′: often we use
the centered inner product sii′=∝an}⌊∇a⌋ketle{txi−¯x,xi′−¯x∝an}⌊∇a⌋ket∇i}ht. The problem then is to
minimize
SC(z1,z2,...,z N) =/summationdisplay
i,i′(sii′−∝an}⌊∇a⌋ketle{tzi−¯z,zi′−¯z∝an}⌊∇a⌋ket∇i}ht)2(14.100)
overz1,z2,...,z N∈IRk. This is attractive because there is an explicit
solution in terms of eigenvectors: see Exercise 14.11. If we have distances
ratherthaninner-products,wecanconvertthemtocentered inner-products
if the distances are Euclidean ;14see (18.31) on page 671 in Chapter 18.
If the similarities are in fact centered inner-products, cl assical scaling is
exactly equivalent to principal components, an inherently linear dimension-
reduction technique. Classical scaling is not equivalent t o least squares
scaling; the loss functions are diﬀerent, and the mapping ca n be nonlinear.
Least squares and classical scaling are referred to as metricscaling meth-
ods, in the sense that the actual dissimilarities or similar ities are approx-
imated.Shephard–Kruskal nonmetric scaling eﬀectively uses only ranks.
Nonmetric scaling seeks to minimize the stress function
SNM(z1,z2,...,z N) =/summationtext
i/ne}ationslash=i′/bracketleftbig
||zi−zi′||−θ(dii′)/bracketrightbig2
/summationtext
i/ne}ationslash=i′||zi−zi′||2(14.101)
over theziand an arbitrary increasing function θ. Withθﬁxed, we min-
imize over ziby gradient descent. With the ziﬁxed, the method of iso-
tonic regression is used to ﬁnd the best monotonic approxima tionθ(dii′)
to||zi−zi′||. These steps are iterated until the solutions stabilize.
Like the self-organizing map and principal surfaces, multi dimensional
scaling represents high-dimensional data in a low-dimensi onal coordinate
system. Principal surfaces and SOMs go a step further, and ap proximate
the original data by a low-dimensional manifold, parametri zed in the low
dimensional coordinate system. In a principal surface and S OM, points
14AnN×Ndistance matrix is Euclidean if the entries represent pairwise Euclidean
distances between Npoints in some dimensional space.

572 14. Unsupervised Learning
First MDS CoordinateSecond MDS Coordinate
-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•
•••
•••
•
••
•••
••••
•
•••
••••••
••
•••
••
•
••
•••
•
••••••
•••
••
••
•••
•
••••
•••
••
••
••
•
••
•••
•
•••
••••
•••
••
FIGURE 14.43. First two coordinates for half-sphere data, from classical mu lti-
dimensional scaling.
close together in the original feature space should map clos e together on
the manifold, but points far apart in feature space might als o map close
together. This is less likely in multidimensional scaling s ince it explicitly
tries to preserve all pairwise distances.
Figure 14.43 shows the ﬁrst two MDS coordinates from classic al scaling
for the half-sphere example. There is clear separation of th e clusters, and
the tighter nature of the red cluster is apparent.
14.9 Nonlinear Dimension Reduction and Local
Multidimensional Scaling
Several methods have been recently proposed for nonlinear d imension re-
duction, similar in spirit to principal surfaces. The idea i s that the data lie
close to an intrinsically low-dimensional nonlinear manif old embedded in a
high-dimensional space. These methods can be thought of as “ ﬂattening”
the manifold, and hence reducing the data to a set of low-dime nsional co-
ordinates that represent their relative positions in the ma nifold. They are
useful for problems where signal-to-noise ratio is very hig h (e.g., physical
systems), and are probably not as useful for observational d ata with lower
signal-to-noise ratios.
The basic goal is illustrated in the left panel of Figure 14.4 4. The data
lie near a parabola with substantial curvature. Classical M DS does not pre-

14.9 Nonlinear Dimension Reduction and Local Multidimensi onal Scaling 573
−5 0 5−15 −10 −5 0Classical MDS
−5 0 5−15 −10 −5 0Local MDS
x1 x1
x2x2
FIGURE 14.44. The orange points show data lying on a parabola, while the blue
points shows multidimensional scaling representations in on e dimension. Classical
multidimensional scaling (left panel) does not preserve the ord ering of the points
along the curve, because it judges points on opposite ends of t he curve to be close
together. In contrast, local multidimensional scaling (right panel) does a good job
of preserving the ordering of the points along the curve.
serve the ordering of the points along the curve, because it j udges points
on opposite ends of the curve to be close together. The right p anel shows
the results of local multi-dimensional scaling , one of the three methods for
non-linear multi-dimensional scaling that we discuss belo w. These meth-
ods use only the coordinates of the points in pdimensions, and have no
other information about the manifold. Local MDS has done a go od job of
preserving the ordering of the points along the curve.
We now brieﬂy describe three new approaches to nonlinear dim ension
reduction and manifold mapping.
Isometric feature mapping (ISOMAP) (Tenenbaum et al., 2000) con-
structs a graph to approximate the geodesic distance betwee n points along
the manifold. Speciﬁcally, for each data point we ﬁnd its nei ghbors—points
within some small Euclidean distance of that point. We const ruct a graph
with an edge between any two neighboring points. The geodesi c distance
between any two points is then approximated by the shortest p ath be-
tween points on the graph. Finally, classical scaling is app lied to the graph
distances, to produce a low-dimensional mapping.
Local linear embedding (Roweis and Saul, 2000) takes a very diﬀerent ap-
proach, trying to preserve the local aﬃne structure of the hi gh-dimensional
data. Each data point is approximated by a linear combinatio n of neigh-
boring points. Then a lower dimensional representation is c onstructed that

574 14. Unsupervised Learning
best preserves these local approximations. The details are interesting, so
we give them here.
1. For each data point xiinpdimensions, we ﬁnd its K-nearest neigh-
borsN(i) in Euclidean distance.
2. We approximate each point by an aﬃne mixture of the points i n its
neighborhood:
min
Wik||xi−/summationdisplay
k∈N(i)wikxk||2(14.102)
over weights wiksatisfyingwik= 0, k /∈N(i),/summationtextN
k=1wik= 1.wik
is the contribution of point kto the reconstruction of point i. Note
that for a hope of a unique solution, we must have K <p.
3. Finally, we ﬁnd points yiin a space of dimension d<pto minimize
N/summationdisplay
i=1||yi−N/summationdisplay
k=1wikyk||2(14.103)
withwikﬁxed.
In step 3, we minimize
tr[(Y−WY)T(Y−WY)] = tr[YT(I−W)T(I−W)Y] (14.104)
whereWisN×N;YisN×d, for some small d < p. The solutions ˆY
are the trailing eigenvectors of M= (I−W)T(I−W). Since1is a trivial
eigenvector with eigenvalue 0, we discard it and keep the nex td. This has
the side eﬀect that 1TY= 0, and hence the embedding coordinates are
mean centered.
Local MDS (Chen and Buja, 2008) takes the simplest and arguably the
most direct approach. We deﬁne Nto be the symmetric set of nearby pairs
of points; speciﬁcally a pair ( i,i′) is inNif pointiis among the K-nearest
neighbors of i′, or vice-versa. Then we construct the stress function
SL(z1,z2,...,z N) =/summationdisplay
(i,i′)∈N(dii′−||zi−zi′||)2
+/summationdisplay
(i,i′)/∈Nw·(D−||zi−zi′||)2.(14.105)
HereDis some large constant and wis a weight. The idea is that points
that are not neighbors are considered to be very far apart; su ch pairs are
given a small weight wso that they don’t dominate the overall stress func-
tion. To simplify the expression, we take w∼1/D, and letD→ ∞.
Expanding (14.105), this gives

14.9 Nonlinear Dimension Reduction and Local Multidimensi onal Scaling 575
FIGURE 14.45. Images of faces mapped into the embedding space described by
the ﬁrst two coordinates of LLE. Next to the circled points, re presentative faces
are shown in diﬀerent parts of the space. The images at the bot tom of the plot
correspond to points along the top right path (linked by solid lin e), and illustrate
one particular mode of variability in pose and expression.

576 14. Unsupervised Learning
SL(z1,z2,...,z N) =/summationdisplay
(i,i′)∈N(dii′−||zi−zi′||)2−τ/summationdisplay
(i,i′)/∈N||zi−zi′||,
(14.106)
whereτ= 2wD. The ﬁrst term in (14.106) tries to preserve local structure
in the data, while the second term encourages the representa tionszi,zi′
for pairs (i,i′) that are non-neighbors to be farther apart. Local MDS
minimizesthestressfunction(14.106)over zi,forﬁxedvaluesofthenumber
of neighbors Kand the tuning parameter τ.
TherightpanelofFigure14.44showstheresultoflocalMDS, usingk= 2
neighbors and τ= 0.01. We used coordinate descent with multiple starting
values to ﬁnd a good minimum of the (nonconvex) stress functi on (14.106).
The ordering of the points along the curve has been largely pr eserved,
Figure 14.45 shows a more interesting application of one of t hese meth-
ods (LLE)15. The data consist of 1965 photographs, digitized as 20 ×28
grayscale images. The result of the ﬁrst two-coordinates of LLE are shown
and reveal some variability in pose and expression. Similar pictures were
produced by local MDS.
In experiments reported in Chen and Buja (2008), local MDS sh ows su-
perior performance, as compared to ISOMAP and LLE. They also demon-
strate the usefulness of local MDS for graph layout. There ar e also close
connections between the methods discussed here, spectral c lustering (Sec-
tion 14.5.3) and kernel PCA (Section 14.5.4).
14.10 The Google PageRank Algorithm
In this section we give a brief description of the original PageRank algo-
rithm used by the Google search engine, an interesting recen t application
of unsupervised learning methods.
We suppose that we have Nweb pages and wish to rank them in terms
of importance. For example, the Npages might all contain a string match
to “statistical learning” and we might wish to rank the pages in terms of
their likely relevance to a websurfer.
ThePageRank algorithm considers a webpage to be important if many
other webpages point to it. However the linking webpages tha t point to a
given page are not treated equally: the algorithm also takes into account
both the importance ( PageRank ) of the linking pages and the number of
outgoing links that they have. Linking pages with higher PageRank are
given more weight, while pages with more outgoing links are g iven less
weight. These ideas lead to a recursive deﬁnition for PageRank , detailed
next.
15Sam Roweis and Lawrence Saul kindly provided this ﬁgure.

14.10 The Google PageRank Algorithm 577
LetLij= 1 if page jpoints to page i, and zero otherwise. Let cj=/summationtextN
i=1Lijequal the number of pages pointed to by page j(number of out-
links). Then the Google PageRanks piare deﬁned by the recursive rela-
tionship
pi= (1−d)+dN/summationdisplay
j=1/parenleftbigLij
cj/parenrightbig
pj (14.107)
wheredis a positive constant (apparently set to 0.85).
The idea is that the importance of page iis the sum of the importances of
pages that point to that page. The sums are weighted by 1 /cj, that is, each
page distributes a total vote of 1 to other pages. The constan tdensures
that each page gets a PageRank of at least 1−d. In matrix notation
p= (1−d)e+d·LD−1
cp (14.108)
whereeis a vector of Nones and Dc= diag(c) is a diagonal matrix with
diagonal elements cj. Introducing the normalization eTp=N(i.e., the
averagePageRank is 1), we can write (14.108) as
p=/bracketleftbig
(1−d)eeT/N+dLD−1
c/bracketrightbig
p
=Ap (14.109)
where the matrix Ais the expression in square braces.
Exploiting a connection with Markov chains (see below), it c an be shown
that the matrix Ahas a real eigenvalue equal to one, and one is its largest
eigenvalue. This means that we can ﬁnd ˆpby the power method: starting
with some p=p0we iterate
pk←Apk−1;pk←Npk
eTpk. (14.110)
The ﬁxed points ˆpare the desired PageRanks .
In the original paper of Page et al. (1998), the authors consi deredPageR-
ankas a model of user behavior, where a random web surfer clicks o n links
at random, without regard to content. The surfer does a rando m walk on
the web, choosing among available outgoing links at random. The factor
1−dis the probability that he does not click on a link, but jumps i nstead
to a random webpage.
Some descriptions of PageRank have (1−d)/Nas the ﬁrst term in def-
inition (14.107), which would better coincide with the rand om surfer in-
terpretation. Then the page rank solution (divided by N) is the stationary
distributionofanirreducible,aperiodicMarkovchainove rtheNwebpages.
Deﬁnition (14.107) also corresponds to an irreducible, ape riodic Markov
chain, with diﬀerent transition probabilities than those f rom the (1−d)/N
version. Viewing PageRank as a Markov chain makes clear why the matrix
Ahas a maximal real eigenvalue of 1. Since Ahas positive entries with

578 14. Unsupervised Learning
Page 2
Page 3Page 4Page 1
FIGURE 14.46. PageRank algorithm: example of a small network
each column summing to one, Markov chain theory tells us that it has a
unique eigenvector with eigenvalue one, corresponding to t he stationary
distribution of the chain (Bremaud, 1999).
A small network is shown for illustration in Figure 14.46. Th e link matrix
is
L=
0 0 1 0
1 0 0 0
1 1 0 1
0 0 0 0
(14.111)
and the number of outlinks is c= (2,1,1,1).
ThePageRank solution is ˆp= (1.49,0.78,1.58,0.15). Notice that page 4
has no incoming links, and hence gets the minimum PageRank of 0.15.
Bibliographic Notes
There are many books on clustering, including Hartigan (197 5), Gordon
(1999) and Kaufman and Rousseeuw (1990). K-means clustering goes back
atleasttoLloyd(1957),Forgy(1965),Jancey(1966)andMac Queen(1967).
Applications in engineering, especially in image compress ion via vector
quantization, can be found in Gersho and Gray (1992). The k-medoid pro-
cedure is described in Kaufman and Rousseeuw (1990). Associ ation rules
are outlined in Agrawal et al. (1995). The self-organizing m ap was proposed
by Kohonen (1989) and Kohonen (1990); Kohonen et al. (2000) g ive a more
recent account. Principal components analysis and multidi mensional scal-
ing are described in standard books on multivariate analysi s, for example,
Mardia et al. (1979). Buja et al. (2008) have implemented a po werful en-
vironment called Ggvis for multidimensional scaling, and t he user manual

Exercises 579
contains a lucid overview of the subject. Figures 14.17, 14. 21 (left panel)
and 14.28 (left panel) were produced in Xgobi, a multidimens ional data
visualization package by the same authors. GGobi is a more re cent im-
plementation (Cook and Swayne, 2007). Goodall (1991) gives a technical
overview of Procrustes methods in statistics, and Ramsay an d Silverman
(1997)discusstheshaperegistrationproblem.Principalc urvesandsurfaces
were proposed in Hastie (1984) and Hastie and Stuetzle (1989 ). The idea of
principal points was formulated in Flury (1990), Tarpey and Flury (1996)
give an exposition of the general concept of self-consisten cy. An excellent
tutorial on spectral clustering can be found in von Luxburg ( 2007); this was
the main source for Section 14.5.3. Luxborg credits Donath a nd Hoﬀman
(1973) and Fiedler (1973) with the earliest work on the subje ct. A history
of spectral clustering my be found in Spielman and Teng (1996 ). Indepen-
dent component analysis was proposed by Comon (1994), with s ubsequent
developments by Bell and Sejnowski (1995); our treatment in Section 14.7
is based on Hyv¨ arinen and Oja (2000). Projection pursuit wa s proposed by
Friedman and Tukey (1974), and is discussed in detail in Hube r (1985). A
dynamic projection pursuit algorithm is implemented in GGo bi.
Exercises
Ex. 14.1 Weights for clustering . Show that weighted Euclidean distance
d(w)
e(xi,xi′) =/summationtextp
l=1wl(xil−xi′l)2
/summationtextp
l=1wl
satisﬁes
d(w)
e(xi,xi′) =de(zi,zi′) =p/summationdisplay
l=1(zil−zi′l)2, (14.112)
where
zil=xil·/parenleftbiggwl/summationtextp
l=1wl/parenrightbigg1/2
. (14.113)
Thus weighted Euclidean distance based on xis equivalent to unweighted
Euclidean distance based on z.
Ex. 14.2 Consider a mixture model density in p-dimensional feature space,
g(x) =K/summationdisplay
k=1πkgk(x), (14.114)
wheregk=N(µk,L·σ2) andπk≥0∀kwith/summationtext
kπk= 1. Here{µk,πk},k=
1,...,Kandσ2are unknown parameters.

580 14. Unsupervised Learning
Suppose we have data x1,x2,...,x N∼g(x) and we wish to ﬁt the mix-
ture model.
1. Write down the log-likelihood of the data
2. Derive an EM algorithm for computing the maximum likeliho od es-
timates (see Section 8.1).
3. Show that if σhas a known value in the mixture model and we take
σ→0, then in a sense this EM algorithm coincides with K-means
clustering.
Ex. 14.3 In Section 14.2.6 we discuss the use of CART or PRIM for con-
structing generalized association rules. Show that a probl em occurs with ei-
therofthesemethodswhenwegeneratetherandomdatafromth eproduct-
marginal distribution; i.e., by randomly permuting the val ues for each of
the variables. Propose ways to overcome this problem.
Ex. 14.4 Cluster the demographic data of Table 14.1 using a classiﬁca tion
tree. Speciﬁcally, generate a reference sample of the same s ize of the train-
ing set, by randomly permuting the values within each featur e. Build a
classiﬁcation tree to the training sample (class 1) and the r eference sample
(class 0) and describe the terminal nodes having highest est imated class 1
probability. Compare the results to the PRIM results near Ta ble 14.1 and
also to the results of K-means clustering applied to the same data.
Ex. 14.5 Generate data with three features, with 30 data points in eac h of
three classes as follows:
θ1=U(−π/8,π/8)
φ1=U(0,2π)
x1= sin(θ1)cos(φ1)+W11
y1= sin(θ1)sin(φ1)+W12
z1= cos(θ1)+W13
θ2=U(π/2−π/4,π/2+π/4)
φ2=U(−π/4,π/4)
x2= sin(θ2)cos(φ2)+W21
y2= sin(θ2)sin(φ2)+W22
z2= cos(θ2)+W23
θ3=U(π/2−π/4,π/2+π/4)
φ3=U(π/2−π/4,π/2+π/4)
x3= sin(θ3)cos(φ3)+W31
y3= sin(θ3)sin(φ3)+W32
z3= cos(θ3)+W33
HereU(a,b) indicates a uniform variate on the range [ a,b] andWjkare
independent normal variates with standard deviation 0 .6. Hence the data

Exercises 581
lie near the surface of a sphere in three clusters centered at (1,0,0), (0,1,0)
and (0,0,1).
Write a program to ﬁt a SOM to these data, using the learning ra tes
given in the text. Carry out a K-means clustering of the same data, and
compare the results to those in the text.
Ex. 14.6 Write programs to implement K-means clustering and a self-
organizing map (SOM), with the prototype lying on a two-dime nsional
grid. Apply them to the columns of the human tumor microarray data, us-
ingK= 2,5,10,20 centroids for both. Demonstrate that as the size of the
SOM neighborhood is taken to be smaller and smaller, the SOM s olution
becomes more similar to the K-means solution.
Ex. 14.7 Derive (14.51) and (14.52) in Section 14.5.1. Show that ˆ µis not
unique, and characterize the family of equivalent solution s.
Ex. 14.8 Derive the solution (14.57) to the Procrustes problem (14.5 6).
Derive also the solution to the Procrustes problem with scal ing (14.58).
Ex. 14.9 Write an algorithm to solve
min
{βℓ,Rℓ}L
1,ML/summationdisplay
ℓ=1||XℓRℓ−M||2
F. (14.115)
Apply it to the three S’s, and compare the results to those sho wn in Fig-
ure 14.26.
Ex.14.10 Derivethesolutiontotheaﬃne-invariantaverageproblem( 14.60).
Apply it to the three S’s, and compare the results to those com puted in
Exercise 14.9.
Ex. 14.11 Classical multidimensional scaling. LetSbe the centered in-
ner product matrix with elements ∝an}⌊∇a⌋ketle{txi−¯x,xj−¯x∝an}⌊∇a⌋ket∇i}ht. Letλ1> λ2>···>
λkbe theklargest eigenvalues of S, with associated eigenvectors Ek=
(e1,e2,...,ek). LetDkbe a diagonal matrix with diagonal entries√λ1,√λ2,...,√λk. Show that the solutions zito the classical scaling problem
(14.100) are the rowsofEkDk.
Ex. 14.12 Consider the sparse PCA criterion (14.71).
1. Show that with Θﬁxed, solving for Vamounts to Kseparate elastic-
net regression problems, with responses the Kelements of ΘTxi.
2. Show that with Vﬁxed, solving for Θamounts to a reduced-rank
version of the Procrustes problem, which reduces to
max
Θtrace(ΘTM) subject to ΘTΘ=IK, (14.116)
whereMandΘare bothp×KwithK≤p. IfM=UDQTis the
SVD ofM, show that the optimal Θ=UQT.

582 14. Unsupervised Learning
Ex. 14.13 Generate 200 data points with three features, lying close to a
helix. In detail, deﬁne X1= cos(s)+0.1·Z1,X2= sin(s)+0.1·Z2,X3=
s+0.1·Z3wherestakes on 200 equally spaced values between 0 and 2 π,
andZ1,Z2,Z3are independent and have standard Gaussian distributions.
(a) Fit a principal curve to the data and plot the estimated co ordinate
functions. Compare them to the underlying functions cos( s),sin(s)
ands.
(b) Fit a self-organizing map to the same data, and see if you c an discover
the helical shape of the original point cloud.
Ex. 14.14 Pre- and post-multiply equation (14.81) by a diagonal matri x
containing the inverse variances of the Xj. Hence obtain an equivalent
decomposition for the correlation matrix, in the sense that a simple scaling
is applied to the matrix A.
Ex. 14.15 Generate 200 observations of three variates X1,X2,X3according
to
X1∼Z1
X2=X1+0.001·Z2
X3= 10·Z3 (14.117)
whereZ1,Z2,Z3are independent standard normal variates. Compute the
leading principal component and factor analysis direction s. Hence show
that the leading principal component aligns itself in the ma ximal variance
directionX3, while the leading factor essentially ignores the uncorrel ated
component X3, and picks up the correlated component X2+X1(Geoﬀrey
Hinton, personal communication).
Ex. 14.16 Consider the kernel principal component procedure outline d in
Section 14.5.4. Argue that the number Mof principal components is equal
to the rank of K, which is the number of non-zero elements in D. Show
that themth component zm(mth column of Z) can be written (up to
centering) as zim=/summationtextN
j=1αjmK(xi,xj), whereαjm=ujm/dm. Show that
the mapping of a new observation x0to themth component is given by
z0m=/summationtextN
j=1αjmK(x0,xj).
Ex. 14.17 Show that with g1(x) =/summationtextN
j=1cjK(x,xj), the solution to (14.66)
is given by ˆ cj=uj1/d1, whereu1is the ﬁrst column of Uin (14.65), and
d1the ﬁrst diagonal element of D. Show that the second and subsequent
principal component functions are deﬁned in a similar manne r (hint: see
Section 5.8.1.)
Ex. 14.18 Consider the regularized log-likelihood for the density es timation
problem arising in ICA,

Exercises 583
1
NN/summationdisplay
i=1[logφ(si)+g(si)]−/integraldisplay
φ(t)eg(t)dt−λ/integraldisplay
{g′′′(t)}2(t)dt.(14.118)
The solution ˆ gis a quartic smoothing spline, and can be written as ˆ g(s) =
ˆq(s) + ˆq⊥(s), whereqis a quadratic function (in the null space of the
penalty). Let q(s) =θ0+θ1s+θ2s2. By examining the stationarity condi-
tions for ˆθk, k= 1,2,3, show that the solution ˆf=φeˆgis a density, and
has mean zero and variance one. If we used a second-derivativ e penalty/integraltext
{g′′(t)}2(t)dtinstead, what simple modiﬁcation could we make to the
problem to maintain the three moment conditions?
Ex. 14.19 IfAisp×porthogonal, show that the ﬁrst term in (14.91) on
page 567
p/summationdisplay
j=1N/summationdisplay
i=1logφ(aT
jxi),
withajthejth column of A, does not depend on A.
Ex. 14.20 Fixed point algorithm for ICA (Hyv¨ arinen et al., 2001). Consider
maximizing C(a) =E{g(aTX)}with respect to a, with||a||= 1 and
Cov(X) =I. Use a Lagrange multiplier to enforce the norm constraint,
and write down the ﬁrst two derivatives of the modiﬁed criter ion. Use the
approximation
E{XXTg′′(aTX)}≈E{XXT}E{g′′(aTX)}
to show that the Newton update can be written as the ﬁxed-poin t update
(14.96).
Ex. 14.21 Consider an undirected graph with non-negative edge weight s
wii′and graph Laplacian L. Suppose there are mconnected components
A1,A2,...,A minthegraph.Showthat thereare meigenvectors of Lcorre-
sponding to eigenvalue zero, and the indicator vectors of th ese components
IA1,IA2,...,I Amspan the zero eigenspace.
Ex. 14.22
(a) Show that deﬁnition (14.108) implies that the sum of the PageRanks
piisN, the number of web pages.
(b) Write a program to compute the PageRank solutions by the power
method using formulation (14.107). Apply it to the network o f Fig-
ure 14.47.
Ex. 14.23 Algorithm for non-negative matrix factorization (Wu and Lange,
2007). A function g(x,y) to said to minorize a functionf(x) if

584 14. Unsupervised Learning
Page 2Page 1
Page 3
Page 4
Page 5Page 6
FIGURE 14.47. Example of a small network.
g(x,y)≤f(x), g(x,x) =f(x) (14.119)
for allx,yin the domain. This is useful for maximizing f(x) since it is easy
to show that f(x) is nondecreasing under the update
xs+1= argmaxxg(x,xs) (14.120)
There are analogous deﬁnitions for majorization , for minimizing a function
f(x). The resulting algorithms are known as MMalgorithms, for “minorize-
maximize” or “majorize-minimize” (Lange, 2004). It also ca n be shown
that the EM algorithm (8.5) is an example of an MM algorithm: s ee Sec-
tion 8.5.3 and Exercise 8.2 for details.
(a) Consider maximization of the function L(W,H) in (14.73), written
here without the matrix notation
L(W,H) =N/summationdisplay
i=1p/summationdisplay
j=1/bracketleftigg
xijlog/parenleftiggr/summationdisplay
k=1wikhkj/parenrightigg
−r/summationdisplay
k=1wikhkj/bracketrightigg
.
Using the concavity of log( x), show that for any set of rvaluesyk≥0
and 0≤ck≤1 with/summationtextr
k=1ck= 1,
log/parenleftiggr/summationdisplay
k=1yk/parenrightigg
≥r/summationdisplay
k=1cklog(yk/ck)
Hence
log/parenleftiggr/summationdisplay
k=1wikhkj/parenrightigg
≥r/summationdisplay
k=1as
ikj
bs
ijlog/parenleftigg
bs
ij
as
ikjwikhkj/parenrightigg
,
where
as
ikj=ws
ikhs
kjandbs
ij=r/summationdisplay
k=1ws
ikhs
kj,
andsindicates the current iteration.

Exercises 585
(b) Hence show that, ignoring constants, the function
g(W,H|Ws,Hs) =N/summationdisplay
i=1p/summationdisplay
j=1r/summationdisplay
k=1xijas
ikj
bs
ij/parenleftig
logwik+loghkj/parenrightig
−N/summationdisplay
i=1p/summationdisplay
j=1r/summationdisplay
k=1wikhkj
minorizesL(W,H).
(c) Set the partial derivatives of g(W,H|Ws,Hs) to zero and hence
derive the updating steps (14.74).
Ex. 14.24 Consider the non-negative matrix factorization (14.72) in the
rank one case ( r= 1).
(a) Show that the updates (14.74) reduce to
wi←wi/summationtextp
j=1xij/summationtextp
j=1wihj
hj←hj/summationtextN
i=1xij/summationtextN
i=1wihj(14.121)
wherewi=wi1,hj=h1j. This is an example of the iterative pro-
portional scaling procedure, applied to the independence model for a
two-way contingency table (Fienberg, 1977, for example).
(b) Show that the ﬁnal iterates have the explicit form
wi=c·/summationtextp
j=1xij/summationtextN
i=1/summationtextp
j=1xij, hk=1
c·/summationtextN
i=1xik/summationtextN
i=1/summationtextp
j=1xij(14.122)
for any constant c >0. These are equivalent to the usual row and
column estimates for a two-way independence model.
Ex. 14.25 Fit a non-negative matrix factorization model to the collec tion
of two’s in the digits database. Use 25 basis elements, and co mpare with a
24- component (plus mean) PCA model. In both cases display th eWand
Hmatrices as in Figure 14.33.

586 14. Unsupervised Learning

This is page 587
Printer: Opaque this
15
Random Forests
15.1 Introduction
Bagging or bootstrap aggregation (section 8.7) is a technique for reducing
the variance of an estimated prediction function. Bagging s eems to work
especially well for high-variance, low-bias procedures, s uch as trees. For
regression, we simply ﬁt the same regression tree many times to bootstrap-
sampled versions of the training data, and average the resul t. For classiﬁ-
cation, a committee of trees each cast a vote for the predicted class.
Boosting in Chapter 10 was initially proposed as a committee method as
well, although unlike bagging, the committee of weak learners evolves over
time, and the members cast a weighted vote. Boosting appears to dominate
bagging on most problems, and became the preferred choice.
Random forests (Breiman, 2001) is a substantial modiﬁcation of bagging
that builds a large collection of de-correlated trees, and then averages them.
On many problems the performance of random forests is very si milar to
boosting,andtheyaresimplertotrainandtune.Asaconsequ ence,random
forests are popular, and are implemented in a variety of pack ages.
15.2 Deﬁnition of Random Forests
The essential idea in bagging (Section 8.7) is to average man y noisy but
approximately unbiased models, and hence reduce the varian ce. Trees are
ideal candidates for bagging, since they can capture comple x interaction

588 15. Random Forests
Algorithm 15.1 Random Forest for Regression or Classiﬁcation.
1. Forb= 1 toB:
(a) Draw a bootstrap sample Z∗of sizeNfrom the training data.
(b) Grow a random-forest tree Tbto the bootstrapped data, by re-
cursively repeating the following steps for each terminal n ode of
the tree, until the minimum node size nminis reached.
i. Selectmvariables at random from the pvariables.
ii. Pick the best variable/split-point among the m.
iii. Split the node into two daughter nodes.
2. Output the ensemble of trees {Tb}B
1.
To make a prediction at a new point x:
Regression: ˆfB
rf(x) =1
B/summationtextB
b=1Tb(x).
Classiﬁcation: LetˆCb(x) be the class prediction of the bth random-forest
tree. Then ˆCB
rf(x) =majority vote{ˆCb(x)}B
1.
structures in the data, and if grown suﬃciently deep, have re latively low
bias. Since trees are notoriously noisy, they beneﬁt greatl y from the averag-
ing.Moreover,sinceeachtreegeneratedinbaggingisident icallydistributed
(i.d.), the expectation of an average of Bsuch trees is the same as the ex-
pectation of any one of them. This means the bias of bagged tre es is the
same as that of the individual (bootstrap) trees, and the onl y hope of im-
provement is through variance reduction. This is in contras t to boosting,
where the trees are grown in an adaptive way to remove bias, an d hence
are not i.d.
An average of Bi.i.d. random variables, each with variance σ2, has vari-
ance1
Bσ2. If the variables are simply i.d. (identically distributed , but not
necessarily independent) with positive pairwise correlat ionρ, the variance
of the average is (Exercise 15.1)
ρσ2+1−ρ
Bσ2. (15.1)
AsBincreases, the second term disappears, but the ﬁrst remains , and
hence the size of the correlation of pairs of bagged trees lim its the beneﬁts
of averaging. The idea in random forests (Algorithm 15.1) is to improve
the variance reduction of bagging by reducing the correlati on between the
trees, without increasing the variance too much. This is ach ieved in the
tree-growing process through random selection of the input variables.
Speciﬁcally, when growing a tree on a bootstrapped dataset:

15.2 Deﬁnition of Random Forests 589
Before each split, select m≤pof the input variables at random
as candidates for splitting.
Typically values for mare√por even as low as 1.
AfterBsuchtrees{T(x;Θb)}B
1aregrown,therandomforest(regression)
predictor is
ˆfB
rf(x) =1
BB/summationdisplay
b=1T(x;Θb). (15.2)
AsinSection10.9(page356),Θ bcharacterizesthe bthrandomforesttreein
terms of split variables, cutpoints at each node, and termin al-node values.
Intuitively, reducing mwill reduce the correlation between any pair of trees
in the ensemble, and hence by (15.1) reduce the variance of th e average.
0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060 0.065 0.070Spam Data
Number of TreesTest ErrorBagging
Random Forest
Gradient Boosting (5 Node)
FIGURE 15.1. Bagging, random forest, and gradient boosting, applied to th e
spam data. For boosting, 5-node trees were used, and the number of trees were
chosen by 10-fold cross-validation ( 2500trees). Each “step” in the ﬁgure corre-
sponds to a change in a single misclassiﬁcation (in a test set of 1536).
Not all estimators can be improved by shaking up the data like this.
It seems that highly nonlinear estimators, such as trees, be neﬁt the most.
For bootstrapped trees, ρis typically small (0 .05 or lower is typical; see
Figure 15.9), while σ2is not much larger than the variance for the original
tree. On the other hand, bagging does not change linearestimates, such
as the sample mean (hence its variance either); the pairwise correlation
between bootstrapped means is about 50% (Exercise 15.4).

590 15. Random Forests
Random forests are popular. Leo Breiman’s1collaborator Adele Cutler
maintains a random forest website2where the software is freely available,
withmorethan3000downloadsreportedby2002.Thereisa randomForest
package in R, maintained by Andy Liaw, available from the CRANwebsite.
The authors make grand claims about the success of random for ests:
“most accurate,” “most interpretable,” and the like. In our experience ran-
dom forests do remarkably well, with very little tuning requ ired. A ran-
dom forest classiﬁer achieves 4 .88% misclassiﬁcation error on the spamtest
data, which compares well with all other methods, and is not s igniﬁcantly
worse than gradient boosting at 4 .5%. Bagging achieves 5 .4% which is
signiﬁcantly worse than either (using the McNemar test outl ined in Ex-
ercise 10.6), so it appears on this example the additional ra ndomization
helps.
RF−1 RF−3 Bagging GBM−1 GBM−60.00 0.05 0.10 0.15Nested SpheresTest Misclassification Error
Bayes Error
FIGURE 15.2. The results of 50simulations from the “nested spheres” model in
IR10. The Bayes decision boundary is the surface of a sphere (addi tive). “RF-3”
refers to a random forest with m= 3, and “GBM-6” a gradient boosted model
with interaction order six; similarly for “RF-1” and “GBM-1.” T he training sets
were of size 2000, and the test sets 10,000.
Figure 15.1 shows the test-error progression on 2500 trees f or the three
methods. In this case there is some evidence that gradient bo osting has
started to overﬁt, although 10-fold cross-validation chos e all 2500 trees.
1Sadly, Leo Breiman died in July, 2005.
2http://www.math.usu.edu/ ∼adele/forests/

15.2 Deﬁnition of Random Forests 591
0 200 400 600 800 10000.32 0.34 0.36 0.38 0.40 0.42 0.44California Housing Data
Number of TreesTest Average Absolute ErrorRF m=2
RF m=6
GBM depth=4
GBM depth=6
FIGURE 15.3. Random forests compared to gradient boosting on the Californ ia
housing data. The curves represent mean absolute error on the test data as a
function of the number of trees in the models. Two random fores ts are shown, with
m= 2andm= 6. The two gradient boosted models use a shrinkage parameter
ν= 0.05in (10.41), and have interaction depths of 4and6. The boosted models
outperform random forests.
Figure 15.2 shows the results of a simulation3comparing random forests
to gradient boosting on the nested spheres problem [Equation (10.2) in
Chapter 10]. Boosting easily outperforms random forests he re. Notice that
smallermis better here, although part of the reason could be that the t rue
decision boundary is additive.
Figure 15.3 compares random forests to boosting (with shrin kage) in a
regression problem, using the California housing data (Sec tion 10.14.1).
Two strong features that emerge are
•Randomforestsstabilizeatabout200trees,whileat1000tr eesboost-
ing continues to improve. Boosting is slowed down by the shri nkage,
as well as the fact that the trees are much smaller.
•Boosting outperforms random forests here. At 1000 terms, th e weaker
boosting model (GBM depth 4) has a smaller error than the stro nger
3Details: The random forests were ﬁt using the R package randomForest 4.5-11 ,
with 500 trees. The gradient boosting models were ﬁt using R package gbm 1.5, with
shrinkage parameter set to 0.05, and 2000 trees.

592 15. Random Forests
0 500 1000 1500 2000 25000.045 0.055 0.065 0.075
Number of TreesMisclassification ErrorOOB Error
Test Error
FIGURE 15.4. ooberror computed on the spamtraining data, compared to the
test error computed on the test set.
random forest (RF m= 6); a Wilcoxon test on the mean diﬀerences
in absolute errors has a p-value of 0 .007. For larger mthe random
forests performed no better.
15.3 Details of Random Forests
We have glossed over the distinction between random forests for classiﬁca-
tionversusregression.Whenusedforclassiﬁcation,arand omforestobtains
a class vote from each tree, and then classiﬁes using majorit y vote (see Sec-
tion 8.7 on bagging for a similar discussion). When used for r egression, the
predictions from each tree at a target point xare simply averaged, as in
(15.2). In addition, the inventors make the following recom mendations:
•For classiﬁcation, the default value for mis⌊√p⌋and the minimum
node size is one.
•For regression, the default value for mis⌊p/3⌋and the minimum
node size is ﬁve.
Inpracticethebestvaluesfortheseparameterswilldepend ontheproblem,
and they should be treated as tuning parameters. In Figure 15 .3m= 6
performs much better than the default value ⌊8/3⌋= 2.
15.3.1 Out of Bag Samples
An important feature of random forests is its use of out-of-bag (oob) sam-
ples:

15.3 Details of Random Forests 593
For each observation zi= (xi,yi), construct its random forest
predictor by averaging onlythose trees corresponding to boot-
strap samples in which zidid notappear.
Anooberror estimate is almost identical to that obtained by N-fold cross-
validation;seeExercise15.2.Henceunlikemanyothernonl inearestimators,
random forests can be ﬁt in one sequence, with cross-validat ion being per-
formed along the way. Once the ooberror stabilizes, the training can be
terminated.
Figure 15.4 shows the oobmisclassiﬁcation error for the spamdata, com-
pared to the test error. Although 2500 trees are averaged her e, it appears
from the plot that about 200 would be suﬃcient.
15.3.2 Variable Importance
Variable importance plots can be constructed for random for ests in exactly
the same way as they were for gradient-boosted models (Secti on 10.13).
At each split in each tree, the improvement in the split-crit erion is the
importancemeasureattributedtothesplittingvariable,a ndisaccumulated
over all the trees in the forest separately for each variable . The left plot
of Figure 15.5 shows the variable importances computed in th is way for
thespamdata; compare with the corresponding Figure 10.6 on page 354 for
gradient boosting. Boosting ignores some variables comple tely, while the
random forest does not. The candidate split-variable selec tion increases
the chance that any single variable gets included in a random forest, while
no such selection occurs with boosting.
Randomforestsalsousethe oobsamplestoconstructadiﬀerent variable-
importance measure, apparently to measure the prediction strength of e ach
variable. When the bth tree is grown, the oobsamples are passed down
the tree, and the prediction accuracy is recorded. Then the v alues for the
jth variable are randomly permuted in the oobsamples, and the accuracy
is again computed. The decrease in accuracy as a result of thi s permuting
is averaged over all trees, and is used as a measure of the impo rtance of
variablejin the random forest. These are expressed as a percent of the
maximum in the right plot in Figure 15.5. Although the rankin gs of the
two methods are similar, the importances in the right plot ar e more uni-
form over the variables. The randomization eﬀectively void s the eﬀect of
a variable, much like setting a coeﬃcient to zero in a linear m odel (Exer-
cise 15.7). This does not measure the eﬀect on prediction wer e this variable
not available, because if the model was reﬁtted without the v ariable, other
variables could be used as surrogates.

594 15. Random Forests
!$removefreeCAPAVEyourCAPMAXhpCAPTOTmoneyouryougeorge000eduhplbusiness1999internet(willallemailrereceiveovermail;650meetinglabsorderaddresspmpeoplemake#creditfontdatatechnology85[labtelnetreportoriginalprojectconferencedirect415857addresses3dcspartstableGini
0 20 40 60 80 100
Variable Importance!remove$CAPAVEhpfreeCAPMAXedugeorgeCAPTOTyourour1999reyouhplbusiness000meetingmoney(willinternet650pmreceiveoveremail;fontmailtechnologyorderalllabs[85addressoriginallabtelnetpeopleprojectdatacreditconference857#415makecsreportdirectaddresses3dpartstableRandomization
0 20 40 60 80 100
Variable Importance
FIGURE 15.5. Variable importance plots for a classiﬁcation random forest
grown on the spamdata. The left plot bases the importance on the Gini split-
ting index, as in gradient boosting. The rankings compare we ll with the rankings
produced by gradient boosting (Figure 10.6 on page 354). The right plot uses oob
randomization to compute variable importances, and tends to spread the impor-
tances more uniformly.

15.3 Details of Random Forests 595
Proximity Plot
12
34
5
6Random Forest Classifier
123456
Dimension 1Dimension 2
X1X2
FIGURE 15.6. (Left): Proximity plot for a random forest classiﬁer grown to
the mixture data. (Right): Decision boundary and training d ata for random forest
on mixture data. Six points have been identiﬁed in each plot.
15.3.3 Proximity Plots
One of the advertised outputs of a random forest is a proximity plot . Fig-
ure15.6showsaproximityplotforthemixturedatadeﬁnedin Section2.3.3
in Chapter 2. In growing a random forest, an N×Nproximity matrix is
accumulated for the training data. For every tree, any pair o foobobser-
vations sharing a terminal node has their proximity increas ed by one. This
proximity matrix is then represented in two dimensions usin g multidimen-
sional scaling (Section 14.8). The idea is that even though t he data may be
high-dimensional, involving mixed variables, etc., the pr oximity plot gives
an indication of which observations are eﬀectively close to gether in the eyes
of the random forest classiﬁer.
Proximity plots for random forests often look very similar, irrespective of
the data, which casts doubt on their utility. They tend to hav e a star shape,
one arm per class, which is more pronounced the better the cla ssiﬁcation
performance.
Since the mixture data are two-dimensional, we can map point s from the
proximityplottotheoriginalcoordinates,andgetabetter understandingof
what they represent. It seems that points in pure regions cla ss-wise map to
the extremities of the star, while points nearer the decisio n boundaries map
nearer the center. This is not surprising when we consider th e construction
of the proximity matrices. Neighboring points in pure regio ns will often
end up sharing a bucket, since when a terminal node is pure, it is no longer

596 15. Random Forests
split by a random forest tree-growing algorithm. On the othe r hand, pairs
of points that are close but belong to diﬀerent classes will s ometimes share
a terminal node, but not always.
15.3.4 Random Forests and Overﬁtting
When the number of variables is large, but the fraction of rel evant variables
small, random forests are likely to perform poorly with smal lm. At each
split the chance can be small that the relevant variables wil l be selected.
Figure 15.7 shows the results of a simulation that supports t his claim. De-
tails are given in the ﬁgure caption and Exercise 15.3. At the top of each
pair we see the hyper-geometric probability that a relevant variable will be
selectedatanysplitbyarandomforesttree(inthissimulat ion,therelevant
variables are all equal in stature). As this probability get s small, the gap
between boosting and random forests increases. When the num ber of rele-
vant variables increases, the performance of random forest s is surprisingly
robust to an increase in the number of noise variables. For ex ample, with 6
relevant and 100 noise variables, the probability of a relev ant variable being
selected at any split is 0.46, assuming m=/radicalbig
(6+100)≈10. According to
Figure15.7,thisdoesnothurttheperformanceofrandomfor estscompared
with boosting. This robustness is largely due to the relativ e insensitivity of
misclassiﬁcation cost to the bias and variance of the probab ility estimates
in each tree. We consider random forests for regression in th e next section.
Another claim is that random forests “cannot overﬁt” the dat a. It is
certainly true that increasing Bdoes not cause the random forest sequence
to overﬁt; like bagging, the random forest estimate (15.2) a pproximates the
expectation
ˆfrf(x) = EΘT(x;Θ) = lim
B→∞ˆf(x)B
rf (15.3)
with an average over Brealizations of Θ. The distribution of Θ here is con-
ditional on the training data. However, this limit can overﬁt the data ; the
average of fully grown trees can result in too rich a model, an d incur unnec-
essary variance. Segal (2004) demonstrates small gains in p erformance by
controlling the depths of the individual trees grown in rand om forests. Our
experience is that using full-grown trees seldom costs much , and results in
one less tuning parameter.
Figure15.8showsthemodesteﬀectofdepthcontrolinasimpl eregression
example. Classiﬁers are less sensitive to variance, and thi s eﬀect of over-
ﬁtting is seldom seen with random-forest classiﬁcation.

15.4 Analysis of Random Forests 597Test Misclassification Error
0.10 0.15 0.20 0.25 0.30Bayes Error
(2, 5) (2, 25) (2, 50) (2, 100) (2, 150)
Number of (Relevant, Noise) Variables0.52 0.34 0.25 0.19 0.15
Random Forest
Gradient Boosting
FIGURE 15.7. A comparison of random forests and gradient boosting on prob -
lems with increasing numbers of noise variables. In each case t he true decision
boundary depends on two variables, and an increasing number o f noise variables
are included. Random forests uses its default value m=√p. At the top of each
pair is the probability that one of the relevant variables is cho sen at any split.
The results are based on 50simulations for each pair, with a training sample of
300, and a test sample of 500. See Exercise 15.3.
15.4 Analysis of Random Forests
In this section we analyze the mechanisms at play with the add itional
randomization employed by random forests. For this discuss ion we focus
on regression and squared error loss, since this gets at the m ain points,
and bias and variance are more complex with 0–1 loss (see Sect ion 7.3.1).
Furthermore, even in the case of a classiﬁcation problem, we can consider
therandom-forestaverageasanestimateoftheclassposter iorprobabilities,
for which bias and variance are appropriate descriptors.
15.4.1 Variance and the De-Correlation Eﬀect
The limiting form ( B→∞) of the random forest regression estimator is
ˆfrf(x) = EΘ|ZT(x;Θ(Z)), (15.4)
where we have made explicit the dependence on the training da taZ. Here
we consider estimation at a single target point x. From (15.1) we see that

598 15. Random Forests
50 30 20 10 51.00 1.05 1.10
Minimum Node SizeMean Squared Test ErrorShallow Deep
FIGURE 15.8. The eﬀect of tree size on the error in random forest regres-
sion. In this example, the true surface was additive in two of t he12variables,
plus additive unit-variance Gaussian noise. Tree depth is con trolled here by the
minimum node size; the smaller the minimum node size, the deep er the trees.
Varˆfrf(x) =ρ(x)σ2(x). (15.5)
Here
•ρ(x) is thesampling correlation between any pair of trees used in the
averaging:
ρ(x) = corr[T(x;Θ1(Z)),T(x;Θ2(Z))], (15.6)
where Θ 1(Z) and Θ 2(Z) are a randomly drawn pair of random forest
trees grown to the randomly sampled Z;
•σ2(x) is the sampling variance of any single randomly drawn tree,
σ2(x) = VarT(x;Θ(Z)). (15.7)
It is easy to confuse ρ(x) with the average correlation between ﬁtted trees
in agivenrandom-forest ensemble; that is, think of the ﬁtted trees as N-
vectors, and compute the average pairwise correlation betw een these vec-
tors, conditioned on the data. This is notthe case; this conditional corre-
lation is not directly relevant in the averaging process, an d the dependence
onxinρ(x) warns us of the distinction. Rather, ρ(x) is the theoretical
correlation between a pair of random-forest trees evaluate d atx, induced
by repeatedly making training sample draws Zfrom the population, and
then drawing a pair of random forest trees. In statistical ja rgon, this is the
correlation induced by the sampling distribution ofZand Θ.
More precisely, the variability averaged over in the calcul ations in (15.6)
and (15.7) is both

15.4 Analysis of Random Forests 599
•conditional on Z: due to the bootstrap sampling and feature sampling
at each split, and
•a result of the sampling variability of Zitself.
In fact, the conditional covariance of a pair of tree ﬁts at xis zero, because
the bootstrap and feature sampling is i.i.d; see Exercise 15 .5.
1 4 7 13 19 25 31 37 43 490.00 0.02 0.04 0.06 0.08
Number of Randomly Selected Splitting Variables mCorrelation between Trees
FIGURE 15.9. Correlations between pairs of trees drawn by a random-forest
regression algorithm, as a function of m. The boxplots represent the correlations
at600randomly chosen prediction points x.
The following demonstrations are based on a simulation mode l
Y=1√
5050/summationdisplay
j=1Xj+ε, (15.8)
with all the Xjandεiid Gaussian. We use 500 training sets of size 100, and
a single set of test locations of size 600. Since regression t rees are nonlinear
inZ, the patterns we see below will diﬀer somewhat depending on t he
structure of the model.
Figure 15.9 shows how the correlation (15.6) between pairs o f trees de-
creases asmdecreases: pairs of tree predictions at xfor diﬀerent training
setsZare likely to be less similar if they do not use the same splitt ing
variables.
In the left panel of Figure 15.10 we consider the variances of single tree
predictors, Var T(x;Θ(Z)) (averaged over 600 prediction points xdrawn
randomlyfromoursimulationmodel).Thisisthetotalvaria nce,andcanbe

600 15. Random Forests
decomposed into two parts using standard conditional varia nce arguments
(see Exercise 15.5):
VarΘ,ZT(x;Θ(Z)) = Var ZEΘ|ZT(x;Θ(Z)) + E ZVarΘ|ZT(x;Θ(Z))
Total Variance = Var Zˆfrf(x) + within-ZVariance
(15.9)
The second term is the within- Zvariance—a result of the randomization,
which increases as mdecreases. The ﬁrst term is in fact the sampling vari-
ance of the random forest ensemble (shown in the right panel) , which de-
creases asmdecreases. The variance of the individual trees does not cha nge
appreciably over much of the range of m, hence in light of (15.5), the vari-
ance of the ensemble is dramatically lower than this tree var iance.
0 10 20 30 40 501.80 1.85 1.90 1.95Single Tree
mVariance
Within Z
Total
0 10 20 30 40 500.65 0.70 0.75 0.80 0.85Random Forest Ensemble
mMean Squared Error and Squared Bias
Variance
Mean Squared Error
Squared Bias
Variance
0.0 0.05 0.10 0.15 0.20
FIGURE 15.10. Simulation results. The left panel shows the average variance o f
a single random forest tree, as a function of m. “Within Z” refers to the average
within-sample contribution to the variance, resulting from t he bootstrap sampling
and split-variable sampling (15.9). “Total” includes the samplin g variability of
Z. The horizontal line is the average variance of a single fully gr own tree (with-
out bootstrap sampling). The right panel shows the average me an-squared error,
squared bias and variance of the ensemble, as a function of m. Note that the
variance axis is on the right (same scale, diﬀerent level). The h orizontal line is
the average squared-bias of a fully grown tree.
15.4.2 Bias
As in bagging, the bias of a random forest is the same as the bia s of any
of the individual sampled trees T(x;Θ(Z)):

15.4 Analysis of Random Forests 601
Bias(x) =µ(x)−EZˆfrf(x)
=µ(x)−EZEΘ|ZT(x;Θ(Z)). (15.10)
This is also typically greater (in absolute terms) than the b ias of an un-
pruned tree grown to Z, since the randomization and reduced sample space
imposerestrictions.Hencetheimprovementsinprediction obtainedbybag-
ging or random forests are solely a result of variance reduction .
Any discussion of bias depends on the unknown true function. Fig-
ure 15.10 (right panel) shows the squared bias for our additi ve model simu-
lation (estimated from the 500 realizations). Although for diﬀerent models
the shape and rate of the bias curves may diﬀer, the general tr end is that
asmdecreases, the bias increases. Shown in the ﬁgure is the mean -squared
error, and we see a classical bias-variance trade-oﬀ in the c hoice ofm. For
allmthe squared bias of the random forest is greater than that for a single
tree (horizontal line).
These patterns suggest a similarity with ridge regression ( Section 3.4.1).
Ridge regression is useful (in linear models) when one has a l arge number
of variables with similarly sized coeﬃcients; ridge shrink s their coeﬃcients
toward zero, and those of strongly correlated variables tow ard each other.
Although the size of the training sample might not permit all the variables
to be in the model, this regularization via ridge stabilizes the model and al-
lows all the variables to have their say (albeit diminished) . Random forests
with small mperform a similar averaging. Each of the relevant variables
get their turn to be the primary split, and the ensemble avera ging reduces
the contribution of any individual variable. Since this sim ulation exam-
ple (15.8) is based on a linear model in all the variables, rid ge regression
achieves a lower mean-squared error (about 0 .45 with df(λopt)≈29).
15.4.3 Adaptive Nearest Neighbors
The random forest classiﬁer has much in common with the k-nearest neigh-
bor classiﬁer (Section 13.3); in fact a weighted version the reof. Since each
tree is grown to maximal size, for a particular Θ∗,T(x;Θ∗(Z)) is the re-
sponse value for one of the training samples4. The tree-growing algorithm
ﬁnds an “optimal” path to that observation, choosing the mos t informative
predictors from those at its disposal. The averaging proces s assigns weights
to these training responses, which ultimately vote for the p rediction. Hence
via the random-forest voting mechanism, those observation scloseto the
target point get assigned weights—an equivalent kernel—whic h combine to
form the classiﬁcation decision.
Figure 15.11 demonstrates the similarity between the decis ion boundary
of 3-nearest neighbors and random forests on the mixture dat a.
4We gloss over the fact that pure nodes are not split further, and hence t here can be
more than one observation in a terminal node

602 15. Random Forests
Random Forest Classifier
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
ooooo
oo
oo
oo
ooo
ooooooo
oo oooo
oo
oo
oooo
o
oo
oo
ooooo
ooo
o
o
ooo
oooooo
oo
o
oo
ooo
o
Training Error: 0.000
Test Error:       0.238
Bayes Error:    0.2103−Nearest Neighbors
o
oooo
ooo
oo
o
oo
oo
ooo
oo
ooo
oo
oo
o
oo
o
oooo
oo
o
oo
oo
ooo
oo
o
ooo
o
oo
oo
o
ooo
oo
o
o
oo
o
oo
ooooo
o
oo
o oo
oo
ooo
oo
oo
oo
oo
oo
ooo
o
ooooo
oooo
ooo
oo
ooo
oo
o
oo
o
ooo
oo ooo
oo
ooooo
oo
oo
oo
ooo
ooooooo
oo oooo
oo
oo
oooo
o
oo
oo
ooooo
ooo
o
o
ooo
oooooo
oo
o
oo
ooo
o
Training Error: 0.130
Test Error:       0.242
Bayes Error:    0.210
FIGURE 15.11. Random forests versus 3-NN on the mixture data. The axis-ori -
ented nature of the individual trees in a random forest lead to decision regions
with an axis-oriented ﬂavor.
Bibliographic Notes
Random forests as described here were introduced by Breiman (2001), al-
though many of the ideas had cropped up earlier in the literat ure in dif-
ferent forms. Notably Ho (1995) introduced the term “random forest,” and
used a consensus of trees grown in random subspaces of the fea tures. The
idea of using stochastic perturbation and averaging to avoi d overﬁtting was
introduced by Kleinberg (1990), and later in Kleinberg (199 6). Amit and
Geman (1997) used randomized trees grown on image features f or image
classiﬁcation problems. Breiman (1996a) introduced baggi ng, a precursor
to his version of random forests. Dietterich (2000b) also pr oposed an im-
provement on bagging using additional randomization. His a pproach was
to rank the top 20 candidate splits at each node, and then sele ct from the
list at random. He showed through simulations and real examp les that this
additional randomization improved over the performance of bagging. Fried-
man and Hall (2007) showed that sub-sampling (without repla cement) is
an eﬀective alternative to bagging. They showed that growin g and aver-
aging trees on samples of size N/2 is approximately equivalent (in terms
bias/variance considerations) to bagging, while using sma ller fractions of
Nreduces the variance even further (through decorrelation) .
There are several free software implementations of random f orests. In
this chapter we used the randomForest package in R, maintained by Andy
Liaw, available from the CRANwebsite. This allows both split-variable se-
lection, as well as sub-sampling. Adele Cutler maintains a r andom forest
websitehttp://www.math.usu.edu/ ∼adele/forests/ where (as of Au-
gust 2008) the software written by Leo Breiman and Adele Cutl er is freely

Exercises 603
available. Their code, and the name “random forests”, is exc lusively li-
censed to Salford Systems for commercial release. The Wekamachine learn-
ing archive http://www.cs.waikato.ac.nz/ml/weka/ at Waikato Univer-
sity, New Zealand, oﬀers a free javaimplementation of random forests.
Exercises
Ex. 15.1 Derive the variance formula (15.1). This appears to fail if ρis
negative; diagnose the problem in this case.
Ex. 15.2 Show that as the number of bootstrap samples Bgets large, the
ooberror estimate for a random forest approaches its N-fold CV error
estimate, and that in the limit, the identity is exact.
Ex. 15.3 Consider the simulation model used in Figure 15.7 (Mease and
Wyner, 2008). Binary observations are generated with proba bilities
Pr(Y= 1|X) =q+(1−2q)·I
J/summationdisplay
j=1Xj>J/2
, (15.11)
whereX∼U[0,1]p, 0≤q≤1
2, andJ≤pis some predeﬁned (even)
number. Describe this probability surface, and give the Bay es error rate.
Ex. 15.4 Supposexi, i= 1,...,Nare iid (µ,σ2). Let ¯x∗
1and ¯x∗
2be two
bootstrap realizations of the sample mean. Show that the sam pling cor-
relation corr(¯ x∗
1,¯x∗
2) =n
2n−1≈50%. Along the way, derive var(¯ x∗
1) and
the variance of the bagged mean ¯ xbag. Here ¯xis alinearstatistic; bagging
produces no reduction in variance for linear statistics.
Ex. 15.5 Show that the sampling correlation between a pair of random-
forest trees at a point xis given by
ρ(x) =VarZ[EΘ|ZT(x;Θ(Z))]
VarZ[EΘ|ZT(x;Θ(Z))]+E ZVarΘ|Z[T(x;Θ(Z)].(15.12)
The term in the numerator is Var Z[ˆfrf(x)], and the second term in the
denominator is the expected conditional variance due to the randomization
in random forests.
Ex. 15.6 Fit a series of random-forest classiﬁers to the spamdata, to explore
the sensitivity to the parameter m. Plot both the ooberror as well as the
test error against a suitably chosen range of values for m.

604 15. Random Forests
Ex. 15.7 Suppose we ﬁt a linear regression model to Nobservations with
responseyiand predictors xi1,...,x ip. Assume that all variables are stan-
dardized to have mean zero and standard deviation one. Let RSSbe the
mean-squaredresidualonthetrainingdata,and ˆβtheestimatedcoeﬃcient.
Denote byRSS∗
jthe mean-squared residual on the training data using the
sameˆβ, but with the Nvalues for the jth variable randomly permuted
before the predictions are calculated. Show that
EP[RSS∗
j−RSS] = 2ˆβ2
j, (15.13)
whereE Pdenotesexpectationwithrespecttothepermutationdistri bution.
Argue that this is approximately true when the evaluations a re done using
an independent test set.

This is page 605
Printer: Opaque this
16
Ensemble Learning
16.1 Introduction
The idea of ensemble learning is to build a prediction model b y combining
the strengths of a collection of simpler base models. We have already seen
a number of examples that fall into this category.
Bagging in Section 8.7 and random forests in Chapter 15 are en semble
methods for classiﬁcation, where a committee of trees each cast a vote for
the predicted class. Boosting in Chapter 10 was initially pr oposed as a
committee method as well, although unlike random forests, t he committee
ofweak learners evolves over time, and the members cast a weighted vote.
Stacking (Section 8.8) is a novel approach to combining the s trengths of
a number of ﬁtted models. In fact one could characterize any d ictionary
method, such as regression splines, as an ensemble method, w ith the basis
functions serving the role of weak learners.
Bayesian methods for nonparametric regression can also be v iewed as
ensemble methods: a large number of candidate models are ave raged with
respect to the posterior distribution of their parameter se ttings (e.g. (Neal
and Zhang, 2006)).
Ensemble learning can be broken down into two tasks: develop ing a pop-
ulation of base learners from the training data, and then com bining them
to form the composite predictor. In this chapter we discuss b oosting tech-
nology that goes a step further; it builds an ensemble model b y conducting
a regularized and supervised search in a high-dimensional s pace of weak
learners.

606 16. Ensemble Learning
An early example of a learning ensemble is a method designed f or multi-
classclassiﬁcationusing error-correcting output codes (DietterichandBakiri,
1995, ECOC). Consider the 10-class digit classiﬁcation pro blem, and the
coding matrix Cgiven in Table 16.1.
TABLE 16.1. Part of a 15-bit error-correcting coding matrix Cfor the10-class
digit classiﬁcation problem. Each column deﬁnes a two-class clas siﬁcation prob-
lem.
Digit C1C2C3C4C5C6···C15
01 1 0 0 0 0 ···1
10 0 1 1 1 1 ···0
21 0 0 1 0 0 ···1
.....................···...
81 1 0 1 0 1 ···1
90 1 1 1 0 0 ···0
Note that the ℓth column of the coding matrix Cℓdeﬁnes a two-class
variable that merges all the original classes into two group s. The method
works as follows:
1. Learn a separate classiﬁer for each of the L= 15 two class problems
deﬁned by the columns of the coding matrix.
2. At a test point x, let ˆpℓ(x) be the predicted probability of a one for
theℓth response.
3. Deﬁneδk(x) =/summationtextL
ℓ=1|Ckℓ−ˆpℓ(x)|, the discriminant function for the
kth class, where Ckℓis the entry for row kand column ℓin Table 16.1.
Each row of Cis a binary code for representing that class. The rows have
more bits than is necessary, and the idea is that the redundan t “error-
correcting” bits allow for some inaccuracies, and can impro ve performance.
In fact, the full code matrix Cabove has a minimum Hamming distance1
of 7 between any pair of rows. Note that even the indicator res ponse coding
(Section 4.2) is redundant, since 10 classes require only ⌈log210 = 4 bits for
theiruniquerepresentation.DietterichandBakiri(1995) showedimpressive
improvements in performance for a variety of multiclass pro blems when
classiﬁcation trees were used as the base classiﬁer.
James and Hastie (1998) analyzed the ECOC approach, and show ed
that random code assignment worked as well as the optimally c onstructed
error-correctingcodes.Theyalsoarguedthatthemainbene ﬁtofthecoding
was in variance reduction (as in bagging and random forests) , because the
diﬀerent coded problems resulted in diﬀerent trees, and the decoding step
(3) above has a similar eﬀect as averaging.
1The Hamming distance between two vectors is the number of mismatches betw een
corresponding entries.

16.2 Boosting and Regularization Paths 607
16.2 Boosting and Regularization Paths
In Section 10.12.2 of the ﬁrst edition of this book, we sugges ted an analogy
between the sequence of models produced by a gradient boosti ng algorithm
and regularized model ﬁtting in high-dimensional feature s paces. This was
primarily motivated by observing the close connection betw een a boosted
version of linear regression and the lasso (Section 3.4.2). These connec-
tions have been pursued by us and others, and here we present o ur current
thinking in this area. We start with the original motivation , which ﬁts more
naturally in this chapter on ensemble learning.
16.2.1 Penalized Regression
Intuition for the success of the shrinkage strategy (10.41) of gradient boost-
ing (page 364 in Chapter 10) can be obtained by drawing analog ies with
penalized linear regression with a large basis expansion. C onsider the dic-
tionary of all possible J-terminal node regressiontrees T={Tk}that could
be realized on the training data as basis functions in IRp. The linear model
is
f(x) =K/summationdisplay
k=1αkTk(x), (16.1)
whereK= card(T). Suppose the coeﬃcients are to be estimated by least
squares. Since the number of such trees is likely to be much la rger than
even the largest training data sets, some form of regulariza tion is required.
Let ˆα(λ) solve
min
α

N/summationdisplay
i=1/parenleftigg
yi−K/summationdisplay
k=1αkTk(xi)/parenrightigg2
+λ·J(α)

, (16.2)
J(α) is a function of the coeﬃcients that generally penalizes la rger values.
Examples are
J(α) =K/summationdisplay
k=1|αk|2ridge regression , (16.3)
J(α) =K/summationdisplay
k=1|αk|lasso, (16.4)
(16.5)
both covered in Section 3.4. As discussed there, the solutio n to the lasso
problem with moderate to large λtends to be sparse; many of the ˆ αk(λ) =
0. That is, only a small fraction of all possible trees enter t he model (16.1).

608 16. Ensemble Learning
Algorithm 16.1 Forward Stagewise Linear Regression.
1. Initialize ˇ αk= 0, k= 1,...,K. Setε >0 to some small constant,
andMlarge.
2. Form= 1 toM:
(a) (β∗,k∗) = argmin β,k/summationtextN
i=1/parenleftig
yi−/summationtextK
l=1ˇαlTl(xi)−βTk(xi)/parenrightig2
.
(b) ˇαk∗←ˇαk∗+ε·sign(β∗).
3. OutputfM(x) =/summationtextK
k=1ˇαkTk(x).
This seems reasonable since it is likely that only a small fra ction of all pos-
sible trees will be relevant in approximating any particula r target function.
However, the relevant subset will be diﬀerent for diﬀerent t argets. Those
coeﬃcients that are not set to zero are shrunk by the lasso in t hat their
absolute values are smaller than their corresponding least squares values2:
|ˆαk(λ)|<|ˆαk(0)|. Asλincreases, the coeﬃcients all shrink, each one
ultimately becoming zero.
Owing to the very large number of basis functions Tk, directly solving
(16.2) with the lasso penalty (16.4) is not possible. Howeve r, a feasible
forward stagewise strategy exists that closely approximat es the eﬀect of
the lasso, and is very similar to boosting and the forward sta gewise Algo-
rithm 10.2. Algorithm 16.1 gives the details. Although phra sed in terms
of tree basis functions Tk, the algorithm can be used with any set of ba-
sis functions. Initially all coeﬃcients are zero in line 1; t his corresponds
toλ=∞in (16.2). At each successive step, the tree Tk∗is selected that
best ﬁts the current residuals in line 2(a). Its correspondi ng coeﬃcient ˇ αk∗
is then incremented or decremented by an inﬁnitesimal amoun t in 2(b),
while all other coeﬃcients ˇ αk, k∝ne}ationslash=k∗are left unchanged. In principle, this
process could be iterated until either all the residuals are zero, orβ∗= 0.
The latter case can occur if K <N, and at that point the coeﬃcient values
represent a least squares solution. This corresponds to λ= 0 in (16.2).
After applying Algorithm 16.1 with M <∞iterations, many of the coef-
ﬁcientswillbezero,namely,thosethathaveyettobeincrem ented.Theoth-
ers will tend to have absolute values smaller than their corr esponding least
squares solution values, |ˇαk(M)|<|ˆαk(0)|. Therefore this M-iteration
solution qualitatively resembles the lasso, with Minversely related to λ.
Figure 16.1 shows an example, using the prostate data studie d in Chap-
ter 3. Here, instead of using trees Tk(X) as basis functions, we use the origi-
2IfK > N, there is in general no unique “least squares value,” since inﬁnitely man y
solutionswillexistthatﬁtthedataperfectly.Wecanpicktheminim umL1-normsolution
amongst these, which is the unique lasso solution.

16.2 Boosting and Regularization Paths 609−0.2 0.0 0.2 0.4 0.6lcavol
lweight
agelbphsvi
lcpgleasonpgg45
0.0 0.5 1.0 1.5 2.0
−0.2 0.0 0.2 0.4 0.6lcavol
lweight
agelbphsvi
lcpgleasonpgg45
0 50 100 150 200
t=/summationtext
k|αk|
CoeﬃcientsCoeﬃcientsLasso Forward Stagewise
Iteration
FIGURE 16.1. Proﬁles of estimated coeﬃcients from linear regression, for t he
prostate data studied in Chapter 3. The left panel shows the re sults from the lasso,
for diﬀerent values of the bound parameter t=/summationtext
k|αk|. The right panel shows
the results of the stagewise linear regression Algorithm 16.1, usingM= 220
consecutive steps of size ε=.01.
nalvariables Xkthemselves;thatis,amultiple linearregressionmodel.Th e
left panel displays the proﬁles of estimated coeﬃcients fro m the lasso, for
diﬀerent values of the bound parameter t=/summationtext
k|αk|. The right panel shows
the results of the stagewise Algorithm 16.1, with M= 250 and ε= 0.01.
[The left and right panels of Figure 16.1 are the same as Figur e 3.10 and
the left panel of Figure 3.19, respectively.] The similarit y between the two
graphs is striking.
Insomesituationstheresemblanceismorethanqualitative .Forexample,
if all of the basis functions Tkare mutually uncorrelated, then as ε↓0,M↑
such thatMǫ→t, Algorithm 16.1 yields exactly the same solution as the
lasso for bound parameter t=/summationtext
k|αk|(and likewise for all solutions along
the path). Of course, tree-based regressors are not uncorre lated. However,
the solution sets are also identical if the coeﬃcients ˆ αk(λ) are all monotone
functions of λ. This is often the case when the correlation between the
variables is low. When the ˆ αk(λ) are not monotone in λ, then the solution
sets are not identical. The solution sets for Algorithm 16.1 tend to change
lessrapidlywithchangingvaluesoftheregularizationpar ameterthanthose
of the lasso.

610 16. Ensemble Learning
Efron et al. (2004) make the connections more precise, by cha racterizing
the exact solution paths in the ε-limiting case. They show that the coeﬃ-
cient paths are piece-wise linear functions, both for the la sso and forward
stagewise. This facilitates eﬃcient algorithms which allo w the entire paths
to be computed with the same cost as a single least-squares ﬁt . Thisleast
angle regression algorithm is described in more detail in Section 3.8.1.
Hastie et al. (2007) show that this inﬁnitesimal forward sta gewise algo-
rithm (FS 0) ﬁts a monotone version of the lasso, which optimally reduce s
at each step the loss function for a given increase in the arc length of the
coeﬃcient path (see Sections 16.2.3 and 3.8.1). The arc-len gth for theǫ>0
case isMǫ, and hence proportional to the number of steps.
Tree boosting (Algorithm 10.3) with shrinkage (10.41) clos ely resembles
Algorithm 16.1, with the learning rate parameter νcorresponding to ε. For
squared error loss, the only diﬀerence is that the optimal tr ee to be selected
at each iteration Tk∗is approximated by the standard top-down greedy
tree-induction algorithm. For other loss functions, such a s the exponential
loss of AdaBoost and the binomial deviance, Rosset et al. (20 04a) show
similar results to what we see here. Thus, one can view tree bo osting with
shrinkage as a form of monotone ill-posed regression on all p ossible (J-
terminal node) trees, with the lasso penalty (16.4) as a regu larizer. We
return to this topic in Section 16.2.3.
The choice of no shrinkage [ ν= 1 in equation (10.41)] is analogous to
forward-stepwise regression, and its more aggressive cous in best-subset se-
lection,whichpenalizesthe numberofnonzerocoeﬃcients J(α) =/summationtext
k|αk|0.
With a small fraction of dominant variables, best subset app roaches often
work well. But with a moderate fraction of strong variables, it is well known
thatsubsetselectioncanbeexcessivelygreedy(Copas,198 3),oftenyielding
poor results when compared to less aggressive strategies su ch as the lasso
or ridge regression. The dramatic improvements often seen w hen shrinkage
is used with boosting are yet another conﬁrmation of this app roach.
16.2.2 The “Bet on Sparsity” Principle
As shown in the previous section, boosting’s forward stagew ise strategy
with shrinkage approximately minimizes the same loss funct ion with a
lasso-style L1penalty. The model is built up slowly, searching through
“model space” and adding shrunken basis functions derived f rom impor-
tant predictors. In contrast, the L2penalty is computationally much easier
to deal with, as shown in Section 12.3.7. With the basis funct ions andL2
penalty chosen to match a particular positive-deﬁnite kern el, one can solve
the corresponding optimization problem without explicitl y searching over
individual basis functions.
However, the sometimes superior performance of boosting ov er proce-
dures such as the support vector machine may be largely due to the im-
plicit use of the L1versusL2penalty. The shrinkage resulting from the

16.2 Boosting and Regularization Paths 611
L1penalty is better suited to sparsesituations, where there are few basis
functions with nonzero coeﬃcients (among all possible choi ces).
We can strengthen this argument through a simple example, ta ken from
Friedman et al. (2004). Suppose we have 10 ,000 data points and our model
is a linear combination of a million trees. If the true popula tion coeﬃcients
of these trees arose from a Gaussian distribution, then we kn ow that in a
Bayesian sense the best predictor is ridge regression (Exer cise 3.6). That is,
we should use an L2rather than an L1penalty when ﬁtting the coeﬃcients.
On the other hand, if there are only a small number (e.g., 1000 ) coeﬃcients
that are nonzero, the lasso ( L1penalty) will work better. We think of this
as asparsescenario, while the ﬁrst case (Gaussian coeﬃcients) is dense.
Note however that in the dense scenario, although the L2penalty is best,
neither method does very well since there is too little data f rom which to
estimate such a large number of nonzero coeﬃcients. This is t he curse of
dimensionality taking its toll. In a sparse setting, we can p otentially do
well with the L1penalty, since the number of nonzero coeﬃcients is small.
TheL2penalty fails again.
In other words, use of the L1penalty follows what we call the “bet on
sparsity” principle for high-dimensional problems:
Use a procedure that does well in sparse problems, since no pr o-
cedure does well in dense problems.
These comments need some qualiﬁcation:
•Foranygivenapplication,thedegreeofsparseness/densen essdepends
on the unknown true target function, and the chosen dictiona ryT.
•The notion of sparse versus dense is relative to the size of th e train-
ing data set and/or the noise-to-signal ratio (NSR). Larger training
sets allow us to estimate coeﬃcients with smaller standard e rrors.
Likewise in situations with small NSR, we can identify more n onzero
coeﬃcients with a given sample size than in situations where the NSR
is larger.
•Thesizeofthedictionaryplaysaroleaswell.Increasingth esizeofthe
dictionary may lead to a sparser representation for our func tion, but
the search problem becomes more diﬃcult leading to higher va riance.
Figure 16.2 illustrates these points in the context of linea r models us-
ing simulation. We compare ridge regression and lasso, both for classiﬁ-
cation and regression problems. Each run has 50 observation s with 300
independent Gaussian predictors. In the top row all 300 coeﬃ cients are
nonzero, generated from a Gaussian distribution. In the mid dle row, only
10 are nonzero and generated from a Gaussian, and the last row has 30
non zero Gaussian coeﬃcients. For regression, standard Gau ssian noise is

612 16. Ensemble Learning
0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0
0.1 0.2 0.3 0.4 0.5Ridge/Gaussian
0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0
0.1 0.2 0.3 0.4 0.5Ridge/Subset 10
0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0
0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Squared Prediction Error Explained
Noise−to−Signal RatioRegression
0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0
0.1 0.2 0.3 0.4 0.5Ridge/Gaussian
0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0
0.1 0.2 0.3 0.4 0.5Ridge/Subset 10
0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0
0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Misclassification Error Explained
Noise−to−Signal RatioClassification
FIGURE 16.2. Simulations that show the superiority of the L1(lasso) penalty
overL2(ridge) in regression and classiﬁcation. Each run has 50observations
with300independent Gaussian predictors. In the top row all 300coeﬃcients are
nonzero, generated from a Gaussian distribution. In the midd le row, only 10are
nonzero, and the last row has 30nonzero. Gaussian errors are added to the linear
predictorη(X)for the regression problems, and binary responses generated via the
inverse-logit transform for the classiﬁcation problems. Scali ng ofη(X)resulted in
the noise-to-signal ratios shown. Lasso is used in the left su b-columns, ridge in the
right. We report the optimal percentage of error explained on test data (relative
to the error of a constant model), displayed as boxplots over 20realizations for
each combination. In the only situation where ridge beats lass o (top row), neither
do well.

16.2 Boosting and Regularization Paths 613
added to the linear predictor η(X) =XTβto produce a continuous re-
sponse. For classiﬁcation the linear predictor is transfor med via the inverse-
logit to a probability, and a binary response is generated. F ive diﬀer-
ent noise-to-signal ratios are presented, obtained by scal ingη(X) prior
to generating the response. In both cases this is deﬁned to be NSR=
Var(Y|η(X))/Var(η(X)). Both the ridge regression and lasso coeﬃcient
paths were ﬁt using a series of 50 values of λcorresponding to a range of
dffrom 1 to 50 (see Chapter 3 for details). The models were evalu ated on
a large test set (inﬁnite for Gaussian, 5000 for binary), and in each case the
value forλwas chosen to minimize the test-set error. We report percent age
variance explained for the regression problems, and percen tage misclassiﬁ-
cation error explained for the classiﬁcation problems (rel ative to a baseline
error of 0.5). There are 20 simulation runs for each scenario.
Note that for the classiﬁcation problems, we are using squar ed-error loss
to ﬁt the binary response. Note also that we do not using the tr aining
data to select λ, but rather are reporting the best possible behavior for
each method in the diﬀerent scenarios. The L2penalty performs poorly
everywhere. The Lasso performs reasonably well in the only t wo situations
where it can (sparse coeﬃcients). As expected the performan ce gets worse
as the NSR increases (less so for classiﬁcation), and as the m odel becomes
denser. The diﬀerences are less marked for classiﬁcation th an for regression.
These empirical results are supported by a large body of theo retical
results (Donoho and Johnstone, 1994; Donoho and Elad, 2003; Donoho,
2006b;CandesandTao,2007)thatsupportthesuperiorityof L1estimation
in sparse settings.
16.2.3 Regularization Paths, Over-ﬁtting and Margins
It has often been observed that boosting “does not overﬁt,” o r more as-
tutely is “slow to overﬁt.” Part of the explanation for this p henomenon was
made earlier for random forests — misclassiﬁcation error is less sensitive to
variance than is mean-squared error, and classiﬁcation is t he major focus
in the boosting community. In this section we show that the re gulariza-
tion paths of boosted models are “well behaved,” and that for certain loss
functions they have an appealing limiting form.
Figure 16.3 shows the coeﬃcient paths for lasso and inﬁnites imal forward
stagewise (FS 0) in a simulated regression setting. The data consists of a
dictionary of 1000 Gaussian variables, strongly correlate d (ρ= 0.95) within
blocks of 20, but uncorrelated between blocks. The generati ng model has
nonzero coeﬃcients for 50 variables, one drawn from each blo ck, and the
coeﬃcient values are drawn from a standard Gaussian. Finall y, Gaussian
noise is added, with a noise-to-signal ratio of 0 .72 (Exercise 16.1.) The
FS0algorithm is a limiting form of algorithm 16.1, where the ste p sizeε
is shrunk to zero (Section 3.8.1). The grouping of the variab les is intended
to mimic the correlations of nearby trees, and with the forwa rd-stagewise

614 16. Ensemble Learning
0.0 0.2 0.4 0.6 0.8 1.0−20 −10 0 10 20 30Standardized CoefficientsLASSO
0.0 0.2 0.4 0.6 0.8 1.0−20 −10 0 10 20 30Standardized CoefficientsForward Stagewise
|α(m)|/|α(∞)| |α(m)|/|α(∞)|
FIGURE 16.3. Comparison of lasso and inﬁnitesimal forward stagewise path s
on simulated regression data. The number of samples is 60and the number of
variables is 1000. The forward-stagewise paths ﬂuctuate less than those of lass o
in the ﬁnal stages of the algorithms.
algorithm,thissetupisintendedasanidealizedversionof gradientboosting
with shrinkage. For both these algorithms, the coeﬃcient pa ths can be
computed exactly, since they are piecewise linear (see the L ARS algorithm
in Section 3.8.1).
Here the coeﬃcient proﬁles are similar only in the early stag es of the
paths. For the later stages, the forward stagewise paths ten d to be mono-
tone and smoother, while those for the lasso ﬂuctuate widely . This is due
to the strong correlations among subsets of the variables —la sso suﬀers
somewhat from the multi-collinearity problem (Exercise 3. 28).
The performance of the two models is rather similar (Figure 1 6.4), and
theyachieveaboutthesameminimum.Inthelaterstagesforw ardstagewise
takes longer to overﬁt, a likely consequence of the smoother paths.
Hastie et al. (2007) show that FS 0solves amonotone version of the lasso
problem for squared error loss. Let Ta=T ∪{−T} be the augmented
dictionary obtained by including a negative copy of every ba sis element
inT. We consider models f(x) =/summationtext
Tk∈TaαkTk(x) with non-negative co-
eﬃcientsαk≥0. In this expanded space, the lasso coeﬃcient paths are
positive, while those of FS 0are monotone nondecreasing.
The monotone lasso path is characterized by a diﬀerential eq uation
∂α
∂ℓ=ρml(α(ℓ)), (16.6)

16.2 Boosting and Regularization Paths 615
o
oo
o
o
oo
oo
oo
oooo
ooo
ooooooooooooooooooooooooooooooo ooooooo ooooooooooooooooooooooooooooooooooooooooo ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo oo ooo
oo
o
o o
oo
ooo
ooo oo
o ooooo
oooooooo ooooo ooo ooooooooooo oooooo o ooo oooo oo ooooo oo oo ooooooo oo oo ooooo oo oo oo oo ooo oo oo oo ooooooo oo ooo oooooo oo ooo oo oooooooo oooo ooo ooooooo ooo ooo ooooo oooo ooo oooooo ooooooo oooooooooooooo ooooo ooooooooooo ooooooooo oo oooo oooooo oo ooooooooooo ooo oo ooo oo ooo oooooooooo ooooo ooooooo ooooooooo oooo oooooo oooo ooo oooooo oooo oo oo oooo ooooo ooooo ooooooo ooooo o o oooooooooooooooooo oo ooo oo oo oo oooo o oo o o o o o oo o o oooo oo o oo o o oo oo o ooo o o o ooo o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o o
0 10 20 30 40 50 60 7025 30 35 40 45 50 55Lasso
Forward Stagewise
|α(m)|Mean Squared Error
FIGURE 16.4. Mean squared error for lasso and inﬁnitesimal forward stagew ise
on the simulated data. Despite the diﬀerence in the coeﬃcient paths, the two
models perform similarly over the critical part of the regulariz ation path. In the
right tail, lasso appears to overﬁt more rapidly.
with initial condition α(0) = 0, where ℓis theL1arc-length of the path
α(ℓ) (Exercise 16.2). The monotone lasso move direction (veloc ity vector)
ρml(α(ℓ)) decreases the loss at the optimal quadratic rate per unit i ncrease
in theL1arc-length of the path. Since ρml
k(α(ℓ))≥0∀k,ℓ, the solution
paths are monotone.
The lasso can similarly be characterized as the solution to a diﬀerential
equation as in (16.6), except that the move directions decre ase the loss
optimally per unit increase in the L1norm of the path. As a consequence,
they are not necessarily positive, and hence the lasso paths need not be
monotone.
In this augmented dictionary, restricting the coeﬃcients t o be positive is
natural, since it avoids an obvious ambiguity. It also ties i n more naturally
with tree boosting—we always ﬁnd trees positively correlate d with the
current residual.
There have been suggestions that boosting performs well (fo r two-class
classiﬁcation) because it exhibits maximal-margin proper ties, much like the
support-vector machines of Chapters 4.5.2 and 12. Schapire et al. (1998)
deﬁne the normalized L1margin of a ﬁtted model f(x) =/summationtext
kαkTk(x) as
m(f) = min
iyif(xi)/summationtextK
k=1|αk|. (16.7)
Here the minimum is taken over the training sample, and yi∈{−1,+1}.
Unlike the L2margin (4.40) of support vector machines, the L1margin
m(f) measures the distance to the closest training point in L∞units (max-
imum coordinate distance).

616 16. Ensemble Learning
−0.3 −0.2 −0.1 0.0 0.1
Number of TreesMargin
0 2K 4K 6K 8K 10K
0.25 0.26 0.27 0.28
Number of TreesTest Error
0 2K 4K 6K 8K 10K
FIGURE 16.5. The left panel shows the L1marginm(f)for the Adaboost clas-
siﬁer on the mixture data, as a function of the number of 4-node trees. The model
was ﬁt using the R package gbm, with a shrinkage factor of 0.02. After10,000
trees,m(f)has settled down. Note that when the margin crosses zero, the t raining
error becomes zero. The right panel shows the test error, whi ch is minimized at
240trees. In this case, Adaboost overﬁts dramatically if run to c onvergence.
Schapire et al. (1998) prove that with separable data, Adabo ost in-
creasesm(f) with each iteration, converging to a margin-symmetric so-
lution. R¨ atsch and Warmuth (2002) prove the asymptotic con vergence of
Adaboost with shrinkage to a L1-margin-maximizing solution. Rosset et
al. (2004a) consider regularized models of the form (16.2) f or general loss
functions. Theyshow that as λ↓0,for particular loss functions the solution
converges to a margin-maximizing conﬁguration. In particu lar they show
this to be the case for the exponential loss of Adaboost, as we ll as binomial
deviance.
Collecting together the results of this section, we reach th e following
summary for boosted classiﬁers:
The sequence of boosted classiﬁers form an L1-regularized mono-
tone path to a margin-maximizing solution.
Ofcoursethemargin-maximizingendofthepathcanbeaveryp oor,overﬁt
solution, as it is in the example in Figure 16.5. Early stoppi ng amounts
to picking a point along the path, and should be done with the a id of a
validation dataset.
16.3 Learning Ensembles
Theinsightslearnedfromtheprevioussectionscanbeharne ssedtoproduce
a more eﬀective and eﬃcient ensemble model. Again we conside r functions

16.3 Learning Ensembles 617
of the form
f(x) =α0+/summationdisplay
Tk∈TαkTk(x), (16.8)
whereTis a dictionary of basis functions, typically trees. For gra dient
boosting and random forests, |T|is very large, and it is quite typical for the
ﬁnal model to involve many thousands of trees. In the previou s section we
arguethatgradientboostingwithshrinkageﬁtsan L1regularizedmonotone
path in this space of trees.
Friedman and Popescu (2003) propose a hybrid approach which breaks
this process down into two stages:
•A ﬁnite dictionary TL={T1(x),T2(x),...,T M(x)}of basis functions
is induced from the training data;
•A family of functions fλ(x) is built by ﬁtting a lasso path in this
dictionary:
α(λ) = argmin
αN/summationdisplay
i=1L[yi,α0+M/summationdisplay
m=1αmTm(xi)]+λM/summationdisplay
m=1|αm|.(16.9)
In its simplest form this model could be seen as a way of post-p rocessing
boosting or random forests, taking for TLthe collection of trees produced
by the gradient boosting or random forest algorithms. By ﬁtt ing the lasso
pathtothesetrees,wewouldtypically useamuchreducedset ,whichwould
save in computations and storage for future predictions. In the next section
wedescribemodiﬁcationsofthisprescriptionthatreducet hecorrelationsin
the ensembleTL, and improve the performance of the lasso post processor.
As an initial illustration, we apply this procedure to a rand om forest
ensemble grown on the spam data.
Figure 16.6 shows that a lasso post-processing oﬀers modest improve-
ment over the random forest (blue curve), and reduces the for est to about
40 trees, rather than the original 1000. The post-processed performance
matches that of gradient boosting. The orange curves repres ent a modiﬁed
version of random forests, designed to reduce the correlati ons between trees
even more. Here a random sub-sample (without replacement) o f 5% of the
training sample is used to grow each tree, and the trees are re stricted to be
shallow (about six terminal nodes). The post-processing oﬀ ers more dra-
matic improvements here, and the training costs are reduced by a factor
of about 100. However, the performance of the post-processe d model falls
somewhat short of the blue curves.
16.3.1 Learning a Good Ensemble
Not all ensembles TLwill perform well with post-processing. In terms of
basis functions, we want a collection that covers the space w ell in places

618 16. Ensemble Learning
0 100 200 300 400 5000.04 0.05 0.06 0.07 0.08 0.09Spam Data
Number of TreesTest ErrorRandom Forest
Random Forest (5%, 6)
Gradient Boost (5 node)
FIGURE 16.6. Application of the lasso post-processing (16.9) to the spam da ta.
The horizontal blue line is the test error of a random forest ﬁt t o the spam data,
using1000trees grown to maximum depth (with m= 7; see Algorithm 15.1).
The jagged blue curve is the test error after post-processing the ﬁrst 500trees
using the lasso, as a function of the number of trees with nonze ro coeﬃcients.
The orange curve/line use a modiﬁed form of random forest, whe re a random
draw of 5% of the data are used to grow each tree, and the trees are force d to
be shallow (typically six terminal nodes). Here the post-proc essing oﬀers much
greater improvement over the random forest that generated t he ensemble.
where they are needed, and are suﬃciently diﬀerent from each other for
the post-processor to be eﬀective.
Friedman and Popescu (2003) gain insights from numerical qu adrature
and importance sampling. They view the unknown function as a n integral
f(x) =/integraldisplay
β(γ)b(x;γ)dγ, (16.10)
whereγ∈Γ indexes the basis functions b(x;γ). For example, if the basis
functions are trees, then γindexes the splitting variables, the split-points
and the values in the terminal nodes. Numerical quadrature a mounts to
ﬁnding a set of Mevaluation points γm∈Γ and corresponding weights
αmso thatfM(x) =α0+/summationtextM
m=1αmb(x;γm) approximates f(x) well over
the domain of x. Importance sampling amounts to sampling γat random,
but giving more weight to relevant regions of the space Γ. Fri edman and
Popescu (2003) suggest a measure of (lack of) relevance that uses the loss
function (16.9):

16.3 Learning Ensembles 619
Q(γ) = min
c0,c1N/summationdisplay
i=1L(yi,c0+c1b(xi;γ)), (16.11)
evaluated on the training data.
If a single basis function were to be selected (e.g., a tree), it would be
the global minimizer γ∗= argmin γ∈ΓQ(γ). Introducing randomness in the
selection of γwould necessarily produce less optimal values with Q(γ)≥
Q(γ∗). They propose a natural measure of the characteristic widthσof the
sampling scheme S,
σ= ES[Q(γ)−Q(γ∗)]. (16.12)
•σtoo narrow suggests too many of the b(x;γm) look alike, and similar
tob(x;γ∗);
•σtoo wide implies a large spread in the b(x;γm), but possibly con-
sisting of many irrelevant cases.
Friedman and Popescu (2003) use sub-sampling as a mechanism for intro-
ducing randomness, leading to their ensemble-generation a lgorithm 16.2.
Algorithm 16.2 ISLE Ensemble Generation.
1.f0(x) = argmin c/summationtextN
i=1L(yi,c)
2. Form= 1 toMdo
(a)γm= argmin γ/summationtext
i∈Sm(η)L(yi,fm−1(xi)+b(xi;γ))
(b)fm(x) =fm−1(x)+νb(x;γm)
3.TISLE={b(x;γ1),b(x;γ2),...,b(x;γM)}.
Sm(η) refers to a subsample of N·η(η∈(0,1]) of the training obser-
vations, typically withoutreplacement. Their simulations suggest picking
η≤1
2, and for large Npickingη∼1/√
N. Reducing ηincreases the
randomness, and hence the width σ. The parameter ν∈[0,1] introduces
memory into the randomization process; the larger ν, the more the pro-
cedure avoids b(x;γ) similar to those found before. A number of familiar
randomization schemes are special cases of Algorithm 16.2:
Bagging hasη= 1, but samples with replacement, and has ν= 0. Fried-
man and Hall (2007) argue that sampling without replacement with
η= 1/2 is equivalent to sampling with replacement with η= 1, and
the former is much more eﬃcient.

620 16. Ensemble Learning
Random forest sampling is similar, with more randomness introduced by
the selection of the splitting variable. Reducing η <1/2 in algo-
rithm 16.2 has a similar eﬀect to reducing min random forests, but
does not suﬀer from the potential biases discussed in Sectio n 15.4.2.
Gradient boosting with shrinkage (10.41) uses η= 1, but typically does
not produce suﬃcient width σ.
Stochastic gradient boosting (Friedman, 1999) follows the recipe exactly.
The authors recommend values ν= 0.1 andη≤1
2, and call their combined
procedure (ensemble generation and post processing) Importance sampled
learning ensemble (ISLE).
Figure 16.7 shows the performance of an ISLE on the spam data. It does
0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060Spam Data
Number of TreesTest ErrorGradient Boosting (5 Node)
Lasso Post−processed
FIGURE 16.7. Importance sampling learning ensemble (ISLE) ﬁt to the spam
data. Here we used η= 1/2,ν= 0.05, and trees with ﬁve terminal nodes. The
lasso post-processed ensemble does not improve the predictio n error in this case,
but it reduces the number of trees by a factor of ﬁve.
not improve the predictive performance, but is able to produ ce a more
parsimonious model. Note that in practice the post-process ing includes
the selection of the regularization parameter λin (16.9), which would be

16.3 Learning Ensembles 621
chosen by cross-validation. Here we simply demonstrate the eﬀects of post-
processing by showing the entire path on the test data.
Figure 16.8 shows various ISLEs on a regression example. The generating
0 500 1000 1500 2000 25001.0 1.5 2.0 2.5 3.0 3.5
Number of TreesMean Squared ErrorGBM (1, 0.01)
GBM (0.1, 0.01)
ISLE  GB
ISLE RF
Random Forest
FIGURE 16.8. Demonstration of ensemble methods on a regression simulation
example. The notation GBM (0.1, 0.01) refers to a gradient boos ted model, with
parameters (η,ν). We report mean-squared error from the true (known) functio n.
Note that the sub-sampled GBM model (green) outperforms the fu ll GBM model
(orange). The lasso post-processed version achieves similar error. The random
forest is outperformed by its post-processed version, but b oth fall short of the
other models.
function is
f(X) = 10·5/productdisplay
j=1e−2X2
j+35/summationdisplay
j=6Xj, (16.13)
whereX∼U[0,1]100(the last 65 elements are noise variables). The re-
sponseY=f(X)+εwhereε∼N(0,σ2); we chose σ= 1.3 resulting in a
signal-to-noise ratio of approximately 2. We used a trainin g sample of size
1000, and estimated the mean squared error E( ˆf(X)−f(X))2by averaging
over a test set of 500 samples. The sub-sampled GBM curve (lig ht blue)
is an instance of stochastic gradient boosting (Friedman, 1999) discussed in
Section 10.12, and it outperforms gradient boosting on this example.

622 16. Ensemble Learning
16.3.2 Rule Ensembles
Here we describe a modiﬁcation of the tree-ensemble method t hat focuses
on individual rules (Friedman and Popescu, 2003). We encoun tered rules
in Section 9.3 in the discussion of the PRIM method. The idea i s to enlarge
an ensemble of trees by constructing a set of rules from each o f the trees
in the collection.
1 2
30
4
5 6X1<2.1 X1≥2.1
X3∈ {M,L} X3∈ {S}
X7<4.5 X7≥4.5
FIGURE 16.9. A typical tree in an ensemble, from which rules can be derived.
Figure 16.9 depicts a small tree, with numbered nodes. The fo llowing
rules can be derived from this tree:
R1(X) =I(X1<2.1)
R2(X) =I(X1≥2.1)
R3(X) =I(X1≥2.1)·I(X3∈ {S})
R4(X) =I(X1≥2.1)·I(X3∈ {M,L})
R5(X) =I(X1≥2.1)·I(X3∈ {S})·I(X7<4.5)
R6(X) =I(X1≥2.1)·I(X3∈ {S})·I(X7≥4.5)(16.14)
A linear expansion in rules 1, 4, 5 and 6 is equivalent to the tr ee itself
(Exercise 16.3); hence (16.14) is an over-complete basis for the tree.
For each tree Tmin an ensembleT, we can construct its mini-ensemble
of rulesTm
RULE, and then combine them all to form a larger ensemble
TRULE=M/uniondisplay
m=1Tm
RULE. (16.15)
This is then treated like any other ensemble, and post-proce ssed via the
lasso or similar regularized procedure.
There are several advantages to this approach of deriving ru les from the
more complex trees:
•The space of models is enlarged, and can lead to improved perf or-
mance.

16.3 Learning Ensembles 623
Rules Rules + Linear0.9 1.0 1.1 1.2 1.3Mean Squared Error
FIGURE 16.10. Mean squared error for rule ensembles, using 20realizations
of the simulation example (16.13).
•Rules are easier to interpret than trees, so there is the pote ntial for
a simpliﬁed model.
•It is often natural to augment TRULEby including each variable Xj
separately as well, thus allowing the ensemble to model line ar func-
tions well.
FriedmanandPopescu(2008)demonstratethepowerofthispr ocedureona
number of illustrative examples, including the simulation example (16.13).
Figure 16.10 shows boxplots of the mean-squared error from t he true model
for twenty realizations from this model. The models were all ﬁt using the
Rulefitsoftware, available on the ESL homepage3, which runs in an auto-
matic mode.
On the same training set as used in Figure 16.8, the rule based model
achieved a mean-squared error of 1.06. Although slightly wo rse than the
best achieved in that ﬁgure, the results are not comparable b ecause cross-
validation was used here to select the ﬁnal model.
Bibliographic Notes
As noted in the introduction, many of the new methods inmachi ne learning
have been dubbed “ensemble” methods. These include neural n etworks
boosting, bagging and random forests; Dietterich (2000a) g ives a survey of
tree-based ensemble methods. Neural networks (Chapter 11) are perhaps
moredeservingofthename,sincetheysimultaneouslylearn theparameters
3ESL homepage: www-stat.stanford.edu/ElemStatLearn

624 16. Ensemble Learning
of the hidden units (basis functions), along with how to comb ine them.
Bishop (2006) discusses neural networks in some detail, alo ng with the
Bayesian perspective (MacKay, 1992; Neal, 1996). Support v ector machines
(Chapter 12) can also be regarded as an ensemble method; they perform
L2regularized model ﬁtting in high-dimensional feature spac es. Boosting
and lasso exploit sparsity through L1regularization to overcome the high-
dimensionality, while SVMs rely on the “kernel trick” chara cteristic of L2
regularization.
C5.0 (Quinlan, 2004) is a commercial tree and rule generatio n package,
with some goals in common with Rulefit.
There is a vast and varied literature often referred to as “co mbining clas-
siﬁers” which abounds in ad-hoc schemes for mixing methods o f diﬀerent
types to achieve better performance. For a principled appro ach, see Kittler
et al. (1998).
Exercises
Ex. 16.1 Describe exactly how to generate the block correlated data u sed
in the simulation in Section 16.2.3.
Ex. 16.2 Letα(t)∈IRpbe a piecewise-diﬀerentiable and continuous coef-
ﬁcient proﬁle, with α(0) = 0. The L1arc-length of αfrom time 0 to tis
deﬁned by
Λ(t) =/integraldisplayt
0|˙α(t)|1dt. (16.16)
Show that Λ( t)≥|α(t)|1, with equality iﬀ α(t) is monotone.
Ex. 16.3 Show that ﬁtting a linear regression model using rules 1, 4, 5 and
6 in equation (16.14) gives the same ﬁt as the regression tree corresponding
to this tree. Show the same is true for classiﬁcation, if a log istic regression
model is ﬁt.
Ex. 16.4 Program and run the simulation study described in Figure 16. 2.

This is page 625
Printer: Opaque this
17
Undirected Graphical Models
17.1 Introduction
A graph consists of a set of vertices (nodes), along with a set of edges join-
ing some pairs of the vertices. In graphical models, each ver tex represents
a random variable, and the graph gives a visual way of underst anding the
joint distribution of the entire set of random variables. Th ey can be use-
ful for either unsupervised or supervised learning. In an undirected graph ,
the edges have no directional arrows. We restrict our discus sion to undi-
rected graphical models, also known as Markov random ﬁelds orMarkov
networks . In these graphs, the absence of an edge between two vertices has
a special meaning: the corresponding random variables are c onditionally
independent, given the other variables.
Figure 17.1 shows an example of a graphical model for a ﬂow-cy tometry
dataset with p= 11 proteins measured on N= 7466 cells, from Sachs
et al. (2005). Each vertex in the graph corresponds to the rea l-valued ex-
pression level of a protein. The network structure was estim ated assuming
a multivariate Gaussian distribution, using the graphical lasso procedure
discussed later in this chapter.
Sparsegraphshavearelativelysmallnumberofedges,andar econvenient
for interpretation. They are useful in a variety of domains, including ge-
nomics and proteomics, where they provide rough models of ce ll pathways.
Much work has been done in deﬁning and understanding the stru cture of
graphical models; see the Bibliographic Notes for referenc es.

626 17. Undirected Graphical Models
Raf
Mek
Plcg
PIP2
PIP3
Erk AktPKAPKCP38Jnk
FIGURE 17.1. Example of a sparse undirected graph, estimated from a ﬂow
cytometry dataset, with p= 11proteins measured on N= 7466cells. The net-
work structure was estimated using the graphical lasso proce dure discussed in this
chapter.
As we will see, the edges in a graph are parametrized by values orpo-
tentialsthat encode the strength of the conditional dependence betw een
the random variables at the corresponding vertices. The mai n challenges in
working with graphical models are model selection (choosin g the structure
of the graph), estimation of the edge parameters from data, a nd compu-
tation of marginal vertex probabilities and expectations, from their joint
distribution. The last two tasks are sometimes called learningandinference
in the computer science literature.
We do not attempt a comprehensive treatment of this interest ing area.
Instead, we introduce some basic concepts, and then discuss a few sim-
ple methods for estimation of the parameters and structure o f undirected
graphical models; methods that relate to the techniques alr eady discussed
in this book. The estimation approaches that we present for c ontinuous
and discrete-valued vertices are diﬀerent, so we treat them separately. Sec-
tions 17.3.1 and 17.3.2 may be of particular interest, as the y describe new,
regression-based procedures for estimating graphical mod els.
There is a large and active literature on directed graphical models or
Bayesian networks ; these are graphical models in which the edges have
directional arrows (but no directed cycles). Directed grap hical models rep-
resent probability distributions that can be factored into products of condi-
tional distributions, and have the potential for causal int erpretations. We
refer the reader to Wasserman (2004) for a brief overview of b oth undi-
rected and directed graphs; the next section follows closel y his Chapter 18.

17.2 Markov Graphs and Their Properties 627
X
XXX
YYYY
Z
ZZ
Z
WWW
(a) (b)
(c) (d)
FIGURE 17.2. Examples of undirected graphical models or Markov networks.
Each node or vertex represents a random variable, and the lack o f an edge between
two nodes indicates conditional independence. For example, in graph (a), Xand
Zare conditionally independent, given Y. In graph (b), Zis independent of each
ofX,Y, andW.
A longer list of useful references is given in the Bibliograp hic Notes on
page 645.
17.2 Markov Graphs and Their Properties
In this section we discuss the basic properties of graphs as m odels for the
joint distribution of a set of random variables. We defer dis cussion of (a)
parametrization and estimation of the edge parameters from data, and (b)
estimation of the topology of a graph, to later sections.
Figure17.2showsfourexamplesofundirectedgraphs.Agrap hGconsists
of a pair (V,E), whereVis a set of vertices and Ethe set of edges (deﬁned
by pairs of vertices). Two vertices XandYare called adjacent if there
is a edge joining them; this is denoted by X∼Y. ApathX1,X2,...,X n
is a set of vertices that are joined, that is Xi−1∼Xifori= 2,...,n. A
complete graph is a graph with every pair of vertices joined by an edge.
AsubgraphU∈Vis a subset of vertices together with their edges. For
example, (X,Y,Z) in Figure 17.2(a) form a path but not a complete graph.
Suppose that we have a graph Gwhose vertex set Vrepresents a set of
random variables having joint distribution P. In a Markov graph G, the
absence of an edge implies that the corresponding random var iables are
conditionally independent given the variables at the other vertices. This is
expressed with the following notation:

628 17. Undirected Graphical Models
No edge joining XandY⇐⇒X⊥Y|rest (17.1)
where “rest” refers to all of the other vertices in the graph. For example
in Figure 17.2(a) X⊥Z|Y. These are known as the pairwise Markov
independencies ofG.
IfA,BandCare subgraphs, then Cis said to separateAandBif every
path between AandBintersects a node in C. For example, Yseparates
XandZin Figures 17.2(a) and (d), and ZseparatesYandWin (d). In
Figure 17.2(b) Zis not connected to X,Y,W so we say that the two sets
are separated by the empty set. In Figure 17.2(c), C={X,Z}separates
YandW.
Separators have the nice property that they break the graph i nto con-
ditionally independent pieces. Speciﬁcally, in a Markov gr aphGwith sub-
graphsA,BandC,
ifCseparatesAandBthenA⊥B|C. (17.2)
These are known as the global Markov properties ofG. It turns out that the
pairwise and global Markov properties of a graph are equival ent (for graphs
with positive distributions). That is, the set of graphs wit h associated prob-
ability distributions that satisfy the pairwise Markov ind ependencies and
global Markov assumptions are the same. This result is usefu l for inferring
global independence relations from simple pairwise proper ties. For example
in Figure 17.2(d) X⊥Z|{Y,W}since it is a Markov graph and there is no
link joining XandZ. ButYalso separates XfromZandWand hence by
the global Markov assumption we conclude that X⊥Z|YandX⊥W|Y.
Similarly we have Y⊥W|Z.
The global Markov property allows us to decompose graphs int o smaller
more manageable pieces and thus leads to essential simpliﬁc ations in com-
putation and interpretation. For this purpose we separate t he graph into
cliques. A cliqueis a complete subgraph— a set of vertices that are all
adjacent to one another; it is called maximal if it is a clique and no other
vertices can be added to it and still yield a clique. The maxim al cliques for
the graphs of Figure 17.2 are
(a){X,Y},{Y,Z},
(b){X,Y,W},{Z},
(c){X,Y},{Y,Z},{Z,W},{X,W}, and
(d){X,Y},{Y,Z},{Z,W}.
Although the following applies to both continuous and discr ete distri-
butions, much of the development has been for the latter. A pr obability
density function fover a Markov graph Gcan be can represented as

17.2 Markov Graphs and Their Properties 629
f(x) =1
Z/productdisplay
C∈CψC(xC) (17.3)
whereCis the set of maximal cliques, and the positive functions ψC(·) are
calledclique potentials . These are not in general density functions1, but
rather are aﬃnities that capture the dependence in XCby scoring certain
instancesxChigher than others. The quantity
Z=/summationdisplay
x∈X/productdisplay
C∈CψC(xC) (17.4)
is the normalizing constant, also known as the partition function. Alterna-
tively, the representation (17.3) implies a graph with inde pendence prop-
erties deﬁned by the cliques in the product. This result hold s for Markov
networksGwith positive distributions, and is known as the Hammersley-
Cliﬀordtheorem (Hammersley and Cliﬀord, 1971; Cliﬀord, 1990).
Many of the methods for estimation and computation on graphs ﬁrst de-
compose the graph into its maximal cliques. Relevant quanti ties are com-
puted in the individual cliques and then accumulated across the entire
graph. A prominent example is the join tree orjunction tree algorithm for
computing marginal and low order probabilities from the joi nt distribution
on a graph. Details can be found in Pearl (1986), Lauritzen an d Spiegel-
halter (1988), Pearl (1988), Shenoy and Shafer (1988), Jens en et al. (1990),
or Koller and Friedman (2007).
XY
Z
FIGURE 17.3. A complete graph does not uniquely specify the higher-order
dependence structure in the joint distribution of the varia bles.
A graphical model does not always uniquely specify the highe r-order
dependence structure of a joint probability distribution. Consider the com-
plete three-node graph in Figure 17.3. It could represent th e dependence
structure of either of the following distributions:
f(2)(x,y,z) =1
Zψ(x,y)ψ(x,z)ψ(y,z);
f(3)(x,y,z) =1
Zψ(x,y,z).(17.5)
The ﬁrst speciﬁes only second order dependence (and can be re presented
with fewer parameters). Graphical models for discrete data are a special
1If the cliques are separated, then the potentials can be densities, but thi s is in general
not the case.

630 17. Undirected Graphical Models
case ofloglinear models for multiway contingency tables (Bishop et al.,
1975, e.g.); in that language f(2)is referred to as the “no second-order
interaction” model.
For the remainder of this chapter we focus on pairwise Markov graphs
(Koller and Friedman, 2007). Here there is a potential funct ion for each
edge (pair of variables as in f(2)above), and at most second–order interac-
tions are represented. These are more parsimonious in terms of parameters,
easier to work with, and give the minimal complexity implied by the graph
structure. The models for both continuous and discrete data are functions
of only the pairwise marginal distributions of the variable s represented in
the edge set.
17.3 Undirected Graphical Models for Continuous
Variables
Here we consider Markov networks where all the variables are continuous.
The Gaussian distribution is almost always used for such gra phical models,
becauseofitsconvenientanalyticalproperties.Weassume thattheobserva-
tionshaveamultivariateGaussiandistributionwithmean µandcovariance
matrixΣ. Since the Gaussian distribution represents at most second -order
relationships,itautomatically encodesapairwiseMarkov graph.Thegraph
in Figure 17.1 is an example of a Gaussian graphical model.
The Gaussian distribution has the property that all conditi onal distri-
butions are also Gaussian. The inverse covariance matrix Σ−1contains
information about the partial covariances between the variables; that is,
the covariances between pairs iandj, conditioned on all other variables.
Inparticular,ifthe ijthcomponentof Θ=Σ−1iszero,thenvariables iand
jare conditionally independent, given the other variables ( Exercise 17.3).
It is instructive to examine the conditional distribution o f one variable
versus the rest, where the role of Θis explicit. Suppose we partition X=
(Z,Y) whereZ= (X1,...,X p−1) consists of the ﬁrst p−1 variables and
Y=Xpis the last. Then we have the conditional distribution of YgiveZ
(Mardia et al., 1979, e.g.)
Y|Z=z∼N/parenleftbig
µY+(z−µZ)TΣ−1
ZZσZY, σYY−σT
ZYΣ−1
ZZσZY/parenrightbig
,(17.6)
where we have partitioned Σas
Σ=/parenleftbiggΣZZσZY
σT
ZYσYY/parenrightbigg
. (17.7)
The conditional mean in (17.6) has exactly the same form as th e pop-
ulation multiple linear regression of YonZ, with regression coeﬃcient
β=Σ−1
ZZσZY[see (2.16) on page 19]. If we partition Θin the same way,
sinceΣΘ=Istandard formulas for partitioned inverses give

17.3 Undirected Graphical Models for Continuous Variables 6 31
θZY=−θYY·Σ−1
ZZσZY, (17.8)
where 1/θYY=σYY−σT
ZYΣ−1
ZZσZY>0. Hence
β=Σ−1
ZZσZY
=−θZY/θYY.(17.9)
We have learned two things here:
•The dependence of YonZin (17.6) is in the mean term alone. Here
we see explicitly that zero elements in βand henceθZYmean that
the corresponding elements of Zare conditionally independent of Y,
given the rest.
•Wecanlearnaboutthis dependencestructurethroughmultip le linear
regression.
ThusΘcaptures all the second-order information (both structura l and
quantitative) needed to describe the conditional distribu tion of each node
given the rest, and is the so-called “natural” parameter for the Gaussian
graphical model2.
Another(diﬀerent)kindofgraphicalmodelisthe covariance graph orrel-
evance network ,inwhichverticesareconnectedbybidirectionaledgesift he
covariance (rather than the partial covariance) between th e corresponding
variables is nonzero. These are popular in genomics, see esp ecially Butte
et al. (2000). The negative log-likelihood from these model s is not convex,
making the computations more challenging (Chaudhuri et al. , 2007).
17.3.1 Estimation of the Parameters when the Graph
Structure is Known
Given some realizations of X, we would like to estimate the parameters
of an undirected graph that approximates their joint distri bution. Suppose
ﬁrst that the graph is complete (fully connected). We assume that we have
Nmultivariate normal realizations xi, i= 1,...,Nwith population mean
µand covariance Σ. Let
S=1
NN/summationdisplay
i=1(xi−¯x)(xi−¯x)T(17.10)
betheempiricalcovariancematrix,with ¯ xthesamplemeanvector.Ignoring
constants, the log-likelihood of the data can be written as
2The distribution arising from a Gaussian graphical model is a Wishar t distribution.
This is a member of the exponential family, with canonical or “natur al” parameter
Θ=Σ−1. Indeed, the partially maximized log-likelihood (17.11) is (up t o constants)
the Wishart log-likelihood.

632 17. Undirected Graphical Models
ℓ(Θ) = logdet Θ−trace(SΘ). (17.11)
In (17.11) we have partially maximized with respect to the me an parameter
µ. The quantity−ℓ(Θ) is a convex function of Θ. It is easy to show that
the maximum likelihood estimate of Σis simply S.
Now to make the graph more useful (especially in high-dimens ional set-
tings) let’s assume that some of the edges are missing; for ex ample, the
edge between PIP3andErkis one of several missing in Figure 17.1. As we
have seen, for the Gaussian distribution this implies that t he correspond-
ing entries of Θ=Σ−1are zero. Hence we now would like to maximize
(17.11) under the constraints that some pre-deﬁned subset o f the parame-
ters are zero. This is an equality-constrained convex optim ization problem,
and a number of methods have been proposed for solving it, in p articular
the iterative proportional ﬁtting procedure (Speed and Kii veri, 1986). This
and other methods are summarized for example in Whittaker (1 990) and
Lauritzen (1996). These methods exploit the simpliﬁcation s that arise from
decomposing the graph into its maximal cliques, as describe d in the previ-
ous section. Here we outline a simple alternate approach, th at exploits the
sparsity in a diﬀerent way. The fruits of this approach will b ecome apparent
later when we discuss the problem of estimation of the graph s tructure.
The idea is based on linear regression, as inspired by (17.6) and (17.9).
In particular, suppose that we want to estimate the edge para metersθijfor
the vertices that are joined to a given vertex i, restricting those that are not
joined to be zero. Then it would seem that the linear regressi on of the node
ivalues on the other relevant vertices might provide a reason able estimate.
But this ignores the dependence structure among the predict ors in this
regression. It turns out that if instead we use our current (m odel-based)
estimate of the cross-product matrix of the predictors when we perform
our regressions, this gives the correct solutions and solve s the constrained
maximum-likelihood problem exactly. We now give details.
To constrain the log-likelihood (17.11), we add Lagrange co nstants for
all missing edges
ℓC(Θ) = logdet Θ−trace(SΘ)−/summationdisplay
(j,k)/ne}ationslash∈Eγjkθjk. (17.12)
The gradient equation for maximizing (17.12) can be written as
Θ−1−S−Γ=0, (17.13)
using the fact that the derivative of logdet ΘequalsΘ−1(Boyd and Van-
denberghe, 2004, for example, page 641). Γis a matrix of Lagrange param-
eters with nonzero values for all pairs with edges absent.
We will show how we can use regression to solve for Θand its inverse
W=Θ−1one row and column at a time. For simplicity let’s focus on the
last row and column. Then the upper right block of equation (1 7.13) can
be written as

17.3 Undirected Graphical Models for Continuous Variables 6 33
w12−s12−γ12= 0. (17.14)
Here we have partitioned the matrices into two parts as in (17 .7): part 1
being the ﬁrst p−1 rows and columns, and part 2 the pth row and column.
WithWand its inverse Θpartitioned in a similar fashion, we have
/parenleftbiggW11w12
wT
12w22/parenrightbigg/parenleftbiggΘ11θ12
θT
12θ22/parenrightbigg
=/parenleftbiggI0
0T1/parenrightbigg
. (17.15)
This implies
w12=−W11θ12/θ22 (17.16)
=W11β (17.17)
whereβ=−θ12/θ22as in (17.9). Now substituting (17.17) into (17.14)
gives
W11β−s12−γ12= 0. (17.18)
These can be interpreted as the p−1 estimating equations for the con-
strained regression of Xpon the other predictors, except that the observed
mean cross-products matrix S11is replaced by W11, the current estimated
covariance matrix from the model.
Now we can solve (17.18) by simple subset regression. Suppos e there are
p−qnonzero elements in γ12—i.e.,p−qedges constrained to be zero. These
p−qrows carry no information and can be removed. Furthermore we can
reduceβtoβ∗by removing its p−qzero elements, yielding the reduced
q×qsystem of equations
W∗
11β∗−s∗
12= 0, (17.19)
with solution ˆβ∗=W∗
11−1s∗
12. This is padded with p−qzeros to give ˆβ.
Although it appears from (17.16) that we only recover the ele mentsθ12
up to a scale factor 1 /θ22, it is easy to show that
1
θ22=w22−wT
12β (17.20)
(using partitioned inverse formulas). Also w22=s22, since the diagonal of
Γin (17.13) is zero.
This leads to the simple iterative procedure given in Algori thm 17.1 for
estimating both ˆWand its inverse ˆΘ, subject to the constraints of the
missing edges.
Note that this algorithm makes conceptual sense. The graph e stimation
problem is not pseparate regression problems, but rather pcoupled prob-
lems. The use of the common Win step (b), in place of the observed
cross-products matrix, couples the problems together in th e appropriate
fashion. Surprisingly, we were not able to ﬁnd this procedur e in the lit-
erature. However it is related to the covariance selection p rocedures of

634 17. Undirected Graphical Models
Algorithm 17.1 A Modiﬁed Regression Algorithm for Estimation of an
Undirected Gaussian Graphical Model with Known Structure.
1. Initialize W=S.
2. Repeat for j= 1,2,...,p,1,...until convergence:
(a) Partition the matrix Winto part 1: all but the jth row and
column, and part 2: the jth row and column.
(b) Solve W∗
11β∗−s∗
12= 0 for the unconstrained edge parameters
β∗, using the reduced system of equations as in (17.19). Obtain
ˆβby padding ˆβ∗with zeros in the appropriate positions.
(c) Update w12=W11ˆβ
3. In the ﬁnal cycle (for each j) solve for ˆθ12=−ˆβ·ˆθ22, with 1/ˆθ22=
s22−wT
12ˆβ.
X1X2 X3
X4S=
10 1 5 4
1 10 2 6
5 2 10 3
4 6 3 10

FIGURE 17.4. A simple graph for illustration, along with the empirical covar i-
ance matrix.
Dempster (1972), and is similar in ﬂavor to the iterative con ditional ﬁtting
procedure for covariance graphs, proposed by Chaudhuri et a l. (2007).
Here is a little example, borrowed from Whittaker (1990). Su ppose that
our model is as depicted in Figure 17.4, along with its empiri cal covariance
matrixS. We apply algorithm (17.1) to this problem; for example, in t he
modiﬁed regression for variable 1 in step (b), variable 3 is l eft out. The
procedure quickly converged to the solutions:
ˆΣ=
10.00 1.001.314.00
1.00 10.00 2.000.87
1.312.00 10.00 3.00
4.000.873.00 10.00
,ˆΣ−1=
0.12−0.010.00−0.05
−0.01 0.11−0.020.00
0.00−0.02 0.11−0.03
−0.050.00−0.03 0.13
.
Note the zeroes in ˆΣ−1, corresponding to the missing edges (1,3) and (2,4).
Note also that the corresponding elements in ˆΣare the only elements dif-
ferent from S. The estimation of ˆΣis an example of what is sometimes
called the positive deﬁnite “completion” of S.

17.3 Undirected Graphical Models for Continuous Variables 6 35
17.3.2 Estimation of the Graph Structure
In most cases we do not know which edges to omit from our graph, and
so would like to try to discover this from the data itself. In r ecent years a
number of authors have proposed the use of L1(lasso) regularization for
this purpose.
Meinshausen and B¨ uhlmann (2006) take a simple approach to t he prob-
lem: rather than trying to fully estimate ΣorΘ=Σ−1, they only estimate
which components of θijare nonzero. To do this, they ﬁt a lasso regression
using each variable as the response and the others as predict ors. The com-
ponentθijis then estimated to be nonzero if either the estimated coeﬃc ient
of variable ionjis nonzero, orthe estimated coeﬃcient of variable jon
iis nonzero (alternatively they use an andrule). They show that asymp-
totically this procedure consistently estimates the set of nonzero elements
ofΘ.
Wecantakeamoresystematicapproachwiththelassopenalty ,following
thedevelopmentoftheprevioussection.Considermaximizi ngthepenalized
log-likelihood
logdetΘ−trace(SΘ)−λ||Θ||1, (17.21)
where||Θ||1is theL1norm—the sum of the absolute values of the elements
ofΣ−1, and we have ignored constants. The negative of this penaliz ed
likelihood is a convex function of Θ.
It turns out that one can adapt the lasso to give the exact maxi mizer of
the penalized log-likelihood. In particular, we simply rep lace the modiﬁed
regression step (b) in Algorithm 17.1 by a modiﬁed lasso step . Here are the
details.
The analog of the gradient equation (17.13) is now
Θ−1−S−λ·Sign(Θ) =0. (17.22)
Here we use sub-gradient notation, with Sign( θjk) = sign(θjk) ifθjk∝ne}ationslash= 0,
else Sign(θjk)∈[−1,1] ifθjk= 0. Continuing the development in the
previous section, we reach the analog of (17.18)
W11β−s12+λ·Sign(β) = 0 (17.23)
(recall that βandθ12haveoppositesigns). Wewill nowseethat thissystem
is exactly equivalent to the estimating equations for a lass o regression.
Consider the usual regression setup with outcome variables yand pre-
dictor matrix Z. There the lasso minimizes
1
2(y−Zβ)T(y−Zβ)+λ·||β||1 (17.24)
[see (3.52) on page 68; here we have added a factor1
2for convenience]. The
gradient of this expression is

636 17. Undirected Graphical Models
Algorithm 17.2 Graphical Lasso.
1. Initialize W=S+λI. The diagonal of Wremains unchanged in
what follows.
2. Repeat for j= 1,2,...p,1,2,...p,... until convergence:
(a) Partition the matrix Winto part 1: all but the jth row and
column, and part 2: the jth row and column.
(b) Solve the estimating equations W11β−s12+λ·Sign(β) = 0
using the cyclical coordinate-descent algorithm (17.26) f or the
modiﬁed lasso.
(c) Update w12=W11ˆβ
3. In the ﬁnal cycle (for each j) solve for ˆθ12=−ˆβ·ˆθ22, with 1/ˆθ22=
w22−wT
12ˆβ.
ZTZβ−ZTy+λ·Sign(β) = 0 (17.25)
So up to a factor 1 /N,ZTyis the analog of s12, and we replace ZTZby
W11, the estimated cross-product matrix from our current model .
The resulting procedure is called the graphical lasso , proposed by Fried-
man et al. (2008b) building on the work of Banerjee et al. (200 8). It is
summarized in Algorithm 17.2.
Friedman et al. (2008b) use the pathwise coordinate descent method
(Section 3.8.6) to solve the modiﬁed lasso problem at each st age. Here are
the details of pathwise coordinate descent for the graphica l lasso algorithm.
LettingV=W11, the update has the form
ˆβj←S/parenleftig
s12j−/summationdisplay
k/ne}ationslash=jVkjˆβk,λ/parenrightig
/Vjj (17.26)
forj= 1,2,...,p−1,1,2,...,p−1,..., whereSis the soft-threshold
operator:
S(x,t) = sign(x)(|x|−t)+. (17.27)
The procedure cycles through the predictors until converge nce.
It is easy to show that the diagonal elements wjjof the solution matrix
Ware simplysjj+λ, and these are ﬁxed in step 1 of Algorithm 17.23.
The graphical lasso algorithm is extremely fast, and can sol ve a moder-
ately sparse problem with 1000 nodes in less than a minute. It is easy to
modify the algorithm to have edge-speciﬁc penalty paramete rsλjk; since
3An alternative formulation of the problem (17.21) can be posed, wh ere we don’t
penalize the diagonal of Θ. Then the diagonal elements wjjof the solution matrix are
sjj, and the rest of the algorithm is unchanged.

17.3 Undirected Graphical Models for Continuous Variables 6 37
λjk=∞will force ˆθjkto be zero, this algorithm subsumes Algorithm 17.1.
By casting the sparse inverse-covariance problem as a serie s of regressions,
one can also quickly compute and examine the solution paths a s a function
of the penalty parameter λ. More details can be found in Friedman et al.
(2008b).
Raf
Mek
Plcg
PIP2
PIP3
Erk AktPKAPKCP38JnkRaf
Mek
Plcg
PIP2
PIP3
Erk AktPKAPKCP38Jnk
Raf
Mek
Plcg
PIP2
PIP3
Erk AktPKAPKCP38JnkRaf
Mek
Plcg
PIP2
PIP3
Erk AktPKAPKCP38Jnkλ= 0 λ= 7λ= 27 λ= 36
FIGURE 17.5. Four diﬀerent graphical-lasso solutions for the ﬂow-cytometr y
data.
Figure 17.1 shows the result of applying the graphical lasso to the ﬂow-
cytometry dataset. Here the lasso penalty parameter λwas set at 14. In
practice it is informative to examine the diﬀerent sets of gr aphs that are
obtained as λis varied. Figure 17.5 shows four diﬀerent solutions. The
graph becomes more sparse as the penalty parameter is increa sed.
Finallynotethatthevaluesatsomeofthenodesinagraphica lmodelcan
be unobserved; that is, missing or hidden. If only some value s are missing
at a node, the EM algorithm can be used to impute the missing va lues

638 17. Undirected Graphical Models
(Exercise 17.9). However, sometimes the entire node is hidd en orlatent.
In the Gaussian model, if a node has all missing values, due to linearity
one can simply average over the missing nodes to yield anothe r Gaussian
model over the observed nodes. Hence the inclusion of hidden nodes does
not enrich the resulting model for the observed nodes; in fac t, it imposes
additionalstructureonitscovariancematrix.Howeverint hediscretemodel
(described next) the inherent nonlinearities make hidden u nits a powerful
way of expanding the model.
17.4 Undirected Graphical Models for Discrete
Variables
Undirected Markov networks with all discrete variables are popular, and
in particular pairwise Markov networks with binary variabl es being the
most common. They are sometimes called Ising models in the statistical
mechanics literature, and Boltzmann machines in the machine learning lit-
erature, where the vertices are referred to as “nodes” or “un its” and are
binary-valued.
In addition, the values at each node can be observed (“visibl e”) or un-
observed (“hidden”). The nodes are often organized in layer s, similar to a
neural network. Boltzmann machines are useful both for unsu pervised and
supervised learning, especially for structured input data such as images,
but have been hampered by computational diﬃculties. Figure 17.6 shows
a restricted Boltzmann machine (discussed later), in which some variables
are hidden, and only some pairs of nodes are connected. We ﬁrs t consider
the simpler case in which all pnodes are visible with edge pairs ( j,k) enu-
merated in E.
Denoting the binary valued variable at node jbyXj, the Ising model
for their joint probabilities is given by
p(X,Θ) = exp/bracketleftig/summationdisplay
(j,k)∈EθjkXjXk−Φ(Θ)/bracketrightig
forX∈X,(17.28)
withX={0,1}p. As with the Gaussian model of the previous section,
only pairwise interactions are modeled. The Ising model was developed in
statistical mechanics, and is now used more generally to mod el the joint
eﬀects of pairwise interactions. Φ( Θ) is the log of the partition function,
and is deﬁned by
Φ(Θ) = log/summationdisplay
x∈X/bracketleftig
exp/parenleftig/summationdisplay
(j,k)∈Eθjkxjxk/parenrightig/bracketrightig
. (17.29)
The partition function ensures that the probabilities add t o one over the
sample space. The terms θjkXjXkrepresent a particular parametrization

17.4 Undirected Graphical Models for Discrete Variables 639
of the (log) potential functions (17.5), and for technical r easons requires
aconstant nodeX0≡1 to be included (Exercise 17.10), with “edges” to
all the other nodes. In the statistics literature, this mode l is equivalent
to a ﬁrst-order-interaction Poisson log-linear model for m ultiway tables of
counts (Bishop et al., 1975; McCullagh and Nelder, 1989; Agr esti, 2002).
The Ising model implies a logistic form for each node conditi onal on the
others (exercise 17.11):
Pr(Xj= 1|X−j=x−j) =1
1+exp(−θj0−/summationtext
(j,k)∈Eθjkxk),(17.30)
whereX−jdenotes all of the nodes except j. Hence the parameter θjk
measures the dependence of XjonXk, conditional on the other nodes.
17.4.1 Estimation of the Parameters when the Graph
Structure is Known
Given some data from this model, how can we estimate the param eters?
Supposewehaveobservations xi= (xi1,xi2,...,x ip)∈{0,1}p, i= 1,...,N.
The log-likelihood is
ℓ(Θ) =N/summationdisplay
i=1logPrΘ(Xi=xi)
=N/summationdisplay
i=1
/summationdisplay
(j,k)∈Eθjkxijxik−Φ(Θ)
 (17.31)
The gradient of the log-likelihood is
∂ℓ(Θ)
∂θjk=N/summationdisplay
i=1xijxik−N∂Φ(Θ)
∂θjk(17.32)
and
∂Φ(Θ)
∂θjk=/summationdisplay
x∈Xxjxk·p(x,Θ)
= EΘ(XjXk) (17.33)
Setting the gradient to zero gives
ˆE(XjXk)−EΘ(XjXk) = 0 (17.34)
where we have deﬁned

640 17. Undirected Graphical Models
ˆE(XjXk) =1
NN/summationdisplay
i=1xijxik, (17.35)
theexpectationtakenwithrespecttotheempiricaldistrib utionofthedata.
Looking at (17.34), we see that the maximum likelihood estim ates simply
match the estimated inner products between the nodes to thei r observed
inner products. This is a standard form for the score (gradie nt) equation
for exponential family models, in which suﬃcient statistic s are set equal to
their expectations under the model.
To ﬁnd the maximum likelihood estimates, we can use gradient search
or Newton methods. However the computation of E Θ(XjXk) involves enu-
meration of p(X,Θ) over 2p−2of the|X|= 2ppossible values of X, and is
not generally feasible for large p(e.g., larger than about 30). For smaller
p, a number of standard statistical approaches are available :
Poisson log-linear modeling , where we treat the problem as a large regres-
sion problem (Exercise 17.12). The response vector yis the vector of
2pcounts in each of the cells of the multiway tabulation of the d ata4.
The predictor matrix Zhas 2prows and up to 1+ p+p2columns that
characterize each of the cells, although this number depend s on the
sparsity of the graph. The computational cost is essentiall y that of a
regression problem of this size, which is O(p42p) and is manageable
forp<20. The Newton updates aretypically computed byiterativel y
reweighted least squares, and the number of steps is usually in the
single digits. See Agresti (2002) and McCullagh and Nelder ( 1989) for
details. Standard software (such as the Rpackageglm) can be used
to ﬁt this model.
Gradient descent requires at most O(p22p−2) computations to compute
the gradient, but may require many more gradient steps than t he
second–order Newton methods. Nevertheless, it can handle s lightly
larger problems with p≤30. These computations can be reduced
by exploiting the special clique structure in sparse graphs , using the
junction-tree algorithm. Details are not given here.
Iterative proportional ﬁtting (IPF) performs cyclical coordinate descent on
the gradient equations (17.34). At each step a parameter is u pdated
so that its gradient equation is exactly zero. This is done in a cyclical
fashion until all the gradients are zero. One complete cycle costs the
sameasagradientevaluation,butmaybemoreeﬃcient.Jirou ´ sekand
Pˇ reuˇ cil (1995) implement an eﬃcient version of IPF, using junction
trees.
4Each of the cell counts is treated as an independent Poisson variable. We g et the
multinomial model corresponding to (17.28) by conditioning on th e total count N(which
is also Poisson under this framework).

17.4 Undirected Graphical Models for Discrete Variables 641
Whenpis large (>30) other approaches have been used to approximate
the gradient.
•The mean ﬁeld approximation (Peterson and Anderson, 1987) e sti-
mates E Θ(XjXk) by E Θ(Xj)EΘ(Xj), and replaces the input vari-
ables by their means, leading to a set of nonlinear equations for the
parameters θjk.
•To obtain near-exact solutions, Gibbs sampling (Section 8. 6) is used
to approximate E Θ(XjXk) by successively sampling from the esti-
mated model probabilities Pr Θ(Xj|X−j) (see e.g. Ripley (1996)).
We have not discussed decomposable models , for which the maximum
likelihood estimates can be found in closed form without any iteration
whatsoever. These models arise, for example, in trees: special graphs with
tree-structured topology. When computational tractabili ty is a concern,
trees represent a useful class of models and they sidestep th e computational
concerns raised in this section. For details, see for exampl e Chapter 12 of
Whittaker (1990).
17.4.2 Hidden Nodes
We can increase the complexity of a discrete Markov network b y including
latent or hidden nodes. Suppose that a subset of the variable sXHare
unobserved or “hidden”, and the remainder XVare observed or “visible.”
Then the log-likelihood of the observed data is
ℓ(Θ) =N/summationdisplay
i=1log[Pr Θ(XV=xiV)]
=N/summationdisplay
i=1/bracketleftig
log/summationdisplay
xH∈XHexp/summationdisplay
(j,k)∈E(θjkxijxik−Φ(Θ))/bracketrightig
.(17.36)
The sum over xHmeans that we are summing over all possible {0,1}values
for the hidden units. The gradient works out to be
dℓ(Θ)
dθjk=ˆEVEΘ(XjXk|XV)−EΘ(XjXk) (17.37)
The ﬁrst term is an empirical average of XjXkif both are visible; if one
or both are hidden, they are ﬁrst imputed given the visible da ta, and then
averaged over the hidden variables. The second term is the un conditional
expectation of XjXk.
The inner expectation in the ﬁrst term can be evaluated using basic rules
of conditional expectation and properties of Bernoulli ran dom variables. In
detail, for observation i

642 17. Undirected Graphical Models
EΘ(XjXk|XV=xiV) =/braceleftbigxijxik ifj,k∈V
xijPrΘ(Xk= 1|XV=xiV) ifj∈V,k∈H
PrΘ(Xj= 1,Xk= 1|XV=xiV) ifj,k∈H.
(17.38)
Now two separate runs of Gibbs sampling are required; the ﬁrs t to estimate
EΘ(XjXk) by sampling from the model as above, and the second to esti-
mate E Θ(XjXk|XV=xiV). In this latter run, the visible units are ﬁxed
(“clamped”) at their observed values and only the hidden var iables are
sampled. Gibbs sampling must be done for each observation in the training
set, at each stage of the gradient search. As a result this pro cedure can be
very slow, even for moderate-sized models. In Section 17.4. 4 we consider
further model restrictions to make these computations mana geable.
17.4.3 Estimation of the Graph Structure
The use of a lasso penalty with binary pairwise Markov networ ks has been
suggested by Lee et al. (2007) and Wainwright et al. (2007). T he ﬁrst au-
thors investigate a conjugate gradient procedure for exact maximization of
a penalized log-likelihood. The bottleneck is the computat ion of E Θ(XjXk)
in the gradient; exact computation via the junction tree alg orithm is man-
ageable for sparse graphs but becomes unwieldy for dense gra phs.
The second authors propose an approximate solution, analog ous to the
Meinshausen and B¨ uhlmann (2006) approach for the Gaussian graphical
model. They ﬁt an L1-penalized logistic regression model to each node as
a function of the other nodes, and then symmetrize the edge pa rameter
estimates in some fashion. For example if ˜θjkis the estimate of the j-k
edge parameter from the logistic model for outcome node j, the “min”
symmetrization sets ˆθjkto either ˜θjkor˜θkj, whichever is smallest in abso-
lute value. The “max” criterion is deﬁned similarly. They sh ow that under
certain conditions either approximation estimates the non zero edges cor-
rectly as the sample size goes to inﬁnity. Hoeﬂing and Tibshi rani (2008)
extend the graphical lasso to discrete Markov networks, obt aining a pro-
cedure which is somewhat faster than conjugate gradients, b ut still must
deal with computation of E Θ(XjXk). They also compare the exact and
approximate solutions in an extensive simulation study and ﬁnd the “min”
or “max” approximations are only slightly less accurate tha n the exact pro-
cedure, both for estimating the nonzero edges and for estima ting the actual
values of the edge parameters, and are muchfaster. Furthermore, they can
handle denser graphs because they never need to compute the q uantities
EΘ(XjXk).
Finally, we point out a key diﬀerence between the Gaussian an d binary
models.IntheGaussiancase,both Σanditsinversewilloftenbeofinterest,
and the graphical lasso procedure delivers estimates for bo th of these quan-
tities. However, the approximation of Meinshausen and B¨ uh lmann (2006)
for Gaussian graphical models, analogous to the Wainwright et al. (2007)

17.4 Undirected Graphical Models for Discrete Variables 643
XjXk
Xℓ
VisibleV1 VisibleV2HiddenH
θjk
FIGURE 17.6. A restricted Boltzmann machine (RBM) in which there are no
connections between nodes in the same layer. The visible units are subdivided to
allow the RBM to model the joint density of feature V1and their labels V2.
approximation for the binary case, only yields an estimate o fΣ−1. In con-
trast, in the Markov model for binary data, Θis the object of interest, and
its inverse is not of interest. The approximate method of Wai nwright et al.
(2007) estimates Θeﬃciently and hence is an attractive solution for the
binary problem.
17.4.4 Restricted Boltzmann Machines
In this section we consider a particular architecture for gr aphical models
inspired by neural networks, where the units are organized i n layers. A
restricted Boltzmann machine (RBM) consists of one layer of visible units
and one layer of hidden units with no connections within each layer. It is
much simpler to compute the conditional expectations (as in (17.37) and
(17.38)) if the connections between hidden units are remove d5. Figure 17.6
shows an example; the visible layer is divided into input var iablesV1and
output variables V2, and there is a hidden layer H. We denote such a
network by
V1↔H↔V 2. (17.39)
For example,V1could be the binary pixels of an image of a handwritten
digit, andV2could have 10 units, one for each of the observed class labels
0-9.
The restricted form of this model simpliﬁes the Gibbs sampli ng for es-
timating the expectations in (17.37), since the variables i n each layer are
independent of one another, given the variables in the other layers. Hence
they can be sampled together, using the conditional probabi lities given by
expression (17.30).
The resulting model is less general than a Boltzmann machine , but is still
useful; for example it can learn to extract interesting feat ures from images.
5We thank Geoﬀrey Hinton forassistance in the preparation of the mater ial on RBMs.

644 17. Undirected Graphical Models
By alternately sampling the variables in each layer of the RB M shown
in Figure 17.6, it is possible to generate samples from the jo int density
model. If theV1part of the visible layer is clamped at a particular feature
vector during the alternating sampling, it is possible to sa mple from the
distribution over labels given V1. Alternatively classiﬁcation of test items
can also be achieved by comparing the unnormalized joint den sities of each
label category with the observed features. We do not need to c ompute the
partition function as it is the same for all of these combinat ions.
As noted the restricted Boltzmann machine has the same gener ic form
as a single hidden layer neural network (Section 11.3). The e dges in the
latter model are directed, the hidden units are usually real -valued, and the
ﬁtting criterion is diﬀerent. The neural network minimizes the error (cross-
entropy) between the targets and their model predictions, c onditional on
the input features. In contrast, the restricted Boltzmann m achine maxi-
mizes the log-likelihood for the joint distribution of all v isible units—that
is, the features and targets. It can extract information fro m the input fea-
tures that is useful for predicting the labels, but, unlike s upervised learning
methods, it may also use some of its hidden units to model stru cture in the
feature vectors that is not immediately relevant for predic ting the labels.
These features may turn out to be useful, however, when combi ned with
features derived from other hidden layers.
Unfortunately, Gibbs sampling in a restricted Boltzmann ma chine can
be very slow, as it can take a long time to reach stationarity. As the net-
work weights get larger, the chain mixes more slowly and we ne ed to run
more steps to get the unconditional estimates. Hinton (2002 ) noticed em-
pirically that learning still works well if we estimate the s econd expectation
in (17.37) by starting the Markov chain at the data and only ru nning for a
few steps (instead of to convergence). He calls this contrastive divergence :
we sampleHgivenV1,V2, thenV1,V2givenHand ﬁnallyHgivenV1,V2
again. The idea is that when the parameters are far from the so lution, it
may be wasteful to iterate the Gibbs sampler to stationarity , as just a single
iteration will reveal a good direction for moving the estima tes.
We now give an example to illustrate the use of an RBM. Using co n-
trastivedivergence,itispossibletotrainanRBMtorecogn izehand-written
digits from the MNIST dataset (LeCun et al., 1998). With 2000 hidden
units, 784 visible units for representing binary pixel inte nsities and one
10-way multinomial visible unit for representing labels, t he RBM achieves
an error rate of 1.9% on the test set. This is a little higher th an the 1.4%
achieved by a support vector machine and comparable to the er ror rate
achieved by a neural network trained with backpropagation. The error rate
of the RBM, however, can be reduced to 1.25% by replacing the 7 84 pixel
intensities by 500 features that are produced from the image s without using
any label information. First, an RBM with 784 visible units a nd 500 hidden
units is trained, using contrastive divergence, to model th e set of images.
Then the hidden states of the ﬁrst RBM are used as data for trai ning a

Exercises 645
FIGURE 17.7. Example of a restricted Boltzmann machine for handwritten
digit classiﬁcation. The network is depicted in the schemati c on the left. Displayed
on the right are some diﬃcult test images that the model classiﬁ es correctly.
second RBM that has 500 visible units and 500 hidden units. Fi nally, the
hidden states of the second RBM are used as the features for tr aining an
RBM with 2000 hidden units as a joint density model. The detai ls and
justiﬁcation for learning features in this greedy, layer-b y-layer way are de-
scribed in Hinton et al. (2006). Figure 17.7 gives a represen tation of the
composite model that is learned in this way and also shows som e examples
of the types of distortion that it can cope with.
Bibliographic Notes
Much work has been done in deﬁning and understanding the stru cture of
graphical models. Comprehensive treatments of graphical m odels can be
found in Whittaker (1990), Lauritzen (1996), Cox and Wermut h (1996),
Edwards (2000), Pearl (2000), Anderson (2003), Jordan (200 4), and Koller
and Friedman (2007). Wasserman (2004) gives a brief introdu ction, and
Chapter 8 of Bishop (2006) gives a more detailed overview. Bo ltzmann
machineswereproposedinAckleyetal.(1985).Ripley(1996 )hasadetailed
chapter on topics in graphical models that relate to machine learning. We
found this particularly useful for its discussion of Boltzm ann machines.
Exercises
Ex. 17.1 For the Markov graph of Figure 17.8, list all of the implied co ndi-
tional independence relations and ﬁnd the maximal cliques.

646 17. Undirected Graphical Models
X1
X2X3X4
X5
X6
FIGURE 17.8.
Ex. 17.2 Consider random variables X1,X2,X3,X4. In each of the following
cases draw a graph that has the given independence relations :
(a)X1⊥X3|X2andX2⊥X4|X3.
(b)X1⊥X4|X2,X3andX2⊥X4|X1,X3.
(c)X1⊥X4|X2,X3,X1⊥X3|X2,X4andX3⊥X4|X1,X2.
Ex. 17.3 LetΣbe the covariance matrix of a set of pvariablesX. Consider
the partial covariance matrix Σa.b=Σaa−ΣabΣ−1
bbΣbabetween the two
subsets of variables Xa= (X1,X2) consisting of the ﬁrst two, and Xb
the rest. This is the covariance matrix between these two var iables, after
linear adjustment for all the rest. In the Gaussian distribu tion, this is the
covariance matrix of the conditional distribution of Xa|Xb. The partial
correlation coeﬃcient ρjk|restbetween the pair Xaconditional on the rest
Xb, is simply computed from this partial covariance. Deﬁne Θ=Σ−1.
1. Show that Σa.b=Θ−1
aa.
2. Show that if any oﬀ-diagonal element of Θis zero, then the partial
correlation coeﬃcient between the corresponding variable s is zero.
3. Showthatifwetreat Θasifitwereacovariancematrix,andcompute
the corresponding “correlation” matrix
R= diag(Θ)−1/2·Θ·diag(Θ)−1/2, (17.40)
thenrjk=−ρjk|rest
Ex. 17.4 Denote by
f(X1|X2,X3,...,X p)
the conditional density of X1givenX2,...,X p. If
f(X1|X2,X3,...,X p) =f(X1|X3,...,X p),
show thatX1⊥X2|X3,...,X p.

Exercises 647
Ex. 17.5 Consider the setup in Section 17.3.1 with no missing edges. S how
that
S11β−s12= 0
are the estimating equations for the multiple regression co eﬃcients of the
last variable on the rest.
Ex. 17.6 Recovery of ˆΘ=ˆΣ−1from Algorithm 17.1 . Use expression (17.16)
to derive the standard partitioned inverse expressions
θ12=−W−1
11w12θ22 (17.41)
θ22= 1/(w22−wT
12W−1
11w12). (17.42)
Sinceˆβ=W−1
11w12, show that ˆθ22= 1/(w22−wT
12ˆβ) andˆθ12=−ˆβˆθ22.
Thusˆθ12is a simply rescaling of ˆβby−ˆθ22.
Ex.17.7Writeaprogramtoimplementthemodiﬁedregressionprocedu rein
Algorithm 17.1 for ﬁtting the Gaussian graphical model with pre-speciﬁed
edges missing. Test it on the ﬂow cytometry data from the book website,
using the graph of Figure 17.1.
Ex. 17.8
(a)Writeaprogramtoﬁtthelassousingthecoordinatedesce ntprocedure
(17.26). Compare its results to those from the larsprogram or some
other convex optimizer, to check that it is working correctl y.
(b) Using the program from (a), write code to implement the gr aphical
lasso (Algorithm 17.2). Apply it to the ﬂow cytometry data fr om the
book website. Vary the regularization parameter and examin e the
resulting networks.
Ex. 17.9 Suppose that we have a Gaussian graphical model in which some
or all of the data at some vertices are missing.
(a) Consider the EM algorithm for a dataset of Ni.i.d. multivariate ob-
servationsxi∈IRpwith mean µand covariance matrix Σ. For each
samplei, letoiandmiindex the predictors that are observed and
missing, respectively. Show that in the E step, the observat ions are
imputed from the current estimates of µandΣ:
ˆxi,mi= E(xi,mi|xi,oi,θ) = ˆµmi+ˆΣmi,oiˆΣ−1
oi,oi(xi,oi−ˆµoi)
(17.43)
while in the M step, µandΣare re-estimated from the empirical
mean and (modiﬁed) covariance of the imputed data:
ˆµj=N/summationdisplay
i=1ˆxij/N

648 17. Undirected Graphical Models
ˆΣjj′=N/summationdisplay
i=1[(ˆxij−ˆµj)(ˆxij′−ˆµj′)+ci,jj′]/N(17.44)
whereci,jj′=ˆΣjj′ifj,j′∈miand zerootherwise. Explain the reason
for the correction term ci,jj′(Little and Rubin, 2002).
(b) Implement the EM algorithm for the Gaussian graphical mo del using
the modiﬁed regression procedure from Exercise 17.7 for the M-step.
(c) For the ﬂow cytometry data on the book website, set the dat a for the
lastprotein Jnkintheﬁrst1000observationstomissing,ﬁtthemodel
of Figure 17.1, and compare the predicted values to the actua l values
forJnk. Compare the results to those obtained from a regression of
Jnkon the other vertices with edges to Jnkin Figure 17.1, using only
the non-missing data.
Ex. 17.10 Using a simple binary graphical model with just two variable s,
show why it is essential to include a constant node X0≡1 in the model.
Ex. 17.11 Show that the Ising model (17.28) for the joint probabilitie s in
a discrete graphical model implies that the conditional dis tributions have
the logistic form (17.30).
Ex. 17.12 Consider a Poisson regression problem with pbinary variables
xij, j= 1,...,pand response variable yiwhich measures the number of
observations with predictor xi∈{0,1}p. The design is balanced, in that all
n= 2ppossible combinations are measured. We assume a log-linear model
for the Poisson mean in each cell
logµ(X) =θ00+/summationdisplay
(j,k)∈Exijxikθjk, (17.45)
usingthesamenotationasinSection17.4.1(includingthec onstantvariable
xi0= 1∀i). We assume the response is distributed as
Pr(Y=y|X=x) =e−µ(x)µ(x)y
y!. (17.46)
Write down the conditional log-likelihood for the observed responsesyi,
and compute the gradient.
(a) Show that the gradient equation for θ00computes the partition func-
tion (17.29).
(b) Show that the gradient equations for the remainder of the parameters
are equivalent to the gradient (17.34).

This is page 649
Printer: Opaque this
18
High-Dimensional Problems: p≫N
18.1 When pis Much Bigger than N
In this chapter we discuss prediction problems in which the n umber of
featurespis much larger than the number of observations N, often written
p≫N. Such problems have become of increasing importance, espec ially in
genomics and other areas of computational biology. We will s ee that high
variance and overﬁtting are a major concern in this setting. As a result,
simple, highly regularized approaches often become the met hods of choice.
The ﬁrst part of the chapter focuses on prediction in both the classiﬁcation
and regression settings, while the second part discusses th e more basic
problem of feature selection and assessment.
To get us started, Figure 18.1 summarizes a small simulation study that
demonstrates the “less ﬁtting is better” principle that app lies whenp≫N.
For each of N= 100 samples, we generated pstandard Gaussian features
Xwith pairwise correlation 0 .2. The outcome Ywas generated according
to a linear model
Y=p/summationdisplay
j=1Xjβj+σε (18.1)
whereεwas generated from a standard Gaussian distribution. For ea ch
dataset, the set of coeﬃcients βjwere also generated from a standard Gaus-
sian distribution. We investigated three cases: p= 20,100,and 1000. The
standard deviation σwas chosen in each case so that the signal-to-noise
ratio Var[E( Y|X)]/σ2equaled 2. As a result, the number of signiﬁcant uni-

650 18. High-Dimensional Problems: p≫N1.0 1.5 2.0 2.5 3.0Relative error
20 9 220 featuresTest Error
1.0 1.5 2.0 2.5 3.0
99 35 7100 features
1.0 1.5 2.0 2.5 3.0
99 87 431000 features
Effective Degrees of Freedom
FIGURE 18.1. Test-error results for simulation experiments. Shown are box -
plots of the relative test errors over 100simulations, for three diﬀerent values
ofp, the number of features. The relative error is the test error d ivided by the
Bayes error, σ2. From left to right, results are shown for ridge regression wit h
three diﬀerent values of the regularization parameter λ:0.001,100and1000. The
(average) eﬀective degrees of freedom in the ﬁt is indicated below each plot.
variate regression coeﬃcients1was 9, 33 and 331, respectively, averaged
over the 100 simulation runs. The p= 1000 case is designed to mimic the
kind of data that we might see in a high-dimensional genomic o r proteomic
dataset, for example.
We ﬁt a ridge regression to the data, with three diﬀerent valu es for the
regularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this
is nearly the same as least squares regression, with a little regularization
just to ensure that the problem is non-singular when p > N. Figure 18.1
shows boxplots oftherelative testerrorachieved bythediﬀ erentestimators
in each scenario. The corresponding average degrees of free dom used in
each ridge-regression ﬁt is indicated (computed using form ula (3.50) on
page 682). The degrees of freedom is a more interpretable parameter t han
λ. We see that ridge regression with λ= 0.001 (20 df) wins when p= 20;
λ= 100 (35 df) wins when p= 100, and λ= 1000 (43 df) wins when
p= 1000.
Here is an explanation for these results. When p= 20, we ﬁt all the way
and we can identify as many of the signiﬁcant coeﬃcients as po ssible with
1We call a regression coeﬃcient signiﬁcant if |/hatwideβj//hatwidesej| ≥2, where ˆβjis the estimated
(univariate) coeﬃcient and /hatwidesejis its estimated standard error.
2For a ﬁxed value of the regularization parameter λ, the degrees of freedom depends
on the observed predictor values in each simulation. Hence we compute the average
degrees of freedom over simulations.

18.2 Nearest Shrunken Centroids 651
low bias. When p= 100, we can identify some non-zero coeﬃcients using
moderate shrinkage. Finally, when p= 1000, even though there are many
nonzero coeﬃcients, we don’t have a hope for ﬁnding them and w e need
to shrink all the way down. As evidence of this, let tj=/hatwideβj//hatwidesej, whereˆβj
is the ridge regression estimate and /hatwidesejits estimated standard error. Then
using the optimal ridge parameter in each of the three cases, the median
value of|tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values
exceeding 2 was equal to 9.8, 1.2 and 0.0.
Ridge regression with λ= 0.001 successfully exploits the correlation in
the features when p<N, but cannot do so when p≫N. In the latter case
there is not enough information in the relatively small numb er of samples
to eﬃciently estimate the high-dimensional covariance mat rix. In that case,
more regularization leads to superior prediction performa nce.
Thus it is not surprising that the analysis of high-dimensio nal data re-
quires either modiﬁcation of procedures designed for the N >pscenario, or
entirely new procedures. In this chapter we discuss example s of both kinds
ofapproachesforhighdimensionalclassiﬁcationandregre ssion;thesemeth-
ods tend to regularize quite heavily, using scientiﬁc conte xtual knowledge
to suggest the appropriate form for this regularization. Th e chapter ends
with a discussion of feature selection and multiple testing .
18.2 Diagonal Linear Discriminant Analysis and
Nearest Shrunken Centroids
Gene expression arrays are an important new technology in bi ology, and
are discussed in Chapters 1 and 14. The data in our next exampl e form
a matrix of 2308 genes (columns) and 63 samples (rows), from a set of
microarray experiments. Each expression value is a log-rat io log(R/G).R
is the amount of gene-speciﬁc RNA in the target sample that hy bridizes
to a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-
sponding amount of RNA from a reference sample. The samples a rose from
small, round blue-cell tumors (SRBCT) found in children, an d are classiﬁed
into four major types: BL (Burkitt lymphoma), EWS (Ewing’s s arcoma),
NB (neuroblastoma), and RMS (rhabdomyosarcoma). There is a n addi-
tional test data set of 20 observations. We will not go into th e scientiﬁc
background here.
Sincep≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to
the data; some sort of regularization is needed. The method w e describe
here is similar to the methods of Section 4.3.1, but with impo rtant modiﬁ-
cations that achieve feature selection. The simplest form o f regularization
assumes that the features are independent within each class , that is, the
within-class covariance matrix is diagonal. Despite the fa ct that features
will rarely be independent within a class, when p≫Nwe don’t have

652 18. High-Dimensional Problems: p≫N
enough data to estimate their dependencies. The assumption of indepen-
dence greatly reduces the number of parameters in the model a nd often
results in an eﬀective and interpretable classiﬁer.
Thus we consider the diagonal-covariance LDA rule for classifying the
classes. The discriminant score [see (4.12) on page 110] for class kis
δk(x∗) =−p/summationdisplay
j=1(x∗
j−¯xkj)2
s2
j+2logπk. (18.2)
Herex∗= (x∗
1,x∗
2,...,x∗
p)Tis a vector of expression values for a test ob-
servation,sjis the pooled within-class standard deviation of the jth gene,
and ¯xkj=/summationtext
i∈Ckxij/Nkis the mean of the Nkvalues for gene jin class
k, withCkbeing the index set for class k. We call ˜xk= (¯xk1,¯xk2,...¯xkp)T
thecentroid of classk. The ﬁrst part of (18.2) is simply the (negative)
standardized squared distance of x∗to thekth centroid. The second part
is a correction based on the class prior probability πk, where/summationtextK
k=1πk= 1.
The classiﬁcation rule is then
C(x∗) =ℓifδℓ(x∗) = max kδk(x∗). (18.3)
We see that the diagonal LDA classiﬁer is equivalent to a near est centroid
classiﬁer after appropriate standardization. It is also a s pecial case of the
naive-Bayes classiﬁer, as described in Section 6.6.3. It as sumes that the
features in each class have independent Gaussian distribut ions with the
same variance.
The diagonal LDA classiﬁer is often eﬀective in high dimensi onal set-
tings. It is also called the “independence rule” in Bickel an d Levina (2004),
who demonstrate theoretically that it will often outperfor m standard lin-
ear discriminant analysis in high-dimensional problems. H ere the diagonal
LDA classiﬁer yielded ﬁve misclassiﬁcation errors for the 2 0 test samples.
One drawback of the diagonal LDA classiﬁer is that it uses all of the fea-
tures (genes), and hence is not convenient for interpretati on. With further
regularization we can do better—both in terms of test error an d inter-
pretability.
We would like to regularize in a way that automatically drops out fea-
tures that are not contributing to the class predictions. We can do this
by shrinking the classwise mean toward the overall mean, for each feature
separately. The result is a regularized version of the neare st centroid clas-
siﬁer, or equivalently a regularized version of the diagona l-covariance form
of LDA. We call the procedure nearest shrunken centroids (NSC).
The shrinkage procedure is deﬁned as follows. Let
dkj=¯xkj−¯xj
mk(sj+s0), (18.4)
where ¯xjis the overall mean for gene j,m2
k= 1/Nk−1/Nands0is a
small positive constant, typically chosen to be the median o f thesjvalues.

18.2 Nearest Shrunken Centroids 653
(0,0)∆
FIGURE 18.2. Soft thresholding function sign(x)(|x|−∆)+is shown in orange,
along with the 45◦line in red.
This constant guards against large dkjvalues that arise from expression
values near zero. With constant within-class variance σ2, the variance of
the contrast ¯ xkj−¯xjin the numerator is m2
kσ2, and hence the form of the
standardization in the denominator. We shrink the dkjtoward zero using
soft thresholding
d′
kj= sign(dkj)(|dkj|−∆)+; (18.5)
see Figure 18.2. Here ∆ is a parameter to be determined; we use d 10-fold
cross-validation in the example (see the top panel of Figure 18.4). Each dkj
is reduced by an amount ∆ in absolute value, and is set to zero i f its value
is less than zero. The soft-thresholding function is shown i n Figure 18.2;
the same thresholding is applied to wavelet coeﬃcients in Se ction 5.9. An
alternative is to use hard thresholding
d′
kj=dkj·I(|dkj|≥∆); (18.6)
we prefer soft-thresholding, as it is a smoother operation a nd typically
works better. The shrunken versions of ¯ xkjare then obtained by reversing
the transformation in (18.4):
¯x′
kj= ¯xj+mk(sj+s0)d′
kj. (18.7)
We then use the shrunken centroids ¯ x′
kjin place of the original ¯ xkjin the
discriminant score (18.2). The estimator (18.7) can also be viewed as a
lasso-style estimator for the class means (Exercise 18.2).
Notice that only the genes that have a nonzero d′
kjfor at least one of the
classes play a role in the classiﬁcation rule, and hence the v ast majority
of genes can often be discarded. In this example, all but 43 ge nes were
discarded, leaving a small interpretable set of genes that c haracterize each
class. Figure 18.3 represents the genes in a heatmap.
Figure 18.4 (top panel) demonstrates the eﬀectiveness of th e shrinkage.
With no shrinkage we make 5/20 errors on the test data, and sev eral errors

654 18. High-Dimensional Problems: p≫N
on the training and CV data. The shrunken centroids achieve z ero test er-
rorsforafairlybroadbandofvaluesfor∆.Thebottompanelo fFigure18.4
shows the four centroids for the SRBCT data (gray), relative to the overall
centroid. The blue bars are shrunken versions of these centr oids, obtained
by soft-thresholding the gray bars, using ∆ = 4 .3. The discriminant scores
(18.2) can be used to construct class probability estimates :
ˆpk(x∗) =e1
2δk(x∗)
/summationtextK
ℓ=1e1
2δℓ(x∗). (18.8)
These can be used to rate the classiﬁcations, or to decide not to classify a
particular sample at all.
Note that other forms of feature selection can be used in this setting,
including hard thresholding. Fan and Fan (2008) show theore tically the
importance of carrying out some kind of feature selection wi th diagonal
linear discriminant analysis in high-dimensional problem s.
18.3 Linear Classiﬁers with Quadratic
Regularization
Ramaswamy et al. (2001) present a more diﬃcult microarray cl assiﬁcation
problem, involving a training set of 144 patients with 14 diﬀ erent types of
cancer, and a test set of 54 patients. Gene expression measur ements were
available for 16 ,063 genes.
Table 18.1 shows the prediction results from eight diﬀerent classiﬁcation
methods. The data from each patient was ﬁrst standardized to have mean
0 and variance 1; this seems to improve prediction accuracy o verall this
example, suggesting that the “shape” of each gene-expressi on proﬁle is
important, rather than the absolute expression levels. In e ach case, the
BL EWS NB RMS
FIGURE 18.3. Heat-map of the chosen 43 genes. Within each of the horizonta l
partitions, we have ordered the genes by hierarchical cluste ring, and similarly
for the samples within each vertical partition. Yellow repres ents over- and blue
under-expression.

18.3 Linear Classiﬁers with Quadratic Regularization 655
0 2 4 6Misclassification Error2308 2059 1223 598 284 159 81 43 23 15 10 5 1Number of Genes0.0 0.2 0.4 0.6 0.8Training
10−fold CV
Test
Amount of Shrinkage ∆
−1.0 −0.5 0.0 0.5 1.0BL0 500 1000 1500 2000
−1.0 −0.5 0.0 0.5 1.0EWS
−1.0 −0.5 0.0 0.5 1.0NB
−1.0 −0.5 0.0 0.5 1.0RMSGene
Centroids: Average Expression Centered at Overall Centroid
FIGURE 18.4. (Top): Error curves for the SRBCT data. Shown are the train-
ing, 10-fold cross-validation, and test misclassiﬁcation err ors as the threshold
parameter ∆is varied. The value ∆ = 4.34is chosen by CV, resulting in a sub-
set of43selected genes. (Bottom): Four centroids proﬁles dkjfor the SRBCT
data (gray), relative to the overall centroid. Each centroid has2308components,
and we see considerable noise. The blue bars are shrunken versi onsd′
kjof these
centroids, obtained by soft-thresholding the gray bars, usi ng∆ = 4.3.

656 18. High-Dimensional Problems: p≫N
TABLE 18.1. Prediction results for microarray data with 14 cancer classes .
Method 1 is described in Section 18.2. Methods 2, 3 and 6 are di scussed in Sec-
tion 18.3, while 4, 7 and 8 are discussed in Section 18.4. Metho d 5 is described in
Section 13.3. The elastic-net penalized multinomial does the b est on the test data,
but the standard error of each test-error estimate is about 3 , so such comparisons
are inconclusive.
Methods CV errors (SE) Test errors Number of
Out of 144 Out of 54 Genes Used
1. Nearest shrunken centroids 35 (5.0) 17 6,520
2.L2-penalized discriminant 25 (4.1) 12 16,063
analysis
3. Support vector classiﬁer 26 (4.2) 14 16,063
4. Lasso regression (one vs all) 30.7 (1.8) 12.5 1,429
5.k-nearest neighbors 41 (4.6) 26 16,063
6.L2-penalized multinomial 26 (4.2) 15 16,063
7.L1-penalized multinomial 17 (2.8) 13 269
8. Elastic-net penalized 22 (3.7) 11.8 384
multinomial
regularization parameter has been chosen to minimize the cr oss-validation
error, and the test error at that value of the parameter is sho wn. When
more than one value of the regularization parameter yields t he minimal
cross-validation error, the average test error at these val ues is reported.
RDA(regularizeddiscriminantanalysis),regularizedmul tinomiallogistic
regression, and the support vector machine are more complex methods that
try to exploit multivariate information in the data. We desc ribe each in
turn, as well as a variety of regularization methods, includ ing bothL1and
L2and some in between.
18.3.1 Regularized Discriminant Analysis
Regularized discriminant analysis (RDA) is described in Se ction 4.3.1. Lin-
ear discriminant analysis involves the inversion of a p×pwithin-covariance
matrix. When p≫N, this matrix can be huge, has rank at most N < p,
and hence is singular. RDA overcomes the singularity issues by regulariz-
ing the within-covariance estimate ˆΣ. Here we use a version of RDA that
shrinksˆΣtowards its diagonal:
ˆΣ(γ) =γˆΣ+(1−γ)diag(ˆΣ),withγ∈[0,1]. (18.9)
Note thatγ= 0 corresponds to diagonal LDA, which is the “no shrinkage”
version of nearest shrunken centroids. The form of shrinkag e in (18.9) is

18.3 Linear Classiﬁers with Quadratic Regularization 657
muchlikeridgeregression(Section3.4.1),whichshrinkst hetotalcovariance
matrix of the features towards a diagonal (scalar) matrix. I n fact, viewing
linear discriminant analysis as linear regression with opt imal scoring of the
categorical response (see (12.57) in Section 12.6), the equ ivalence becomes
more precise.
Thecomputationalburdenofinvertingthislarge p×pmatrixisovercome
using the methods discussed in Section 18.3.5. The value of γwas chosen
by cross-validation in line 2 of Table 18.1; all values of γ∈(0.002,0.550)
gave the same CV and test error. Further development of RDA, i ncluding
shrinkage of the centroids in addition to the covariance mat rix, can be
found in Guo et al. (2006).
18.3.2 Logistic Regression with Quadratic Regularization
Logistic regression (Section 4.4) can be modiﬁed in a simila r way, to deal
with thep≫Ncase. With Kclasses, we use a symmetric version of the
multiclass logistic model (4.17) on page 119:
Pr(G=k|X=x) =exp(βk0+xTβk)/summationtextK
ℓ=1exp(βℓ0+xTβℓ). (18.10)
This hasKcoeﬃcient vectors of log-odds parameters β1,β2,...,β K. We
regularize the ﬁtting by maximizing the penalized log-like lihood
max
{β0k,βk}K
1/bracketleftiggN/summationdisplay
i=1logPr(gi|xi)−λ
2K/summationdisplay
k=1||βk||2
2/bracketrightigg
. (18.11)
This regularization automatically resolves the redundanc y in the paramet-
rization, and forces/summationtextK
k=1ˆβkj= 0, j= 1,...,p(Exercise 18.3). Note that
the constant terms βk0are not regularized (and so one should be set to
zero). The resulting optimization problem is convex, and ca n be solved by
a Newton algorithm or other numerical techniques. Details a re given in Zhu
and Hastie (2004). Friedman et al. (2010) provide software f or computing
the regularization path for the two- and multiclass logisti c regression mod-
els. Table 18.1, line 6 reports the results for the multiclas s logistic regres-
sion model, referred to there as “multinomial”. It can be sho wn (Rosset
et al., 2004a) that for separable data, as λ→0, the regularized (two-
class) logistic regression estimate (renormalized) conve rges to the maximal
margin classiﬁer (Section 12.2). This gives an attractive a lternative to the
support-vector machine, discussed next, especially in the multiclass case.
18.3.3 The Support Vector Classiﬁer
The support vector classiﬁer is described for the two-class case in Sec-
tion 12.2. When p > N, it is especially attractive because in general the

658 18. High-Dimensional Problems: p≫N
classes are perfectly separable by a hyperplane unless ther e are identical
feature vectors in diﬀerent classes. Without any regulariz ation the support
vector classiﬁer ﬁnds the separating hyperplane with the la rgest margin;
that is, the hyperplane yielding the biggest gap between the classes in
the training data. Somewhat surprisingly, when p≫Nthe unregularized
support vector classiﬁer often works about as well as the bes t regularized
version. Overﬁtting often does not seem to be a problem, part ly because of
the insensitivity of misclassiﬁcation loss.
There are many diﬀerent methods for generalizing the two-cl ass support-
vector classiﬁer to K >2 classes. In the “one versus one” ( ovo) approach,
we compute all/parenleftbigK
2/parenrightbig
pairwise classiﬁers. For each test point, the predicted
class is the one that wins the most pairwise contests. In the “ one versus all”
(ova) approach, each class is compared to all of the others in Ktwo-class
comparisons. To classify a test point, we compute the conﬁde nces (signed
distancefromthehyperplane)foreachofthe Kclassiﬁers.Thewinneristhe
class with the highest conﬁdence. Finally, Vapnik (1998) an d Weston and
Watkins (1999) suggested (somewhat complex) multiclass cr iteria which
generalize the two-class criterion (12.7).
Tibshirani and Hastie (2007) propose the margin tree classiﬁer, in which
support-vector classiﬁers are used in a binary tree, much as in CART
(Chapter 9). The classes are organized in a hierarchical man ner, which can
be useful for classifying patients into diﬀerent cancer typ es, for example.
Line 3 of Table 18.1 shows the results for the support vector c lassiﬁer
using the ovamethod; Ramaswamy et al. (2001) reported (and we con-
ﬁrmed) that this approach worked best for this problem. The e rrors are
very similar to those in line 6, as we might expect from the com ments
at the end of the previous section. The error rates are insens itive to the
choice ofC[the regularization parameter in (12.8) on page 420], for va lues
ofC >0.001. Sincep > N, the support vector hyperplane can perfectly
separate the training data by setting C=∞.
18.3.4 Feature Selection
Feature selection is an important scientiﬁc requirement fo r a classiﬁer when
pislarge.Neitherdiscriminantanalysis,logisticregress ion,northesupport-
vector classiﬁer perform feature selection automatically , because all use
quadratic regularization. All features have nonzero weigh ts in both models.
Ad-hoc methods for feature selection have been proposed, fo r example,
removing genes with small coeﬃcients, and reﬁtting the clas siﬁer. This is
doneinabackwardstepwisemanner,startingwiththesmalle stweightsand
moving on to larger weights. This is known as recursive feature elimination
(Guyon et al., 2002). It was not successful in this example; R amaswamy
et al. (2001) report, for example, that the accuracy of the su pport-vector
classiﬁer starts to degrade as the number of genes is reduced from the full

18.3 Linear Classiﬁers with Quadratic Regularization 659
set of 16,063. This is rather remarkable, as the number of training sam ples
is only 144. We do not have an explanation for this behavior.
All three methods discussed in this section (RDA, LR and SVM) can
be modiﬁed to ﬁt nonlinear decision boundaries using kernel s. Usually the
motivation for such an approach is to increase the model comp lexity. With
p≫Nthe models are already suﬃciently complex and overﬁtting is always
adanger.Yetdespitethehighdimensionality,radialkerne ls(Section12.3.3)
sometimes deliver superior results in these high dimension al problems. The
radial kernel tends to dampen inner products between points far away from
each other, which in turn leads to robustness to outliers. Th is occurs often
in high dimensions, and may explain the positive results. We tried a radial
kernel with the SVM in Table 18.1, but in this case the perform ance was
inferior.
18.3.5 Computational Shortcuts When p≫N
Thecomputationaltechniquesdiscussedinthissectionapp lytoanymethod
that ﬁts a linear model with quadratic regularization on the coeﬃcients.
That includes all the methods discussed in this section, and many more.
Whenp > N, the computations can be carried out in an N-dimensional
space, rather than p, via the singular value decomposition introduced in
Section 14.5. Here is the geometric intuition: just like two points in three-
dimensional space always lie on a line, Npoints inp-dimensional space lie
in an (N−1)-dimensional aﬃne subspace.
Given theN×pdata matrix X, let
X=UDVT(18.12)
=RVT(18.13)
be the singular-value decomposition (SVD) of X; that is, Visp×Nwith
orthonormal columns, UisN×Northogonal, and Da diagonal matrix
with elements d1≥d2≥dN≥0. The matrix RisN×N, with rows rT
i.
As a simple example, let’s ﬁrst consider the estimates from a ridge re-
gression:
ˆβ= (XTX+λI)−1XTy. (18.14)
Replacing XbyRVTand after some further manipulations, this can be
shown to equal
ˆβ=V(RTR+λI)−1RTy (18.15)
(Exercise 18.4). Thus ˆβ=Vˆθ, where ˆθis the ridge-regression estimate
using theNobservations ( ri,yi),i= 1,2,...,N. In other words, we can
simply reduce the data matrix from XtoR, and work with the rows of
R. This trick reduces the computational cost from O(p3) toO(pN2) when
p>N.

660 18. High-Dimensional Problems: p≫N
These results can be generalized to allmodels that are linear in the
parameters and have quadratic penalties. Consider any supe rvised learning
problem where we use a linear function f(X) =β0+XTβto model a
parameter in the conditional distribution of Y|X. We ﬁt the parameters β
by minimizing some loss function/summationtextN
i=1L(yi,f(xi)) over the data with a
quadratic penalty on β. Logistic regression is a useful example to have in
mind. Then we have the following simple theorem:
Letf∗(ri) =θ0+rT
iθwithrideﬁned in (18.13), and consider the pair of
optimization problems:
(ˆβ0,ˆβ) = arg min
β0,β∈IRpN/summationdisplay
i=1L(yi,β0+xT
iβ)+λβTβ; (18.16)
(ˆθ0,ˆθ) = arg min
θ0,θ∈IRNN/summationdisplay
i=1L(yi,θ0+rT
iθ)+λθTθ.(18.17)
Then the ˆβ0=ˆθ0, andˆβ=Vˆθ.
The theorem says that we can simply replace the pvectorsxiby the
N-vectorsri, and perform our penalized ﬁt as before, but with far fewer
predictors. The N-vector solution ˆθis then transformed back to the p-
vector solution via a simple matrix multiplication. This re sult is part of
the statistics folklore, and deserves to be known more widel y—see Hastie
and Tibshirani (2004) for further details.
Geometrically, we are rotating the features to a coordinate system in
which all but the ﬁrst Ncoordinates are zero. Such rotations are allowed
since the quadratic penalty is invariant under rotations, a nd linear models
are equivariant.
This result can be applied to many of the learning methods dis cussed
in this chapter, such as regularized (multiclass) logistic regression, linear
discriminant analysis (Exercise 18.6), and support vector machines. It also
applies to neural networks with quadratic regularization ( Section 11.5.2).
Note, however, that it does not apply to methods such as the la sso, which
uses nonquadratic ( L1) penalties on the coeﬃcients.
Typically we use cross-validation to select the parameter λ. It can be
seen (Exercise 18.12) that we only need to construct Ronce, on the original
data, and use it as the data for each of the CV folds.
The support vector “kernel trick” of Section 12.3.7 exploit s the same re-
duction used in this section, in a slightly diﬀerent context . Suppose we have
at our disposal the N×Ngram (inner-product) matrix K=XXT. From
(18.12) we have K=UD2UT, and soKcaptures the same information as
R. Exercise 18.13 shows how we can exploit the ideas in this sec tion to ﬁt
a ridged logistic regression with Kusing its SVD.

18.4 Linear Classiﬁers with L1Regularization 661
18.4 Linear Classiﬁers with L1Regularization
The methods of Section 18.3 use an L2penalty to regularize their pa-
rameters, just as in ridge regression. All of the estimated c oeﬃcients are
nonzero, and hence no feature selection is performed. In thi s section we dis-
cuss methods that use L1penalties instead, and hence provide automatic
feature selection.
Recall the lasso of Section 3.4.2,
min
β1
2N/summationdisplay
i=1/parenleftig
yi−β0−p/summationdisplay
j=1xijβj/parenrightig2
+λp/summationdisplay
j=1|βj|,(18.18)
which we have written in the Lagrange form (3.52). As discuss ed there, the
use of theL1penalty causes a subset of the solution coeﬃcients ˆβjto be
exactly zero, for a suﬃciently large value of the tuning para meterλ.
In Section 3.8.1 we discussed the LARS algorithm, an eﬃcient procedure
for computing the lasso solution for all λ. Whenp>N(as in this chapter),
asλapproaches zero, the lasso ﬁts the training data exactly. In fact, by
convex duality one can show that when p > Nthe number of non-zero
coeﬃcients is at most Nfor all values of λ(Rosset and Zhu, 2007, for
example). Thus the lasso provides a (severe) form of feature selection.
Lasso regression can be applied to a two-class classiﬁcatio n problem by
codingtheoutcome ±1,andapplyingacutoﬀ(usually0)tothepredictions.
For more than two classes, there are many possible approache s, including
theovaandovomethods discussed in Section 18.3.3. We tried the ova-
approach on the cancer data in Section 18.3. The results are s hown in
line (4) of Table 18.1. Its performance is among the best.
A more natural approach for classiﬁcation problems is to use the lasso
penalty to regularize logistic regression. Several implem entations have been
proposedintheliterature,includingpathalgorithmssimi lartoLARS(Park
and Hastie, 2007). Because the paths are piecewise smooth bu t nonlinear,
exact methods are slower than the LARS algorithm, and are les s feasible
whenpis large.
Friedman et al. (2010) provide very fast algorithms for ﬁtti ngL1-pen-
alized logistic and multinomial regression models. They us e the symmetric
multinomial logistic regression model as in (18.10) in Sect ion 18.3.2, and
maximize the penalized log-likelihood
max
{β0k,βk∈IRp}K
1
N/summationdisplay
i=1logPr(gi|xi)−λK/summationdisplay
k=1p/summationdisplay
j=1|βkj|
; (18.19)
compare with (18.11). Their algorithm computes the exact so lution at a
pre-chosen sequence of values for λby cyclical coordinate descent (Sec-
tion 3.8.6), and exploits the fact that solutions are sparse whenp≫N,

662 18. High-Dimensional Problems: p≫N
as well as the fact that solutions for neighboring values of λtend to be
very similar. This method was used in line (7) of Table 18.1, w ith the over-
all tuning parameter λchosen by cross-validation. The performance was
similar to that of the best methods, except here the automati c feature se-
lection chose 269 genes altogether. A similar approach is us ed in Genkin
et al. (2007); although they present their model from a Bayes ian point of
view, they in fact compute the posterior mode, which solves t he penalized
maximum-likelihood problem.
−8 −7 −6 −5 −4 −3 −2 −10.0 0.5 1.0 1.5 2.0
−8 −7 −6 −5 −4 −3 −2 −10.0 0.5 1.0 1.5 2.0CoeﬃcientsCoeﬃcients
log(λ) log(λ)Lasso Elastic Net
FIGURE 18.5. Regularized logistic regression paths for the leukemia data. T he
left panel is the lasso path, the right panel the elastic-net pat h withα= 0.8. At
the ends of the path (extreme left), there are 19nonzero coeﬃcients for the lasso,
and39for the elastic net. The averaging eﬀect of the elastic net resu lts in more
non-zero coeﬃcients than the lasso, but with smaller magnitud es.
In genomic applications, there are often strong correlatio ns among the
variables; genes tend to operate in molecular pathways. The lasso penalty
is somewhat indiﬀerent to the choice among a set of strong but corre-
lated variables (Exercise 3.28). The ridge penalty, on the o ther hand, tends
to shrink the coeﬃcients of correlated variables toward eac h other (Exer-
cise 3.29 on page 99). The elastic net penalty (Zou and Hastie, 2005) is a
compromise, and has the form
p/summationdisplay
j=1/parenleftbig
α|βj|+(1−α)β2
j/parenrightbig
. (18.20)
Thesecondtermencourageshighlycorrelatedfeaturestobe averaged,while
the ﬁrst term encourages a sparse solution in the coeﬃcients of these aver-

18.4 Linear Classiﬁers with L1Regularization 663
aged features. The elastic net penalty can be used with any li near model,
in particular for regression or classiﬁcation.
Hence the multinomial problem above with elastic-net penal ty becomes
max
{β0k,βk∈IRp}K
1
N/summationdisplay
i=1logPr(gi|xi)−λK/summationdisplay
k=1p/summationdisplay
j=1/parenleftbig
α|βkj|+(1−α)β2
kj/parenrightbig
.
(18.21)
The parameter αdetermines the mix of the penalties, and is often pre-
chosen on qualitative grounds. The elastic net can yield mor e thatNnon-
zero coeﬃcients when p > N, a potential advantage over the lasso. Line
(8) in Table 18.1 uses this model, with αandλchosen by cross-validation.
We used a sequence of 20 values of αbetween 0.05 and 1.0, and a 100
values ofλuniform on the log scale covering the entire range. Values of
α∈[0.75,0.80] gave the minimum CV error, with values of λ<0.001 for all
tied solutions. Although it has the lowest test error among a ll methods, the
margin is small and not signiﬁcant. Interestingly, when CV i s performed
separately for each value of α, a minimum test error of 8 .8 is achieved at
α= 0.10, but this is not the value chosen in the two-dimensional CV .
−8 −7 −6 −5 −4 −3 −2 −10.0 0.1 0.2 0.3 0.4Misclassification ErrorTraining
Test
10−fold CV
−8 −7 −6 −5 −4 −3 −2 −10 5 10 15 20 25 30Deviance
log(λ) log(λ)
FIGURE 18.6. Training, test, and 10-fold cross validation curves for lasso lo gis-
tic regression on the leukemia data. The left panel shows miscla ssiﬁcation errors,
the right panel shows deviance.
Figure 18.5 shows the lasso and elastic-net coeﬃcient paths on the two-
class leukemia data (Golub et al., 1999). There are 7129 gene -expression
measurements on 38 samples, 27 of them in class ALL (acute lym phocytic
leukemia), and 11 in class AML (acute myelogenous leukemia) . There is
also a test set with 34 samples (20, 14). Since the data are lin early separa-
ble, the solution is undeﬁned at λ= 0 (Exercise 18.11), and degrades for
very small values of λ. Hence the paths have been truncated as the ﬁtted
probabilities approach 0 and 1. There are 19 non-zero coeﬃci ents in the
left plot, and 39 in the right. Figure 18.6 (left panel) shows the misclas-

664 18. High-Dimensional Problems: p≫N
siﬁcation errors for the lasso logistic regression on the tr aining and test
data, as well as for 10-fold cross-validation on the trainin g data. The right
panel uses binomial deviance to measure errors, and is much s moother. The
small sample sizes lead to considerable sampling variance i n these curves,
even though individual curves are relatively smooth (see, f or example, Fig-
ure 7.1 on page 220). Both of these plots suggest that the limi ting solution
λ↓0 is adequate, leading to 3/34 misclassiﬁcations in the test set. The
corresponding ﬁgures for the elastic net are qualitatively similar and are
not shown.
Forp≫N, the limiting coeﬃcients diverge for all regularized logis tic
regression models, so in practical software implementatio ns a minimum
value forλ>0 is either explicitly or implicitly set. However, renormal ized
versions of the coeﬃcients converge, and these limiting sol utions can be
thought of as interesting alternatives to the linear optima l separating hy-
perplane (SVM). With α= 0 the limiting solution coincides with the SVM
(see end of Section 18.3.2), but all the 7129 genes are select ed. Withα= 1,
the limiting solution coincides with an L1separating hyperplane (Rosset
et al., 2004a), and includes at most 38 genes. As αdecreases from 1, the
elastic-net solutions include more genes in the separating hyperplane.
18.4.1 Application of Lasso to Protein Mass Spectroscopy
Protein mass spectrometry has become a popular technology f or analyzing
the proteins in blood, and can be used to diagnose a disease or understand
the processes underlying it.
For each blood serum sample i, we observe the intensity xijfor many
time of ﬂight valuestj. This intensity is related to the number of particles
observed to take approximately tjtime to pass from the emitter to the
detector during a cycle of operation of the machine. The time of ﬂight has
a known relationship to the mass over charge ratio ( m/z) of the constituent
proteins in the blood. Hence the identiﬁcation of a peak in th e spectrum
at a certain tjtells us that there is a protein with a corresponding mass
and charge. The identity of this protein can then be determin ed by other
means.
Figure 18.7 shows an example taken from Adam et al. (2003). It shows
the average spectra for healthy patients and those with pros tate cancer.
There are 16,898 m/zsites in total, ranging in value from 2000 to 40,000.
The full dataset consists of 157 healthy patients and 167 wit h cancer, and
the goal is to ﬁnd m/zsites that discriminate between the two groups.
This is an example of functional data; the predictors can be viewed as a
function of m/z. There has been much interest in this problem in the past
few years; see e.g. Petricoin et al. (2002).
The data were ﬁrst standardized (baseline subtraction and n ormaliza-
tion), and we restricted attention to m/zvalues between 2000 and 40,000
(spectra outside of this range were not of interest). We then applied near-

18.4 Linear Classiﬁers with L1Regularization 665
2e+03 5e+03 1e+04 2e+04 5e+04 1e+05 2e+0510 20 30 40
m/zIntensityNormal
Cancer
FIGURE 18.7. Protein mass spectrometry data: average proﬁles from normal
and prostate cancer patients.
est shrunken centroids and lasso regression to the data, wit h the results for
both methods shown in Table 18.2.
By ﬁtting harder to the data, the lasso achieves a considerab ly lower
test error rate. However, it may not provide a scientiﬁcally useful solu-
tion. Ideally, protein mass spectrometry resolves a biolog ical sample into
its constituent proteins, and these should appear as peaks i n the spectra.
The lasso doesn’t treat peaks in any special way, so not surpr isingly only
some of the non-zero lasso weights were situated near peaks i n the spectra.
Furthermore, the same protein may yield a peak at slightly di ﬀerentm/z
values in diﬀerent spectra. In order to identify common peak s, some kind
ofm/zwarping is needed from sample to sample.
To address this, we applied a standard peak-extraction algo rithm to each
spectrum, yielding a total of 5178 peaks in the 217 training s pectra. Our
idea was to pool the collection of peaks from all patients, an d hence con-
struct a set of common peaks. For this purpose, we applied hie rarchical
clustering to the positions of these peaks along the log m/zaxis. We cut
the resulting dendrogram horizontally at height log(0 .005)3, and computed
averages ofthepeakpositions ineachresultingcluster.Th isprocessyielded
728 common clusters and their corresponding peak centers.
Given these 728 common peaks, we determined which of these we re
present in each individual spectrum, and if present, the hei ght of the peak.
A peak height of zero was assigned if that peak was not found. T his pro-
duced a 217×728 matrix of peak heights as features, which was used in a
lasso regression. We scored the test spectra for the same 728 peaks.
3Use of the value 0 .005 means that peaks with positions less than 0.5% apart are
considered the same peak, a fairly common assumption.

666 18. High-Dimensional Problems: p≫N
TABLE 18.2. Results for the prostate data example. The standard deviation for
the test errors is about 4.5.
Method Test Errors/108 Number of Sites
1. Nearest shrunken centroids 34 459
2. Lasso 22 113
3. Lasso on peaks 28 35
The prediction results for this application of the lasso to t he peaks are
shown in the last line of Table 18.2: it does fairly well, but n ot as well
as the lasso on the raw spectra. However, the ﬁtted model may b e more
useful to the biologist as it yields 35 peak positions for fur ther study. On
the other hand, the results suggest that there may be useful d iscriminatory
information between the peaks of the spectra, and the positi ons of the lasso
sites from line (2) of the table also deserve further examina tion.
18.4.2 The Fused Lasso for Functional Data
In the previous example, the features had a natural order, de termined by
the mass-to-charge ratio m/z. More generally, we may have functional fea-
turesxi(t) that are ordered according to some index variable t. We have
already discussed several approaches for exploiting such s tructure.
We can represent xi(t) by their coeﬃcients in a basis of functions in t,
such as splines, wavelets or Fourier bases, and then apply a r egression using
these coeﬃcients as predictors. Equivalently, one can inst ead represent the
coeﬃcients of the original features in these bases. These ap proaches are
described in Section 5.3.
In the classiﬁcation setting, we discuss the analogous appr oach of penal-
izeddiscriminantanalysisinSection12.6.Thisusesapena ltythatexplicitly
controls the resulting smoothness of the coeﬃcient vector.
The above methods tend to smooth the coeﬃcients uniformly. H ere we
present a more adaptive strategy that modiﬁes the lasso pena lty to take
into account the ordering of the features. The fused lasso (Tibshirani et
al., 2005) solves
min
β∈IRp/braceleftiggN/summationdisplay
i=1(yi−β0−p/summationdisplay
j=1xijβj)2+λ1p/summationdisplay
j=1|βj|+λ2p−1/summationdisplay
j=1|βj+1−βj|/bracerightigg
.(18.22)
This criterion is strictly convex in β, so a unique solution exists. The ﬁrst
penalty encourages the solution to be sparse, while the seco nd encourages
it to be smooth in the index j.
The diﬀerence penalty in (18.22) assumes an uniformly space d indexj. If
instead the underlying index variable thas nonuniform values tj, a natural
generalization of (18.22) would be based on divided diﬀeren ces

18.4 Linear Classiﬁers with L1Regularization 667
0 200 400 600 800 1000−2 0 2 4
Genome orderlog2 ratio
FIGURE 18.8. Fused lasso applied to CGH data. Each point represents the
copy-number of a gene in a tumor sample, relative to that of a con trol (on the log
base-2 scale).
λ2p−1/summationdisplay
j=1|βj+1−βj|
|tj+1−tj|. (18.23)
This amounts to having a penalty modiﬁer for each of the terms in the
series.
A particularly useful special case arises when the predicto r matrix X=
IN, theN×Nidentity matrix. This is a special case of the fused lasso,
used to approximate a sequence {yi}N
1. Thefused lasso signal approximator
solves
min
β∈IRN/braceleftiggN/summationdisplay
i=1(yi−β0−βi)2+λ1N/summationdisplay
i=1|βi|+λ2N−1/summationdisplay
i=1|βi+1−βi|/bracerightigg
.(18.24)
Figure 18.8 shows an example taken from Tibshirani and Wang ( 2007). The
data in the panel come from a Comparative Genomic Hybridizat ion (CGH)
array, measuring the approximate log (base-two) ratio of th e number of
copies of each gene in a tumor sample, as compared to a normal s ample.
The horizontal axis represents the chromosomal location of each gene. The
ideaisthatincancercells,genesareoftenampliﬁed(dupli cated)ordeleted,
and it is of interest to detect these events. Furthermore, th ese events tend
to occur in contiguous regions. The smoothed signal estimat e from the
fused lasso signal approximator is shown in dark red (with ap propriately
chosen values for λ1andλ2). The signiﬁcantly nonzero regions can be used
to detect locations of gains and losses of genes in the tumor.
There is also a two-dimensional version of the fused lasso, i n which the
parameters are laid out in a grid of pixels, and a penalty is ap plied to the

668 18. High-Dimensional Problems: p≫N
ﬁrst diﬀerences to the left, right, above and below the targe t pixel. This
can be useful for denoising or classifying images. Friedman et al. (2007)
develop fast generalized coordinate descent algorithms fo r the one- and
two-dimensional fused lasso.
18.5 Classiﬁcation When Features are Unavailable
In some applications the objects under study are more abstra ct in nature,
and it is not obvious how to deﬁne a feature vector. As long as w e can ﬁll
in anN×Nproximity matrix of similarities between pairs of objects in our
database,itturnsoutwecanputtousemanyoftheclassiﬁers inourarsenal
by interpreting the proximities as inner-products. Protei n structures fall
into this category, and we explore an example in Section 18.5 .1 below.
In other applications, such as document classiﬁcation, fea ture vectors are
available but can be extremely high-dimensional. Here we ma y not wish
to compute with such high-dimensional data, but rather stor e the inner-
products between pairs of documents. Often these inner-pro ducts can be
approximated by sampling techniques.
Pairwise distances serve a similar purpose, because they ca n be turned
into centered inner-products. Proximity matrices are disc ussed in more de-
tail in Chapter 14.
18.5.1 Example: String Kernels and Protein Classiﬁcation
An important problem in computational biology is to classif y proteins into
functional and structural classes based on their sequence s imilarities. Pro-
tein molecules are strings of amino acids, diﬀering in both l ength and com-
position. In the example we consider, the lengths vary betwe en 75–160
amino-acidmolecules,eachofwhichcanbeoneof20diﬀerent types,labeled
using letters. Here are two examples, of length 110 and 153, r espectively:
IPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTL ESQTVQGGTV
ERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDY LQEFLGVMNTEWI
PHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAER LQENLQAYRTFHVLLA
RLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEA DGMLFEKK
LWGLKV LQELSQWTVRSIHDLRFISSHQTGIP
There have been many proposals for measuring the similarity between a
pair of protein molecules. Here we focus on a measure based on the count
of matching substrings (Leslie et al., 2004), such as the LQEabove.
To construct our features, we count the number of times that a given
sequence of length moccurs in our string, and we compute this number

18.5 Classiﬁcation When Features are Unavailable 669
for all possible sequences of length m. Formally, for a string x, we deﬁne a
feature map
Φm(x) ={φa(x)}a∈Am (18.25)
whereAmis the set of subsequences of length m, andφa(x) is the number
of times that “ a” occurs in our string x. Using this, we deﬁne the inner
product
Km(x1,x2) =∝an}⌊∇a⌋ketle{tΦm(x1),Φm(x2)∝an}⌊∇a⌋ket∇i}ht, (18.26)
which measures the similarity between the two strings x1, x2.This can be
used to drive, for example, a support vector classiﬁer for cl assifying strings
into diﬀerent protein classes.
Now the number of possible sequences ais|Am|= 20m, which can be
very large for moderate m, and the vast majority of the subsequences do
not match the strings in our training set. It turns out that we can compute
theN×Ninner-product matrix or string kernel Km(18.26) eﬃciently
using tree-structures, without actually computing the ind ividual vectors.
This methodology, and the data to follow, come from Leslie et al. (2004).4
The data consist of 1708 proteins in two classes— negative (1 663) and
positive (45). The two examples above, which we will call “ x1” and “x2”,
are from this set. We have marked the occurrences of subseque nceLQE,
which appears in both proteins. There are 203possible subsequences, so
Φ3(x) will be a vector of length 8000. For this example φLQE(x1) = 1 and
φLQE(x2) = 2.
Using software from Leslie et al. (2004), we computed the str ing kernel
form= 4, which was then used in a support vector classiﬁer to ﬁnd th e
maximal margin solution in this 204= 160,000-dimensional feature space.
We used 10-fold cross-validation to compute the SVM predict ions on all of
thetrainingdata.TheorangecurveinFigure18.9showsthec ross-validated
ROC curve for the support vector classiﬁer, computed by vary ing the cut-
point on the real-valued predictions from the cross-valida ted support vector
classiﬁer. The area under the curve is 0.84. Leslie et al. (20 04) show that
the string kernel method is competitive with, but perhaps no t as accurate
as, more specialized methods for protein string matching.
Many other classiﬁers can be computed using only the informa tion in the
kernel matrix; some details are given in the next section. Th e results for
the nearest centroid classiﬁer (green), and distance-weig hted one-nearest
neighbors (blue) are shown in Figure 18.9. Their performanc e is similar to
that of the support vector classiﬁer.
4We thank Christina Leslie for her help and for providing the data, w hich is available
on our book website.

670 18. High-Dimensional Problems: p≫N
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0ROC Curves for String Kernel
SpecificitySensitivity
SVM  0.84
Nearest Centroid  0.84
One−Nearest Neighbor 0.86
FIGURE 18.9. Cross-validated ROC curves for protein example using the stri ng
kernel. The numbers next to each method in the legend give the ar ea under the
curve, an overall measure of accuracy. The SVM achieves bett er sensitivities than
the other two, which achieve better speciﬁcities.
18.5.2 Classiﬁcation and Other Models Using Inner-Product
Kernels and Pairwise Distances
There are a number of other classiﬁers, besides the support- vector ma-
chine, that can be implemented using only inner-product mat rices. This
also implies they can be “kernelized” like the SVM.
Anobviousexampleisnearest-neighborclassiﬁcation,sin cewecantrans-
form pairwise inner-products to pairwise distances:
||xi−xi′||2=∝an}⌊∇a⌋ketle{txi,xi∝an}⌊∇a⌋ket∇i}ht+∝an}⌊∇a⌋ketle{txi′,xi′∝an}⌊∇a⌋ket∇i}ht−2∝an}⌊∇a⌋ketle{txi,xi′∝an}⌊∇a⌋ket∇i}ht. (18.27)
A variation of 1-NN classiﬁcation is used in Figure 18.9, whi ch produces
a continuous discriminant score needed to construct a ROC cu rve. This
distance-weighted 1-NN makes use of the distance of a test po ints to the
closest member of each class; see Exercise 18.14.
Nearest-centroid classiﬁcation follows easily as well. Fo r training pairs
(xi,gi), i= 1,...,N, a test point x0, and class centroids ¯ xk,k= 1,...,K
we can write
||x0−¯xk||2=∝an}⌊∇a⌋ketle{tx0,x0∝an}⌊∇a⌋ket∇i}ht−2
Nk/summationdisplay
gi=k∝an}⌊∇a⌋ketle{tx0,xi∝an}⌊∇a⌋ket∇i}ht+1
N2
k/summationdisplay
gi=k/summationdisplay
gi′=k∝an}⌊∇a⌋ketle{txi,xi′∝an}⌊∇a⌋ket∇i}ht,(18.28)

18.5 Classiﬁcation When Features are Unavailable 671
Hence we can compute the distance of the test point to each of t he cen-
troids, and perform nearest centroid classiﬁcation. This a lso implies that
methods like K-means clustering can also be implemented, us ing only the
inner products of the data points.
Logistic and multinomial regression with quadratic regula rization can
also be implemented with inner-product kernels; see Sectio n 12.3.3 and
Exercise 18.13. Exercise 12.10 derives linear discriminan t analysis using an
inner-product kernel.
Principal components can be computed using inner-product k ernels as
well; since this is frequently useful, we give some details. Suppose ﬁrst
that we have a centered data matrix X, and let X=UDVTbe its SVD
(18.12). Then Z=UDis the matrix of principal component variables (see
Section 14.5.1). But if K=XXT, then it follows that K=UD2UT, and
hence we can compute Zfrom the eigen decomposition of K. IfXisnot
centered, then we can center it using ˜X= (I−M)X, whereM=1
N11T
is the mean operator. Thus we compute the eigenvectors of the double-
centered kernel (I−M)K(I−M) for the principal components from an
uncentered inner-product matrix. Exercise 18.15 explores this further, and
Section 14.5.4 discusses in more detail kernel PCA for gener al kernels, such
as the radial kernel used in SVMs.
If instead we had available only the pairwise (squared) Eucl idean dis-
tances between observations,
∆2
ii′=||xi−xi′||2, (18.29)
it turns out we can do all of the above as well. The trick is to co nvert the
pairwise distances to centered inner-products, and then pr oceed as before.
We write
∆2
ii′=||xi−¯x||2+||xi′−¯x||2−2∝an}⌊∇a⌋ketle{txi−¯x,xi′−¯x∝an}⌊∇a⌋ket∇i}ht. (18.30)
Deﬁning B={−∆2
ii′/2}, we double center B:
˜K= (I−M)B(I−M); (18.31)
it is easy to check that ˜Kii′=∝an}⌊∇a⌋ketle{txi−¯x,xi′−¯x∝an}⌊∇a⌋ket∇i}ht, the centered inner-product
matrix.
Distancesandinner-productsalsoallowustocomputetheme doidineach
class—the observation with smallest average distance to oth er observations
in that class. This can be used for classiﬁcation (closest me doids), as well as
to drivek-medoids clustering (Section 14.3.10). With abstract data objects
likeproteins,medoidshaveapracticaladvantageovermean s.Themedoidis
oneofthetrainingexamples,andcanbedisplayed.Wetriedc losestmedoids
in the example in the next section (see Table 18.3), and its pe rformance is
disappointing.
It is useful to consider what we cannotdo with inner-product kernels and
distances:

672 18. High-Dimensional Problems: p≫N
TABLE 18.3. Cross-validated error rates for the abstracts example. The ne arest
shrunken centroids ended up using no-shrinkage, but does us e a word-by-word
standardization (section 18.2). This standardization giv es it a distinct advantage
over the other methods.
Method CV Error (SE)
1. Nearest shrunken centroids 0.17 (0.05)
2. SVM 0.23 (0.06)
3. Nearest medoids 0.65 (0.07)
4. 1-NN 0.44 (0.07)
5. Nearest centroids 0.29 (0.07)
•Wecannotstandardizethevariables;standardizationsign iﬁcantlyim-
proves performance in the example in the next section.
•We cannot assess directly the contributions of individual v ariables.
In particular, we cannot perform individual t-tests, ﬁt the nearest
shrunkencentroidsmodel,orﬁtanymodelthatusesthelasso penalty.
•Wecannotseparatethegoodvariablesfromthenoise:allvar iablesget
an equal say. If, as is often the case, the ratio of relevant to irrelevant
variables is small, methods that use kernels are not likely t o work as
well as methods that do feature selection.
18.5.3 Example: Abstracts Classiﬁcation
This somewhat whimsical example serves to illustrate a limi tation of ker-
nel approaches. We collected the abstracts from 48 papers, 1 6 each from
Bradley Efron (BE), Trevor Hastie and Rob Tibshirani (HT) (f requent co-
authors), and Jerome Friedman (JF). We extracted all unique words from
these abstracts, and deﬁned features xijto be the number of times word
jappears in abstract i. This is the so-called bag of words representation.
Quotations, parentheses and special characters were ﬁrst r emoved from the
abstracts, and all characters were converted to lower case. We also removed
the word “we”, which could unfairly discriminate HT abstrac ts from the
others.
There were 4492 total words, of which p= 1310 were unique. We sought
to classify the documents into BE, HT or JF on the basis of the f eatures
xij. Although it is artiﬁcial, this example allows us to assess t he possible
degradation in performance if information speciﬁc to the ra w features is
not used.
Weﬁrstappliedthenearestshrunkencentroidclassiﬁertot hedata,using
10-foldcross-validation.Itessentiallychosenoshrinka ge,andsousedallthe
features; see the ﬁrst line of Table 18.3. The error rate is 17 %; the number
of features can be reduced to about 500 without much loss in ac curacy.

18.5 Classiﬁcation When Features are Unavailable 673
Note that the nearest shrunken classiﬁer requires the raw fe ature matrix
Xin order to standardize the features individually. Figure 1 8.10 shows the
predictivebayesusingthanalgorithmareproceduretechnologyvaluesaccuracyvariableswheninferencethosebayesianfrequentistproposepresentedmethodproblemsBE HT JF
FIGURE 18.10. Abstracts example: top 20scores from nearest shrunken cen-
troids. Each score is the standardized diﬀerence in frequen cy for the word in the
given class (BE, HT or JF) versus all classes. Thus a positive sc ore (to the right
of the vertical grey zero lines) indicates a higher frequency in that class; a negative
score indicates a lower relative frequency.
top 20 discriminating words, with a positive score indicati ng that a word
appears more in that class than in the other classes.
Someofthesetermsmakesense:forexample“frequentist”an d“Bayesian”
reﬂectEfron’sgreateremphasisonstatisticalinference. However,manyoth-
ers are surprising, and reﬂect personal writing styles: for example, Fried-
man’s use of “presented” and HT’s use of “propose”.
We then applied the support vector classiﬁer with linear ker nel and no
regularization, using the “all pairs” ( ovo) method to handle the three
classes (regularization of the SVM did not improve its perfo rmance). The
result is shown in Table 18.3. It does somewhat worse than the nearest
shrunken centroid classiﬁer.
Asmentioned,theﬁrstlineofTable18.3representsnearest shrunkencen-
troids (with no shrinkage). Denote by sjthe pooled within-class standard
deviation for feature j, ands0the median of the sjvalues. Then line (1)
also corresponds to nearest centroid classiﬁcation, after ﬁrst standardizing
each feature by sj+s0[recall (18.4) on page 652].
Line (3) shows that the performance of nearest medoids is ver y poor,
something which surprised us. It is perhaps due to the small s ample sizes

674 18. High-Dimensional Problems: p≫N
and high dimensions, with medoids having much higher varian ce than
means. The performance of the one-nearest neighbor classiﬁ er is also poor.
The performance of the nearest centroid classiﬁer is also sh own in Ta-
ble 18.3 in line (5): it is better than nearest medoids, but wo rse than that
of nearest shrunken centroids, even with no shrinkage. The d iﬀerence seems
to be the standardization of each feature that is done in near est shrunken
centroids. This standardization is important here, and req uires access to
the individual feature values. Nearest centroids uses a sph erical metric, and
relies on the fact that the features are in similar units. The support vector
machine estimates a linear combination of the features and c an better deal
with unstandardized features.
18.6 High-Dimensional Regression: Supervised
Principal Components
In this section we describe a simple approach to regression a nd generalized
regression that is especially useful when p≫N. We illustrate the method
on another microarray data example. The data is taken from Ro senwald
et al. (2002) and consists of 240 samples from patients with d iﬀuse large
B-cell lymphoma (DLBCL), with gene expression measurement s for 7399
genes. The outcome is survival time, either observed or righ t censored. We
randomly divided the lymphoma samples into a training set of size 160 and
a test set of size 80.
Although supervised principal components is useful for lin ear regression,
its most interesting applications may be in survival studie s, which is the
focus of this example.
We have not yet discussed regression with censored survival data in this
book; it represents a generalized form of regression in whic h the outcome
variable (survival time) is only partly observed for some in dividuals. Sup-
pose for example we carry out a medical study that lasts for 36 5 days, and
for simplicity all subjects are recruited on day one. We migh t observe one
individual to die 200 days after the start of the study. Anoth er individ-
ual might still be alive at 365 days when the study ends. This i ndividual
is said to be “right censored” at 365 days. We know only that he or she
livedat least365 days. Although we do not know how long past 365 days
the individual actually lived, the censored observation is still informative.
This is illustrated in Figure 18.11. Figure 18.12 shows the s urvival curve
estimated by the Kaplan–Meier method for the 80 patients in t he test set.
See for example Kalbﬂeisch and Prentice (1980) for a descrip tion of the
Kaplan–Meier method.
Our objective in this example is to ﬁnd a set of features (gene s) that
can predict the survival of an independent set of patients. T his could be

18.6 High-Dimensional Regression: Supervised Principal Co mponents 675
Time(days)Patient
0 100 200 300 365 1234
FIGURE 18.11. Censored survival data. For illustration there are four pati ents.
The ﬁrst and third patients die before the study ends. The sec ond patient is alive
at the end of the study ( 365days), while the fourth patient is lost to follow-up
before the study ends. For example, this patient might have mo ved out of the
country. The survival times for patients two and four are sai d to be “censored.”
0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0l
lllllllll lll
ll ll ll l lllll
l ll l l l lPr(T≥t)
MonthstSurvival Function
FIGURE 18.12. Lymphoma data. The Kaplan–Meier estimate of the survival
function for the 80patients in the test set, along with one-standard-error curv es.
The curve estimates the probability of surviving past tmonths. The ticks indicate
censored observations.

676 18. High-Dimensional Problems: p≫N
Poor Cell Type Good Cell Type
Survival Time
FIGURE 18.13. Underlying conceptual model for supervised principal compo -
nents. There are two cell types, and patients with the good ce ll type live longer on
the average. Supervised principal components estimate the cell type, by averaging
the expression of genes that reﬂect it.
useful as a prognostic indicator to aid in choosing treatmen ts, or to help
understand the biological basis for the disease.
The underlying conceptual model for supervised principal c omponents
is shown in Figure 18.13. We imagine that there are two cell ty pes, and
patients with the good cell type live longer on the average. H owever there
is considerable overlap in the two sets of survival times. We might think
of survival time as a “noisy surrogate” for cell type. A fully supervised
approach would give the most weight to those genes having the strongest
relationship with survival. These genes are partially, but not perfectly, re-
lated to cell type. If we could instead discover the underlyi ng cell types of
the patients, often reﬂected by a sizable signature of genes acting together
in pathways, then we might do a better job of predicting patie nt survival.
Although the cell type in Figure 18.13 is discrete, it is usef ul to imagine
a continuous cell type, deﬁne by some linear combination of t he features.
We will estimate the cell type as a continuous quantity, and t hen discretize
it for display and interpretation.
Howcanweﬁndthelinearcombinationthatdeﬁnestheimporta ntunder-
lying cell types?Principal components analysis (Section 1 4.5) is an eﬀective
method for ﬁnding linear combinations of features that exhi bit large varia-
tion in a dataset. But what we seek here are linear combinatio ns with both
high variance andsigniﬁcant correlation with the outcome. The lower right
panel of Figure 18.14 shows the result of applying standard p rincipal com-
ponents in this example; the leading component does not corr elate strongly
with survival (details are given in the ﬁgure caption).
Hence we want to encourage principal component analysis to ﬁ nd linear
combinations of features that have high correlation with th e outcome. To
do this, we restrict attention to features which by themselv es have a siz-
able correlation with the outcome. This is summarized in the supervised
principal components Algorithm 18.1, and illustrated in Figure 18.14.
The details in steps (1) and (2b) will depend on the type of out come
variable. For a standard regression problem, we use the univ ariate linear
least squares coeﬃcients in step (1) and a linear least squar es model in

18.6 High-Dimensional Regression: Supervised Principal Co mponents 677
7399 7350 50 27 1 Genes
PatientsSupervised PC
1 80 160
Absolute Cox Score0 2 40 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0Probability of Survivallow score
high scoreBest Single Gene
P=0.15
0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0Probability of SurvivalSupervised Principal Component − 27 Genes
P=0.006
0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0
MonthsProbability of SurvivalPrincipal Component − 7399 Genes
P=0.14
FIGURE 18.14. Supervised principal components on the lymphoma data. The
left panel shows a heatmap of a subset of the gene-expression t raining data. The
rows are ordered by the magnitude of the univariate Cox-scor e, shown in the mid-
dle vertical column. The top 50 and bottom 50genes are shown. The supervised
principal component uses the top 27genes (chosen by 10-fold CV). It is repre-
sented by the bar at the top of the heatmap, and is used to order the columns
of the expression matrix. In addition, each row is multiplied b y the sign of the
Cox-score. The middle panel on the right shows the survival cu rves on the test
data when we create a low and high group by splitting this superv ised PC at zero
(training data mean). The curves are well separated, as indi cated by the p-value
for the log-rank test. The top panel does the same, using the to p-scoring gene on
the training data. The curves are somewhat separated, but no t signiﬁcantly. The
bottom panel uses the ﬁrst principal component on all the gen es, and the separa-
tion is also poor. Each of the top genes can be interpreted as no isy surrogates for
a latent underlying cell-type characteristic, and supervised principal components
uses them all to estimate this latent factor.

678 18. High-Dimensional Problems: p≫N
Algorithm 18.1 Supervised Principal Components.
1. Compute the standardized univariate regression coeﬃcie nts for the
outcome as a function of each feature separately.
2. Foreachvalueofthethreshold θfromthelist0≤θ1<θ2<···<θK:
(a) Form a reduced data matrix consisting of only those featu res
whose univariate coeﬃcient exceeds θin absolute value, and
compute the ﬁrst mprincipal components of this matrix.
(b) Use these principal components in a regression model to p redict
the outcome.
3. Pickθ(andm) by cross-validation.
step (2b). For survival problems, Cox’s proportional hazar ds regression
modeliswidelyused;henceweusethescoretestfromthismod elinstep(1)
and the multivariate Cox model in step (2b). The details are n ot essential
forunderstandingthebasicmethod;theymaybefoundinBair etal.(2006).
Figure 18.14 shows the results of supervised principal comp onents in this
example. We used a Cox-score cutoﬀ of 3.53, yielding 27 genes , where the
value 3.53 was found through 10-fold cross-validation. We t hen computed
the ﬁrst principal component ( m= 1) using just this subset of the data,
as well as its value for each of the test observations. We incl uded this as
a quantitative predictor in a Cox regression model, and its l ikelihood-ratio
signiﬁcance was p= 0.005. When dichotomized (using the mean score on
the training data as a threshold), it clearly separates the p atients in the
test set into low and high risk groups (middle-right panel of Figure 18.14,
p= 0.006).
The top-right panel of Figure 18.14 uses the top scoring gene (dichot-
omized) alone as a predictor of survival. It is not signiﬁcan t on the test set.
Likewise, the lower-right panel shows the dichotomized pri ncipal compo-
nent using all the training data, which is also not signiﬁcan t.
Our procedure allows m>1 principal components in step (2a). However,
the supervision in step (1) encourages the principal compon ents to align
with the outcome, and thus in most cases only the ﬁrst or ﬁrst f ew com-
ponents tend to be useful for prediction. In the mathematica l development
below, we consider only the ﬁrst component, but extensions t o more than
one component can be derived in a similar way.
18.6.1 Connection to Latent-Variable Modeling
A formal connection between supervised principal componen ts and the un-
derlyingcelltypemodel(Figure18.13)canbeseenthrougha latentvariable
model for the data. Suppose we have a response variable Ywhich is related

18.6 High-Dimensional Regression: Supervised Principal Co mponents 679
to an underlying latent variable Uby a linear model
Y=β0+β1U+ε. (18.32)
Inaddition,wehavemeasurementsonasetoffeatures Xjindexedby j∈P
(for pathway), for which
Xj=α0j+α1jU+ǫj, j∈P. (18.33)
The errorsεandǫjare assumed to have mean zero and are independent of
all other random variables in their respective models.
We also have many additional features Xk,k∝ne}ationslash∈Pwhich are independent
ofU. We would like to identify P, estimateU, and hence ﬁt the predic-
tion model (18.32). This is a special case of a latent-struct ure model, or
single-component factor-analysis model (Mardia et al., 19 79, see also Sec-
tion 14.7). The latent factor Uis a continuous version of the cell type
conceptualized in Figure 18.13.
The supervised principal component algorithm can be seen as a method
for ﬁtting this model:
•The screening step (1) estimates the set P.
•Given/hatwideP, the largest principal component in step (2a) estimates the
latent factor U.
•Finally, the regression ﬁt in step (2b) estimates the coeﬃci ent in
model (18.32).
Step (1) is natural, since on average the regression coeﬃcie nt is nonzero
only ifα1jis non-zero. Hence this step should select the features j∈P.
Step (2a) is natural if we assume that the errors ǫjhave a Gaussian dis-
tribution, with the same variance. In this case the principa l component is
the maximum likelihood estimate for the single factor model (Mardia et
al., 1979). The regression in (2b) is an obvious ﬁnal step.
Supposethereareatotalof pfeatures,with p1featuresintherelevantset
P. Then ifpandp1grow butp1is small relative to p, one can show (under
reasonable conditions) that the leading supervised princi pal component
is consistent for the underlying latent factor. The usual le ading principal
component may not be consistent, since it can be contaminate d by the
presence of a large number of “noise” features.
Finally, suppose that the threshold used in step (1) of the su pervised
principal component procedure yields a large number of feat ures for com-
putation of the principal component. Then for interpretati onal purposes, as
well as for practical uses, we would like some way of ﬁnding a r educed a set
of features that approximates the model. Pre-conditioning (Section 18.6.3)
is one way of doing this.

680 18. High-Dimensional Problems: p≫N
18.6.2 Relationship with Partial Least Squares
Supervised principal components is closely related to part ial least squares
regression (Section 3.5.2). Bair et al. (2006) found that th e key to the good
performance of supervised principal components was the ﬁlt ering out of
noisyfeaturesinstep(2a).Partialleastsquares(Section 3.5.2)downweights
noisy features, but does not throw them away; as a result a lar ge number
of noisy features can contaminate the predictions. However , a modiﬁcation
of the partial least squares procedure has been proposed tha t has a similar
ﬂavor to supervised principal components [Brown et al. (199 1),Nadler and
Coifman (2005), for example]. We select the features as in st eps (1) and
(2a) of supervised principal components, but then apply PLS (rather than
principal components) to these features. For our current di scussion, we call
this “thresholded PLS.”
Thresholded PLScan beviewed as anoisy version of supervise dprincipal
components, and hence we might not expect it to work as well in practice.
Assume the variables are all standardized. The ﬁrst PLS vari ate has the
form
z=/summationdisplay
j∈P∝an}⌊∇a⌋ketle{ty,xj∝an}⌊∇a⌋ket∇i}htxj, (18.34)
andcanbethoughtofasanestimateofthelatentfactor Uinmodel(18.33).
In contrast, the supervised principal components directio nˆusatisﬁes
ˆu=1
d2/summationdisplay
j∈P∝an}⌊∇a⌋ketle{tˆu,xj∝an}⌊∇a⌋ket∇i}htxj, (18.35)
wheredis the leading singular value of XP. This follows from the deﬁnition
of the leading principal component. Hence thresholded PLS u ses weights
which are the inner product of ywith each of the features, while supervised
principalcomponentsusesthefeaturestoderivea“self-co nsistent”estimate
ˆu. Since many features contribute to the estimate ˆu, rather than just the
single outcome y, we can expect ˆuto be less noisy than z. In fact, if there
arep1features in the set P, andN, pandp1go to inﬁnity with p1/N→0,
then it can be shown using the techniques in Bair et al. (2006) that
z=u+Op(1)
ˆu=u+Op(/radicalbig
p1/N), (18.36)
whereuis the true (unobservable) latent variable in the model (18. 32),
(18.33).
We now present a simulation example to compare the methods nu meri-
cally. There are N= 100 samples and p= 5000 genes. We generated the
data as follows:

18.6 High-Dimensional Regression: Supervised Principal Co mponents 681
FIGURE 18.15. Heatmap of the outcome (left column) and ﬁrst 500genes from
a realization from model (18.37). The genes are in the columns, and the samples
are in the rows.
xij=/braceleftigg
3+ǫijifi≤50,
4+ǫijifi>50j= 1,...,50
xij=/braceleftigg
1.5+ǫijif 1≤i≤25 or 51≤i≤75
5.5+ǫijif 26≤i≤50 or 76≤i≤100j= 51,...,250
xij=ǫij j= 251,...,5000
yi= 2·1
50/summationtext50
j=1xij+εi
(18.37)
whereǫijandεiare independent normal random variables with mean 0 and
standard deviations 1 and 1.5, respectively. Thus in the ﬁrs t 50 genes, there
isanaveragediﬀerenceof1unitbetweensamples1–50and51– 100,andthis
diﬀerence correlates with the outcome y. The next 200 genes have a large
average diﬀerence of 4 units between samples (1–25, 51–75) a nd (26–50,
76–100), but this diﬀerence is uncorrelated with the outcom e. The rest of
the genes are noise. Figure 18.15 shows a heatmap of a typical realization,
with the outcome at the left, and the ﬁrst 500 genes to the righ t.
We generated 100 simulations from this model, and summarize the test
error results in Figure 18.16. The test errors of principal c omponents and
partial least squares are shown at the right of the plot; both are badly
aﬀected by the noisy features in the data. Supervised princi pal components
and thresholded PLS work best over a wide range of the number o f selected
features, with the former showing consistently lower test e rrors.
While this example seems “tailor-made” for supervised prin cipal com-
ponents, its good performance seems to hold in other simulat ed and real
datasets (Bair et al., 2006).
18.6.3 Pre-Conditioning for Feature Selection
Supervisedprincipalcomponentscanyieldlowertesterror sthancompeting
methods, as shown in Figure 18.16. However, it does not alway s produce a
sparse model involving only a small number of features (gene s). Even if the
thresholding in Step (1) of the algorithm yields a relativel y small number

682 18. High-Dimensional Problems: p≫N1.00 1.05 1.10 1.15 1.20 1.25
Number of FeaturesRelative Root Mean Square Test Error
0 50 100 150 200 250 300 ... 5000Thresholded PLS
Supervised Principal Components
FIGURE 18.16. Root mean squared test error ( ±one standard error), for
supervised principal components and thresholded PLS on 100realizations from
model (18.37). All methods use one component, and the errors are relative to
the noise standard deviation (the Bayes error is 1.0). For both methods, diﬀerent
values for the ﬁltering threshold were tried and the number of fe atures retained
is shown on the horizontal axis. The extreme right points cor respond to regular
principal components and partial least squares, using all th e genes.
of features, it may be that some of the omitted features have s izable inner
products with the supervised principal component (and coul d act as a good
surrogate). In addition, highly correlated features will t end to be chosen
together, and there may be great deal of redundancy in the set of selected
features.
The lasso (Sections 18.4 and 3.4.2), on the other hand, produ ces a sparse
modelfromthedata.Howdothetesterrorsofthetwomethodsc ompareon
thesimulatedexampleofthelastsection?Figure18.17show sthetesterrors
for one realization from model (18.37) for the lasso, superv ised principal
components, and the pre-conditioned lasso (described belo w).
We see that supervised principal components (orange curve) reaches its
lowest error when about 50 features are included in the model , which is
the correct number for the simulation. Although a linear mod el in the ﬁrst
50 features is optimal, the lasso (green) is adversely aﬀect ed by the large
number of noisy features, and starts overﬁtting when far few er are in the
model.
Can we get the low test error of supervised principal compone nts along
with the sparsity of the lasso? This is the goal of pre-conditioning (Paul
et al., 2008). In this approach, one ﬁrst computes the superv ised principal
component predictor ˆ yifor each observation in the training set (with the

18.7 Feature Assessment and the Multiple-Testing Problem 6 83
0 50 100 150 200 2503.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4
Number of Features in ModelMean Test ErrorLasso
Supervised Principal Components
Preconditioned Lasso
FIGURE 18.17. Test errors for the lasso, supervised principal components,
and pre-conditioned lasso, for one realization from model (18 .37). Each model is
indexed by the number of non-zero features. The supervised p rincipal component
path is truncated at 250features. The lasso self-truncates at 100, the sample size
(see Section 18.4). In this case, the pre-conditioned lasso a chieves the lowest error
with about 25features.
threshold selected by cross-validation). Then we apply the lasso with ˆyias
the outcome variable, in place of the usual outcome yi. All features are used
in the lasso ﬁt, not just those that were retained in the thres holding step
in supervised principal components. The idea is that by ﬁrst denoising the
outcome variable, the lasso should not be as adversely aﬀect ed by the large
number of noise features. Figure 18.17 shows that pre-condi tioning (purple
curve) has been successful here, yielding much lower test er ror than the
usuallasso,andaslow(inthiscase)asforsupervisedprinc ipalcomponents.
It also can achieve this using less features. The usual lasso , applied to
the raw outcome, starts to overﬁt more quickly than the pre-c onditioned
version. Overﬁtting is not a problem, since the outcome vari able has been
denoised. We usually select the tuning parameter for the pre -conditioned
lasso on more subjective grounds, like parsimony.
Pre-conditioning can be applied in a variety of settings, us ing initial
estimates other than supervised principal components and p ost-processors
other than the lasso. More details may be found in Paul et al. ( 2008).
18.7 Feature Assessment and the Multiple-Testing
Problem
In the ﬁrst part of this chapter we discuss prediction models in thep≫N
setting. Here we consider the more basic problem of assessin g the signif-

684 18. High-Dimensional Problems: p≫N
icance of each of the pfeatures. Consider the protein mass spectrometry
example of Section 18.4.1. In that problem, the scientist mi ght not be inter-
ested in predicting whether a given patient has prostate can cer. Rather the
goal might be to identify proteins whose abundance diﬀers be tween nor-
mal and cancer samples, in order to enhance understanding of the disease
and suggest targets for drug development. Thus our goal is to assess the
signiﬁcance of individual features. This assessment is usu ally done without
the use of a multivariate predictive model like those in the ﬁ rst part of this
chapter. The feature assessment problem moves our focus fro m prediction
to the traditional statistical topic of multiple hypothesis testing . For the
remainder of this chapter we will use Minstead ofpto denote the number
of features, since we will frequently be referring to p-values.
TABLE 18.4. Subset of the 12,625genes from microarray study of radiation
sensitivity. There are a total of 44samples in the normal group and 14in the
radiation sensitive group; we only show three samples from eac h group.
Normal Radiation Sensitive
Gene 1 7.85 29.74 29.50 ...17.20 -50.75 -18.89 ...
Gene 2 15.44 2.70 19.37 ...6.57 -7.41 79.18 ...
Gene 3 -1.79 15.52 -3.13 ...-8.32 12.64 4.75 ...
Gene 4 -11.74 22.35 -36.11 ...-52.17 7.24 -2.32 ...
...........................
Gene 12,625 -14.09 32.77 57.78 ...-32.84 24.09 -101.44 ...
Consider, for example, the microarray data in Table 18.4, ta ken from a
study on the sensitivity of cancer patients to ionizing radi ation treatment
(Rieger et al., 2004). Each row consists of the expression of genes in 58
patient samples: 44 samples were from patients with a normal reaction, and
14 from patients who had a severe reaction to radiation. The m easurements
were made on oligo-nucleotide microarrays. The object of th e experiment
was to ﬁnd genes whose expression was diﬀerent in the radiati on sensitive
group of patients. There are M= 12,625 genes altogether; the table shows
the data for some of the genes and samples for illustration.
To identify informative genes, we construct a two-sample t-statistic for
each gene.
tj=¯x2j−¯x1j
sej, (18.38)
where ¯xkj=/summationtext
i∈Cℓxij/Nℓ. HereCℓare the indices of the Nℓsamples in
groupℓ, whereℓ= 1 is the normal group and ℓ= 2 is the sensitive group.
The quantity se jis the pooled within-group standard error for gene j:

18.7 Feature Assessment and the Multiple-Testing Problem 6 85
−4 −2 0 2 40 200 400 600 800
t−statistics
FIGURE 18.18. Radiation sensitivity microarray example. A histogram of th e
12,625t-statistics comparing the radiation-sensitive versus ins ensitive groups.
Overlaid in blue is the histogram of the t-statistics from 1000permutations of the
sample labels.
sej= ˆσj/radicalig
1
N1+1
N2; ˆσ2
j=1
N1+N2−2/parenleftigg/summationdisplay
i∈C1(xij−¯x1j)2+/summationdisplay
i∈C2(xij−¯x2j)2/parenrightigg
.
(18.39)
A histogram of the 12,625 t-statistics is shown in orange in F igure 18.18,
ranging in value from −4.7 to 5.0. If thetjvalues were normally distributed
we could consider any value greater than two in absolute valu e to be sig-
niﬁcantly large. This would correspond to a signiﬁcance lev el of about 5%.
Here there are 1189 genes with |tj|≥2. However with 12,625 genes we
would expect many large values to occur by chance, even if the group-
ing is unrelated to any gene. For example, if the genes were in dependent
(which they are surely not), the number of falsely signiﬁcan t genes would
have a binomial distribution with mean 12 ,625·0.05 = 631.3 and standard
deviation 24.5; the actual 1189 is way out of range.
How do we assess the results for all 12,625 genes? This is call ed themul-
tiple testing problem. We can start as above by computing a p-value for
each gene. This can be done using the theoretical t-distribution probabil-
ities, which assumes the features are normally distributed . An attractive
alternative approach is to use the permutation distributio n, since it avoids
assumptions about the distribution of the data. We compute ( in principle)
allK=/parenleftbig58
14/parenrightbig
permutations of the sample labels, and for each permutation
kcompute the t-statisticstk
j. Then the p-value for gene jis

686 18. High-Dimensional Problems: p≫N
pj=1
KK/summationdisplay
k=1I(|tk
j|>|tj|). (18.40)
Of course,/parenleftbig58
14/parenrightbig
is a large number (around 1013) and so we can’t enumer-
ate all of the possible permutations. Instead we take a rando m sample of
the possible permutations; here we took a random sample of K= 1000
permutations.
To exploit the fact that the genes are similar (e.g., measure d on the
same scale), we can instead pool the results for all genes in c omputing the
p-values.
pj=1
MKM/summationdisplay
j′=1K/summationdisplay
k=1I(|tk
j′|>|tj|). (18.41)
This also gives more granular p-values than does (18.40), since there many
more values in the pooled null distribution than there are in each individual
null distribution.
Using this set of p-values, we would like to test the hypotheses:
H0j= treatment has no eﬀect on gene j
versus (18.42)
H1j= treatment has an eﬀect on gene j
for allj= 1,2,...,M. We reject H0jat levelαifpj< α. This test has
type-I error equal to α; that is, the probability of falsely rejecting H0jisα.
Now with many tests to consider, it is not clear what we should use
as an overall measure of error. Let Ajbe the event that H0jis falsely
rejected; by deﬁnition Pr( Aj) =α. Thefamily-wise error rate (FWER)
is the probability of at least one false rejection, and is a co mmonly used
overall measure of error. In detail, if A=∪M
j=1Ajis the event of at least
one false rejection, then the FWER is Pr( A). Generally Pr( A)≫αfor
largeM, and depends on the correlation between the tests. If the tes ts are
independent each with type-I error rate α, then the family-wise error rate
of the collection of tests is (1 −(1−α)M). On the other hand, if the tests
have positive dependence, that is Pr( Aj|Ak)>Pr(Aj), then the FWER
will be less than (1 −(1−α)M). Positive dependence between tests often
occurs in practice, in particular in genomic studies.
One of the simplest approaches to multiple testing is the Bonferroni
method. It makes each individual test more stringent, in ord er to make the
FWER equal to at most α: we reject H0jifpj<α/M. It is easy to show
that the resulting FWER is ≤α(Exercise 18.16). The Bonferroni method
can be useful if Mis relatively small, but for large Mit is too conservative,
that is, it calls too few genes signiﬁcant.
In our example, if we test at level say α= 0.05, then we must use the
threshold 0.05/12,625 = 3.9×10−6. None of the 12 ,625 genes had a p-value
this small.

18.7 Feature Assessment and the Multiple-Testing Problem 6 87
There are variations to this approach that adjust the indivi dualp-values
to achieve an FWER of at most α, with some approaches avoiding the
assumption of independence; see, e.g., Dudoit et al. (2002b ).
18.7.1 The False Discovery Rate
A diﬀerent approach to multiple testing does not try to contr ol the FWER,
but focuses instead on the proportion of falsely signiﬁcant genes. As we will
see, this approach has a strong practical appeal.
Table 18.5 summarizes the theoretical outcomes of Mhypothesis tests.
Note that the family-wise error rate is Pr( V≥1). Here we instead focus
TABLE 18.5. Possible outcomes from Mhypothesis tests. Note that Vis the
number of false-positive tests; the type-I error rate is E(V)/M0. The type-II error
rate isE(T)/M1, and the power is 1−E(T)/M1.
Called Called
Not Signiﬁcant Signiﬁcant Total
H0True U VM0
H0False T SM1
Total M−R RM
on thefalse discovery rate
FDR = E(V/R). (18.43)
In the microarray setting, this is the expected proportion o f genes that
are incorrectly called signiﬁcant, among the Rgenes that are called signif-
icant. The expectation is taken over the population from whi ch the data
are generated. Benjamini and Hochberg (1995) ﬁrst proposed the notion of
false discovery rate, and gave a testing procedure (Algorit hm 18.2) whose
FDR is bounded by a user-deﬁned level α. The Benjamini–Hochberg (BH)
procedure is based on p-values; these can be obtained from an asymptotic
approximation to the test statistic (e.g., Gaussian), or a p ermutation dis-
tribution, as is done here.
If the hypotheses are independent, Benjamini and Hochberg ( 1995) show
that regardless of how many null hypotheses are true and rega rdless of the
distribution of the p-values when the null hypothesis is false, this procedure
has the property
FDR≤M0
Mα≤α. (18.45)
For illustration we chose α= 0.15. Figure 18.19 shows a plot of the or-
deredp-valuesp(j), and the line with slope 0 .15/12625.

688 18. High-Dimensional Problems: p≫N
Algorithm 18.2 Benjamini–Hochberg (BH) Method.
1. Fix the false discovery rate αand letp(1)≤p(2)≤···≤p(M)denote
the ordered p-values
2. Deﬁne
L= max/braceleftig
j:p(j)<α·j
M/bracerightig
. (18.44)
3. Reject all hypotheses H0jfor whichpj≤p(L), the BH rejection
threshold.
Genes ordered by p−valuep−value
1 5 10 50 1005*10^−6 5*10^−5 5*10^−4 5*10^−3••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• ••••••••••••••••••••••• •••• ••••••• • •• • ••• •••••••••••••••••••• • ••••••• •••• •••• • ••• •• •••• •• •••••••• ••••••••• • ••••• •••••• ••••• ••••• • ••• •••• • •••••• •• ••• ••••• • •• • •• •• ••••• •• ••• ••••• ••• • •• • •• •• ••• •• • ••• •• ••••••• ••• • • • •• •• •• • • •• •• •••••• • ••• • •
FIGURE 18.19. Microarray example continued. Shown is a plot of the ordered
p-valuesp(j)and the line 0.15·(j/12,625), for the Benjamini–Hochberg method.
The largest jfor which the p-valuep(j)falls below the line, gives the BH threshold.
Here this occurs at j= 11, indicated by the vertical line. Thus the BH method
calls signiﬁcant the 11genes (in red) with smallest p-values.

18.7 Feature Assessment and the Multiple-Testing Problem 6 89
Algorithm 18.3 The Plug-in Estimate of the False Discovery Rate.
1. CreateKpermutations of the data, producing t-statisticstk
jfor fea-
turesj= 1,2,...,Mand permutations k= 1,2,...,K.
2. For a range of values of the cut-point C, let
Robs=M/summationdisplay
j=1I(|tj|>C),/hatwideE(V) =1
KM/summationdisplay
j=1K/summationdisplay
k=1I(|tk
j|>C).(18.46)
3. Estimate the FDR by /hatwideFDR =/hatwideE(V)/Robs.
Starting at the left and moving right, the BH method ﬁnds the l ast time
that thep-values fall below the line. This occurs at j= 11, so we reject
the 11 genes with smallest p-values. Note that the cutoﬀ occurs at the 11th
smallestp-value, 0.00012, and the 11th largest of the values |tj|is 4.101
Thus we reject the 11 genes with |tj|≥4.101.
From our brief description, it is not clear how the BH procedu re works;
that is, why the corresponding FDR is at most 0 .15, the value used for α.
Indeed,theproofofthisfactisquitecomplicated(Benjami niandHochberg,
1995).
A more direct way to proceed is a plug-inapproach. Rather than starting
with a value for α, we ﬁx a cut-point for our t-statistics, say the value
4.101 that appeared above. The number of observed values |tj|equal or
greater than 4 .101 is 11. The total number of permutation values |tk
j|equal
or greater than 4 .101 is 1518, for an average of 1518 /1000 = 1.518 per
permutation. Thus a direct estimate of the false discovery r ate is/hatwideFDR =
1.518/11≈14%. Note that 14% is approximately equal to the value of
α= 0.15 used above (the diﬀerence is due to discreteness). This pr ocedure
is summarized in Algorithm 18.3. To recap:
The plug-in estimate of FDR of Algorithm 18.3 is equivalent t o the BH
procedure of Algorithm 18.2, using the permutation p-values (18.40).
This correspondence between the BH method and the plug-in es timate is
not a coincidence. Exercise 18.17 shows that they are equiva lent in general.
Note that this procedure makes no reference to p-values at all, but rather
works directly with the test statistics.
The plug-in estimate is based on the approximation
E(V/R)≈E(V)
E(R), (18.47)
and in general /hatwideFDR is a consistent estimate of FDR(Storey, 2002; Storey et
al., 2004). Note that the numerator /hatwideE(V) actually estimates ( M/M0)E(V),

690 18. High-Dimensional Problems: p≫N
since the permutation distribution uses MratherM0null hypotheses.
Hence if an estimate of M0is available, a better estimate of FDR can be
obtained from ( ˆM0/M)·/hatwideFDR. Exercise 18.19 shows a way to estimate M0.
The most conservative (upwardly biased) estimate of FDR use sM0=M.
Equivalently, an estimate of M0can be used to improve the BH method,
through relation (18.45).
The reader might be surprised that we chose a value as large as 0.15 for
α, the FDR bound. We must remember that the FDR is not the same as
type-I error, for which 0.05 is the customary choice. For the scientist, the
false discovery rate is the expected proportion of false pos itive genes among
the list of genes that the statistician tells him are signiﬁc ant. Microarray
experiments with FDRs as high as 0.15 might still be useful, e specially if
they are exploratory in nature.
18.7.2 Asymmetric Cutpoints and the SAM Procedure
In the testing methods described above, we used the absolute value of the
test statistic tj, and hence applied the same cut-points to both positive and
negative values of the statistic. In some experiments, it mi ght happen that
most or all of the diﬀerentially expressed genes change in th e positive direc-
tion (or all in the negative direction). For this situation i t is advantageous
to derive separate cut-points for the two cases.
Thesigniﬁcance analysis of microarrays (SAM) approach oﬀers a way of
doing this. The basis of the SAM method is shown in Figure 18.2 0. On the
vertical axis we have plotted the ordered test statistics t(1)≤t(2)≤···≤
t(M), while the horizontal axis shows the expected order statist ics from the
permutations of the data: ˜t(j)= (1/K)/summationtextK
k=1tk
(j), wheretk
(1)≤tk
(2)≤···≤
tk
(M)are the ordered test statistics from permutation k.
Two lines are drawn, parallel to the 45◦line, ∆ units away. Starting at
the origin and moving to the right, we ﬁnd the ﬁrst place that t he genes
leave the band. This deﬁnes the upper cutpoint Chiand all genes beyond
that point are called signiﬁcant (marked red). Similarly we ﬁnd the lower
cutpoint Clowfor genes in the bottom left corner. Thus each value of the
tuning parameter ∆ deﬁnes upper and lower cutpoints, and the plug-in
estimate /hatwideFDR for each of these cutpoints is estimated as before. Typic ally
arangeofvaluesof∆andassociated /hatwideFDRvaluesarecomputed,fromwhich
a particular pair are chosen on subjective grounds.
The advantage of the SAM approach lies in the possible asymme try of
the cutpoints. In the example of Figure 18.20, with ∆ = 0 .71 we obtain
11 signiﬁcant genes; they are all in the upper right. The data points in the
bottom left never leave the band, and hence Clow=−∞. Hence for this
value of ∆, no genes are called signiﬁcant on the left (negati ve) side. We
do not impose symmetry on the cutpoints, as was done in Sectio n 18.7.1,
as there is no reason to assume similar behavior at the two end s.

18.7 Feature Assessment and the Multiple-Testing Problem 6 91
Expected Order Statisticst−statistic
−4 −2 0 2 4−4 −2 0 2 4
•••••••••••••• • •••••••••••• • ••••••• ••••• •• •••••••••••••••• •• • •••••• •••• • ••• • • •••••• • •••• ••••• •••• •• ••• •••• • •••• ••• •• • •• • •• • ••• • ••••• • •• •••• •• •• •••• • ••• •• ••••••• •• •••••••• •• ••• • ••• ••• •••• •• •• •••• •• •• •• •• ••• • •• • •• •• ••• •• •••• ••• ••• •• • •• ••• •• •• •• ••• •• •• •• •• ••• ••• •••• •• •••• •• •• •• •• •• •• •• •• ••• •• •• ••• •• ••• •• ••• ••• •• ••• •••• •••• •• •• •• •• •• •• •• •• • ••••• •• ••• ••• ••• •• •• •• •• •• ••• •• ••• •• •• •• ••• • ••• ••• •• •• •• •• •• •• ••• •• •• •• ••• •• ••• •• •• •• • •• •• •• •••• •• •• •• •• •• •• ••• ••• •• •• ••• •• •• •• • •• •• •• ••• •• •• ••• •• •• •• ••• •• •• •• •• •• ••• •• •• •• •• •• •• ••• •• •• •• •• •• ••• •••• •• •• •• •• ••• •• •••• •• •• •• •• •• ••• •• ••• •• •• •• •• ••• ••• •• ••• •• •• •• •• •• •• •• ••• •• •••• •• ••• ••• •• • •••• •• •• •• •• •• •• •• •• •• •• ••• •• •• •• •• ••• •• •• •• •• ••• •• •• •• ••• •• •• •• ••• • •• •• •• •• •• ••• •• •• •• •• • ••• •• •• •• •• •• •• ••• •• •• ••• •• ••• •• •• •• •• ••• •• •• •• ••• •• •• •• •• •• ••• •• •• •• •• •• •• •• •• ••• •• ••• ••• •• ••• •• ••• •• •• ••• •• •• •• •• ••• •• •• •••• ••• •• •• •• •• ••• •• ••• •• •• •••• •• •• •• ••• •• •• •• •• •• •• •• •• •• ••• •• ••• ••• •• •• •• •• •• •• ••• •• •• ••• •• •• •• •• ••• •• •• •• •• ••• •• •• ••• ••• •• •• •• ••• •• •• •• •• •• •• •• ••• •• ••• •• •• •• ••• •• •• • ••• •• •• •• •• •• •••• •• •• •• ••• •• •• •• •• •• •• ••• •• •• •• •• •• ••• •• ••• •• •• •• •• ••• •• •• •• •• •• ••• •• •• ••• •• ••• •• •• ••• •• ••• •• •• ••• •• ••• •• •• •• •• ••• •• •• •• ••• •• •• •• •• ••• •• •• •• •••• •• •• ••• •• •• • ••• •• •• ••• •• •• •• •• •• •• ••• •• ••• •• •• •• •• •• ••• •• •••• •• •• ••• •• ••• •• •• •• •• •• •• ••• •• •••• •• ••• •• ••• •• •• •• •• •• ••• •• •• •• ••• •• •••••• •• •• ••• ••• •• •• ••• •• •• ••• •• ••• •• ••• ••• ••• •• •• ••• • ••• •• •• •• •••• •• •• •• •• ••• •• •• •• ••• • •••• •• ••• •• •• •• •• ••• •• •• •••• •••• •• •• ••• ••• •• •• •• ••• •• •• •• ••• •• •• •• •• •• •• •• •••• •• •• ••• •• •• ••••• •• •• •• ••• •• •• •••• •• ••• •• •• •• •• •• ••• •• • •• ••• •• ••• •• • ••• ••• •• •• ••• •• • •• •• ••• •• • ••• ••• ••• •• ••• ••• ••• •• • ••• •• •• • •• •• •• ••• ••• • ••• ••• •• •• •• •• •• •• •• •• ••• •• •• •• •• •• •• •• ••• ••• •• ••• ••• •• • •••••••• •• • •••• • ••• •• ••• •• •• •• •• •••• ••• •• •• • ••••••• •• ••• •• • ••••• •••• •• ••••• •• • •••• • ••••• • •• • ••• ••• •• •• • ••••• •• •• •• • •••••• ••• •••• ••••••• •• • ••• • •• • •••• ••• •• ••••••••••• •• • • • • ••••• • • ••••• •••••• •••••••••••• • ••••••• ••••••••••••••••••••••
••••••••••••
∆
Chi
FIGURE 18.20. SAM plot for the radiation sensitivity microarray data. On th e
vertical axis we have plotted the ordered test statistics, wh ile the horizontal axis
shows the expected order statistics of the test statistics f rom permutations of the
data. Two lines are drawn, parallel to the 45◦line,∆units away from it. Starting
at the origin and moving to the right, we ﬁnd the ﬁrst place that the genes leave
the band. This deﬁnes the upper cut-point Chiand all genes beyond that point are
called signiﬁcant (marked in red). Similarly we deﬁne a lower cut pointClow. For
the particular value of ∆ = 0.71in the plot, no genes are called signiﬁcant in the
bottom left.

692 18. High-Dimensional Problems: p≫N
Thereissomesimilaritybetweenthisapproachandtheasymm etrypossi-
blewithlikelihood-ratiotests.Supposewehavealog-like lihoodℓ0(tj)under
the null-hypothesis of no eﬀect, and a log-likelihood ℓ(tj) under the alterna-
tive. Then a likelihood ratio test amounts to rejecting the n ull-hypothesis
if
ℓ(tj)−ℓ0(tj)>∆, (18.48)
for some ∆. Depending on the likelihoods, and particularly t heir relative
values, this can result in a diﬀerent threshold for tjthan for−tj. The SAM
procedure rejects the null-hypothesis if
|t(j)−˜t(j)|>∆ (18.49)
Again, the threshold for each t(j)depends on the corresponding value of
the null value ˜t(j).
18.7.3 A Bayesian Interpretation of the FDR
There is an interesting Bayesian view of the FDR, developed i n Storey
(2002) and Efron and Tibshirani (2002). First we need to deﬁn e thepositive
false discovery rate (pFDR) as
pFDR = E/bracketleftbiggV
R/vextendsingle/vextendsingle/vextendsingle/vextendsingleR>0/bracketrightbigg
. (18.50)
The additional term positiverefers to the fact that we are only interested
in estimating an error rate where positive ﬁndings have occu rred. It is
this slightly modiﬁed version of the FDR that has a clean Baye sian inter-
pretation. Note that the usual FDR [expression (18.43)] is n ot deﬁned if
Pr(R= 0)>0.
Let Γ be a rejection region for a single test; in the example ab ove we used
Γ = (−∞,−4.10)∪(4.10,∞). Suppose that Midentical simple hypothe-
sis tests are performed with the i.i.d. statistics t1,...,tMand rejection
region Γ. We deﬁne a random variable Zjwhich equals 0 if the jth null
hypothesis is true, and 1 otherwise. We assume that each pair (tj,Zj) are
i.i.d random variables with
tj|Zj∼(1−Zj)·F0+Zj·F1 (18.51)
for some distributions F0andF1. This says that each test statistic tjcomes
from one of two distributions: F0if the null hypothesis is true, and F1
otherwise. Letting Pr( Zj= 0) =π0, marginally we have:
tj∼π0·F0+(1−π0)·F1. (18.52)
Then it can be shown (Efron et al., 2001; Storey, 2002) that

18.8 Bibliographic Notes 693
pFDR(Γ) = Pr( Zj= 0|tj∈Γ). (18.53)
Hence under the mixture model (18.51), the pFDR is the poster ior proba-
bility that the null hypothesis it true, given that test stat istic falls in the
rejection region for the test; that is, given that we reject t he null hypothesis
(Exercise 18.20).
The false discovery rate provides a measure of accuracy for t ests based
on an entire rejection region, such as |tj|≥2. But if the FDR of such a test
is say 10%, then a gene with say tj= 5 will be more signiﬁcant than a gene
withtj= 2. Thus it is of interest to derive a local (gene-speciﬁc) ve rsion
of the FDR. The q-value(Storey, 2003) of a test statistic tjis deﬁned to
be the smallest FDR over all rejection regions that reject tj. That is, for
symmetric rejection regions, the q-value fortj= 2 is deﬁned to be the
FDR for the rejection region Γ = {−(∞,−2)∪(2,∞)}. Thus the q-value
fortj= 5 will be smaller than that for tj= 2, reﬂecting the fact that tj= 5
is more signiﬁcant than tj= 2. The local false discovery rate (Efron and
Tibshirani, 2002) at t=t0is deﬁned to be
Pr(Zj= 0|tj=t0). (18.54)
This is the (positive) FDR for an inﬁnitesimal rejection reg ion surrounding
the valuetj=t0.
18.8 Bibliographic Notes
Many references were given at speciﬁc points in this chapter ; we give some
additional ones here. Dudoit et al. (2002a) give an overview and compar-
ison of discrimination methods for gene expression data. Le vina (2002)
does some mathematical analysis comparing diagonal LDA to f ull LDA, as
p,N→∞withp>N. She shows that with reasonable assumptions diago-
nal LDA has a lower asymptotic error rate than full LDA. Tibsh irani et al.
(2001a)andTibshiranietal.(2003)proposedthenearestsh runken-centroid
classiﬁer. Zhu and Hastie (2004) study regularized logisti c regression. High-
dimensional regression and the lasso are very active areas o f research, and
many references are given in Section 3.8.5. The fused lasso w as proposed
by Tibshirani et al. (2005), while Zou and Hastie (2005) intr oduced the
elastic net. Supervised principal components is discussed in Bair and Tib-
shirani (2004) and Bair et al. (2006). For an introduction to the analysis
of censored survival data, see Kalbﬂeisch and Prentice (198 0).
Microarray technology has led to a ﬂurry of statistical rese arch: see for
example the books by Speed (2003), Parmigiani et al. (2003), Simon et al.
(2004), and Lee (2004).
ThefalsediscoveryratewasproposedbyBenjaminiandHochb erg(1995),
and studied and generalized in subsequent papers by these au thors and

694 18. High-Dimensional Problems: p≫N
many others. A partial list of papers on FDR may be found on Yoa v Ben-
jamini’s homepage. Some more recent papers include Efron an d Tibshirani
(2002), Storey (2002), Genovese and Wasserman (2004), Stor ey and Tib-
shirani (2003) and Benjamini and Yekutieli (2005). Dudoit e t al. (2002b)
review methods for identifying diﬀerentially expressed ge nes in microarray
studies.
Exercises
Ex. 18.1 For a coeﬃcient estimate ˆβj, letˆβj/||ˆβj||2be the normalized ver-
sion. Show that as λ→∞, the normalized ridge-regression estimates con-
verge to the renormalized partial-least-squares one-comp onent estimates.
Ex.18.2Nearest shrunken centroids and the lasso. Considera(naiveBayes)
Gaussian model for classiﬁcation in which the features j= 1,2,...,pare
assumed to be independent within each class k= 1,2,...,K. With ob-
servationsi= 1,2,...,NandCkequal to the set of indices of the Nk
observations in class k, we observe xij∼N(µj+µjk,σ2
j) fori∈Ckwith/summationtextK
k=1µjk= 0. Set ˆσ2
j=s2
j, the pooled within-class variance for feature j,
and consider the lasso-style minimization problem
min
{µj,µjk}

1
2p/summationdisplay
j=1K/summationdisplay
k=1/summationdisplay
i∈Ck(xij−µj−µjk)2
s2
j+λ/radicalbig
Nkp/summationdisplay
j=1K/summationdisplay
k=1|µjk|
sj.

(18.55)
Show that the solution is equivalent to the nearest shrunken centroid es-
timator (18.7), with s0set to zero, and m2
kequal to 1/Nkinstead of
1/Nk−1/Nas before.
Ex. 18.3 Show that the ﬁtted coeﬃcients for the regularized multicla ss
logistic regression problem (18.10) satisfy/summationtextK
k=1ˆβkj= 0, j= 1,...,p.
What about the ˆβk0? Discuss issues with these constant parameters, and
how they can be resolved.
Ex. 18.4 Derive the computational formula (18.15) for ridge regress ion.
[Hint: Use the ﬁrst derivative of the penalized sum-of-squares cr iterion to
show that if λ>0, thenˆβ=XTsfor somes∈IRN.]
Ex. 18.5 Prove the theorem (18.16)–(18.17) in Section 18.3.5, by dec om-
posingβand the rows of Xinto their projections into the column space of
Vand its complement in IRp.
Ex. 18.6 Show how the theorem in Section 18.3.5 can be applied to regu-
larized discriminant analysis [Equations 4.14 and (18.9)] .

Exercises 695
Ex. 18.7 Consider a linear regression problem where p≫N, and assume
the rank of XisN. Let the SVD of X=UDVT=RVT, whereRis
N×Nnonsingular, and Visp×Nwith orthonormal columns.
(a) Show that there are inﬁnitely many least-squares soluti ons all with
zero residuals.
(b) Show that the ridge-regression estimate for βcan be written
ˆβλ=V(RTR+λI)−1RTy (18.56)
(c) Show that when λ= 0, the solution ˆβ0=VD−1UTyhas residuals
all equal to zero, and is unique in that it has the smallest Euc lidean
norm amongst all zero-residual solutions.
Ex. 18.8 Data Piling . Exercise 4.2 shows that the two-class LDA solution
can be obtained by a linear regression of a binary response ve ctorycon-
sisting of−1s and +1s. The prediction ˆβTxfor anyxis (up to a scale and
shift) the LDA score δ(x). Suppose now that p≫N.
(a) Consider the linear regression model f(x) =α+βTxﬁt to a binary
responseY∈ {−1,+1}. Using Exercise 18.7, show that there are
inﬁnitely many directions deﬁned by ˆβin IRponto which the data
project to exactly two points, one for each class. These are known as
data piling directions (Ahn and Marron, 2005).
(b) Show that the distance between the projected points is 2 /||ˆβ||, and
hence these directions deﬁne separating hyperplanes with t hat mar-
gin.
(c) Argue that there is a single maximal data piling direction for which
this distance is largest, and is deﬁned by ˆβ0=VD−1UTy=X−y,
whereX=UDVTis the SVD of X.
Ex. 18.9 Compare the data piling direction of Exercise 18.8 to the dir ection
of the optimal separating hyperplane (Section 4.5.2) quali tatively. Which
makes the widest margin, and why? Use a small simulation to de monstrate
the diﬀerence.
Ex. 18.10 Whenp≫N, linear discriminant analysis (see Section 4.3) is
degenerate because the within-class covariance matrix Wis singular. One
version of regularized discriminant analysis (4.14) repla cesWby a ridged
versionW+λI, leading to a regularized discriminant function δλ(x) =
xT(W+λI)−1(¯x1−¯x−1). Show that δ0(x) = lim λ↓0δλ(x) corresponds to
the maximal data piling direction deﬁned in Exercise 18.8.
Ex. 18.11 Suppose you have a sample of Npairs (xi,yi), withyibinary
andxi∈IR1. Suppose also that the two classes are separable; e.g., for e ach

696 18. High-Dimensional Problems: p≫N
pairi,i′withyi= 0 andyi′= 1,xi′−xi≥Cfor someC >0. You wish
to ﬁt a linear logistic regression model logitPr( Y= 1|X) =α+βXby
maximum-likelihood. Show that ˆβis undeﬁned.
Ex. 18.12 Suppose we wish to select the ridge parameter λby 10-fold cross-
validation in a p≫Nsituation (for any linear model). We wish to use the
computational shortcuts described in Section 18.3.5. Show that we need
only to reduce the N×pmatrixXto theN×NmatrixRonce, and can
use it in all the cross-validation runs.
Ex. 18.13 Suppose our p>Npredictors are presented as an N×Ninner-
product matrix K=XXT, and we wish to ﬁt the equivalent of a linear
logistic regression model in the original features with qua dratic regulariza-
tion. Our predictions are also to be made using inner product s; a newx0
is presented as k0=Xx0. LetK=UD2UTbe the eigen-decomposition of
K. Show that the predictions are given by ˆf0=kT
0ˆα, where
(a) ˆα=UD−1ˆβ, and
(b)ˆβis the ridged logistic regression estimate with input matri xR=
UD.
Argue that the same approach can be used for any appropriate k ernel
matrixK.
Ex. 18.14 Distance weighted 1-NN classiﬁcation . Consider the 1-nearest-
neighbor method (Section 13.3) in a two-class classiﬁcatio n problem. Let
d+(x0) be the shortest distance to a training observation in class +1, and
likewised−(x0) the shortest distance for class −1. LetN−be the number
of samples in class −1,N+the number in class +1, and N=N−+N+.
(a) Show that
δ(x0) = logd−(x0)
d+(x0)(18.57)
can be viewed as a nonparametric discriminant function corr espond-
ing to 1-NN classiﬁcation. [ Hint: Show that ˆf+(x0) =1
N+d+(x0)can
be viewed as a nonparametric estimate of the density in class +1 at
x0].
(b) How would you modify this function to introduce class pri or probabil-
itiesπ+andπ−diﬀerent from the sample-priors N+/NandN−/N?
(c) How would you generalize this approach for K-NN classiﬁc ation?
Ex. 18.15 Kernel PCA. In Section 18.5.2 we show how to compute the
principal component variables Zfrom an uncentered inner-product matrix
K. We compute the eigen-decomposition ( I−M)K(I−M) =UD2UT,
withM=11T/N, and then Z=UD. Suppose we have the inner-product

Exercises 697
vectork0, containing the Ninner-products between a new point x0and
each of the xiin our training set. Show that the (centered) projections of
x0onto the principal-component directions are given by
z0=D−1UT(I−M)[k0−K1/N]. (18.58)
Ex. 18.16 Bonferroni method for multiple comparisons. Suppose we are in
a multiple-testing scenario with null hypotheses H0j,j= 1,2,...,M, and
corresponding p-valuespj,i= 1,2,...,M. LetAbe the event that at least
one null hypothesis is falsely rejected, and let Ajbe the event that the
jth null hypothesis is falsely rejected. Suppose that we use t he Bonferroni
method, rejecting the jth null hypothesis if pj<α/M.
(a) Show that Pr( A)≤α. [Hint: Pr(Aj∪Aj′) = Pr(Aj) + Pr(Aj′)−
Pr(Aj∩Aj′)]
(b) If the hypotheses H0j,j= 1,2,...,M, are independent, then Pr( A) =
1−Pr(AC) = 1−/producttextM
j=1Pr(AC
j) = 1−(1−α/M)M. Use this to show
that Pr(A)≈αin this case.
Ex. 18.17 Equivalence between Benjamini–Hochberg and plug-in metho ds.
(a) In the notation of Algorithm 18.2, show that for rejectio n threshold
p0=p(L), a proportion of at most p0of the permuted values tk
j
exceed|T|(L)where|T|(L)is theLth largest value among the |tj|.
Hence show that the plug-in FDR estimate /hatwideFDR is less than or equal
top0·M/L=α.
(b) Show that the cut-point |T|(L+1)produces a test with estimated FDR
greater than α.
Ex. 18.18 Use result (18.53) to show that
pFDR =π0·{Type I error of Γ}
π0·{Type I error of Γ}+π1{Power of Γ}(18.59)
(Storey, 2003).
Ex. 18.19 Consider the data in Table 18.4 of Section (18.7), available from
the book website.
(a) Using a symmetric two-sided rejection region based on th et-statistic,
compute the plug-in estimate of the FDR for various values of the
cut-point.
(b) Carry out the BH procedure for various FDR levels αand show the
equivalence of your results, with those from part (a).

698 18. High-Dimensional Problems: p≫N
(c) Let (q.25,q.75) be the quartiles of the t-statistics from the permuted
datasets.Let ˆ π0={#tj∈(q.25,q.75)}/(.5M),andset ˆπ0= min(ˆπ0,1).
Multiply the FDR estimates from (a) by ˆ π0and examine the results.
(d) Give a motivation for the estimate in part (c).
(Storey, 2003)
Ex. 18.20 Proof of result (18.53). Write
pFDR = E/parenleftigg
V
R|R>0/parenrightigg
(18.60)
=M/summationdisplay
k=1E/bracketleftigg
V
R|R=k/bracketrightigg
Pr(R=k|R>0) (18.61)
Use the fact that given R=k,Vis a binomial random variable, with k
trials and probability of success Pr( H= 0|T∈Γ), to complete the proof.

This is page 699
Printer: Opaque this
References
Abu-Mostafa, Y. (1995). Hints, Neural Computation 7: 639–671.
Ackley, D. H., Hinton, G. and Sejnowski, T. (1985). A learnin g algorithm
for Boltzmann machines, Trends in Cognitive Sciences 9: 147–169.
Adam, B.-L., Qu, Y., Davis, J. W., Ward, M. D., Clements, M. A. ,
Cazares, L. H., Semmes, O. J., Schellhammer, P. F., Yasui, Y. ,
Feng, Z. and Wright, G. (2003). Serum protein ﬁngerprinting cou-
pled with a pattern-matching algorithm distinguishes pros tate cancer
from benign prostate hyperplasia and healthy mean, Cancer Research
63(10): 3609–3614.
Agrawal, R., Mannila, H., Srikant, R., Toivonen, H. and Verk amo, A. I.
(1995). Fast discovery of association rules, Advances in Knowledge
Discovery and Data Mining , AAAI/MIT Press, Cambridge, MA.
Agresti, A. (1996). An Introduction to Categorical Data Analysis , Wiley,
New York.
Agresti, A. (2002). Categorical Data Analysis (2nd Ed.) , Wiley, New York.
Ahn,J.andMarron,J.(2005). Thedirectionofmaximaldatap ilinginhigh
dimensional space, Technical report , Statistics Department, University
of North Carolina, Chapel Hill.
Akaike, H. (1973). Information theory and an extension of th e maximum
likelihood principle, Second International Symposium on Information
Theory, pp. 267–281.

700 References
Allen, D. (1974). The relationship between variable select ion and data
augmentation and a method of prediction, Technometrics 16: 125–7.
Ambroise, C. and McLachlan, G. (2002). Selection bias in gen e extraction
on the basis of microarray gene-expression data, Proceedings of the
National Academy of Sciences 99: 6562–6566.
Amit, Y. and Geman, D. (1997). Shape quantization and recogn ition with
randomized trees, Neural Computation 9: 1545–1588.
Anderson, J. and Rosenfeld, E. (eds) (1988). Neurocomputing: Foundations
of Research , MIT Press, Cambridge, MA.
Anderson, T. (2003). An Introduction to Multivariate Statistical Analysis,
3rd ed., Wiley, New York.
Bach, F. and Jordan, M. (2002). Kernel independent componen t analysis,
Journal of Machine Learning Research 3: 1–48.
Bair, E. and Tibshirani, R. (2004). Semi-supervised method s to predict
patient survival from gene expression data, PLOS Biology 2: 511–522.
Bair, E., Hastie, T., Paul, D. and Tibshirani, R. (2006). Pre diction by
supervised principal components, Journal of the American Statistical
Association 101: 119–137.
Bakin, S. (1999). Adaptive regression and model selection i n data mining
problems, Technical report , PhD. thesis, Australian National Univer-
sity, Canberra.
Banerjee, O., Ghaoui, L. E. and d’Aspremont, A. (2008). Mode l selection
through sparse maximum likelihood estimation for multivar iate gaus-
sianorbinarydata, Journal of Machine Learning Research 9:485–516.
Barron, A. (1993). Universal approximation bounds for supe rpositions of a
sigmoid function, IEEE Transactions on Information Theory 39: 930–
945.
Bartlett, P. and Traskin, M. (2007). Adaboost is consistent ,in
B.Sch¨ olkopf,J.PlattandT.Hoﬀman(eds), Advances in Neural Infor-
mation Processing Systems 19 , MIT Press, Cambridge, MA, pp. 105–
112.
Becker, R., Cleveland, W. and Shyu, M. (1996). The visual des ign and con-
trol of trellis display, Journal of Computational and Graphical Statis-
tics5: 123–155.
Bell, A. and Sejnowski, T. (1995). An information-maximiza tion approach
to blind separation and blind deconvolution, Neural Computation
7: 1129–1159.

References 701
Bellman, R. E. (1961). Adaptive Control Processes , Princeton University
Press.
Benjamini, Y. and Hochberg, Y. (1995). Controlling the fals e discovery
rate: a practical and powerful approach to multiple testing ,Journal of
the Royal Statistical Society Series B. 85: 289–300.
Benjamini, Y. and Yekutieli, Y. (2005). False discovery rat e controlling
conﬁdence intervals for selected parameters, Journal of the American
Statistical Association 100: 71–80.
Bickel, P. and Levina, E. (2004). Some theory for Fisher’s li near discrim-
inant function,“Naive Bayes”, and some alternatives when t here are
many more variables than observations, Bernoulli 10: 989–1010.
Bickel, P. J., Ritov, Y. and Tsybakov, A. (2008). Simultaneo us analysis of
lasso and Dantzig selector, Annals of Statistics . to appear.
Bishop, C. (1995). Neural Networks for Pattern Recognition , Clarendon
Press, Oxford.
Bishop, C. (2006). Pattern Recognition and Machine Learning , Springer,
New York.
Bishop, Y., Fienberg, S. and Holland, P. (1975). Discrete Multivariate
Analysis, MIT Press, Cambridge, MA.
Boyd, S. and Vandenberghe, L. (2004). Convex Optimization , Cambridge
University Press.
Breiman, L. (1992). The little bootstrap and other methods f or dimension-
ality selection in regression: X-ﬁxed prediction error, Journal of the
American Statistical Association 87: 738–754.
Breiman, L. (1996a). Bagging predictors, Machine Learning 26: 123–140.
Breiman, L. (1996b). Stacked regressions, Machine Learning 24: 51–64.
Breiman,L.(1998). Arcingclassiﬁers(withdiscussion), Annals of Statistics
26: 801–849.
Breiman, L. (1999). Prediction games and arcing algorithms ,Neural Com-
putation 11(7): 1493–1517.
Breiman, L. (2001). Random forests, Machine Learning 45: 5–32.
Breiman, L. and Friedman, J. (1997). Predicting multivaria te responses
in multiple linear regression (with discussion), Journal of the Royal
Statistical Society Series B. 59: 3–37.

702 References
Breiman, L. and Ihaka, R. (1984). Nonlinear discriminant an alysis via
scaling and ACE, Technical report , University of California, Berkeley.
Breiman, L. and Spector, P. (1992). Submodel selection and e valuation
in regression: the X-random case, International Statistical Review
60: 291–319.
Breiman, L., Friedman, J., Olshen, R. and Stone, C. (1984). Classiﬁcation
and Regression Trees , Wadsworth, New York.
Bremaud, P. (1999). Markov Chains: Gibbs Fields, Monte Carlo Simula-
tion, and Queues , Springer, New York.
Brown, P., Spiegelman, C. and Denham, M. (1991). Chemometri cs and
spectral frequency selection, Transactions of the Royal Society of Lon-
don Series A. 337: 311–322.
Bruce, A. and Gao, H. (1996). Applied Wavelet Analysis with S-PLUS ,
Springer, New York.
B¨ uhlmann, P. and Hothorn, T. (2007). Boosting algorithms: regulariza-
tion, prediction and model ﬁtting (with discussion), Statistical Science
22(4): 477–505.
Buja, A., Hastie, T. and Tibshirani, R. (1989). Linear smoot hers and
additive models (with discussion), Annals of Statistics 17: 453–555.
Buja,A.,Swayne,D.,Littman,M.,Hofmann,H.andChen,L.(2 008). Data
vizualization with multidimensional scaling, Journal of Computational
and Graphical Statistics . to appear.
Bunea, F., Tsybakov, A. and Wegkamp, M. (2007). Sparsity ora cle inequal-
ities for the lasso, Electronic Journal of Statistics 1: 169–194.
Burges, C. (1998). A tutorial on support vector machines for pattern recog-
nition,Knowledge Discovery and Data Mining 2(2): 121–167.
Butte, A., Tamayo, P., Slonim, D., Golub, T. and Kohane, I. (2 000).
Discovering functional relationships between RNA express ion and
chemotherapeutic susceptibility using relevance network s,Proceedings
of the National Academy of Sciences pp. 12182–12186.
Candes, E. (2006). Compressive sampling, Proceedings of the Interna-
tional Congress of Mathematicians , European Mathematical Society,
Madrid, Spain.
Candes, E. and Tao, T. (2007). The Dantzig selector: Statist ical estimation
when p is much larger than n, Annals of Statistics 35(6): 2313–2351.

References 703
Chambers, J. and Hastie, T. (1991). Statistical Models in S ,
Wadsworth/Brooks Cole, Paciﬁc Grove, CA.
Chaudhuri, S., Drton, M. and Richardson, T. S. (2007). Estim ation of a
covariance matrix with zeros, Biometrika 94(1): 1–18.
Chen, L. and Buja, A. (2008). Local multidimensional scalin g for nonlinear
dimension reduction, graph drawing and proximity analysis ,Journal
of the American Statistical Association .
Chen, S. S., Donoho, D. and Saunders, M. (1998). Atomic decom position
by basis pursuit, SIAM Journal on Scientiﬁc Computing 20(1): 33–61.
Cherkassky, V. and Ma, Y. (2003). Comparison of model select ion for
regression, Neural computation 15(7): 1691–1714.
Cherkassky, V. and Mulier, F. (2007). Learning from Data (2nd Edition) ,
Wiley, New York.
Chui, C. (1992). An Introduction to Wavelets , Academic Press, London.
Cliﬀord, P. (1990). Markov random ﬁelds in statistics, inG. R. Grimmett
and D. J. A. Welsh (eds), Disorder in Physical Systems. A Volume in
Honour of John M. Hammersley , Clarendon Press, Oxford, pp. 19–32.
Comon, P. (1994). Independent component analysis—a new conc ept?,Sig-
nal Processing 36: 287–314.
Cook, D. and Swayne, D. (2007). Interactive and Dynamic Graphics for
Data Analysis; with R and GGobi , Springer, New York. With con-
tributions from A. Buja, D. Temple Lang, H. Hofmann, H. Wickh am
and M. Lawrence.
Cook, N. (2007). Use and misuse of the receiver operating cha racteristic
curve in risk prediction, Circulation 116(6): 928–35.
Copas, J. B. (1983). Regression, prediction and shrinkage ( with discus-
sion),Journal of the Royal Statistical Society, Series B, Methodo logical
45: 311–354.
Cover, T. and Hart, P. (1967). Nearest neighbor pattern clas siﬁcation,
IEEE Transactions on Information Theory IT-11: 21–27.
Cover, T. and Thomas, J. (1991). Elements of Information Theory , Wiley,
New York.
Cox, D. and Hinkley, D. (1974). Theoretical Statistics , Chapman and Hall,
London.

704 References
Cox, D. and Wermuth, N. (1996). Multivariate Dependencies: Models,
Analysis and Interpretation , Chapman and Hall, London.
Cressie, N. (1993). Statistics for Spatial Data (Revised Edition) , Wiley-
Interscience, New York.
Csiszar, I. and Tusn´ ady, G. (1984). Information geometry a nd alternat-
ing minimization procedures, Statistics & Decisions Supplement Issue
1: 205–237.
Cutler, A. and Breiman, L. (1994). Archetypal analysis, Technometrics
36(4): 338–347.
Dasarathy, B. (1991). Nearest Neighbor Pattern Classiﬁcation Techniques ,
IEEE Computer Society Press, Los Alamitos, CA.
Daubechies, I. (1992). Ten Lectures on Wavelets , Society for Industrial and
Applied Mathematics, Philadelphia, PA.
Daubechies, I., Defrise, M. and De Mol, C. (2004). An iterati ve threshold-
ing algorithm for linear inverse problems with a sparsity co nstraint,
Communications on Pure and Applied Mathematics 57: 1413–1457.
de Boor, C. (1978). A Practical Guide to Splines , Springer, New York.
Dempster, A. (1972). Covariance selection, Biometrics 28: 157–175.
Dempster, A., Laird, N. and Rubin, D. (1977). Maximum likeli hood from
incomplete data via the EM algorithm (with discussion), Journal of
the Royal Statistical Society Series B 39: 1–38.
Devijver, P. and Kittler, J. (1982). Pattern Recognition: A Statistical Ap-
proach, Prentice-Hall, Englewood Cliﬀs, N.J.
Dietterich, T. (2000a). Ensemble methods in machine learni ng,Lecture
Notes in Computer Science 1857: 1–15.
Dietterich, T. (2000b). An experimental comparison of thre e methods for
constructing ensembles of decision trees: bagging, boosti ng, and ran-
domization, Machine Learning 40(2): 139–157.
Dietterich, T. and Bakiri, G. (1995). Solving multiclass le arning problems
via error-correcting output codes, Journal of Artiﬁcial Intelligence Re-
search2: 263–286.
Donath, W. E. and Hoﬀman, A. J. (1973). Lower bounds for the pa rtition-
ing of graphs, IBM Journal of Research and Development pp. 420–425.
Donoho, D. (2006a). Compressed sensing, IEEE Transactions on Informa-
tion Theory 52(4): 1289–1306.

References 705
Donoho, D.(2006b). Formostlarge underdeterminedsystems ofequations,
theminimal ℓ1-normsolutionisthesparsestsolution, Communications
on Pure and Applied Mathematics 59: 797–829.
Donoho, D. and Elad, M. (2003). Optimally sparse representa tion from
overcomplete dictionaries via ℓ1-norm minimization, Proceedings of
the National Academy of Sciences 100: 2197–2202.
Donoho, D. and Johnstone, I. (1994). Ideal spatial adaptati on by wavelet
shrinkage, Biometrika 81: 425–455.
Donoho, D. and Stodden, V. (2004). When does non-negative ma trix
factorization give a correct decomposition into parts?, inS. Thrun,
L. Saul and B. Sch¨ olkopf (eds), Advances in Neural Information Pro-
cessing Systems 16 , MIT Press, Cambridge, MA.
Duan, N. and Li, K.-C. (1991). Slicing regression: a link-fr ee regression
method, Annals of Statistics 19: 505–530.
Duchamp, T. and Stuetzle, W. (1996). Extremal properties of principal
curves in the plane, Annals of Statistics 24: 1511–1520.
Duda, R., Hart, P. and Stork, D. (2000). Pattern Classiﬁcation (2nd Edi-
tion), Wiley, New York.
Dudoit, S., Fridlyand, J. and Speed, T. (2002a). Comparison of discrimi-
nation methods for the classiﬁcation of tumors using gene ex pression
data,Journal of the American Statistical Association 97(457): 77–87.
Dudoit, S., Yang, Y., Callow, M. and Speed, T. (2002b). Stati stical meth-
ods for identifying diﬀerentially expressed genes in repli cated cDNA
microarray experiments, Statistica Sinica pp. 111–139.
Edwards, D. (2000). Introduction to Graphical Modelling, 2nd Edition ,
Springer, New York.
Efron, B. (1975). The eﬃciency of logistic regression compa red to normal
discriminant analysis, Journal of the American Statistical Association
70: 892–898.
Efron, B. (1979). Bootstrap methods: another look at the jac kknife,Annals
of Statistics 7: 1–26.
Efron, B. (1983). Estimating the error rate of a prediction r ule: some
improvements on cross-validation, Journal of the American Statistical
Association 78: 316–331.
Efron, B. (1986). How biased is the apparent error rate of a pr ediction
rule?,Journal of the American Statistical Association 81: 461–70.

706 References
Efron, B. and Tibshirani, R. (1991). Statistical analysis i n the computer
age,Science253: 390–395.
Efron, B. and Tibshirani, R. (1993). An Introduction to the Bootstrap ,
Chapman and Hall, London.
Efron, B. and Tibshirani, R. (1996). Using specially design ed exponential
families for density estimation, Annals of Statistics 24(6): 2431–2461.
Efron, B. and Tibshirani, R. (1997). Improvements on cross- validation: the
632+ bootstrap: method, Journal of the American Statistical Associ-
ation92: 548–560.
Efron,B.andTibshirani,R.(2002). Microarrays,empirica lBayesmethods,
and false discovery rates, Genetic Epidemiology 1: 70–86.
Efron, B., Hastie, T. and Tibshirani, R. (2007). Discussion of “Dantzig
selector” by Candes and Tao, Annals of Statistics 35(6): 2358–2364.
Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (200 4). Least angle
regression (with discussion), Annals of Statistics 32(2): 407–499.
Efron, B., Tibshirani, R., Storey, J. and Tusher, V. (2001). Empirical
Bayes analysis of a microarray experiment, Journal of the American
Statistical Association 96: 1151–1160.
Evgeniou, T., Pontil, M. and Poggio, T. (2000). Regularizat ion networks
and support vector machines, Advances in Computational Mathemat-
ics13(1): 1–50.
Fan, J. and Fan, Y. (2008). High dimensional classiﬁcation u sing features
annealed independence rules, Annals of Statistics . to appear.
Fan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Appli-
cations, Chapman and Hall, London.
Fan, J. and Li, R. (2005). Variable selection via nonconcave penalized
likelihoodanditsoracleproperties, Journal of the American Statistical
Association 96: 1348–1360.
Fiedler, M. (1973). Algebraic connectivity of graphs, Czechoslovak Mathe-
matics Journal 23(98): 298–305.
Fienberg, S. (1977). The Analysis of Cross-Classiﬁed Categorical Data ,
MIT Press, Cambridge.
Fisher, R. A. (1936). The use of multiple measurements in tax onomic
problems, Eugen.7: 179–188.

References 707
Fisher, W. (1958). On grouping for maximum homogeniety, Journal of the
American Statistical Association 53(284): 789–798.
Fix, E. and Hodges, J. (1951). Discriminatory analysis—nonp arametric
discrimination: Consistency properties, Technical Report 21-49-004,4 ,
U.S. Air Force, School of Aviation Medicine, Randolph Field , TX.
Flury, B. (1990). Principal points, Biometrika 77: 33–41.
Forgy, E. (1965). Cluster analysis of multivariate data: eﬃ ciency vs. inter-
pretability of classiﬁcations, Biometrics 21: 768–769.
Frank, I. and Friedman, J. (1993). A statistical view of some chemometrics
regression tools (with discussion), Technometrics 35(2): 109–148.
Freund, Y. (1995). Boosting a weak learning algorithm by maj ority,Infor-
mation and Computation 121(2): 256–285.
Freund, Y. and Schapire, R. (1996a). Experiments with a new b oosting
algorithm, Machine Learning: Proceedings of the Thirteenth Interna-
tional Conference , Morgan Kauﬀman, San Francisco, pp. 148–156.
Freund, Y. and Schapire, R. (1996b). Game theory, on-line pr ediction and
boosting, Proceedings of the Ninth Annual Conference on Computa-
tional Learning Theory , Desenzano del Garda, Italy, pp. 325–332.
Freund, Y. and Schapire, R. (1997). A decision-theoretic ge neralization of
online learning and an application to boosting, Journal of Computer
and System Sciences 55: 119–139.
Friedman, J. (1987). Exploratory projection pursuit, Journal of the Amer-
ican Statistical Association 82: 249–266.
Friedman, J. (1989). Regularized discriminant analysis, Journal of the
American Statistical Association 84: 165–175.
Friedman, J. (1991). Multivariate adaptive regression spl ines (with discus-
sion),Annals of Statistics 19(1): 1–141.
Friedman, J. (1994a). Flexible metric nearest-neighbor cl assiﬁcation, Tech-
nical report , Stanford University.
Friedman, J. (1994b). An overview of predictive learning an d function
approximation, inV. Cherkassky, J. Friedman and H. Wechsler (eds),
From Statistics to Neural Networks , Vol. 136 of NATO ISI Series F ,
Springer, New York.
Friedman, J. (1996). Another approach to polychotomous cla ssiﬁcation,
Technical report , Stanford University.

708 References
Friedman, J. (1997). On bias, variance, 0-1 loss and the curs e of dimen-
sionality, Journal of Data Mining and Knowledge Discovery 1: 55–77.
Friedman, J. (1999). Stochastic gradient boosting, Technical report , Stan-
ford University.
Friedman, J. (2001). Greedy function approximation: A grad ient boosting
machine, Annals of Statistics 29(5): 1189–1232.
Friedman, J. and Fisher, N. (1999). Bump hunting in high dime nsional
data,Statistics and Computing 9: 123–143.
Friedman, J. and Hall, P. (2007). On bagging and nonlinear es timation,
Journal of Statistical Planning and Inference 137: 669–683.
Friedman, J. and Popescu, B. (2003). Importance sampled lea rning ensem-
bles,Technical report , Stanford University, Department of Statistics.
Friedman, J. and Popescu, B. (2008). Predictive learning vi a rule ensem-
bles,Annals of Applied Statistics, to appear .
Friedman, J. and Silverman, B. (1989). Flexible parsimonio us smoothing
and additive modelling (with discussion), Technometrics 31: 3–39.
Friedman, J. and Stuetzle, W. (1981). Projection pursuit re gression, Jour-
nal of the American Statistical Association 76: 817–823.
Friedman, J. and Tukey, J. (1974). A projection pursuit algo rithm for
exploratory data analysis, IEEE Transactions on Computers, Series
C23: 881–889.
Friedman, J., Baskett, F. and Shustek, L. (1975). An algorit hm for ﬁnding
nearest neighbors, IEEE Transactions on Computers 24: 1000–1006.
Friedman, J., Bentley, J. and Finkel, R. (1977). An algorthm for ﬁnd-
ing best matches in logarithmic expected time, ACM Transactions on
Mathematical Software 3: 209–226.
Friedman, J., Hastie, T. and Tibshirani, R. (2000). Additiv e logistic re-
gression: a statistical view of boosting (with discussion) ,Annals of
Statistics 28: 337–307.
Friedman, J., Hastie, T. and Tibshirani, R. (2008a). Respon se to “Mease
and Wyner: Evidence contrary to the statistical view of boos ting”,
Journal of Machine Learning Research 9: 175–180.
Friedman, J., Hastie, T. and Tibshirani, R. (2008b). Sparse inverse covari-
ance estimation with the graphical lasso, Biostatistics 9: 432–441.

References 709
Friedman,J.,Hastie,T.andTibshirani,R.(2010). Regular izationpathsfor
generalized linear models via coordinate descent, Journal of Statistical
Software 33(1): 1–22.
Friedman, J., Hastie, T., Hoeﬂing, H. and Tibshirani, R. (20 07). Pathwise
coordinate optimization, Annals of Applied Statistics 2(1): 302–332.
Friedman, J., Hastie, T., Rosset, S., Tibshirani, R. and Zhu , J. (2004).
Discussion of three boosting papers by Jiang, Lugosi and Vay atis, and
Zhang,Annals of Statistics 32: 102–107.
Friedman, J., Stuetzle, W. and Schroeder, A. (1984). Projec tion pursuit
density estimation, Journal of the American Statistical Association
79: 599–608.
Fu, W. (1998). Penalized regressions: the bridge vs. the las so,Journal of
Computational and Graphical Statistics 7(3): 397–416.
Furnival, G. and Wilson, R. (1974). Regression by leaps and b ounds,Tech-
nometrics 16: 499–511.
Gelfand, A. and Smith, A. (1990). Sampling based approaches to calculat-
ing marginal densities, Journal of the American Statistical Association
85: 398–409.
Gelman, A., Carlin, J., Stern, H. and Rubin, D. (1995). Bayesian Data
Analysis, CRC Press, Boca Raton, FL.
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibb s distribu-
tions and the Bayesian restoration of images, IEEE Transactions on
Pattern Analysis and Machine Intelligence 6: 721–741.
Genkin, A., Lewis, D. and Madigan, D. (2007). Large-scale Ba yesian logis-
tic regression for text categorization, Technometrics 49(3): 291–304.
Genovese, C. and Wasserman, L. (2004). A stochastic process approach to
false discovery rates, Annals of Statistics 32(3): 1035–1061.
Gersho, A. and Gray, R. (1992). Vector Quantization and Signal Compres-
sion, Kluwer Academic Publishers, Boston, MA.
Girosi, F., Jones, M. and Poggio, T. (1995). Regularization theory and
neural network architectures, Neural Computation 7: 219–269.
Golub, G. and Van Loan, C. (1983). Matrix Computations , Johns Hopkins
University Press, Baltimore.
Golub, G., Heath, M. and Wahba, G. (1979). Generalized cross -validation
as a method for choosing a good ridge parameter, Technometrics
21: 215–224.

710 References
Golub, T., Slonim, D., Tamayo, P., Huard, C., Gaasenbeek, M. , Mesirov,
J., Coller, H., Loh, M., Downing, J., Caligiuri, M., Bloomﬁe ld, C. and
Lander, E. (1999). Molecular classiﬁcation of cancer: Clas s discovery
and class prediction by gene expression monitoring, Science286: 531–
536.
Goodall, C. (1991). Procrustes methods in the statistical a nalysis of shape,
Journal of the Royal Statistical Society, Series B 53: 285–321.
Gordon, A. (1999). Classiﬁcation (2nd edition) , Chapman and Hall/CRC
Press, London.
Green, P. and Silverman, B. (1994). Nonparametric Regression and Gener-
alized Linear Models: A Roughness Penalty Approach , Chapman and
Hall, London.
Greenacre, M. (1984). Theory and Applications of Correspondence Analy-
sis, Academic Press, New York.
Greenshtein, E. and Ritov, Y. (2004). Persistence in high-d imensional lin-
earpredictorselectionandthevirtueofoverparametrizat ion,Bernoulli
10: 971–988.
Guo, Y., Hastie, T. and Tibshirani, R. (2006). Regularized l inear discrim-
inant analysis and its application in microarrays, Biostatistics 8: 86–
100.
Guyon, I., Gunn, S., Nikravesh, M. and Zadeh, L. (eds) (2006) .Feature
Extraction, Foundations and Applications , Springer, New York.
Guyon,I.,Weston,J.,Barnhill,S.andVapnik,V.(2002). Ge neselectionfor
cancer classiﬁcation using support vector machines, Machine Learning
46: 389–422.
Hall, P. (1992). The Bootstrap and Edgeworth Expansion , Springer, New
York.
Hammersley, J. M. and Cliﬀord, P. (1971). Markov ﬁeld on ﬁnit e graphs
and lattices, unpublished.
Hand, D. (1981). Discrimination and Classiﬁcation , Wiley, Chichester.
Hanley, J. and McNeil, B. (1982). The meaning and use of the ar ea under
a receiver operating characteristic (roc) curve, Radiology 143: 29–36.
Hart, P. (1968). The condensed nearest-neighbor rule, IEEE Transactions
on Information Theory 14: 515–516.
Hartigan, J. A. (1975). Clustering Algorithms , Wiley, New York.

References 711
Hartigan, J. A. and Wong, M. A. (1979). [(Algorithm AS 136] A k-means
clustering algorithm (AS R39: 81v30 p355-356), Applied Statistics
28: 100–108.
Hastie, T. (1984). Principal Curves and Surfaces , PhD thesis, Stanford
University.
Hastie, T. and Herman, A. (1990). An analysis of gestational age, neona-
tal size and neonatal death using nonparametric logistic re gression,
Journal of Clinical Epidemiology 43: 1179–90.
Hastie, T. and Simard, P. (1998). Models and metrics for hand written digit
recognition, Statistical Science 13: 54–65.
Hastie, T. and Stuetzle, W. (1989). Principal curves, Journal of the Amer-
ican Statistical Association 84(406): 502–516.
Hastie, T. and Tibshirani, R. (1987). Nonparametric logist ic and propor-
tional odds regression, Applied Statistics 36: 260–276.
Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models , Chap-
man and Hall, London.
Hastie, T. and Tibshirani, R. (1996a). Discriminant adapti ve nearest-
neighbor classiﬁcation, IEEE Pattern Recognition and Machine In-
telligence 18: 607–616.
Hastie, T. and Tibshirani, R. (1996b). Discriminant analys is by Gaussian
mixtures, Journal of the Royal Statistical Society Series B. 58: 155–
176.
Hastie, T. and Tibshirani, R. (1998). Classiﬁcation by pair wise coupling,
Annals of Statistics 26(2): 451–471.
Hastie, T. and Tibshirani, R. (2003). Independent componen ts analysis
through product density estimation, inS. T. S. Becker and K. Ober-
mayer (eds), Advances in Neural Information Processing Systems 15 ,
MIT Press, Cambridge, MA, pp. 649–656.
Hastie, T. and Tibshirani, R. (2004). Eﬃcient quadratic reg ularization for
expression arrays, Biostatistics 5(3): 329–340.
Hastie, T. and Zhu, J. (2006). Discussion of “Support vector machines
with applications” by Javier Moguerza and Alberto Munoz, Statistical
Science21(3): 352–357.
Hastie, T., Botha, J. and Schnitzler, C. (1989). Regression with an ordered
categorical response, Statistics in Medicine 43: 884–889.

712 References
Hastie, T., Buja, A. and Tibshirani, R. (1995). Penalized di scriminant
analysis, Annals of Statistics 23: 73–102.
Hastie, T., Kishon, E., Clark, M. and Fan, J. (1992). A model f or
signature veriﬁcation, Technical report , AT&T Bell Laboratories.
http://www-stat.stanford.edu/ ∼hastie/Papers/signature.pdf .
Hastie, T., Rosset, S., Tibshirani, R. and Zhu, J. (2004). Th e entire reg-
ularization path for the support vector machine, Journal of Machine
Learning Research 5: 1391–1415.
Hastie, T., Taylor, J., Tibshirani, R. and Walther, G. (2007 ). Forward
stagewise regression and the monotone lasso, Electronic Journal of
Statistics 1: 1–29.
Hastie, T., Tibshirani, R. and Buja, A. (1994). Flexible dis criminant analy-
sis by optimal scoring, Journal of the American Statistical Association
89: 1255–1270.
Hastie, T., Tibshirani, R. and Buja, A. (2000). Flexible dis criminant and
mixture models, inJ. Kay and M. Titterington (eds), Statistics and
Artiﬁcial Neural Networks , Oxford University Press.
Hastie, T., Tibshirani, R. and Friedman, J. (2003). A note on “Compari-
son of model selection for regression” by Cherkassky and Ma, Neural
computation 15(7): 1477–1480.
Hathaway, R. J. (1986). Another interpretation of the EM alg orithm for
mixture distributions, Statistics & Probability Letters 4: 53–56.
Hebb, D. (1949). The Organization of Behavior , Wiley, New York.
Hertz, J., Krogh, A. and Palmer, R. (1991). Introduction to the Theory of
Neural Computation , Addison Wesley, Redwood City, CA.
Hinton, G. (1989). Connectionist learning procedures, Artiﬁcial Intelli-
gence40: 185–234.
Hinton, G. (2002). Training products of experts by minimizi ng contrastive
divergence, Neural Computation 14: 1771–1800.
Hinton, G., Osindero, S. and Teh, Y.-W. (2006). A fast learni ng algorithm
for deep belief nets, Neural Computation 18: 1527–1554.
Ho,T.K.(1995). Randomdecisionforests, inM.KavavaughandP.Storms
(eds),Proc. Third International Conference on Document Analysis
and Recognition , Vol. 1, IEEE Computer Society Press, New York,
pp. 278–282.

References 713
Hoeﬂing, H. and Tibshirani, R. (2008). Estimation of sparse Markov net-
works using modiﬁed logistic regression and the lasso, subm itted.
Hoerl, A. E. and Kennard, R. (1970). Ridge regression: biase d estimation
for nonorthogonal problems, Technometrics 12: 55–67.
Hothorn, T. and B¨ uhlmann, P. (2006). Model-based boosting in high di-
mensions, Bioinformatics 22(22): 2828–2829.
Huber, P. (1964). Robust estimation of a location parameter ,Annals of
Mathematical Statistics 53: 73–101.
Huber, P. (1985). Projection pursuit, Annals of Statistics 13: 435–475.
Hunter, D. and Lange, K. (2004). A tutorial on MM algorithms, The
American Statistician 58(1): 30–37.
Hyv¨ arinen, A. and Oja, E. (2000). Independent component an alysis: algo-
rithms and applications, Neural Networks 13: 411–430.
Hyv¨ arinen, A., Karhunen, J. and Oja, E. (2001). Independent Component
Analysis, Wiley, New York.
Izenman, A. (1975). Reduced-rank regression for the multiv ariate linear
model,Journal of Multivariate Analysis 5: 248–264.
Jacobs, R., Jordan, M., Nowlan, S. and Hinton, G. (1991). Ada ptive mix-
tures of local experts, Neural computation 3: 79–87.
Jain, A. and Dubes, R. (1988). Algorithms for Clustering Data , Prentice-
Hall, Englewood Cliﬀs, N.J.
James, G. and Hastie, T. (1998). The error coding method and P ICTs,
Journal of Computational and Graphical Statistics 7(3): 377–387.
Jancey, R. (1966). Multidimensional group analysis, Australian Journal of
Botany14: 127–130.
Jensen, F. V., Lauritzen, S. and Olesen, K. G. (1990). Bayesi an updating
in recursive graphical models by local computation, Computational
Statistics Quarterly 4: 269–282.
Jiang, W. (2004). Process consistency for Adaboost, Annals of Statistics
32(1): 13–29.
Jirou´ sek, R. and Pˇ reuˇ cil, S. (1995). On the eﬀective impl ementation of the
iterative proportional ﬁtting procedure, Computational Statistics and
Data Analysis 19: 177–189.
Johnson, N. (2008). A study of the NIPS feature selection cha llenge, Sub-
mitted.

714 References
Joliﬀe, I. T., Trendaﬁlov, N. T. and Uddin, M. (2003). A modiﬁ ed principal
component technique based on the lasso, Journal of Computational
and Graphical Statistics 12: 531–547.
Jones,L.(1992). AsimplelemmaongreedyapproximationinH ilbertspace
and convergence rates for projection pursuit regression an d neural
network training, Annals of Statistics 20: 608–613.
Jordan, M. (2004). Graphical models, Statistical Science (Special Issue on
Bayesian Statistics) 19: 140–155.
Jordan, M. and Jacobs, R. (1994). Hierachical mixtures of ex perts and the
EM algorithm, Neural Computation 6: 181–214.
Kalbﬂeisch, J. and Prentice, R. (1980). The Statistical Analysis of Failure
Time Data , Wiley, New York.
Kaufman, L. and Rousseeuw, P. (1990). Finding Groups in Data: An In-
troduction to Cluster Analysis , Wiley, New York.
Kearns, M. and Vazirani, U. (1994). An Introduction to Computational
Learning Theory , MIT Press, Cambridge, MA.
Kittler, J., Hatef, M., Duin, R. and Matas, J. (1998). On comb ining classi-
ﬁers,IEEE Transaction on Pattern Analysis and Machine Intellige nce
20(3): 226–239.
Kleinberg,E.M.(1990). Stochasticdiscrimination, Annals of Mathematical
Artiﬁcial Intelligence 1: 207–239.
Kleinberg, E. M. (1996). An overtraining-resistant stocha stic modeling
method for pattern recognition, Annals of Statistics 24: 2319–2349.
Knight, K. and Fu, W. (2000). Asymptotics for lasso-type est imators,
Annals of Statistics 28(5): 1356–1378.
Koh, K., Kim, S.-J. and Boyd, S. (2007). An interior-point me thod
for large-scale L1-regularized logistic regression, Journal of Machine
Learning Research 8: 1519–1555.
Kohavi, R. (1995). A study of cross-validation and bootstra p for accu-
racy estimation and model selection, International Joint Conference
on Artiﬁcial Intelligence (IJCAI) , Morgan Kaufmann, pp. 1137–1143.
Kohonen, T. (1989). Self-Organization and Associative Memory (3rd edi-
tion), Springer, Berlin.
Kohonen, T. (1990). The self-organizing map, Proceedings of the IEEE
78: 1464–1479.

References 715
Kohonen, T., Kaski, S., Lagus, K., Saloj¨ arvi, J., Paatero, A. and Saarela,
A. (2000). Self-organization of a massive document collect ion,IEEE
Transactions on Neural Networks 11(3): 574–585. Special Issue on
Neural Networks for Data Mining and Knowledge Discovery.
Koller, D. and Friedman, N. (2007). Structured Probabilistic Models , Stan-
ford Bookstore Custom Publishing. (Unpublished Draft).
Kressel, U. (1999). Pairwise classiﬁcation and support vec tor machines,
inB. Sch¨ olkopf, C. Burges and A. Smola (eds), Advances in Ker-
nel Methods - Support Vector Learning , MIT Press, Cambridge, MA.,
pp. 255–268.
Lambert, D. (1992). Zero-inﬂated Poisson regression, with an application
to defects in manufacturing, Technometrics 34(1): 1–14.
Lange, K. (2004). Optimization , Springer, New York.
Lauritzen, S. (1996). Graphical Models , Oxford University Press.
Lauritzen, S. and Spiegelhalter, D. (1988). Local computat ions with proba-
bilitiesongraphicalstructuresandtheirapplicationtoe xpertsystems,
J. Royal Statistical Society B. 50: 157–224.
Lawson, C. and Hansen, R. (1974). Solving Least Squares Problems ,
Prentice-Hall, Englewood Cliﬀs, NJ.
Le Cun, Y. (1989). Generalization and network design strate gies,Techni-
cal Report CRG-TR-89-4 , Department of Computer Science, Univ. of
Toronto.
Le Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard,
W. and Jackel, L. (1990). Handwritten digit recognition wit h a back-
propogation network, inD. Touretzky (ed.), Advances in Neural In-
formation Processing Systems , Vol. 2, Morgan Kaufman, Denver, CO,
pp. 386–404.
Le Cun, Y., Bottou, L., Bengio, Y. and Haﬀner, P. (1998). Grad ient-based
learning applied to document recognition, Proceedings of the IEEE
86(11): 2278–2324.
Leathwick, J., Elith, J., Francis, M., Hastie, T. and Taylor , P. (2006). Vari-
ation in demersal ﬁsh species richness in the oceans surroun ding new
zealand: an analysis using boosted regression trees, Marine Ecology
Progress Series 77: 802–813.
Leathwick, J., Rowe, D., Richardson, J., Elith, J. and Hasti e, T. (2005).
Using multivariate adaptive regression splines to predict the distribu-
tions of New Zealand’s freshwater diadromous ﬁsh, Freshwater Biology
50: 2034–2051.

716 References
Leblanc, M. and Tibshirani, R. (1996). Combining estimates in regres-
sion and classiﬁcation, Journal of the American Statistical Association
91: 1641–1650.
LeCun, Y., Bottou, L., Bengio, Y. and Haﬀner, P. (1998). Grad ient-based
learning applied to document recognition, Proceedings of the IEEE
86(11): 2278–2324.
Lee,D.andSeung,H.(1999). Learningthepartsofobjectsby non-negative
matrix factorization, Nature401: 788.
Lee, D. and Seung, H. (2001). Algorithms for non-negative ma trix factor-
ization,Advances in Neural Information Processing Systems, (NIPS
2001), Vol. 13, Morgan Kaufman, Denver., pp. 556–562.
Lee, M.-L. (2004). Analysis of Microarray Gene Expression Data , Kluwer
Academic Publishers.
Lee, S.-I., Ganapathi, V. and Koller, D. (2007). Eﬃcient str ucture learning
of markov networks using l1-regularization, inB. Sch¨ olkopf, J. Platt
and T. Hoﬀman (eds), Advances in Neural Information Processing
Systems 19 , MIT Press, Cambridge, MA, pp. 817–824.
Leslie, C., Eskin, E., Cohen, A., Weston, J. and Noble, W. S. ( 2004). Mis-
match string kernels for discriminative protein classiﬁca tion,Bioinfor-
matics20(4): 467–476.
Levina, E. (2002). Statistical issues in texture analysis , PhD thesis, De-
partment. of Statistics, University of California, Berkel ey.
Lin, H., McCulloch, C., Turnbull, B., Slate, E. and Clark, L. (2000). A
latent class mixed model for analyzing biomarker trajector ies in lon-
gitudinal data with irregularly scheduled observations, Statistics in
Medicine 19: 1303–1318.
Lin, Y. and Zhang, H. (2006). Component selection and smooth ing in
smoothing spline analysis of variance models, Annals of Statistics
34: 2272–2297.
Little, R. and Rubin, D. (2002). Statistical Analysis with Missing Data
(2nd Edition) , Wiley, New York.
Lloyd,S.(1957). LeastsquaresquantizationinPCM., Technical report ,Bell
Laboratories. Published in 1982 in IEEE Transactions on Inf ormation
Theory28128-137.
Loader, C. (1999). Local Regression and Likelihood , Springer, New York.

References 717
Loh, W. and Vanichsetakul, N. (1988). Tree structured class iﬁcation via
generalized discriminant analysis, Journal of the American Statistical
Association 83: 715–728.
Lugosi, G. and Vayatis, N. (2004). On the bayes-risk consist ency of regu-
larized boosting methods, Annals of Statistics 32(1): 30–55.
Macnaughton Smith, P., Williams, W., Dale, M. and Mockett, L . (1965).
Dissimilarity analysis: a new technique of hierarchical su bdivision, Na-
ture202: 1034–1035.
MacKay, D. (1992). A practical Bayesian framework for backp ropagation
neural networks, Neural Computation 4: 448–472.
MacQueen, J. (1967). Some methods for classiﬁcation and ana lysis of mul-
tivariate observations, Proceedings of the Fifth Berkeley Symposium
on Mathematical Statistics and Probability, eds. L.M. LeCa m and J.
Neyman, University of California Press, pp. 281–297.
Madigan, D. and Raftery, A. (1994). Model selection and acco unting for
model uncertainty using Occam’s window, Journal of the American
Statistical Association 89: 1535–46.
Mardia,K.,Kent,J.andBibby,J.(1979). Multivariate Analysis ,Academic
Press.
Mason, L., Baxter, J., Bartlett, P. and Frean, M. (2000). Boo sting algo-
rithms as gradient descent, 12: 512–518.
Massart, D., Plastria, F. and Kaufman, L. (1983). Non-hiera rchical clus-
tering with MASLOC, The Journal of the Pattern Recognition Society
16: 507–516.
McCullagh, P. and Nelder, J. (1989). Generalized Linear Models , Chapman
and Hall, London.
McCulloch, W. and Pitts, W. (1943). A logical calculus of the ideas immi-
nent in nervous activity, Bulletin of Mathematical Biophysics 5: 115–
133. Reprinted in Anderson and Rosenfeld (1988), pp 96-104.
McLachlan, G. (1992). Discriminant Analysis and Statistical Pattern
Recognition , Wiley, New York.
Mease, D. and Wyner, A. (2008). Evidence contrary to the stat istical view
of boosting (with discussion), Journal of Machine Learning Research
9: 131–156.
Meinshausen, N. (2007). Relaxed lasso, Computational Statistics and Data
Analysis 52(1): 374–393.

718 References
Meinshausen, N. and B¨ uhlmann, P. (2006). High-dimensiona l graphs and
variable selection with the lasso, Annals of Statistics 34: 1436–1462.
Meir, R. and R¨ atsch, G. (2003). An introduction to boosting and leverag-
ing,inS. Mendelson and A. Smola (eds), Lecture notes in Computer
Science,AdvancedLecturesinMachineLearning,Springer,NewYork .
Michie, D., Spiegelhalter, D. and Taylor, C. (eds) (1994). Machine Learn-
ing, Neural and Statistical Classiﬁcation , Ellis Horwood Series in Ar-
tiﬁcial Intelligence, Ellis Horwood.
Morgan,J.N.andSonquist,J.A.(1963). Problemsintheanal ysisofsurvey
data, and a proposal, Journal of the American Statistical Association
58: 415–434.
Murray, W., Gill, P. and Wright, M. (1981). Practical Optimization , Aca-
demic Press.
Myles, J. and Hand, D. (1990). The multiclass metric problem in nearest
neighbor classiﬁcation, Pattern Recognition 23: 1291–1297.
Nadler, B. and Coifman, R. R. (2005). An exact asymptotic for mula for the
error in CLS and in PLS: The importance of dimensional reduct ion in
multivariate calibration, Journal of Chemometrics 102: 107–118.
Neal, R. (1996). Bayesian Learning for Neural Networks , Springer, New
York.
Neal, R. and Hinton, G. (1998). A view of the EM algorithm that justiﬁes
incremental, sparse, and other variants; in Learning in Gra phical Mod-
els, M. Jordan (ed.) , Dordrecht: Kluwer Academic Publishers, Boston,
MA., pp. 355–368.
Neal, R. and Zhang, J. (2006). High dimensional classiﬁcati on with
bayesian neural networks and dirichlet diﬀusion trees, inI. Guyon,
S. Gunn, M. Nikravesh and L. Zadeh (eds), Feature Extraction, Foun-
dations and Applications , Springer, New York, pp. 265–296.
Onton, J. and Makeig, S. (2006). Information-based modelin g of event-
related brain dynamics, inNeuper and Klimesch (eds), Progress in
Brain Research , Vol. 159, Elsevier, pp. 99–120.
Osborne, M., Presnell, B. and Turlach, B. (2000a). A new appr oach to
variableselectioninleastsquaresproblems, IMA Journal of Numerical
Analysis 20: 389–404.
Osborne, M., Presnell, B. and Turlach, B. (2000b). On the las so and its
dual,Journal of Computational and Graphical Statistics 9: 319–337.

References 719
Pace, R. K. and Barry, R. (1997). Sparse spatial autoregress ions,Statistics
and Probability Letters 33: 291–297.
Page, L., Brin, S., Motwani, R. and Winograd, T. (1998). The
pagerank citation ranking: bringing order to the web, Tech-
nical report , Stanford Digital Library Technologies Project.
http://citeseer.ist.psu.edu/page98pagerank.html .
Park,M.Y.andHastie,T.(2007). l1-regularizationpathalgorithmforgen-
eralized linear models, Journal of the Royal Statistical Society Series
B69: 659–677.
Parker,D.(1985). Learninglogic, Technical Report TR-87 ,CambridgeMA:
MIT Center for Research in Computational Economics and Mana ge-
ment Science.
Parmigiani, G., Garett, E. S., Irizarry, R. A. and Zeger, S. L . (eds) (2003).
The Analysis of Gene Expression Data , Springer, New York.
Paul, D., Bair, E., Hastie, T. and Tibshirani, R. (2008). “Pr e-conditioning”
for feature selection and regression in high-dimensional p roblems, An-
nals of Statistics 36(4): 1595–1618.
Pearl, J. (1986). On evidential reasoning in a hierarchy of h ypotheses,
Artiﬁcial Intelligence 28: 9–15.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: networks of
plausible inference , Morgan Kaufmann, San Francisco, CA.
Pearl, J. (2000). Causality: Models, Reasoning and Inference , Cambridge
University Press.
Peterson and Anderson, J. R. (1987). A mean ﬁeld theory learn ing algo-
rithm for neural networks, Complex Systems 1: 995–1019.
Petricoin, E. F., Ardekani, A. M., Hitt, B. A., Levine, P. J., Fusaro, V.,
Steinberg, S. M., Mills, G. B., Simone, C., Fishman, D. A., Ko hn,
E. and Liotta, L. A. (2002). Use of proteomic patterns in seru m to
identify ovarian cancer, Lancet359: 572–577.
Platt, J. (1999). Fast Training of Support Vector Machines using Sequen-
tial Minimal Optimization; in Advances in Kernel Methods—S upport
Vector Learning, B. Sch¨ olkopf and C. J. C. Burges and A. J. Sm ola
(eds), MIT Press, Cambridge, MA., pp. 185–208.
Quinlan, R. (1993). C4.5: Programs for Machine Learning , Morgan Kauf-
mann, San Mateo.
Quinlan, R. (2004). C5.0, www.rulequest.com .

720 References
Ramaswamy, S., Tamayo, P., Rifkin, R., Mukherjee, S., Yeang , C., Angelo,
M.,Ladd,C.,Reich,M.,Latulippe,E.,Mesirov,J.,Poggio, T.,Gerald,
W., Loda, M., Lander, E. and Golub, T. (2001). Multiclass can cer
diagnosis using tumor gene expression signature, PNAS98: 15149–
15154.
Ramsay, J. and Silverman, B. (1997). Functional Data Analysis , Springer,
New York.
Rao, C. R. (1973). Linear Statistical Inference and Its Applications , Wiley,
New York.
R¨ atsch, G. and Warmuth, M. (2002). Maximizing the margin wi th boost-
ing,Proceedings of the 15th Annual Conference on Computational
Learning Theory , pp. 334–350.
Ravikumar, P., Liu, H., Laﬀerty, J. and Wasserman, L. (2008) . Spam:
Sparse additive models, inJ. Platt, D. Koller, Y. Singer and S. Roweis
(eds),Advances in Neural Information Processing Systems 20 , MIT
Press, Cambridge, MA, pp. 1201–1208.
Ridgeway, G. (1999). The state of boosting, Computing Science and Statis-
tics31: 172–181.
Rieger, K., Hong, W., Tusher, V., Tang, J., Tibshirani, R. an d Chu, G.
(2004). Toxicity from radiation therapy associated with ab normal
transcriptional responses to DNA damage, Proceedings of the National
Academy of Sciences 101: 6634–6640.
Ripley,B.D.(1996). Pattern Recognition and Neural Networks ,Cambridge
University Press.
Rissanen, J. (1983). A universal prior for integers and esti mation by mini-
mum description length, Annals of Statistics 11: 416–431.
Robbins, H. and Munro, S. (1951). A stochastic approximatio n method,
Annals of Mathematical Statistics 22: 400–407.
Roosen, C. and Hastie, T. (1994). Automatic smoothing splin e projection
pursuit,Journal of Computational and Graphical Statistics 3:235–248.
Rosenblatt, F. (1958). The perceptron: a probabilistic mod el for infor-
mation storage and organization in the brain, Psychological Review
65: 386–408.
Rosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the
Theory of Brain Mechanisms , Spartan, Washington, D.C.

References 721
Rosenwald, A., Wright, G., Chan, W. C., Connors, J. M., Campo , E.,
Fisher, R. I., Gascoyne, R. D., Muller-Hermelink, H. K., Sme land,
E. B. and Staudt, L. M. (2002). The use of molecular proﬁling t o
predict survival after chemotherapy for diﬀuse large b-cel l lymphoma,
The New England Journal of Medicine 346: 1937–1947.
Rosset, S. and Zhu, J. (2007). Piecewise linear regularized solution paths,
Annals of Statistics 35(3): 1012–1030.
Rosset,S.,Zhu,J.andHastie,T.(2004a). Boostingasaregu larizedpathto
a maximum margin classiﬁer, Journal of Machine Learning Research
5: 941–973.
Rosset, S., Zhu, J. and Hastie, T. (2004b). Margin maximizin g loss func-
tions,inS. Thrun, L. Saul and B. Sch¨ olkopf (eds), Advances in Neural
Information Processing Systems 16 , MIT Press, Cambridge, MA.
Rousseauw, J., du Plessis, J., Benade, A., Jordaan, P., Kotz e, J., Jooste, P.
and Ferreira, J. (1983). Coronary risk factor screening in t hree rural
communities, South African Medical Journal 64: 430–436.
Roweis, S. T. and Saul, L. K. (2000). Locally linear embeddin g,Science
290: 2323–2326.
Rumelhart, D., Hinton, G. and Williams, R. (1986). Learning internal rep-
resentations by error propagation, inD. Rumelhart and J. McClelland
(eds),Parallel Distributed Processing: Explorations in the Micr ostruc-
ture of Cognition , The MIT Press, Cambridge, MA., pp. 318–362.
Sachs, K., Perez, O., Pe’er, D., Lauﬀenburger, D. and Nolan, G. (2005).
Causalprotein-signalingnetworksderivedfrommultipara metersingle-
cell data, Science308: 523–529.
Schapire, R. (1990). The strength of weak learnability, Machine Learning
5(2): 197–227.
Schapire, R. (2002). The boosting approach to machine learn ing: an
overview, inD. Denison, M. Hansen, C. Holmes, B. Mallick and B. Yu
(eds),MSRI workshop on Nonlinear Estimation and Classiﬁcation ,
Springer, New York.
Schapire, R. and Singer, Y. (1999). Improved boosting algor ithms using
conﬁdence-rated predictions, Machine Learning 37(3): 297–336.
Schapire, R., Freund, Y., Bartlett, P. and Lee, W. (1998). Bo osting the
margin: a new explanation for the eﬀectiveness of voting met hods,
Annals of Statistics 26(5): 1651–1686.

722 References
Sch¨ olkopf,B.,Smola,A.andM¨ uller,K.-R.(1999). Kernel principalcompo-
nentanalysis, inB.Sch¨ olkopf,C.BurgesandA.Smola(eds), Advances
in Kernel Methods—Support Vector Learning , MIT Press, Cambridge,
MA, USA, pp. 327–352.
Schwarz, G. (1978). Estimating the dimension of a model, Annals of Statis-
tics6(2): 461–464.
Scott, D. (1992). Multivariate Density Estimation: Theory, Practice, and
Visualization , Wiley, New York.
Seber, G. (1984). Multivariate Observations , Wiley, New York.
Segal, M. (2004). Machine learning benchmarks and random fo rest regres-
sion,Technical report , eScholarship Repository, University of Califor-
nia.http://repositories.edlib.org/cbmb/bench rfregn.
Shao, J. (1996). Bootstrap model selection, Journal of the American Sta-
tistical Association 91: 655–665.
Shenoy, P. and Shafer, G. (1988). An axiomatic framework for Bayesian
and belief-function propagation, AAAI Workshop on Uncertainty in
AI, North-Holland, pp. 307–314.
Short,R.andFukunaga,K.(1981). Theoptimaldistancemeas urefornear-
est neighbor classiﬁcation, IEEE Transactions on Information Theory
27: 622–627.
Silverman, B. (1986). Density Estimation for Statistics and Data Analysis ,
Chapman and Hall, London.
Silvey, S. (1975). Statistical Inference , Chapman and Hall, London.
Simard, P., Cun, Y. L. and Denker, J. (1993). Eﬃcient pattern recognition
using a new transformation distance, Advances in Neural Information
Processing Systems , Morgan Kaufman, San Mateo, CA, pp. 50–58.
Simon, R. M., Korn, E. L., McShane, L. M., Radmacher, M. D., Wr ight,
G. and Zhao, Y. (2004). Design and Analysis of DNA Microarray
Investigations , Springer, New York.
Sj¨ ostrand,K.,Rostrup,E.,Ryberg,C.,Larsen,R.,Studho lme,C.,Baezner,
H., Ferro, J., Fazekas, F., Pantoni, L., Inzitari, D. and Wal demar,
G. (2007). Sparse decomposition and modeling of anatomical shape
variation, IEEE Transactions on Medical Imaging 26(12): 1625–1635.
Speed, T. and Kiiveri, H. T. (1986). Gaussian Markov distrib utions over
ﬁnite graphs, Annals of Statistics 14: 138–150.

References 723
Speed, T. (ed.) (2003). Statistical Analysis of Gene Expression Microarray
Data, Chapman and Hall, London.
Spiegelhalter, D., Best, N., Gilks, W. and Inskip, H. (1996) . Hepatitis
B: a case study in MCMC methods, inW. Gilks, S. Richardson and
D. Spegelhalter (eds), Markov Chain Monte Carlo in Practice , Inter-
disciplinary Statistics, Chapman and Hall, London, pp. 21– 43.
Spielman, D. A. and Teng, S.-H. (1996). Spectral partitioni ng works: Pla-
nar graphs and ﬁnite element meshes, IEEE Symposium on Founda-
tions of Computer Science , pp. 96–105.
Stamey, T., Kabalin, J., McNeal, J., Johnstone, I., Freiha, F., Redwine, E.
and Yang, N. (1989). Prostate speciﬁc antigen in the diagnos is and
treatment of adenocarcinoma of the prostate II radical pros tatectomy
treated patients, Journal of Urology 16: 1076–1083.
Stone, C., Hansen, M., Kooperberg, C. and Truong, Y. (1997). Polynomial
splinesandtheirtensorproducts(withdiscussion), Annals of Statistics
25(4): 1371–1470.
Stone, M. (1974). Cross-validatory choice and assessment o f statistical
predictions, Journal of the Royal Statistical Society Series B 36: 111–
147.
Stone, M. (1977). An asymptotic equivalence of choice of mod el by cross-
validation and Akaike’s criterion, Journal of the Royal Statistical So-
ciety Series B. 39: 44–7.
Stone, M. and Brooks, R. J. (1990). Continuum regression: cr oss-validated
sequentially constructed prediction embracing ordinary l east squares,
partial least squares and principal components regression (Corr: V54
p906-907), Journal of the Royal Statistical Society, Series B 52: 237–
269.
Storey, J. (2002). A direct approach to false discovery rate s,Journal of the
Royal Statistical Society B. 64(3): 479–498.
Storey, J. (2003). The positive false discovery rate: A Baye sian interpreta-
tion and the q-value, Annals of Statistics 31: 2013–2025.
Storey,J.andTibshirani,R.(2003). Statisticalsigniﬁca nceforgenomewide
studies,Proceedings of the National Academy of Sciences 100-: 9440–
9445.
Storey, J., Taylor, J. and Siegmund, D. (2004). Strong contr ol, conservative
point estimation, and simultaneous conservative consiste ncy of false
discovery rates: A uniﬁed approach., Journal of the Royal Statistical
Society, Series B 66: 187–205.

724 References
Surowiecki, J. (2004). The Wisdom of Crowds: Why the Many are Smarter
than the Few and How Collective Wisdom Shapes Business, Eco-
nomics, Societies and Nations. , Little, Brown.
Swayne, D., Cook, D. and Buja, A. (1991). Xgobi: Interactive dynamic
graphics in the X window system with a link to S, ASA Proceedings
of Section on Statistical Graphics , pp. 1–8.
Tanner, M. and Wong, W. (1987). The calculation of posterior distribu-
tionsbydataaugmentation(withdiscussion), Journal of the American
Statistical Association 82: 528–550.
Tarpey, T. and Flury, B. (1996). Self-consistency: A fundam ental concept
in statistics, Statistical Science 11: 229–243.
Tenenbaum, J. B., de Silva, V. and Langford, J. C. (2000). A gl obal
geometric framework for nonlinear dimensionality reducti on,Science
290: 2319–2323.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso,
Journal of the Royal Statistical Society, Series B 58: 267–288.
Tibshirani, R. and Hastie, T. (2007). Margin trees for high- dimensional
classiﬁcation, Journal of Machine Learning Research 8: 637–652.
Tibshirani, R. and Knight, K. (1999). Model search and infer ence by boot-
strap “bumping, Journal of Computational and Graphical Statistics
8: 671–686.
Tibshirani, R. and Wang, P. (2007). Spatial smoothing and ho t spot de-
tection for CGH data using the fused lasso, Biostatistics 9: 18–29.
Tibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2001 a). Diagnosis
of multiple cancer types by shrunken centroids of gene expre ssion,
Proceedings of the National Academy of Sciences 99: 6567–6572.
Tibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2003 ). Class
prediction by nearest shrunken centroids, with applicatio ns to DNA
microarrays, Statistical Science 18(1): 104–117.
Tibshirani, R., Saunders, M., Rosset, S., Zhu, J. and Knight , K. (2005).
Sparsity and smoothness via the fused lasso, Journal of the Royal
Statistical Society, Series B 67: 91–108.
Tibshirani, R., Walther, G. and Hastie, T. (2001b). Estimat ing the number
of clusters in a dataset via the gap statistic, Journal of the Royal
Statistical Society, Series B. 32(2): 411–423.
Tropp, J. (2004). Greed is good: algorithmic results for spa rse approxima-
tion,IEEE Transactions on Information Theory 50: 2231– 2242.

References 725
Tropp, J. (2006). Just relax: convex programming methods fo r identify-
ing sparse signals in noise, IEEE Transactions on Information Theory
52: 1030–1051.
Valiant, L. G. (1984). A theory of the learnable, Communications of the
ACM27: 1134–1142.
van der Merwe, A. and Zidek, J. (1980). Multivariate regress ion analysis
and canonical variates, The Canadian Journal of Statistics 8: 27–39.
Vapnik, V. (1996). The Nature of Statistical Learning Theory , Springer,
New York.
Vapnik, V. (1998). Statistical Learning Theory , Wiley, New York.
Vidakovic, B. (1999). Statistical Modeling by Wavelets , Wiley, New York.
von Luxburg, U. (2007). A tutorial on spectral clustering, Statistics and
Computing 17(4): 395–416.
Wahba, G. (1980). Spline bases, regularization, and genera lized cross-
validation for solving approximation problems with large q uantities
of noisy data, Proceedings of the International Conference on Approx-
imation theory in Honour of George Lorenz , Academic Press, Austin,
Texas, pp. 905–912.
Wahba, G. (1990). Spline Models for Observational Data , SIAM, Philadel-
phia.
Wahba, G., Lin, Y. and Zhang, H. (2000). GACV for support vect or ma-
chines,inA. Smola, P. Bartlett, B. Sch¨ olkopf and D. Schuurmans
(eds),Advances in Large Margin Classiﬁers , MIT Press, Cambridge,
MA., pp. 297–311.
Wainwright, M. (2006). Sharp thresholds for noisy and high- dimensional
recovery of sparsity using ℓ1-constrained quadratic programming,
Technical report , Department of Statistics, University of California,
Berkeley.
Wainwright, M. J., Ravikumar, P. and Laﬀerty, J. D. (2007). H igh-
dimensional graphical model selection using ℓ1-regularized logistic re-
gression, inB. Sch¨ olkopf, J. Platt and T. Hoﬀman (eds), Advances
in Neural Information Processing Systems 19 , MIT Press, Cambridge,
MA, pp. 1465–1472.
Wasserman, L. (2004). All of Statistics: a Concise Course in Statistical
Inference , Springer, New York.
Weisberg, S. (1980). Applied Linear Regression , Wiley, New York.

726 References
Werbos, P. (1974). Beyond Regression , PhD thesis, Harvard University.
Weston, J. and Watkins, C. (1999). Multiclass support vecto r machines, in
M.Verleysen(ed.), Proceedings of ESANN99 ,D.FactoPress,Brussels.
Whittaker, J. (1990). Graphical Models in Applied Multivariate Statistics ,
Wiley, Chichester.
Wickerhauser, M. (1994). Adapted Wavelet Analysis from Theory to Soft-
ware, A.K. Peters Ltd, Natick, MA.
Widrow, B. and Hoﬀ, M. (1960). Adaptive switching circuits, IRE
WESCON Convention record , Vol. 4. pp 96-104; Reprinted in An-
dersen and Rosenfeld (1988).
Wold, H. (1975). Soft modelling by latent variables: the non linear iterative
partial least squares (NIPALS) approach, Perspectives in Probability
and Statistics, In Honor of M. S. Bartlett , pp. 117–144.
Wolpert, D. (1992). Stacked generalization, Neural Networks 5: 241–259.
Wu, T. and Lange, K. (2007). The MM alternative to EM, unpubli shed.
Wu, T. and Lange, K. (2008). Coordinate descent procedures f or lasso
penalized regression, Annals of Applied Statistics 2(1): 224–244.
Yee, T. and Wild, C. (1996). Vector generalized additive mod els,Journal
of the Royal Statistical Society, Series B. 58: 481–493.
Yuan, M. and Lin, Y. (2007). Model selection and estimation i n regression
with grouped variables, Journal of the Royal Statistical Society, Series
B68(1): 49–67.
Zhang, P. (1993). Model selection via multifold cross-vali dation,Annals of
Statistics 21: 299–311.
Zhang, T. and Yu, B. (2005). Boosting with early stopping: co nvergence
and consistency, Annals of Statistics 33: 1538–1579.
Zhao, P. and Yu, B. (2006). On model selection consistency of lasso,Jour-
nal of Machine Learning Research 7: 2541–2563.
Zhao, P., Rocha, G. and Yu, B. (2008). The composite absolute penalties
for grouped and hierarchichal variable selection, Annals of Statistics .
(to appear).
Zhu, J. and Hastie, T. (2004). Classiﬁcation of gene microar rays by penal-
ized logistic regression, Biostatistics 5(2): 427–443.
Zhu, J., Zou, H., Rosset, S. and Hastie, T. (2005). Multiclas s adaboost,
Unpublished.

References 727
Zou, H. (2006). The adaptive lasso and its oracle properties ,Journal of
the American Statistical Association 101: 1418–1429.
Zou, H. and Hastie, T. (2005). Regularization and variable s election via
the elastic net, Journal of the Royal Statistical Society Series B.
67(2): 301–320.
Zou, H., Hastie, T. and Tibshirani, R. (2006). Sparse princi pal com-
ponent analysis, Journal of Computational and Graphical Statistics
15(2): 265–28.
Zou, H., Hastie, T. and Tibshirani, R. (2007). On the degrees of freedom
of the lasso, Annals of Statistics 35(5): 2173–2192.

728 References

This is page 729
Printer: Opaque this
Author Index
Abu-Mostafa, Y. 95, 474
Ackley, D. H. 645
Adam, B.-L. 664
Agrawal, R. 489–491, 578
Agresti, A. 385, 638, 640
Ahn, J. 695
Akaike, H. 257
Allen, D. 257
Ambroise, C. 247
Amit, Y. 602
Anderson, J. R. 641
Anderson, T. 645
Angelo, M. 654, 658
Ardekani, A. M. 664
Bach, F. 569
Baezner, H. 551
Bair, E. 676, 679–683, 693
Bakin, S. 90
Bakiri, G. 605, 606
Banerjee, O. 636
Barnhill, S. 658
Barron, A. 415
Barry, R. 371
Bartlett, P. 384, 615Baskett, F. 480
Baxter, J. 384
Becker, R. 369
Bell, A. 578
Bellman, R. E. 22
Benade, A. 122
Bengio, Y. 404, 407, 408, 414, 644
Benjamini, Y. 687, 689, 693
Bentley, J. 480
Best, N. 292
Bibby, J. 94, 135, 441, 539, 559,
578, 630, 679
Bickel, P. 652
Bickel, P. J. 89
Bishop, C. 38, 233, 414, 623, 645
Bishop, Y. 629, 638
Bloomﬁeld, C. 663
Boser, B. 404, 414
Botha, J. 334
Bottou, L. 404, 407, 408, 414, 644
Boyd, S. 125, 632
Breiman,L.85,243,251,257,292,
308, 310, 334, 339, 367,
384, 451, 453, 455, 554,
587, 602

730 Author Index
Bremaud, P. 577
Brin, S. 577
Brooks, R. J. 81
Brown, P. 679
Bruce, A. 181
B¨ uhlmann, P. 87, 361, 384
Buja, A. 110, 297, 441, 446, 451,
455, 565, 574, 576, 578
Bunea, F. 91
Burges, C. 455
Butte, A. 631
Caligiuri, M. 663
Callow, M. 686, 693
Campo, E. 674
Candes, E. 86, 89, 613
Carlin, J. 292
Cazares, L. H. 664
Chambers, J. 334
Chan, W. C. 674
Chaudhuri, S. 631, 633
Chen, L. 574, 576, 578
Chen, S. S. 68, 94
Cherkassky, V. 38, 239, 257
Chu, G. 684, 693
Chui, C. 181
Clark, L. 331
Clark, M. 539
Clements, M. A. 664
Cleveland, W. 369
Cliﬀord, P. 629
Cohen, A. 668, 669
Coifman, R. R. 679
Coller, H. 663
Comon, P. 578
Connors, J. M. 674
Cook, D. 565, 578
Cook, N. 317
Copas, J. B. 94, 610
Cover, T. 257, 465, 481
Cox, D. 292, 645
Cressie, N. 171
Csiszar, I. 292
Cun, Y. L. 407, 471, 481
Cutler, A. 554Dale, M. 526
Dasarathy, B. 480, 481
d’Aspremont, A. 636
Daubechies, I. 92, 181
Davis, J. W. 664
de Boor, C. 181
De Mol, C. 92
de Silva, V. 573
Defrise, M. 92
Dempster, A. 292, 449, 633
Denham, M. 679
Denker, J. 404, 407, 414, 471, 481
Devijver, P. 480
Dietterich, T. 286, 602, 605, 606,
623
Donath, W. E. 578
Donoho, D. 68, 86, 91, 94, 179,
181, 554, 613
Downing, J. 663
Drton, M. 631, 633
du Plessis, J. 122
Duan, N. 480
Dubes, R. 508, 522
Duchamp, T. 541
Duda, R. 38, 135
Dudoit, S. 686, 693
Duin, R. 624
Edwards, D. 645
Efron, B. 73, 86, 90, 94, 97, 98,
128, 231, 254, 257, 292,
334, 568, 609, 692, 693
Elad, M. 613
Elith, J. 375, 376, 378
Eskin, E. 668, 669
Evgeniou, T. 168, 181, 455
Fan, J. 92, 216, 539, 654
Fan, Y. 654
Fazekas, F. 551
Feng, Z. 664
Ferreira, J. 122
Ferro, J. 551
Fiedler, M. 578
Fienberg, S. 585, 629, 638

Author Index 731
Finkel, R. 480
Fisher, N. 334
Fisher, R. A. 136, 455
Fisher, R. I. 674
Fisher, W. 310
Fishman, D. A. 664
Fix, E. 481
Flury, B. 578
Forgy, E. 578
Francis, M. 375, 376, 378
Frank, I. 81, 82, 94
Frean, M. 384
Freiha, F. 3, 49
Freund, Y. 337, 383, 384, 615
Fridlyand, J. 693
Friedman,J.38,81,82,85,92–94,
111, 121, 126, 251, 257,
258, 308, 310, 334, 339,
345, 365, 367, 384, 391,
414, 437, 451, 453, 475,
480, 565, 578, 602, 611,
617–621, 623, 636, 657,
661, 667
Friedman, N. 629, 630, 645
Fu, W. 91, 92
Fukunaga, K. 475
Furnival, G. 57
Fusaro, V. 664
Gaasenbeek, M. 663
Ganapathi, V. 642
Gao, H. 181
Gascoyne, R. D. 674
Gelfand, A. 292
Gelman, A. 292
Geman, D. 292, 602
Geman, S. 292
Genkin, A. 661
Genovese, C. 693
Gerald, W. 654, 658
Gersho, A. 514, 515, 526, 578
Ghaoui, L. E. 636
Gijbels, I. 216
Gilks, W. 292
Gill, P. 96, 421Girosi, F. 168, 174, 181, 415
Golub, G. 257, 335, 535
Golub, T. 631, 654, 658, 663
Goodall, C. 578
Gordon, A. 578
Gray, R. 514, 515, 526, 578
Green, P. 181, 183, 334
Greenacre, M. 455
Greenshtein, E. 91
Guo, Y. 657
Guyon, I. 658
Haﬀner,P.404,407,408,414,644
Hall, P. 292, 602, 619
Hammersley, J. M. 629
Hand, D. 135, 475
Hanley, J. 317
Hansen, M. 328
Hansen, R. 93
Hart, P. 38, 135, 465, 480, 481
Hartigan, J. A. 510, 578
Hastie, T. 72, 73, 78, 86, 88, 90,
92–94, 97, 98, 110, 121,
122, 126, 137, 174, 216,
257, 297, 299, 304, 334,
339, 345, 348, 349, 375,
376, 378, 384, 385, 414,
428, 431, 434, 437, 441,
446, 451, 455, 475, 478,
480, 481, 519, 539, 550,
565, 568, 578, 606, 609–
611, 614, 615, 636, 657,
658, 660–662, 664, 667,
676, 679–683, 693
Hatef, M. 624
Hathaway, R. J. 292
Heath, M. 257
Hebb, D. 414
Henderson, D. 404, 414
Herman, A. 334
Hertz, J. 414
Hinkley, D. 292
Hinton,G.292,334,408,414,644,
645
Hitt, B. A. 664

732 Author Index
Ho, T. K. 602
Hochberg, Y. 687, 689, 693
Hodges, J. 481
Hoeﬂing, H. 92, 93, 642, 667
Hoerl, A. E. 64, 94
Hoﬀ, M. 396, 414
Hoﬀman, A. J. 578
Hofmann, H. 578
Holland, P. 629, 638
Hong, W. 684
Hothorn, T. 87, 361, 384
Howard, R. 404, 414
Huard, C. 663
Hubbard, W. 404, 414
Huber, P. 349, 414, 435, 565, 578
Hunter, D. 294
Hyv¨ arinen, A. 560, 562, 578, 583
Ihaka, R. 455
Inskip, H. 292
Inzitari, D. 551
Izenman, A. 84
Jackel, L. 404, 414
Jacobs, R. 334
Jain, A. 508, 522
James, G. 606
Jancey, R. 578
Jensen, F. V. 629
Jiang, W. 384
Jirou´ sek, R. 640
Johnson, N. 412
Johnstone, I. 3, 49, 73, 86, 94, 97,
98, 179, 181, 609, 613
Joliﬀe, I. T. 550
Jones, L. 415
Jones, M. 168, 174, 181, 415
Jooste, P. 122
Jordaan, P. 122
Jordan, M. 334, 569, 645
Kabalin, J. 3, 49
Kalbﬂeisch, J. 674, 693
Karhunen, J. 583
Kaski, S. 531, 532, 578Kaufman, L. 517, 526, 578
Kearns, M. 380
Kennard, R. 64, 94
Kent, J. 94, 135, 441, 539, 559,
578, 630, 679
Kiiveri, H. T. 632
Kim, S.-J. 125
Kishon, E. 539
Kittler, J. 480, 624
Kleinberg, E. M. 602
Knight, K. 91, 292, 666, 693
Koh, K. 125
Kohane, I. 631
Kohavi, R. 243, 257
Kohn, E. 664
Kohonen, T. 462, 481, 531, 532,
578
Koller, D. 629, 630, 642, 645
Kooperberg, C. 328
Korn, E. L. 693
Kotze, J. 122
Kressel, U. 437
Krogh, A. 414
Ladd, C. 654, 658
Laﬀerty, J. 90, 304
Laﬀerty, J. D. 642
Lagus, K. 531, 532, 578
Laird, N. 292, 449
Lambert, D. 376
Lander, E. 654, 658, 663
Lange, K. 92, 294, 583, 584
Langford, J. C. 573
Larsen, R. 551
Latulippe, E. 654, 658
Lauﬀenburger, D. 625
Lauritzen, S. 629, 632, 645
Lawson, C. 93
Le Cun, Y. 404, 406–408, 414
Leathwick, J. 375, 376, 378
Leblanc, M. 292
LeCun, Y. 644
Lee, D. 552, 553
Lee, M.-L. 693
Lee, S.-I. 642

Author Index 733
Lee, W. 384, 615
Leslie, C. 668, 669
Levina, E. 652, 693
Levine, P. J. 664
Lewis, D. 661
Li, K.-C. 480
Li, R. 92
Lin, H. 331
Lin, Y. 90, 304, 428, 455
Liotta, L. A. 664
Little, R. 332, 647
Littman, M. 578
Liu, H. 90, 304
Lloyd, S. 481, 578
Loader, C. 209, 216
Loda, M. 654, 658
Loh, M. 663
Loh, W. 310
Lugosi, G. 384
Ma, Y. 257
Macnaughton Smith, P. 526
MacKay, D. 623
MacQueen, J. 481, 578
Madigan, D. 257, 292, 661
Makeig, S. 564, 565
Mannila, H. 489–491, 578
Mardia, K. 94, 135, 441, 539, 559,
578, 630, 679
Marron, J. 695
Mason, L. 384
Massart, D. 517
Matas, J. 624
McCullagh, P. 638, 640
McCulloch, C. 331
McCulloch, W. 414
McLachlan, G. 135, 247
McNeal, J. 3, 49
McNeil, B. 317
McShane, L. M. 693
Mease, D. 384, 603
Meinshausen, N. 91, 635, 642
Meir, R. 384
Mesirov, J. 654, 658, 663
Mills, G. B. 664Mockett, L. 526
Morgan, J. N. 334
Motwani, R. 577
Mukherjee, S. 654, 658
Mulier, F. 38, 239
Muller-Hermelink, H. K. 674
M¨ uller, K.-R. 547, 548
Munro, S. 397
Murray, W. 96, 421
Myles, J. 475
Nadler, B. 679
Narasimhan, B. 693
Neal, R. 268, 292, 409–412, 414,
605, 623
Nelder, J. 638, 640
Noble, W. S. 668, 669
Nolan, G. 625
Nowlan, S. 334
Oja, E. 560, 562, 578, 583
Olesen, K. G. 629
Olshen,R.251,308,310,334,367,
451, 453
Onton, J. 564, 565
Osborne, M. 76, 94
Osindero, S. 644
Paatero, A. 531, 532, 578
Pace, R. K. 371
Page, L. 577
Palmer, R. 414
Pantoni, L. 551
Park, M. Y. 94, 126, 661
Parker, D. 414
Paul, D. 676, 679–683, 693
Pearl, J. 629, 645
Pe’er, D. 625
Perez, O. 625
Peterson 641
Petricoin, E. F. 664
Pitts, W. 414
Plastria, F. 517
Platt, J. 453
Poggio,T.168,174,181,415,455,
654, 658

734 Author Index
Pontil, M. 168, 181, 455
Popescu, B. 617–619, 621, 623
Prentice, R. 674, 693
Presnell, B. 76, 94
Pˇ reuˇ cil, S. 640
Qu, Y. 664
Quinlan, R. 312, 334, 624
Radmacher, M. D. 693
Raftery, A. 257, 292
Ramaswamy, S. 654, 658
Ramsay, J. 181, 578
Rao, C. R. 455
R¨ atsch, G. 384, 615
Ravikumar, P. 90, 304, 642
Redwine, E. 3, 49
Reich, M. 654, 658
Richardson, J. 375
Richardson, T. S. 631, 633
Ridgeway, G. 361
Rieger, K. 684
Rifkin, R. 654, 658
Ripley, B. D. 38, 131, 135, 136,
234, 308, 310, 400, 414,
415, 455, 468, 480, 481,
641, 645
Rissanen, J. 257
Ritov, Y. 89, 91
Robbins, H. 397
Rocha, G. 90
Roosen, C. 414
Rosenblatt, F. 102, 129, 414
Rosenwald, A. 674
Rosset, S. 89, 98, 348, 349, 385,
426, 428, 434, 610, 611,
615, 657, 661, 664, 666,
693
Rostrup, E. 551
Rousseauw, J. 122
Rousseeuw, P. 517, 526, 578
Rowe, D. 375
Roweis, S. T. 573
Rubin, D. 292, 332, 449, 647
Rumelhart, D. 414Ryberg, C. 551
Saarela, A. 531, 532, 578
Sachs, K. 625
Saloj¨ arvi, J. 531, 532, 578
Saul, L. K. 573
Saunders, M. 68, 94, 666, 693
Schapire, R. 337, 380, 383, 384,
615
Schellhammer, P. F. 664
Schnitzler, C. 334
Sch¨ olkopf, B. 547, 548
Schroeder, A. 391
Schwarz, G. 233, 257
Scott, D. 216
Seber, G. 94
Segal, M. 596
Sejnowski, T. 578, 645
Semmes, O. J. 664
Seung, H. 552, 553
Shafer, G. 629
Shao, J. 257
Shenoy, P. 629
Short, R. 475
Shustek, L. 480
Shyu, M. 369
Siegmund, D. 689
Silverman, B. 181, 183, 216, 334,
486, 567, 578
Silvey, S. 292
Simard, P. 407, 471, 480, 481
Simon, R. M. 693
Simone, C. 664
Singer, Y. 384
Sj¨ ostrand, K. 551
Slate, E. 331
Slonim, D. 631, 663
Smeland, E. B. 674
Smith, A. 292
Smola, A. 547, 548
Sonquist, J. A. 334
Spector, P. 243, 257
Speed, T. 632, 686, 693
Spiegelhalter, D. 292, 629
Spiegelman, C. 679

Author Index 735
Spielman, D. A. 578
Srikant, R. 489–491, 578
Stamey, T. 3, 49
Staudt, L. M. 674
Steinberg, S. M. 664
Stern, H. 292
Stodden, V. 554
Stone, C. 251, 308, 310, 328, 334,
367, 451, 453
Stone, M. 81, 257
Storey, J. 689, 692, 693, 697, 698
Stork, D. 38, 135
Studholme, C. 551
Stuetzle, W. 391, 414, 541, 578
Surowiecki, J. 286
Swayne, D. 565, 578
Tamayo, P. 631, 654, 658, 663
Tang, J. 684
Tanner, M. 292
Tao, T. 89, 613
Tarpey, T. 578
Taylor, J. 88, 94, 610, 614, 689
Taylor, P. 375, 376, 378
Teh, Y.-W. 644
Tenenbaum, J. B. 573
Teng, S.-H. 578
Thomas, J. 257
Tibshirani, R. 73, 78, 86, 88, 90,
92–94, 97, 98, 110, 121,
122, 126, 137, 216, 257,
292, 297, 299, 304, 334,
339, 345, 384, 428, 431,
434, 437, 441, 446, 451,
455, 475, 478, 480, 481,
519, 550, 565, 568, 609–
611, 614, 636, 642, 657,
658, 660, 661, 666, 667,
676, 679–684, 692, 693
Toivonen, H. 489–491, 578
Traskin, M. 384
Trendaﬁlov, N. T. 550
Tropp, J. 91
Truong, Y. 328
Tsybakov, A. 89, 91Tukey, J. 414, 565, 578
Turlach, B. 76, 94
Turnbull, B. 331
Tusher, V. 684, 692
Tusn´ ady, G. 292
Uddin, M. 550
Valiant, L. G. 380
van der Merwe, A. 84
Van Loan, C. 335, 535
Vandenberghe, L. 632
Vanichsetakul, N. 310
Vapnik, V. 38, 102, 132, 135, 171,
257, 438, 455, 658
Vayatis, N. 384
Vazirani, U. 380
Verkamo, A. I. 489–491, 578
Vidakovic, B. 181
von Luxburg, U. 578
Wahba,G.168,169,181,257,268,
428, 429, 455
Wainwright, M. 91
Wainwright, M. J. 642
Waldemar, G. 551
Walther, G. 88, 94, 519, 610, 614
Wang, P. 667
Ward, M. D. 664
Warmuth, M. 615
Wasserman, L. 90, 304, 626, 645,
693
Watkins, C. 658
Wegkamp, M. 91
Weisberg, S. 94
Werbos, P. 414
Wermuth, N. 645
Weston, J. 658, 668, 669
Whittaker, J. 632, 633, 641, 645
Wickerhauser, M. 181
Widrow, B. 396, 414
Wild, C. 300
Williams, R. 414
Williams, W. 526
Wilson, R. 57

736 Author Index
Winograd, T. 577
Wold, H. 94
Wolpert, D. 292
Wong, M. A. 510
Wong, W. 292
Wright, G. 664, 674, 693
Wright, M. 96, 421
Wu, T. 92, 294, 583
Wyner, A. 384, 603
Yang, N. 3, 49
Yang, Y. 686, 693
Yasui, Y. 664
Yeang, C. 654, 658
Yee, T. 300
Yekutieli, Y. 693
Yu, B. 90, 91, 384
Yuan, M. 90Zhang, H. 90, 304, 428, 455
Zhang, J. 409–412, 605
Zhang, P. 257
Zhang, T. 384
Zhao, P. 90, 91
Zhao, Y. 693
Zhu, J. 89, 98, 174, 348, 349, 385,
426, 428, 434, 610, 611,
615, 657, 661, 664, 666,
693
Zidek, J. 84
Zou, H. 72, 78, 92, 349, 385, 550,
662, 693

This is page 737
Printer: Opaque this
Index
L1regularization, seeLasso
Activation function, 392–395
AdaBoost, 337–346
Adaptive lasso, 92
Adaptive methods, 429
Adaptive nearest neighbor meth-
ods, 475–478
Adaptive wavelet ﬁltering, 181
Additive model, 295–304
Adjusted response, 297
Aﬃne set, 130
Aﬃne-invariant average, 482, 540
AIC,seeAkaike information cri-
terion
Akaikeinformationcriterion(AIC),
230
Analysis of deviance, 124
Applications
abstracts, 672
aorta, 204
bone, 152
California housing, 371–372,
591
countries, 517demographics, 379–380
document, 532
ﬂow cytometry, 637
galaxy, 201
heart attack, 122, 146, 207
lymphoma, 674
marketing, 488
microarray, 5, 505, 532
nested spheres, 590
New Zealand ﬁsh, 375–379
nuclear magnetic resonance,
176
ozone, 201
prostatecancer,3,49,61,608
proteinmassspectrometry,664
satellite image, 470
skin of the orange, 429–432
spam, 2, 300–304, 313, 320,
328, 352, 593
vowel, 440, 464
waveform, 451
ZIP code, 4, 404, 536–539
Archetypal analysis, 554–557
Association rules, 492–495, 499–
501

738 Index
Automaticrelevancedetermination,
411
Automatic selectionofsmoothing
parameters , 156
B-Spline, 186
Back-propagation, 392–397, 408–
409
Backﬁtting, 297, 391
Backward
selection, 58
stepwise selection, 59
Backward pass, 396
Bagging, 282–288, 409, 587
Basis expansions and regulariza-
tion, 139–189
Basisfunctions,141,186,189,321,
328
Batch learning, 397
Baum–Welch algorithm, 272
Bayes
classiﬁer, 21
factor, 234
methods, 233–235, 267–272
rate, 21
Bayesian, 409
Bayesianinformationcriterion(BIC),
233
Benjamini–Hochbergmethod,688
Best-subset selection, 57, 610
Between class covariance matrix,
114
Bias, 16, 24, 37, 160, 219
Bias-variance decomposition, 24,
37, 219
Bias-variance tradeoﬀ, 37, 219
BIC,seeBayesianInformationCri-
terion
Boltzmann machines, 638–648
Bonferroni method, 686
Boosting, 337–386, 409
as lasso regression, 607–609
exponentiallossandAdaBoost,
343
gradient boosting, 358implementations, 360
margin maximization, 613
numerical optimization, 358
partial-dependenceplots,369
regularization path, 607
shrinkage, 364
stochastic gradient boosting,
365
tree size, 361
variable importance, 367
Bootstrap,249,261–264,267,271–
282, 587
relationshiptoBayesianmethod,
271
relationshiptomaximumlike-
lihood method, 267
Bottom-up clustering, 520–528
Bump hunting, seePatient rule
induction method
Bumping, 290–292
C5.0, 624
Canonical variates, 441
CART,seeClassiﬁcation and re-
gression trees
Categorical predictors, 10, 310
Censored data, 674
Classicalmultidimensionalscaling,
570
Classiﬁcation, 22, 101–137, 305–
317, 417–429
Classiﬁcationandregressiontrees
(CART), 305–317
Clique, 628
Clustering, 501–528
k-means, 509–510
agglomerative, 523–528
hierarchical, 520–528
Codebook, 515
Combinatorial algorithms, 507
Combining models, 288–290
Committee, 289, 587, 605
Comparison of learning methods,
350–352
Complete data, 276

Index 739
Complexity parameter, 37
Computational shortcuts
quadratic penalty, 659
Condensing procedure, 480
Conditional likelihood, 31
Confusion matrix, 301
Conjugate gradients, 396
Consensus, 285–286
Convolutional networks, 407
Coordinate descent, 92, 636, 668
COSSO, 304
Cost complexity pruning, 308
Covariance graph, 631
Cpstatistic, 230
Cross-entropy, 308–310
Cross-validation, 241–245
Cubic smoothing spline, 151–153
Cubic spline, 151–153
Curse of dimensionality, 22–26
Dantzig selector, 89
Data augmentation, 276
Daubechies symmlet-8 wavelets,
176
De-correlation, 597
Decision boundary, 13–15, 21
Decision trees, 305–317
Decoder, 515, seeencoder
Decomposable models, 641
Degrees of freedom
in an additive model, 302
in ridge regression, 68
of a tree, 336
ofsmoothermatrices,153–154,
158
Delta rule, 397
Demmler-Reinschbasisforsplines,
156
Density estimation, 208–215
Deviance, 124, 309
Diagonallineardiscriminantanal-
ysis, 651–654
Dimension reduction, 658
for nearest neighbors, 479
Discrete variables, 10, 310–311Discriminant
adaptivenearestneighborclas-
siﬁer, 475–480
analysis, 106–119
coordinates, 108
functions, 109–110
Dissimilarity measure, 503–504
Dummy variables, 10
Early stopping, 398
Eﬀective degrees of freedom, 17,
68,153–154,158,232,302,
336
Eﬀective number of parameters,
15,68,153–154,158,232,
302, 336
Eigenvaluesofasmoothermatrix,
154
Elastic net, 662
EM algorithm, 272–279
asamaximization-maximization
procedure, 277
for two component Gaussian
mixture, 272
Encoder, 514–515
Ensemble, 616–623
Ensemble learning, 605–624
Entropy, 309
Equivalent kernel, 156
Error rate, 219–230
Error-correcting codes, 606
Estimates of in-sample prediction
error, 230
Expectation-maximizationalgorithm,
seeEM algorithm
Extra-sample error, 228
Falsediscoveryrate,687–690,692,
693
Feature, 1
extraction, 150
selection, 409, 658, 681–683
Feed-forwardneuralnetworks,392–
408

740 Index
Fisher’s linear discriminant, 106–
119, 438
Flexiblediscriminantanalysis,440–
445
Forward
selection, 58
stagewise, 86, 608
stagewise additive modeling,
342
stepwise, 73
Forward pass algorithm, 395
Fourier transform, 168
Frequentist methods, 267
Function approximation, 28–36
Fused lasso, 666
Gap statistic, 519
Gating networks, 329
Gauss-Markov theorem, 51–52
Gauss-Newton method, 391
Gaussian(normal)distribution,16
Gaussian graphical model, 630
Gaussian mixtures, 273, 463, 492,
509
Gaussian radial basis functions,
212
GBM,seeGradient boosting
GBMpackage, seeGradientboost-
ing
GCV,seeGeneralizedcross-validation
GEM (generalized EM), 277
Generalization
error, 220
performance, 220
Generalized additive model, 295–
304
Generalizedassociationrules,497–
499
Generalized cross-validation, 244
Generalizedlineardiscriminantanal-
ysis, 438
Generalized linear models, 125
Gibbs sampler, 279–280, 641
for mixtures, 280
Gini index, 309Global Markov property, 628
Gradient Boosting, 359–361
Gradient descent, 358, 395–397
Graph Laplacian, 545
Graphical lasso, 636
Grouped lasso, 90
Haar basis function, 176
Hammersley-Cliﬀordtheorem,629
Hard-thresholding, 653
Hat matrix, 46
Helix, 582
Hessian matrix, 121
Hidden nodes, 641–642
Hidden units, 393–394
Hierarchical clustering, 520–528
Hierarchical mixtures of experts,
329–332
High-dimensional problems, 649
Hints, 96
Hyperplane, seeSeparating Hy-
perplane
ICA,seeIndependentcomponents
analysis
Importance sampling, 617
In-sample prediction error, 230
Incomplete data, 332
Independentcomponentsanalysis,
557–570
Independent variables, 9
Indicator response matrix, 103
Inference, 261–294
Information
Fisher, 266
observed, 274
Information theory, 236, 561
Inner product, 53, 668, 670
Inputs, 10
Instability of trees, 312
Intercept, 11
Invariance manifold, 471
Invariant metric, 471
Inverse wavelet transform, 179

Index 741
IRLS,seeIteratively reweighted
least squares
Irreducible error, 224
Ising model, 638
ISOMAP, 572
Isometric feature mapping, 572
Iterativeproportionalscaling,585
Iterativelyreweightedleastsquares
(IRLS), 121
Jensen’s inequality, 293
Join tree, 629
Junction tree, 629
K-means clustering, 460, 509–514
K-medoid clustering, 515–520
K-nearestneighborclassiﬁers,463
Karhunen-Loevetransformation(prin-
cipal components), 66–
67, 79, 534–539
Karush-Kuhn-Tucker conditions,
133, 420
Kernel
classiﬁcation, 670
density classiﬁcation, 210
density estimation, 208–215
function, 209
logistic regression, 654
principalcomponent,547–550
string, 668–669
trick, 660
Kernelmethods,167–176,208–215,
423–438, 659
Knot, 141, 322
Kriging, 171
Kruskal-Shephard scaling, 570
Kullback-Leibler distance, 561
Lagrange multipliers, 293
Landmark, 539
Laplacian, 545
Laplacian distribution, 72
LAR,seeLeast angle regression
Lasso,68–69,86–90,609,635,636,
661fused, 666
Latent
factor, 674
variable, 678
Learning, 1
Learning rate, 396
Learningvectorquantization,462
Least angle regression, 73–79, 86,
610
Least squares, 11, 32
Leave-one-outcross-validation,243
LeNet, 406
Likelihood function, 265, 273
Linear basis expansion, 139–148
Linear combination splits, 312
Lineardiscriminantfunction,106–
119
Linear methods
for classiﬁcation, 101–137
for regression, 43–99
Linear models and least squares,
11
Linear regression of an indicator
matrix, 103
Linear separability, 129
Linear smoother, 153
Link function, 296
LLE,seeLocal linear embedding
Local false discovery rate, 693
Local likelihood, 205
Local linear embedding, 572
Localmethodsinhighdimensions,
22–27
Local minima, 400
Local polynomial regression, 197
Local regression, 194, 200
Localizationintime/frequency,175
Loess (local regression), 194, 200
Log-linear model, 639
Log-odds ratio (logit), 119
Logistic (sigmoid) function, 393
Logistic regression, 119–128, 299
Logit (log-odds ratio), 119
Lossfunction,18,21,219–223,346
Loss matrix, 310

742 Index
Lossless compression, 515
Lossy compression, 515
LVQ,seeLearning Vector Quan-
tization
Mahalanobis distance, 441
Majority vote, 337
Majorization, 294, 553
Majorize-Minimizealgorithm,294,
584
MAP (maximum aposteriori) es-
timate, 270
Margin, 134, 418
Market basket analysis, 488, 499
MarkovchainMonteCarlo(MCMC)
methods, 279
Markov graph, 627
Markov networks, 638–648
MARS,seeMultivariate adaptive
regression splines
MART,seeMultiple additive re-
gression trees
Maximum likelihood estimation,
31, 261, 265
MCMC, seeMarkovChainMonte
Carlo Methods
MDL,seeMinimum description
length
Mean ﬁeld approximation, 641
Mean squared error, 24, 285
Memory-based method, 463
Metropolis-Hastingsalgorithm,282
Minimumdescriptionlength(MDL),
235
Minorization, 294, 553
Minorize-Maximizealgorithm,294,
584
Misclassiﬁcation error, 17, 309
Missing data, 276, 332–333
Missing predictor values, 332–333
Mixing proportions, 214
Mixturediscriminantanalysis,449–
455
Mixture modeling, 214–215, 272–
275, 449–455, 692Mixture of experts, 329–332
Mixtures and the EM algorithm,
272–275
MM algorithm, 294, 584
Mode seekers, 507
Modelaveragingandstacking,288
Model combination, 289
Model complexity, 221–222
Modelselection,57,222–223,230–
231
Modiﬁed regression, 634
Monte Carlo method, 250, 495
Mother wavelet, 178
Multidimensionalscaling,570–572
Multidimensional splines, 162
Multiedit algorithm, 480
Multilayer perceptron, 400, 401
Multinomial distribution, 120
Multiple additive regression trees
(MART), 361
Multiple hypothesis testing, 683–
693
Multiple minima, 291, 400
Multiple outcome shrinkage and
selection, 84
Multiple outputs, 56, 84, 103–106
Multipleregressionfromsimpleuni-
variate regression, 52
Multiresolution analysis, 178
Multivariate adaptive regression
splines(MARS),321–327
Multivariatenonparametricregres-
sion, 445
Nadaraya–Watson estimate, 193
Naive Bayes classiﬁer, 108, 210–
211, 694
Natural cubic splines, 144–146
Nearest centroids, 670
Nearest neighbor methods, 463–
483
Nearest shrunken centroids, 651–
654, 694
Network diagram, 392
Neural networks, 389–416

Index 743
Newton’smethod(Newton-Raphson
procedure), 120–122
Non-negativematrixfactorization,
553–554
Nonparametriclogisticregression,
299–304
Normal (Gaussian) distribution,
16, 31
Normal equations, 12
Numerical optimization, 395–396
Object dissimilarity, 505–507
Online algorithm, 397
Optimal scoring, 445, 450–451
Optimalseparatinghyperplane,132–
135
Optimismofthetrainingerrorrate,
228–230
Orderedcategorical(ordinal)pre-
dictor, 10, 504
Ordered features, 666
Orthogonal predictors, 53
Overﬁtting, 220, 228–230, 364
PageRank, 576
Pairwise distance, 668
Pairwise Markov property, 628
Parametric bootstrap, 264
Partialdependenceplots,369–370
Partial least squares, 80–82, 680
Partition function, 638
Parzen window, 208
Pasting, 318
Pathalgorithm,73–79,86–89,432
Patientruleinductionmethod(PRIM),
317–321, 499–501
Peeling, 318
Penalization, 607, seeregulariza-
tion
Penalizeddiscriminantanalysis,446–
449
Penalized polynomial regression,
171
Penalizedregression,34,61–69,171
Penalty matrix, 152, 189Perceptron, 392–416
Piecewisepolynomialsandsplines,
36, 143
Posterior
distribution, 268
probability, 233–235, 268
Power method, 577
Pre-conditioning, 681–683
Prediction accuracy, 329
Prediction error, 18
Predictive distribution, 268
PRIM,seePatient rule induction
method
Principal components, 66–67, 79–
80, 534–539, 547
regression, 79–80
sparse, 550
supervised, 674
Principalcurvesandsurfaces,541–
544
Principal points, 541
Prior distribution, 268–272
Procrustes
average, 540
distance, 539
Projection pursuit, 389–392, 565
regression, 389–392
Prototype classiﬁer, 459–463
Prototype methods, 459–463
Proximity matrices, 503
Pruning, 308
QR decomposition, 55
Quadraticapproximationsandin-
ference, 124
Quadratic discriminant function,
108, 110
Radial basis function (RBF) net-
work, 392
Radial basis functions, 212–214,
275, 393
Radial kernel, 548
Random forest, 409, 587–604
algorithm, 588

744 Index
bias, 596–601
comparison to boosting, 589
example, 589
out-of-bag ( oob), 592
overﬁt, 596
proximity plot, 595
variable importance, 593
variance, 597–601
Rao score test, 125
Rayleigh quotient, 116
Receiver operating characteristic
(ROC) curve, 317
Reduced-rank linear discriminant
analysis, 113
Regression,11–14,43–99,200–204
Regression spline, 144
Regularization, 34, 167–176
Regularizeddiscriminantanalysis,
112–113, 654
Relevance network, 631
Representer of evaluation, 169
ReproducingkernelHilbertspace,
167–176, 428–429
Reproducing property, 169
Responsibilities, 274–275
Ridge regression, 61–68, 650, 659
Risk factor, 122
Robust ﬁtting, 346–350
Rosenblatt’s perceptron learning
algorithm, 130
Rug plot, 303
Ruleﬁt, 623
SAM,690–693, seeSigniﬁcanceAnal-
ysis of Microarrays
Sammon mapping, 571
SCAD, 92
Scaling of the inputs, 398
Schwarz’s criterion, 230–235
Score equations, 120, 265
Self-consistencyproperty,541–543
Self-organizing map (SOM), 528–
534
Sensitivity of a test, 314–317
Separating hyperplane, 132–135Separatinghyperplanes,136,417–
419
Separator, 628
Shape average, 482, 540
Shrinkage methods, 61–69, 652
Sigmoid, 393
Signiﬁcance Analysis of Microar-
rays, 690–693
Similarity measure, seeDissimi-
larity measure
Single index model, 390
Singular value decomposition, 64,
535–536, 659
singular values, 535
singular vectors, 535
Sliced inverse regression, 480
Smoother, 139–156, 192–199
matrix, 153
Smoothingparameter,37,156–161,
198–199
Smoothing spline, 151–156
Soft clustering, 512
Soft-thresholding, 653
Softmax function, 393
SOM,seeSelf-organizing map
Sparse, 175, 304, 610–613, 636
additive model, 91
graph, 625, 635
Speciﬁcity of a test, 314–317
Spectral clustering, 544–547
Spline, 186
additive, 297–299
cubic, 151–153
cubic smoothing, 151–153
interaction, 428
regression, 144
smoothing, 151–156
thin plate, 165
Squared error loss, 18, 24, 37, 219
SRM,seeStructuralriskminimiza-
tion
Stacking(stackedgeneralization),
290
Starting values, 397
Statistical decision theory, 18–22

Index 745
Statistical model, 28–29
Steepest descent, 358, 395–397
Stepwise selection, 60
Stochastic approximation, 397
Stochasticsearch(bumping),290–
292
Stress function, 570–572
Structuralriskminimization(SRM),
239–241
Subset selection, 57–60
Supervised learning, 2
Supervisedprincipalcomponents,
674–681
Supportvectorclassiﬁer,417–421,
654
multiclass, 657
Support vector machine, 423–437
SURE shrinkage method, 179
Survival analysis, 674
Survival curve, 674
SVD,seeSingular value decom-
position
Symmlet basis, 176
Tangent distance, 471–475
Tanh activation function, 424
Target variables, 10
Tensor product basis, 162
Test error, 220–223
Test set, 220
Thin plate spline, 165
Thinning strategy, 189
Trace of a matrix, 153
Training epoch, 397
Training error, 220–223
Training set, 219–223
Tree for regression, 307–308
Tree-based methods, 305–317
Trees for classiﬁcation, 308–310
Trellis display, 202Undirected graph, 625–648
Universal approximator, 390
Unsupervisedlearning,2,485–585
Unsupervised learning as super-
vised learning, 495–497
Validation set, 222
Vapnik-Chervonenkis(VC)dimen-
sion, 237–239
Variable importance plot, 594
Variable types and terminology, 9
Variance, 16, 25, 37, 158–161, 219
between, 114
within, 114, 446
Variance reduction, 588
Varying coeﬃcient models, 203–
204
VCdimension, seeVapnik–Chervon-
enkis dimension
Vector quantization, 514–515
Voronoi regions, 510
Wald test, 125
Wavelet
basis functions, 176–179
smoothing, 174
transform, 176–179
Weak learner, 383, 605
Weakest link pruning, 308
Webpages, 576
Website for book, 8
Weight decay, 398
Weight elimination, 398
Weights in a neural network, 395
Withinclasscovariancematrix,114,
446