1
A Systematic Literature Review of
Retrieval-Augmented Generation: Techniques,
Metrics, and Challenges
Andrew Brown, Muhammad Roman, and Barry Devereux
Abstract—This systematic review of the research literature
on retrieval-augmented generation (RAG) provides a focused
analysis of the most highly cited studies published between
2020 and May 2025. A total of 128 articles met our inclusion
criteria. The records were retrieved from ACM Digital Library,
IEEE Xplore, Scopus, ScienceDirect, and the Digital Bibliography
and Library Project (DBLP). RAG couples a neural retriever
with a generative language model, grounding output in up-
to-date, non-parametric memory while retaining the semantic
generalisation stored in model weights. Guided by the PRISMA
2020 framework, we (i) specify explicit inclusion and exclusion
criteria based on citation count and research questions, (ii) cat-
alogue datasets, architectures, and evaluation practices, and (iii)
synthesise empirical evidence on the effectiveness and limitations
of RAG. To mitigate citation-lag bias, we applied a lower citation-
count threshold to papers published in 2025 so that emerging
breakthroughs with naturally fewer citations were still captured.
This review clarifies the current research landscape, highlights
methodological gaps, and charts priority directions for future
research.
I. INTRODUCTION
Large Language Models (LLMs) have, over the past five
years, transformed the way researchers and practitioners pro-
cess text. Retrieval-Augmented Generation (RAG) addresses
key shortcomings of these models, such as hallucinated
facts, stale world knowledge, and the challenges posed by
knowledge-intensive and domain-specific queries, by allowing
a generative model to query an external corpus at inference
time, combiningparametricmemory learnt during pre-training
withnon-parametricevidence retrieved on demand [1].
Traditional retrieval systems locate relevant passages but
cannot compose new text; purely generative models produce
fluent language, yet risk factual errors when outside knowledge
is required. RAG integrates both paradigms, offering factual
grounding without sacrificing fluency.
Since Meta AI introduced RAG in 2020 [1], the field
has diversified rapidly, incorporating hybrid retrievers, it-
erative retrieval loops, graph-based retrieval, and domain-
specific pipelines have been proposed. However, the results
are fragmented and the evaluation protocols are still evolving.
Therefore, a transparent, protocol-driven synthesis of RAG is
required. Consequently, we follow the PRISMA 2020 state-
ment (Preferred Reporting Items for Systematic Reviews and
Meta-Analyses) [2] to ensure transparency and reproducibility
in the development of a systematic review of the state-of-the-
art of RAG research. Each published paper selected for this
review progressed through the four PRISMA flow stages —identification, selection, eligibility, and inclusion — while the
reviewers verified that the study addressed at least one of our
research questionsandmet the predefined inclusion/exclusion
criteria. To focus on work that has demonstrably shaped the
field, we place special emphasis on the most frequently cited
RAG articles, including only the most highly cited studies pub-
lished between 2020 and May 2025.1This citation-based filter
serves as the primary gate in our PRISMA workflow, ensuring
that the review concentrates on influential contributions while
maintaining reproducibility.
The present study addresses these gaps by offering a
citation-weighted, PRISMA-compliant systematic synthesis of
128 influential RAG studies that maps datasets, architectures,
evaluation metrics, and open research challenges, thus ad-
vancing the field toward more aligned, robust, and scalable
retrieval-augmented systems.
This review is aimed at both NLP researchers, who can use
it to identify gaps and promising directions, and NLP engi-
neers seeking practical guidance on applying RAG techniques.
Our review catalogues datasets, novel methods, evaluation
metrics, and RAG deployment challenges. In doing so, it
delivers a cohesive overview of RAG architectures and offers
actionable insights to inspire future innovations.
Based on this aim, we formulate four research questions
(Table I). Throughout the review, we treat the original RAG
architecture of Lewiset al.[1] - Dense Passage Retriever plus
sequence-to-sequence generator - as thestandard baseline.
Variants are characterised relative to this reference point.
The remainder of this paper is organised as follows. Section
II details the methodology employed in this review, including
search strategies and inclusion criteria. Section III presents
the results, categorising the studies according to key themes
and findings. Section IV discusses the implications of these
findings, addressing both the strengths and challenges of RAG.
Finally, Section VI concludes the paper, summarising the
key insights with concrete recommendations for researchers
and engineers building the next wave of knowledge-aware
language models.
II. METHODOLOGY
This systematic literature review adhered to the strategy
and reporting guidelines of the Preferred Reporting Items
for Systematic Reviews and Meta-Analyses (PRISMA 2020)
[2]. The PRISMA framework is recognised across various
1Citation statistics were collected from Semantic Scholar on 13 May 2025.arXiv:2508.06401v3  [cs.DL]  9 Sep 2025

2
TABLE I: Summary of the research questions that guide this
systematic review.
Index Research Question Goal
RQ1 What thematic topics have
already been addressed by
highly cited RAG studies?Summarises the main topics in the
field, outlining the current state of
knowledge and identifying gaps in
the literature.
RQ2 What are the innovative
methods and approaches
compared to the standard
retrieval-augmented gener-
ation?Provides a thorough overview of
current research on RAG, assisting
researchers and engineers in identi-
fying common methodologies, ex-
isting studies, and exploring novel
approaches in the field.
RQ3 What are the most fre-
quently used metrics for
evaluating the effective-
ness of retrieval-augmented
generation systems?By identifying relevant metrics, re-
searchers can conduct meaningful
comparative analyses of systems,
essential for benchmarking and ad-
vancing the field.
RQ4 What are the key
challenges and limitations
associated with retrieval-
augmented generation
techniques?Identifies research gaps, enabling
researchers to propose solutions or
suggest areas for further explo-
ration.
Fig. 1: PRISMA 2020 flow diagram showing the stages of
article selection in this systematic review
research domains for its robust approach to literature reviews.
The review process was structured into three main phases:
Identification, Screening, and Inclusion.
We used the standard PRISMA flow diagram, as shown in
Figure 1, which encompasses searches exclusively in specific
databases and registers, although these are not detailed here.
This section elaborates on our selection of the systematic
review framework, detailing the search strategy, inclusion
and exclusion criteria, data extraction processes, and quality
assessment procedures to underscore our commitment to a
transparent and reproducible methodology.A. Systematic Review Framework Selection
The PRISMA 2020 guidelines provide an extensive frame-
work for systematic reviews, especially suitable for multi-
disciplinary fields such as RAG. These guidelines emphasise
updated methodological standards, including the synthesis of
findings, the assessment of study biases, and the inclusion of
various study designs. In contrast, Kitchenham’s guidelines
[3], designed specifically for software engineering literature
reviews, do not offer the necessary interdisciplinary breadth re-
quired for RAG research. Similarly, Evidence-Based Software
Engineering (EBSE) [3] focuses primarily on the application
of evidence-based principles to software engineering and does
not adequately address the broader theoretical and application-
based questions relevant to RAG. Therefore, PRISMA 2020
is valuable for facilitating the synthesis of various study
methodologies and goals, which aligns well with the evolving
and interdisciplinary nature of RAG research.
B. Database Selection
We used four digital databases and the DBLP bibliographic
index to improve coverage and deduplication. We targeted five
key electronic resources, chosen for their extensive repositories
and relevance to our research topics:
1) ACM Digital Library: https://dl.acm.org
2) IEEE Xplore: https://ieeexplore.ieee.org/
3) Scopus: https://www.scopus.com/
4) ScienceDirect: https://www.sciencedirect.com/
5) Digital Bibliography and Library Project (DBLP; bibli-
ographic index):
https://dblp.org/
C. Inclusion and Exclusion Criteria
In this section, we define the eligibility criteria for selecting
studies for our systematic review. We focus on articles pub-
lished between 2020 and 2025; this time frame coincides with
the significant introduction of the RAG framework by Meta
AI [1], a key milestone in natural language processing (NLP)
research. Our selection includes studies that explicitly address
the RAG framework or explore systems with similar function-
alities, ensuring that our review comprehensively captures the
latest innovations in this area.
a) Inclusion Criteria::
1)Focus:Studies must address RAG or similar systems
that rely on retrieval to support text output.
2)Publication Date and Citations:Only works from Jan-
uary 2020 to May 2025 are accepted. For 2025 publica-
tions, a minimum of 15 citations is required; for those
from 2024 or earlier, at least 30 citations are needed.
3)Original Contributions:Only works that present new
experimental data or fresh ideas are considered.
4)Input and Output:Studies may use various input types
(e.g., text, images, audio) if retrieval is central, but the
final output must be text.

3
TABLE II: Search queries used with each database.
Database Query
ACM Digital Library Title:(retrieval AND augmented AND generation) OR Abstract:(retrieval AND augmented AND generation)
IEEE Xplore ("Document Title": retrieval augmented generation) OR ("Publication Title": retrieval augmented generation) OR ("Abstract":
retrieval augmented generation)
Scopus TITLE-ABS-KEY ( retrieval AND augmented AND generation )
ScienceDirect Title, abstract, keywords: retrieval AND augmented AND generation
DBLP retrieval augmented generation
b) Exclusion Criteria::
1)Relevance:Works that do not pertain to the topic are
removed.
2)Language:Studies not published in English are ex-
cluded.
3)Duplicates and Access:Duplicate works or those with
unavailable full text are omitted.
D. Search Strategy and Search Terms
We based our search terms on the core concept of the
RAG framework by breaking down “retrieval augmented
generation” into three parts: “retrieval”, “augmented”, and
“generation”. These parts became the basis for our search
terms used in titles, abstracts, and keywords. Table II lists the
detailed queries for each database. Our systematic approach,
combining the main keywords with related phrases such as
"retrieval augmented text generation", gathered a wide range
of relevant literature on RAG.
E. Search Process
We queried five well-established digital databases and a bib-
liographic inde (ACM Digital Library, IEEE Xplore, Scopus,
ScienceDirect, and DBLP) to collect relevant articles. The
results were exported in BibTeX, CSV , or Excel formats as
provided by the source. A Python script converted BibTeX
files into Excel format, gathering key details such as titles,
abstracts, publication years, authors, author counts, and journal
names into one data table. Duplicate entries were first auto-
matically removed by the script, followed by a manual check
to verify accuracy.
F . Screening Process
Articles were screened against a set inclusion and exclusion
criteria linked to our research questions. Missing abstracts
were retrieved from the original databases and manually
added. Following PRISMA guidelines, a reviewer handled
initial screening, full text review, and data extraction, while
a second reviewer independently checked the results to reduce
bias. This dual-review method strengthens the review’s reli-
ability. The process, illustrated in Figure 1, consisted of an
initial screening and a review of the full text.1) Initial Screening:After removing duplicates and apply-
ing date and citation filters, two of the present authors (R 1,
R2) independently screenedalltitles and abstracts (n= 202).
Each record was labelled1(include) or0(exclude) against
the predefined eligibility criteria (§II-C). To aid, but not
replace, human judgement, we provided both reviewers with
LLM-generated suggestions fromdeepseek-ai/DeepSeek-R1-
Distill-Llama-70B; final decisions remained entirely with the
reviewers.
LLM-assisted suggestions.To support decision-making,
not replace human judgement, we provided both reviewers
with five independent generations fromdeepseek-ai/DeepSeek-
R1-Distill-Llama-70Bfor each record. The LLM was
prompted with our research questions and inclusion/exclusion
criteria; its five binary recommendations were then collapsed
into a single suggestion by majority vote. The final selection
decisions remained exclusively with the human reviewers.
2) Full Text Screening:Full texts were retrieved from
the original sources indexed by our selected databases and
the DBLP bibliographic index. During full-text screening,
we applied a quality assurance protocol assessing soundness,
validity, reliability, and statistical rigour to ensure the inclusion
of only high-quality studies. During the screening, each article
was evaluated against predefined criteria for inclusion and
exclusion, which encompassed the scope and methodological
robustness of the study, and was categorised with a ’0’ for
exclusion or ’1’ for inclusion. Moreover, we encountered
challenges concerning the interchangeable use of terms such
as RAG, retriever+reader models, and retrieval-augmented
LLMs. To address these challenges, we concentrated on clearly
differentiating the retriever and generator components, thereby
streamlining the analysis while ensuring a comprehensive
comprehension of the fundamental elements.
G. Data Extraction
Data extraction and management were handled using
Google Sheets for organising data and EndNote for managing
references. The data extracted from the articles were compiled
into a structured database designed for easy access during
subsequent analysis, synthesis, and reporting. Each entry was
verified against the original articles to identify and correct
any discrepancies, such as mismatched values or missing
information.
a) Data-extraction workbook.:All coded variables, their
operational definitions, and the raw study-level entries are

4
available in a publicly accessible Google Sheets workbook2
After verification, the data were synthesised to address the
research questions of the systematic review. The synthesis
used methods suited to the nature of the data and the review
objectives, primarily through a descriptive approach that sum-
marised and explained the data patterns by identifying trends,
differences and similarities between studies. This method
enabled us to draw meaningful conclusions from the diverse
data collected during the review.
1) Data Extraction Methodology: Domains, Specific Tasks,
Technique and Results:The data extraction process followed
our research question and eligibility criteria, focusing on
topics, methods, and evaluation metrics. It recorded details
such as Domain Area, which defines the field addressed by
each study. For datasets, both public and private sets were
included. The framework and components of the RAG system
were documented, listing the "Retrieval Mechanism", "Chunk-
ing Mechanism", "Vector Space Encoder", and "Generation
Model" while excluding any components not mentioned in
the paper. All data were organised in a workbook under clear
headings for easy access and analysis, providing complete
coverage for detailed review.
A single reviewer, using a RAG framework, independently
extracted the data to confirm accuracy and reliability. The
framework treated each article as a separate knowledge source,
queried by the specific data required. This approach simplified
the review process and offered a method of verifying the de-
tails. Using this framework confirmed that the data collection
was complete and consistent with the research criteria and
objectives.
However, the RAG framework poses two major challenges.
The first is the risk of hallucination, where the system may
generate information that does not exist. The second is that
key data might be absent from the retrieved passages. Despite
the framework’s benefits in improving speed and precision,
these issues call for careful cross-checking of the extracted
data to maintain its authenticity and reliability. Addressing
these challenges is essential to preserve the integrity of the
data extraction process.
2) Dataset Identification Methodology:We systematically
examined all studies in this review and used citation tracking
to identify and extract relevant datasets. The extracted infor-
mation was organised in Google Sheets to form a structured,
navigable database, ensuring the inclusion of the most impact-
ful and widely used resources. This organisation supported
more effective analysis and comparison, making sure that the
most relevant and impactful datasets were included.
Each entry lists its source reference, full official name and
common abbreviation, content overview, intended use, and
frequency of citations, allowing researchers to assess scope
and suitability. The relevance and popularity of each dataset
are highlighted by the number of papers that have used it,
indicating its significant impact and widespread adoption in
the field.
As shown in Table IV (see Appendix A, Table IV), each
dataset with the extracted fields: dataset name; content descrip-
2RAG_Data_Extraction.xlsxtion, which includes details such as the number of questions;
intended use, which may be described as designed to or as a
high overview; citation frequency, which indicates the number
of times the dataset has been mentioned in the reviewed
academic papers.
III. RESULTS
We identified 4721 records; after removing duplicates
(1494), out-of-range (158) and below-threshold items (2867),
202 were screened; 144 full texts were assessed; 128 studies
were included (reasons in Fig. 1).
A. Excluded Studies
Following the screening of the title and abstract, 144 can-
didate records were recovered in full and assessed against the
predefined inclusion criteria. Sixteen of these were excluded
during the full text screening for the reasons summarised
below. The reasons for exclusion were categorised as follows:
•Irrelevance of Primary Focus (n = 7):Papers whose pri-
mary contributions lay outside the augmented generation
of retrieval, e.g. robustness of dense search, long-context
benchmarks, general GenIR evaluation or system-level
optimisations, where RAG appeared only as a peripheral
baseline or illustrative example [4]–[10].
•Insufficient Emphasis or Ancillary Treatment (n = 7):
Studies that incorporated RAG merely as an auxiliary
component within broader investigations—such as LLM-
human hybrids for marketing research, domain-specific
LLM development, knowledge graph construction work-
flows, multimodal agent toolkits, healthcare task automa-
tion, cost-effective classification or materials modelling
pipelines—without substantive and dedicated analysis of
RAG itself [11]–[17].
•Methodological Distinction (n = 2):Works focused on
conceptually distinct paradigms from RAG, specifically
generative retrieval or generation-augmented retrieval,
which invert the standard RAG pipeline by predicting
document identifiers rather than conditioning the genera-
tion on the retrieved content [18], [19].
All exclusion decisions were systematically documented to
ensure methodological rigour, transparency, and reproducibil-
ity.
B. Yearly Distribution of Identified Articles
Across 2020–2025, the number of identified articles in-
creased year on year from 2020 to 2023, with a pronounced
increase in 2024. As of 13/05/2025, the count for 2025 is
lower because the year is incomplete. Figure 2 visualises the
annual distribution; year-specific totals are listed in Table III.
These counts reflect the records that remained after dedupli-
cation and application of the eligibility criteria (Section II-C),
including the citation thresholds (≥30for publications up to
2024;≥15for 2025). Consequently, year-to-year comparisons
should be interpreted in light of (i) the staged indexing of
databases and (ii) the partial coverage of 2025 at the time of
the last search.

5
Fig. 2: Yearly distribution of identified articles from 2020 to
2025
Fig. 3: Distribution of Studies by Domain: This bar chart
shows the percentage of studies conducted in various areas.
C. Domain Characteristics of Included Studies
Studies were coded to a singleprimarydomain for propor-
tional reporting; secondary tags (e.g., multimodal, conversa-
tional) were retained for analysis but are not double-counted
in the primary distribution. Coding rules and examples appear
in Table III. Proportions below refer to the included studies
(Fig. 3).
Knowledge-intensive tasks accounted for 27.34%, followed
by open-domain question answering (ODQA) at 15.62%,
software engineering 10.16% and medical 8.59%. Evaluation
comprised 7.03%. The “Other” category (7.03%) covers nine
single-study niches: networking, counterfactual augmentation,
content creation, personalisation, legal QA, recommender sys-
tems, chemistry, disaster response and personalised search.
Multimodal and conversational AI each represented 4.69%;
security/vulnerabilities and biomedical 3.91% each; education
3.12%; information extraction 2.34%; and finance 1.56%.
These distributions indicate a concentration of work in
knowledge-intensive and ODQA settings, with substantial ac-
tivity in software engineering and medical applications and
a long tail of niche areas. Full per-study domain labels and
secondary tags are provided in Table III; percentages may not
sum exactly due to rounding.

6TABLE III: Study characteristics of 128 included RAG papers by domain: datasets, chunking mechanisms, retrieval mechanisms, vector-space encoders,
and generation models.
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Knowledge-Intensive Tasks
•Adversarial NLI; Chain-of-thought;
FACTUALITYPROMPTS;
FLAN; HANS; Lambada;
Pre-training Corpus; RACE;
REALTOXICITYPROMPTS; Self-
Instruct; SODA; WinoGrande;
Word-in-Context [20]
•CoQA; CommonsenseQA;
DialogSum; DROP; FactKG;
FreebaseQA; Natural Questions
(NQ); PwC Reading-
Comprehension Corpus; QuAIL;
SamSum; SQuAD v2; Web
Questions (WebQA); WikiQA;
Wikipedia dump (December 2021)
[21]
•codeparrot/github-jupyter dataset;
DigMinecraft; Google search;
GSM8K; GSMHard; HumanEval;
HumanEval+; MBPP; MBPP+;
MC-TextWorld; Minecraft Wiki
[22]
•Dress Code Standards.pdf; Payment
Insurance Calculation.txt; Web-
based data; YouTube video content
[23]
•Enron Email; HealthcareMagic-
101; W3C-Email; Wikitext-103
[24]•100-token passages [25]
•100-word chunks [1]
•6-10 sentences [26]
•A decompose-then-
recompose algorithm splits
each retrieved document
into smaller strips, filters
out irrelevant portions, and
reassembles the relevant
parts. [27]
•Align passage segmentation
with paragraph boundaries.
[28]
•Approximately 300 words
each [29]
•Combine short paragraphs
when possible. [28]
•Each doctor-patient dialogue
as an individual chunk. [24]
•Each document as a distinct
chunk. [24]
•Each email as a separate data
piece. [24]
•Fixed-length 100-words.
[30]
•Fixed-length passages aver-
aging≈180tokens [21]
•Fixed-size 1200-token
chunks [31]
•Fixed-size 64 tokens [20],
[32]•Adaptive Retrieval: The
model generates a special
“Retrieve” reflection token
to determine on demand
whether external knowledge
is needed. [33]
•BM25 [34]
•Combines a retrieval-
augmented generator with a
memory selector. Iteratively
refines and improves the
generation process. [35]
•Composite structured
prompting strategy that
includes a command
component (e.g., “Please
repeat all the context”) to
extract the retrieved content
effectively. [24]
•Contriever: Uses a con-
trastive learning framework
without supervision. [29]
•Dense Retrieval [1], [20],
[22]–[30], [32], [36]–[46]
•Dense Retrieval - Dynam-
ically triggered by RIND
based on the LLM’s infor-
mation needs. [47]
•Dense Retrieval - FAISS
[21], [48]•ANCE (Dense) [26]
•Alibaba-NLP/gte-large-en-
v1.5 (Dense) [49]
•BAAI/LLM-Embedder
(Dense) [49]
•BAAI/bge-base-en (Dense)
[49]
•BAAI/bge-base-en-v1.5
(Dense) [49]
•BAAI/bge-large-en (Dense)
[49]
•BAAI/bge-large-en-v1.5
(Dense) [49]
•BAAI/bge-small-en (Dense)
[49]
•BAAI/bge-small-en-v1.5
(Dense) [49]
•BERT (Dense) [37]
•BERT-base (Dense) [1]
•BERT-based (Dense) [20],
[26], [28]
•BGE (Dense) [26]
•BGE-Base (Dense) [21]
•BGE-Large (Dense) [21]
•BM25 (Sparse) [30], [49],
[50]
•CLIP Variants (Dense) [26]
•ColBERTv2 (Dense) [21],
[45]
•Contriever (Dense) [27],
[29], [47], [49]•ActSTD [51]
•Alpaca [27]
•Alpaca-13B [33]
•Alpaca-7B [33]
•BART [1], [41]
•BART-Large [28]
•Both fine-tuned small models and
few-shot prompted LLMs [35]
•CRAG [27]
•ChatGPT [33]
•CodeLlama-7B [22]
•DiffTraj [51]
•Evidentiality prediction: An ad-
ditional decoder is used for pre-
dicting the evidentiality of each
passage [42]
•FLAN-T5 xlarge [52]
•FLAN-T5 xxlarge [52]
•Flan-T5 [26], [43]
•Fusion-in-Decoder [41], [42]
•GPT-3.5 [22]
•GPT-3.5-turbo [23], [24], [45],
[49]
•GPT-3.5-turbo-0613 [30], [40],
[51]
•GPT-4 [22], [37]
•GPT-4-0613 [30], [40]
•GPT-4o-mini [31], [51]
•GPT-Neo-1.3B [24]
•GPT-like decoder [20]
•GPT4All [23]
(Continued on next page)

7Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Knowledge-Intensive Tasks Continued...
•Databricks-Dolly-15K; SQuAD;
TREC DL19 and TREC DL20;
lyft_2021 [49]
•MultiFieldQA-en; Qasper;
QMSum; QuALITY [29]
•Amazon Book Reviews; Avocado
Email; Wikipedia Corpus [46]
•BigPatent; DailyDialog; JRC-
Acquis; XSum [35]
•Bamboogle; Feverous [47]
•CCNet; CREAK; CSQA2.0 [41]
•CuratedTrec; SearchQA [1]
•AI2 Reasoning Challenge (ARC);
BioASQ; ConceptNet; Physical
Interaction Question Answering
(PIQA); RiddleSense; UMLS [52]
•C4; RealNews [32]
•BioChatter Benchmark [53]
•Curated Golden Evaluation; Histor-
ical Issue Tickets [37]
•Infineon Developer Community Fo-
rum Questions; Infineon Product
Documents [36]
•Current Events; MMLU Bench-
mark [48]
•Biography [27]
•CNN/DailyMail [21], [40]
•RAGTruth; Yelp. 2021 [40]
•GPT4-Alpaca; LIMA; Oasst1; Ope-
nAssistant; Open-Orca; WizardLM
[26], [34]•Flexible Intervals 32 tokens
[32]
•K-hop ego-graphs [54]
•Max of 2000 tokens each
chunk [22]
•Overlapping - half the chunk
size [26]
•Parse each support ticket
into a tree structure instead
of fixed-length chunks. [37]
•Passages [41]
•Represent sections like Sum-
mary, Description, and Steps
to Reproduce as tree nodes.
[37]
•Sentence-level Chunking
[49]
•Sentences [43]
•Sentences or sub-sentence
[33]
•Sentences: 64 tokens [50]
•Sliding Window Chunking
Technique [49]
•Small-to-Big Chunking
Technique [49]
•Splits large documents into
smaller text chunks - sen-
tence, paragraph level or of-
ten configured to a maxi-
mum size (e.g., 500 charac-
ters) [23]•Dense Retrieval and Bing
Search Engine Retrieval [50]
•Dual-level retrieval [31]
•Dynamic Content Prediction
- generative model fore-
casts upcoming content and
dynamically forms retrieval
queries. [50]
•Efficient K-hop Subgraph
Retrieval [54]
•Evolving-Based Retrieval
[51]
•External search engines -
primarily DuckDuckGo [34]
•Graph database query (e.g.,
Cypher) [37]
•Hybrid with HyDE [49]
•LLM-driven query reformu-
lation to finalise sub-graph
[37]
•Large-scale web search. [27]
•Learning-Based Retrieval
[51]
•Multimodal [26]
•Retrieval is dynamically
triggered by RIND based
on the LLM’s information
needs. [25]•DPR (Dense) [21]
•Dragon (Dense) [21], [29]
•E5 (Dense) [26], [37]
•E5-Large (Dense) [21]
•E5-Mistral (Dense) [21]
•GPT4All [23]
•GTR (Dense) [46]
•Graph Attention Network
(GAT) [52]
•Graph Transformer (Dense)
[38]
•OpenAI’s text-embedding-3-
large (Dense) [34]
•SFR (Dense) [21]
•SentenceBERT (Dense) [54]
•SentenceBERT (SBERT)
(Dense) [38]
•T5 encoder (Dense) [42],
[44]
•all-MiniLM-L6-v2 (Dense)
[24]
•bge-large-en (Dense) [48]
•bge-large-en-v1.5 (Dense)
[24]
•e5-large-v2 (Dense) [30]
•e5-base-v2 (Dense) [24]
•embedding OpenAI (No
name) [23]
•intfloat/e5-large-v2 (Dense)
[49]
•intfloat/e5-small-v2 (Dense)
[49]•Internvl2.5-8B [26]
•Llama-13b-Chat [24]
•Llama-2 [27], [43]
•Llama-2-13B [38]
•Llama-2-13B (with LoRA fine-
tuning) [26]
•Llama-2-13B-Chat [25], [30],
[40]
•Llama-2-13b-chat-hf (for scale
comparison) [54]
•Llama-2-70B [29]
•Llama-2-70B-Chat (4-bitQ) [30],
[40]
•Llama-2-7B [26], [29], [38]
•Llama-2-7B-Chat [25], [30], [34],
[40]
•Llama-2-7b-chat-hf (LoRA fine-
tuning) [54]
•Llama-2-7b-chat-hf (frozen LLM)
[54]
•Llama-3-8B [51]
•Llama-3-8B-instruct [26]
•Llama-7b-Chat [24]
•Llama2-13B [33]
•Llama2-70B [39]
•Llama2-7B [33], [39], [48]
•Llama2-FT7B (retrieval-fine-
tuned Llama2) [33]
•Llava-7B [26]
•Llava-ov-7B [26]
(Continued on next page)

8Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Knowledge-Intensive Tasks Continued...
•GPT4-Alpaca; LIMA; Oasst1; Ope-
nAssistant; Open-Orca; WizardLM
[26], [34]
•ALCE-ASQA; ARC-Challenge;
Knowledge-intensive datasets
(Natural Questions, Wizard of
Wikipedia, FEVER, OpenBookQA,
ARC-Easy, ASQA); TriviaQA-
unfiltered [33]
•GraphQA Benchmark [54]
•ExplaGraphs; WebQSP [38], [54]
•SceneGraphs [38]
•UltraDomain - Agriculture; Ultra-
Domain - CS; UltraDomain - Legal;
UltraDomain - Mixed [31]
•Osaka Personal Activity Trajectory;
Tokyo Personal Activity Trajectory
[51]
•2WikiMultiHopQA [25], [26], [34],
[47], [49], [50]
•ASQA [26], [34], [49], [50]
•ASQA-hint; WikiAsp [50]
•ELI5 [43], [45]
•FEVER [1], [42]–[45], [49]
•FaVIQ-Ambig [42]
•Natural Questions; Natural Ques-
tions (NQ); Google Natural Ques-
tions [1], [20], [21], [28], [39],
[41]–[44], [46], [49]
•HotpotQA [21], [25], [26], [29],
[34], [39], [41], [43]–[47], [49]
•IIRC [25]
•KILT [30]•The graph is converted into
CSV-style representations by
processing its nodes and
edges. [38]
•Token length 256 [48]
•Token-level Chunking [49]
•Truncate overly long para-
graphs. [28]
•100 words [26]•Retrieves subgraphs based
on query relevance using
the Prize-Collecting Steiner
Tree optimization. [38]
•Sparse Retrieval [26], [30]
•Subgraph Retrieval [52]•jinaai/jina-embeddings-v2-
small-en (Dense) [49]
•multilingual-e5-large
(Dense) [30]
•sentence transformer
(Dense) [32]
•sentence-transformers/all-
mpnet-base-v2 (Dense)
[49]
•text-embedding-3-small
(Dense) [39]
•text-embedding-ada-002
(Dense) [22], [29]
•thenlper/gte-base (Dense)
[49]
•thenlper/gte-small (Dense)
[49]•MiniGPT-4 [38]
•Mistral-7B [39], [48]
•Mistral-7B-Instruct [21], [30],
[40]
•Mixtral-8×7B [21], [39]
•NeMo GPT-43B (proprietary)
[29]
•Orca2-7B [48]
•PaLM-2-S [46]
•PaLM-2-XXS [46]
•Perplexity.ai [33]
•Qwen-1.5-14B [26]
•Qwen2-vl-7B [26]
•RETRO+ [32]
•RETRO++ [20]
•RETRO-582M [32]
•Ret-ChatGPT [33]
•Ret-Llama2-chat [33]
•SAIL-7B [33]
•SELF-RAG-13B [33]
•SELF-RAG-7B [33]
•SelfRAG-Llama-2 [27]
•T5 [41]
•T5-Base [44]
•T5-Large [44]
•T5-XL [44]
•Toolformer-6B [33]
•TrajGAIL [51]
•Vicuna-13B [45]
•Vicuna-13B-v1.5 [25]
•text-davinci-003 [47], [50]
(Continued on next page)

9Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Knowledge-Intensive Tasks Continued...
•MMLU [39], [49]
•MS MARCO [1], [21], [40], [49]
•MuSiQue [26], [29], [34], [45],
[47], [49]
•NarrativeQA [21], [29], [49]
•OpenBookQA (OBQA) [26], [34],
[49], [52]
•PopQA [26], [27], [33], [34]
•PubHealth [27], [33], [49]
•PubMedQA [21], [49], [52]
•StrategyQA [25], [45], [47], [50]
•T-REx [28], [44], [45]
•TACRED [28]
•TruthfulQA [20], [21], [49]
•WebQuestions [1], [41], [49]
•WikiMultiHopQA [45]
•Wikipedia [27], [30], [32], [39],
[41]
•Wizard of Wikipedia [42]–[44]
•Zero-Shot RE [26], [28], [44], [45]
(Continued on next page)

10Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Open-Domain Question Answering
•2WikiMQA [55], [56]
•2WikiMultiHopQA (2WikiMulti-
hopQA) [57], [58]
•AGNews; AdvBench; BBQ; MS
MARCO; SST-2 [59]
•CoQA; SQuAD; TREC-COVID
[60]
•Conceptual Caption; LAION; Mul-
timodalQA; Probably-Asked Ques-
tions; Visual Question Answering;
WebQA [61]
•CORD-19; COVID-19 QA;
NewsQA; QAConv [62]
•Encyclopedic-VQA; InfoSeek;
LLaV A-Instruct [63]
•EntityQuestion; WitQA [64]
•FreshQA; Google Search; ToolQA
[65]
•HotpotQA (HQA) [55]–[60], [66]–
[68]
•IIRC; PDFTriage [56]
•MetaQA [69]
•Mintaka [67]
•MuSiQue [56], [58]
•MultifieldQA-en; Qasper [66]
•Natural Questions (NQ) [55], [57]–
[60], [66]–[68], [70]–[73]
•OK-VQA [74]
•PopQA [64], [65]•100 words [55], [62], [73]
•Aggregate knowledge-graph
triples into aggregated tex-
tual statements [69]
•Batch Grounding: Retrieved
documents are processed in
user-defined batches (e.g., 3
docs at a time); grounding
stops once evidence is cited.
[58]
•For PDFs, extract pages and
tables as separate nodes. [56]
•Group short documents into
longer units (from less than
1k tokens to around 4k to-
kens). [66]
•Image is divided into image
patches using a sliding win-
dow with a set stride [74]
•Image-only entries [61]
•Image-text pairs [61]
•Individual Sentences [64]
•Maximum sequence length
of 256 tokens [59]
•Non-overlapping segments
of 100 words [70], [72]
•Question Decomposition:
The LLM breaks the
original multi-hop question
into simpler single-hop
sub-questions before
retrieval/encoding. [58]•Asynchronous Updates - Re-
encode and re-index the
knowledge base during train-
ing. [62]
•BM25 [60]
•Contriever-MS-MARCO re-
triever [55]
•Dense Retrieval [57]–[62],
[65]–[68], [70]–[73]
•Document-level retrieval
with CLIP [63]
•Explicit Knowledge
Retrieval [74]
•Google API [58]
•Hierarchical two-step
retrieval [63]
•Implicit Knowledge
Retrieval [74]
•Iteratively retrieval - candi-
date relations for the cur-
rent entity set, then select
and rank the most relevant
ones using LLM prompts
and weighted voting. [69]
•Knowledge Graph Traver-
sal: LLM-based KG traver-
sal agent [56]
•Passage-level retrieval with
Contriever [63]
•Sparse Retrieval [58], [60],
[66], [67], [73]•ADORE (Dense) [70]
•BCEmbedding (Dense) [68]
•BERT-base (Dense) [73]
•BERT-based (Dense) [62],
[64]
•BGE cross-encoder reranker
(Dense) [68]
•BGE-Large-En-V1.5
(Dense) [59]
•BM25 (Sparse) [58], [64],
[67], [70], [73]
•CLIP (Dense) [63]
•CLIP model - ViT-B/16
(Dense) [74]
•ColBERTv2 (Dense) [58],
[68]
•Contriever (Dense) [63]–
[65], [70], [73]
•Dense Passage Retriever
(Dense) [68]
•Dual Encoder: BERT (unsu-
pervised training procedure)
(Dense) [72]
•E5-Mistral-7B (Dense) [66]
•Elastic Learned Sparse
Encoder (ELSER) (Sparse)
[60]
•KNN-MDR (Fine-tuned)
(Dense) [56]
•KNN-ST (Dense) [56]
•MPNet (Dense) [67]
•Multimodal encoder - T5
and ViT (text and image)
(Dense) [61]•BART [62]
•Blended RAG; Flan-T5-XXL;
GLaM (one-shot); GLaM (zero-
shot); PaLM540B (one-shot);
RAG-end2end; RAG-original
[60]
•ChatGPT [56], [67], [69]
•Claude-3-Opus; Claude-3.5-
Sonnet; DeepSeek-V2-Chat;
Gemini-1.5-Pro; GPT-4-turbo;
GPT-4o [66]
•Decoder (no name) [61]
•Falcon-7B (4-BitQ); Llama-2-7B
(4-BitQ); MPT-7B (4-BitQ); Phi-
2-2.7B (4-BitQ) [70]
•Flan-250M; Flan-Large; Flan-XL;
T0 [67]
•Flan-T5-base; Flan-T5-large;
Flan-T5-small; Flan-T5-xl; Flan-
T5-xxl; Llama-2-Chat; Llama-
3-Chat; MiniCPM; Mistral;
StableLM2; Zephyr [64]
•Flan-T5XL; Vicuna-7B [63]
•Fusion-in-Decoder [72]
•GPT-2; GPT-J; GPT-Neo; OPT
[73]
•GPT-3; Llama-2-70B-Chat [69]
•GPT-3.5; Phi-2-2.7B [65], [68]
•GPT-3.5-turbo [58], [59]
•GPT-4 [65], [68], [71]
•GPT-Neo-1.3B; Llama-13B-Chat;
Llama-7B-Chat [59]
(Continued on next page)

11Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Open-Domain Question Answering Continued...
•RealNews; The Pile; WikiText-103
[73]
•RealTimeQA [65], [71]
•Reddit Webis-TLDR-17 Dataset
[70]
•StrategyQA [55], [58]
•TriviaQA (TQA) [55], [57], [65],
[68], [71]–[73]
•WebQuestions (WebQ) [59], [71],
[72]
•WebQSP (WebQuestionsSP) [67]–
[69]
•Wikidata [67], [74]
•Wikipedia (English; December
2018 and 2017/2018 dumps;
passage corpus) [55], [57], [58],
[63]–[65], [67], [68], [70]–[73]•Split each document into
individual passages (text
blocks) [56]
•Split long units into fixed-
size chunks of 512 tokens.
[66]
•Summary Paragraph [64]
•Text-only entries [61]
•Use 4K-token chunking
when applicable. [66]
•Use each node (passage,
page, or table) [56]
•600 characters [63]•RankLLaMA (Dense) [68]
•Sentence Transformers
(Dense) [60]
•Spider (Dense) [73]
•T5 (text) (Dense) [61]
•TAGME Entity Linking
(Sparse + Semantic) [56]
•TF-IDF (Sparse) [56]
•UAE-Large-V1 (Dense) [59]
•ViT (images) (Dense) [61]
•bge-large-en-v1.5 (Dense)
[66]
•transformer-based encoder
(BERT-based) (Dense) [70]•Llama [56], [73]
•Llama-2-13B; Llama-3-8B;
Qwen-1.5-0.5B; Qwen-1.5-1.8B;
Qwen-1.5-14B; Qwen-1.5-4B;
Qwen-1.5-7B; Qwen2-7B [68]
•Llama-33B; text-davinci-002 [57]
•Mistral-7B [58], [68]
•Self-RAG-7B [65]
•T5 (fine-tuned) [56]
•T5-780M [55]
•TinyLlama-1.1B [64], [65]
•text-davinci-003 [55], [57]
•encoder–decoder transformer ar-
chitecture (initialised with models
like T5 or BART) [74]
(Continued on next page)

12Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Software Engineering
•ACE04, ACE05 [75]
•AI Tutor [76]
•BioASQ [76]
•C Code Summarization Dataset
[77], [78]
•CASIE [75]
•CoNLL03, CoNLL03 [75]
•Code Refinement [79]
•CodeMatcher [80]
•CodeSearchNet, CodeXGLUE [77],
[80]
•Cognitive Reviewer [76]
•Concode [77]
•CrossCodeEval [81]
•CrossCodeLongEval [81]
•Defects4J [79]
•Django [80]
•Hearthstone [80]
•InferredBugs [82]
•Multi-programming Language
Commit Message [83]
•NL2Bash [84]
•NLC2CMD [84]
•NYT [75]
•PyTorrent [80]
•Python Code Summarization
Dataset [78]
•RTLLM [85]
•RepoEval [81], [86]
•ServiceNow Internal Data [87]
•TFix [79]
•The Stack [81]
•VerilogEval [85]
•VerilogEval-syntax [85]•20 lines per chunk [81]
•50 lines per chunk [81]
•Code Property Graphs from
source code [78]
•Code Segments [82]
•Code Snippets [84]
•Code diff and commit mes-
sage [83]
•Code snippets, bug-fix pairs,
or other programming lan-
guage constructs [79]
•Fixed-size sliding window
[81]
•Fragment-alignment [81]
•Heuristics-based chunking:
use punctuation and para-
graph breaks. [76]
•Partition code files using
a sliding window approach
[86]
•Semantic chunking: use the
text’s inherent semantics.
[76]
•Sentence [75]
•Stride = ½ chunk size (over-
lap) [81]•A curated retrieval database
is used, where compiler error
messages (error tags) are ex-
actly matched to stored hu-
man solutions. [85]
•Anonymous sentence
embedding-based retrieval
strategy [75]
•Dense Retrieval [76], [77],
[81], [82], [86], [87]
•Dense Retrieval - Lucene
[80]
•Header2Code [80]
•Hybrid Patch Retriever:
Lexical-based and Semantic-
based [79]
•Iterative Retrieval [86]
•NL2Code [80]
•NL2NL. [80]
•Retrieving Similar Code [78]
•Semantic Code Diff Re-
triever [83]
•Semantic and Lexical Simi-
larity [84]
•Sparse Retrieval [77], [81],
[86]
•Supports Unimodal and Bi-
modal [77]
•The retrieval process
enriches the input prompt
with specific instructions
and demonstrations for
syntax error resolution. [85]•BM25 (Sparse) [77], [79]
•BiLSTM (Dense) [78]
•Bidirectional transformer
encoder model (no name)
(Dense) [82]
•CodeBERT [80]
•CodeBERT (fine-tuned)
(Dense) [84]
•CodeBERT and GraphCode-
BERT (named SCODE-R)
(Dense) [77]
•CodeDiff Encoder (Dense)
[83]
•CodeT5’s encoder (Dense)
[79]
•Commit Message Encoder
(Dense) [83]
•Jaccard token-set (Sparse)
[81]
•MPNet (Dense) [75]
•RoBERTa [80]
•Sentence-BERT [80]
•TF-IDF (Sparse) [81]
•UniXcoder (Dense) [81],
[86]
•Weighted n-gram (Sparse)
[81]
•all-mpnet-base-v2 (Dense)
[87]
•bag-of-words (Sparse) [86]
•gtr-t5-base (Dense) [87]
•gtr-t5-large (Dense) [87]
•gtr-t5-xl (Dense) [87]
•gtr-t5-xxl (Dense) [87]•CODEGEN-350M, 2B, 6B [86]
•ChatGPT [76]
•CodeGPT [80]
•CodeGen-Mono-2B, 16B [81]
•CodeGen25-7B [81]
•CodeLlama-16B [81]
•CodeLlama-7B [81], [87]
•CodeT5 [79]
•Exemplar Guider [83]
•GPT-3.5 [85]
•GPT-3.5-turbo [86]
•GPT-3.5-turbo-0613 [81]
•GPT-4 [76], [85]
•LSTM [78], [80]
•Mistral-7B-v0.1 [87]
•PLBART (named SCODE-G)
[77]
•Repoformer-1B, 3B, 7B, 16B [81]
•StarCoder-16B [81]
•StarCoderBase-15.5B [87]
•StarCoderBase-1B,3B,7B [81],
[87]
•Three encoders [83]
•Transformer-based architecture -
decoder component [78], [80],
[83], [84]
•code-cushman-001 (Codex large
language model - fine-tuned) [82]
•gpt-3.5-turbo [80]
•gpt-3.5-turbo-16k-0613 [85]
•text-davinci-002 [75]
•text-davinci-003 [75], [80]
(Continued on next page)

13Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Medical
•14 Clinical Scenarios [88]
•30 American Association for the
Study of Liver Diseases [89]
•35 Preoperative Guidelines [88]
•Apnea-ECG [90]
•Biomedical Instructions, Clinical
Practice Guidelines [91]
•CXR-PRO [92]
•Dynamed [93]
•European Association for the Study
of the Liver [94]
•Harvard-FairVLMed [95]
•Hospital Neurology Discharge
Summaries [96]
•Human-generated responses [88]
•IU-Xray [95]
•LiveQA, MMLU, MedInstruct,
MedMCQA, MedQA, Medical
Textbook, MedicationQA, Mol-
Instructions, PubMed Central
Full-text, PubMed Abstract [91]
•MIMIC-CXR [95]
•MS-CXR [92]
•Northern American HCV Guide-
lines [94]
•Online Sources Nursing Knowledge
JSON [96]
•PTB-XL, PTB-XL+ [90]
•Patient Inquiry Dataset, Patient
Symptom Record Dataset [96]
•PubMed Clinical Papers, Scoliosis
Research Society’s, UpToDate [93]•1000 tokens with an overlap
of 100 tokens [88]
•128 words with a 32-word
overlap [91]
•2000 tokens [93]
•Apnea-ECG: Split record-
ings into one-minute seg-
ments. [90]
•Cleans text [94]
•Converts tables into text-
based lists [94]
•Full medical reports [95]
•Hierarchical sections [97]
•Labels paragraphs [94]
•LangChain’s RecursiveChar-
acterTextSplitter [97]
•PTB-XL+: Use pre-made
ECG features as natural
chunks. [90]
•Paragraphs [97]
•single-disease/ topic JSON
entries (one entry = one
chunk) [96]•Dense Retrieval [90], [91],
[93]–[96], [98]
•Microsoft Azure Cognitive
Search services [89]
•Multimodal Retrieval -
Dense Retrieval [97]
•Pinecone’s Retrieval Agent
[88]
•Similarity of embeddings
[92]•ALBEF (Dense) [92]
•BM25 (Sparse) [97]
•BioClinicalBERT (Dense)
[95]
•Contriever (Dense) [97]
•Employs Microsoft Azure
OpenAI’s ADA Text
Embedding Version 2 model
(text-embedding-ada-002)
(Dense) [89]
•MPNet (Dense) [93]
•MedCPT (Dense) [91], [97]
•OpenAI’s text-embedding-
ada-002 (Dense) [88]
•ResNet-50 (Dense) [95]
•SPECTER (Dense) [97]
•text-embedding-ada-002
(Dense) [90], [96]•Alpaca [91]
•ChatGPT [93]
•Flan-T5 [91]
•GPT-3.5 [88], [90], [91], [97]
•GPT-3.5-turbo [89], [92], [96]
•GPT-4 [89], [91], [92], [95]–[98]
•GPT-4-turbo [94]
•GPT-4.0 [88]
•Galactica [91]
•LLaV A-Med-1.0 [95]
•LLaV A-Med-1.5-7B [95]
•Llama 3-70B [96]
•Llama-2-13B [88], [90]
•Llama-2-13b [89]
•Llama-2-70B [97]
•Llama-2-7B [88], [90]
•Llama2 [91]
•MEDITRON [91]
•MEDITRON-70B [97]
•Med-Flamingo [95]
•Med-PaLM [91]
•MedAlpaca [91]
•MedVInT [95]
•Mixtral-8×7B [97]
•PMC-LLaMA [91]
•PMC-Llama-13B [97]
•RadFM [95]
•Self-BioRAG [91]
•Self-RAG [91]
•text-davinci-003 [92]
(Continued on next page)

14Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Other
•1,000-User Benchmark Subset [99]
•Aggregated flood event listings
[100]
•Amazon Movie Reviews, Human-
Edited Counterfactuals Subset
of IMDb, IMDb Movie Review,
MNLI, WaNLI, Wikipedia, Yelp
Reviews [101]
•Australian Open Legal QA, Case-
HOLD, LEDGAR, Harvard Law
Case Corpus, LEDGAR, Legal-
Bench collection [102]
•Bing Search Logs [99]
•Census/projection-disaggregated
gridded population [100]
•ChEBI-20 Dataset, Colossal Clean
Crawled Corpus, ZINC-15 [103]
•Facebook Books, MovieLens100K
[104]
•FloodBrain ablation study dataset,
FloodBrain evaluation dataset [100]
•LaMP [105]
•Microsoft Research Paraphrase
Corpus [106]
•OpenStreetMap Planet dump [100]
•ParaSCI-ACL [106]
•Quora Question Pairs 140K, 50K
[106]
•ReliefWeb flood reports [100]
•The human cost of disasters
(2000–2019) [100]•A prefiltering strategy to
manage the token limit im-
posed by the API. [104]
•Bing Search Logs May–
July 2023) comprising user
queries and clicked results,
filtered and sampled to 1,000
users for evaluation [99]
•Each legal case is broken
down into question, sup-
port snippet, extracted enti-
ties, and an answer. [102]
•Full-text pages from
Wikipedia and a curated
set of 500 high-traffic
news domains, retained to
maximize reliable entity
linking [99]
•LangChain with a chunk size
(set as 1000 tokens) and
chunk overlap (200 tokens)
[107]
•Small prompt segments (p1:s
and q1:t) [106]•BM25-based Caption Re-
trieval [103]
•Counterfactual Dense Re-
trieval (CF-DPR) [101]
•Dense Retrieval [99], [106],
[107]
•Google Custom Search
[100]
•Morgan Fingerprints-based
Molecule Retrieval [103]
•ROPG-KD [105]
•ROPG-RL [105]
•Three-pronged retrieval ap-
proach: Intra query match-
ing, Inter context match-
ing, Hybrid weighted re-
trieval [102]•AnglE-BERT (Dense) [102]
•BERT (Dense) [102]
•Contriever (Dense) [99],
[105]
•LegalBERT (Dense) [102]
•Morgan Fingerprints:
Converts molecular SMILES
representations into binary
bit vectors that capture
the presence or absence
of chemical substructures.
[103]
•Sparse encoding mechanism,
effectively capturing detailed
structural features of
molecules. [103]
•Two independent BERT en-
coders (Dense) [101]
•paraphrase-mpnet-base-v2
(Dense) [106]•Bloom [107]
•ChatGPT-3.5-turbo [104]
•Flan-T5-XXL-11B [105]
•GPT-3 [101]
•GPT-3.5 [100]
•GPT-3.5-turbo [99], [103]
•GPT-4 [99], [100], [102], [107]
•GPT-4-0314 [103]
•GPT2 Large [106]
•GPT2 Medium [106]
•Gemini [107]
•Llama-2 [107]
•Llama-2-7B [103]
•Mistral-7B [102]
•PaLM-Text-Bison [100]
(Continued on next page)

15Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Evaluation
•2WikiMultihopQA [108]
•ClashEval Drug Dosage [109]
•ClashEval Locations [109]
•ClashEval Names [109]
•ClashEval News [109]
•ClashEval Sports Records [109]
•ClashEval Wikipedia Dates [109]
•EN.MC [108]
•En.QA [108]
•FEVER [110]
•Factual Recall Questions [111]
•False Premise Questions [111]
•General Legal Research [111]
•HotpotQA [108], [110]
•Jurisdiction or Time-Specific Re-
search [111]
•MuSiQue [108]
•MultiFieldQA [108]
•MultiHop-RAG dataset [112]
•MultiRC [110]
•NarrativeQA [108]
•Natural Questions [110]
•QMSum [108]
•Qasper [108]
•RGB (Retrieval-Augmented Gener-
ation Benchmark) [113]
•ReCoRD [110]
•WikiEval [114]
•Wizards of Wikipedia [110]•Fixed-size sliding windows
of 300 words per chunk
[108]•Dense Retrieval [108],
[109], [111]•Contriever (Dense) [108]
•Dragon (Dense) [108]•Ask Practical Law AI proprietary
LLM [111]
•Claude-3-Opus [109]
•Claude-3.5-Sonnet [109]
•GPT-3.5-turbo [108]
•GPT-3.5-turbo-0125 [109]
•GPT-4-turbo-2024-04-09 [111]
•GPT-4o [108], [109]
•Gemini-1.5-Flash [109]
•Gemini-1.5-Pro [108]
•Lexis+ AI proprietary LLM [111]
•Llama-3-8B-instruct [109]
•Westlaw AI-Assisted Research
(GPT-4-based) [111]
(Continued on next page)

16Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Multimodal
•ActivityNet Captions, MSR-VTT,
MSVD, V ATEX [115]
•CC12M, CC3M, CCS, COYO-
700M, NoCaps, SBU [116]
•COCO [117]
•Common Objects in Context [118]
•Flickr30k [116], [118]
•Google Search corpus [119]
•LAION [120]
•MS-COCO [116], [120]
•OKVQA [119]•Image and Sentences pairs
[118]
•Single Image [117]
•Subword [116]
•Uniformly sample frames or
clips, up to 25 per video -
Apply temporal deformable
convolution. [115]•Dense Retrieval [115]–
[117], [119]
•Multimodal Retriever -
Dense Retrieval [120]
•Similarity Search [118]•BERT-base (Dense) [119]
•Byte Pair Encoding (Dense)
[117]
•CLIP (Dense) [116]–[118],
[120]
•LXMERT (image and text)
(Dense) [118]
•Transformer-based encoder
(Dense) [116]
•Temporal deformable con-
volutional encoder (Dense)
[115]•CM3 Model [120]
•RETRO [116]
•T5 [119]
•Transformer encoder-decoder ar-
chitecture [117]
•Transformer-based GPT-2 model
[118]
•V&L encoder is used with a de-
coder for image captioning [118]
•fully convolutional decoder [115]
Domain: Conversational AI
•CoQA [121]
•CCNet [122]
•ConvFinQA (CFQA) [121]
•DoQA [121]
•Doc2Dial (D2D) [121]
•Emotion-Specific Dialogue [123]
•Gender-Specific Dialogue [123]
•HybriDial (HDial) [121]
•INSCIT [121]
•LightQA, LightWild, OpenQA-NQ
RN358
•MultiWOZ 2.1 [124]
•QReCC, QuAC, SQA, TCQA [121]
•Sentiment-Specific Dialogue [123]
•Weibo [125]
•Wikipedia, Wizard of the Internet
[122]
•Wizard of Wikipedia [122], [126]•100 Words [122]
•First 256 tokens (Search En-
gine) [122]
•Fixed-length text chunks
( 300 words) [121]
•Same as RAG DPR (Token)
(Lewis et al., 2020b) [126]
•Summarized memory slot
[125]•Dense Knowledge Retrieval
(DKR) [124]
•Dense Retrieval [121]–
[123], [126]
•Memory module [125]
•Search Engine Retrieval
(Bing Search API) [122]•Dragon (Dense) [121]
•E5-unsupervised (Dense)
[121]
•GRU (Dense) [125]
•Pre-trained DPR model
from the KILT Benchmark
(Dense) [122]
•RoBERTa (Dense) [124]
•Same as RAG DPR (Token)
(Lewis et al., 2020b) (Dense)
[126]•BART-Large [122], [124], [126]
•BlenderBot [122]
•ChatQA-1.0 7B, 8B, 13B, 22B,
70B 13B [121]
•Command R+ (104 B) [121]
•Fusion-in-Decoder [122]
•GPT-2–based [123]
•GPT-3.5-Turbo-0613, GPT-4-
0613, GPT-4-Turbo-2024-04-09,
GPT-SFT-22B, GPT-SFT-8B,
Llama-2-13B-Chat, Llama-2-
70B-Chat, Llama-2-7B-Chat,
Llama-3-70B-Instruct, Llama-3-
8B-Instruct, Llama2-SFT-13B,
Llama2-SFT-70B, Llama3-
ChatQA-1.5-70B, Llama3-
ChatQA-1.5-8B [121]
•Seq2Seq [125]
•T5 [122], [126]
(Continued on next page)

17Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Security/Vulnerabilities
•Agent-Driver [127]
•EHRAgent [127]
•Enron Email corpus [128]
•Harry Potter Series (Books3 subset)
[129]
•HotpotQA [128]
•MS MARCO [128], [130]
•Natural Questions [128], [130]
•SQuAD [130]
•StrategyQA [127]
•WikiASP [130]
•WikiQA question set [129]
•Wikipedia [129]•Documents are split into
fixed-length contiguous pas-
sages [128]
•Fixed-size overlapping
chunks [129]•Dense Retrieval [127],
[128], [130], [131]
•ORQA [127]
•REALM [127]
•Sparse Retrieval [129]•ANCE (Dense) [127]
•BGE (Dense) [127]
•BM25 (Sparse) [129]
•Contriever (Dense) [128],
[130]
•Contriever-MS MARCO
(Dense) [128]
•DPR encoder (Dense) [128]
•Dense Passage Retriever
(Dense) [127]
•JinaBERT (Dense) [130]
•LLaMA Embedding (Dense)
[130]
•ORQA (Dense) [127]
•Proprietary dense encoder
inside NVIDIA Chat-with-
RTX (Dense) [128]
•REALM (Dense) [127]
•text-embedding-ada-002 en-
coder (Dense) [127]•Claude-3-Opus [130]
•Customized GPT instances [131]
•GPT-3.5 [131]
•GPT-3.5-turbo [127], [128]
•GPT-4 [128]–[131]
•Gemma-2B, Gemma-7B [128]
•Llama-2-13B-Chat, 70B-Chat,
7B-Chat [129]
•Llama-2-7b-chat-hf [130]
•Llama-3-70B [127]
•Llama-3-8B [127], [128]
•Llama-3-8B-instruct, Mistral-
7B-Instruct, Mixtral-8×7B,
Platypus 2-Instruct-70B, Qwen-
1.5-72B-Chat, SOLAR-10.7B,
WizardLM-13B [129]
•Mistral 7B-int4 [128]
•Mistral-7B [131]
•Vicuna-13B [128], [129]
•Vicuna-7B [128]
Domain: Biomedical
•ADInt; Ade-corpus-v2; ChemProt;
DDI; GIT; GIT-RE; MTsample;
UMLS [132]
•Alzheimer’s Knowledge Base
(AlzKB) [133]
•BioChatter Benchmark [53]
•Multiple Choice Questions
Dataset; RAG Comparison Dataset;
True/False Dataset [134]
•PubMed Clinical Papers [135]•Extracts multiple contextual
associations (chunks) from
SPOKE via REST-API calls.
[134]
•Nodes and Edges [133]
•Split text into chunks of five
consecutive tokens/words.
[132]•Dense Retrieval [53], [132],
[134], [135]
•Dense Retrieval - Weaviate
[133]
•SPOKE’s REST-API [134]
•Sparse Retrieval [133]•MedLLaMA-13B (Dense)
[132]
•MiniLM (Dense) [134]
•OpenAI’s text-embedding-
ada-002 (Dense) [135]
•PubMedBert (Dense) [134]
•all-MiniLM-L6-v2 (Dense)
[134]•BioGPT; KRAGEN’s LLM;
OpenChat [133]
•GPT-3.5; Microsoft’s
Prometheus; text-davinci-003
[135]
•GPT-3.5-turbo [134]
•GPT-4 [134], [135]
•GPT-4 (in zero-shot settings);
Llama-3.1-8B; MedLlama-13B
[132]
•Llama-2-13B [132], [134]
(Continued on next page)

18Table III continued from previous page
Datasets Chunking Mechanism Retrieval Mechanism Vector Space Encoder Generation Model
Domain: Education
•GPT-generated answer-eval corpus;
lecture materials; MongoDB logs;
MongoDB QA; TAM questionnaire
responses [136]
•Lay-language synthesis corpus;
UMLS; Wikipedia [137]
•Data-mining and text-analytics
course materials; Lumos-QG-
generated QA dataset [138]
•Math Nation queries; OpenStax
Prealgebra [139]•Character-level text splitting
of PDFs into fixed-length
[138]
•LangChain TextSplitter
[136]
•Textbook by sub-section
[139]•Dense Retrieval [137], [139]
•Dense Retrieval - LangChain
[138]
•Dense Retrieval - Weaviate
[136]
•MongoDB [136]•BERTBase (Dense) [137]
•text-embedding-ada-002
(Dense) [136], [138], [139]•BART [137]
•GPT-3.5-turbo [136], [138]
•GPT-4 [136], [138]
•gpt-3.5-turbo-0613 [139]
Domain: Information Extraction
•ACE 2005 [140]
•De-identified electronic health
records [141]
•RAMS [142]
•WikiEvents [140], [142]•Fixed-size chunks (600 char-
acters) [141]
•Sentence [142]
•Structured data (like weight
tables) is split row by row
[141]•Adaptive Hybrid Retrieval;
Context-Consistency
Retrieval; Schema-
Consistency Retrieval
[142]
•Dense Retrieval [140], [141]•Sentence Transformer
(Dense) [141]
•SentenceBERT (Dense)
[140], [142]•BART-Large [140]
•Llama-2-13B [141]
•T5 [142]
Domain: Financial
•AlphaFin-Test subset [143]
•FinanceBench [144]
•Financial News [143]
•Financial Reports [143]
•Financial Reports CoT [143]
•Real-time Market Data [143]
•Research datasets [143]
•StockQA [143]•128/256/512-token chunks;
Chipper-based structure
extraction; merge strategy
[144]
•Coarse: ChatGPT document
summaries; Fine: RefGPT
generates multiple Q&A per
document; embed each sum-
mary/Q&A chunk [143]•Dense Retrieval [143], [144]•BGE (Dense) [143]
•SGPT (Dense) [143]
•multi-qa-mpnet-base-dot-v1
(Dense) [144]•ChatGLM2-6B [143]
•GPT-4 [144]
•Mixtral-8×7B [144]
•StockGPT [143]

19
IV. DISCUSSION
A. What are the key topics that are already addressed in RAG?
1) Retrieval mechanism:Retrieval-augmented generation
systems depend uniformly on an external retriever to select
a relevant context for a language model. In general, the
mechanisms surveyed fall into five interrelated categories.
Sparse term-based methods(e.g., BM25) remain vital
for their efficiency and interpretability, yet they struggle with
semantic recall and gaps [73]. Dense retrievers, built on dual
encoder networks such as DPR, map queries and documents
into continuous vector spaces and leverage the maximum
inner-product search for semantic matching [1]. Hybrid ap-
proaches combine sparse pruning of candidates with dense re-
ranking to balance recall and precision across domains [29].
Encoder–decoder query generators reformulate inputs,
especially conversational or multihop questions, into stan-
dalone search queries, improving recall at the cost of added
latency [122]. Reclassification modules (e.g., CRAG) apply
lightweight evaluators or preference-aligned models to reorder
initial top k results, mitigating noisy retrievals, and aligning
outputs with downstream generation needs [27].
By organising passages or entities intoknowledge graphs,
graph retrieval methods extract sub-graphs or paths most
relevant to a query. Steiner tree formulations collecting prizes
yield coherent multi-hop contexts with explicit reasoning
chains, albeit at significant computational cost for large graphs
[38], [56].
Iterative frameworks interleave retrieval and genera-
tion: LLM outputs refine subsequent queries, progressively
bridging semantic gaps in complex tasks [47]. Although this
feedback loop improves multistep reasoning, it incurs in-
creased latency and requires careful stopping criteria to prevent
error propagation [86].
Specialised retrievers adapt core architectures to dif-
ferent data modalities or domains, such as code snippet
retrieval by edit distance scoring [78], multimodal CLIP-based
retrieval for image captioning [120], or clinical report retrieval
using vision language embeddings [74]. These systems achieve
high task relevance but demand bespoke engineering and
corpus maintenance.
Together, these mechanisms illustrate a landscape where ad-
vances in semantic embeddings, input optimisation, structural
reasoning, adaptive feedback, and domain adaptation coalesce
to enrich the context of LLM. Each category presents distinct
trade-off between efficiency, scalability, interpretability, and
domain generality, highlighting open avenues for unified,
explainable, and resource-efficient retrieval in future RAG
research.
2) Vector Database:The vector database is fundamental to
RAG, enabling fast similarity searches over dense embeddings
through approximate nearest neighbor (ANN) techniques such
as hierarchical navigable small world (HNSW) graphs and
FAISS-based flat or inverted indices, which achieve sub-
millisecond Maximum Inner Product Search (MIPS) per-
formance in production settings but must negotiate accu-
racy–latency trade-offs and memory footprint constraints [1],
[28], [122]. Research has extended these core indexing meth-ods to distributed and dynamic environments, employing GPU-
sharded indices and cloud-native services like Pinecone to
ingest and serve millions of vectors across training and
inference pipelines; however, synchronization latency, up-
date throughput, and cost-efficiency remain pressing con-
cerns [72], [88]. Concurrently, domain-specific vector stores
have emerged—tailored for code retrieval (e.g., RepoCoder),
biomedical concept embeddings (Chroma), financial knowl-
edge bases, and multimodal memory systems (MuRAG, Re-
ViLM)—to address the unique representational, alignment,
and privacy demands of specialized data [61], [86], [134].
Finally, managed vector database offerings integrated via
frameworks such as LangChain, LlamaIndex, Weaviate, and
Qdrant have streamlined deployment in commercial RAG
pipelines, albeit at the expense of potential vendor lock-in,
hybrid architecture complexity, and unpredictable operational
costs [37], [136].
Despite the maturity of these infrastructures, several cross-
cutting research gaps persist. Notably, adaptive indexing al-
gorithms capable of real-time inserts and deletes without
degrading search performance are under-explored, while cost-
aware scaling strategies that balance query latency against
infrastructure expenditure remain scarce. Moreover, ensuring
seamless interoperability across heterogeneous vector database
services and embedding formats presents an ongoing challenge
and a fertile avenue for future RAG innovation.
3) Document chunking:Document chunking is the decom-
position of large inputs into smaller, retrievable units. It is a
critical preprocessing step in RAG. Highly cited studies have
converged on four principal approaches:
Static fixed-length segmentation.Early RAG architectures
adopt uniform, size-bounded splits to simplify indexing and
conform to transformer context limits. Common configura-
tions include 100-word segments [1], [30], fixed-size 64-token
chunks (with optional 32-token flexible intervals) [32], and
approximately 600-character spans [63]. These static splits
require minimal linguistic preprocessing and integrate readily
with vector stores (e.g., FAISS), but frequently bisect semantic
units, resulting in context loss and a trade-off between index
growth (for smaller chunks) and retrieval precision (for larger
ones).
Semantic boundary–aware splitting.To preserve dis-
course coherence, the subsequent work aligns the chunk
boundaries with the inherent text structure. Techniques in-
clude sentence-level chunking, where each sentence becomes
a chunk [75], and paragraph-level segmentation, merging
short paragraphs and truncating overly long ones [28]. More
advanced methods leverage hierarchical section markers (e.g.,
PDF sub-sections) to define semantically coherent units [97],
[138]. These approaches mitigate fragmentation and often
improve retrieval relevance, at the cost of additional prepro-
cessing complexity and the absence of standardised coherence
metrics.
Domain and modality specific chunking.Recognising that
different types of data exhibit unique structures, specialised
chunking strategies have been developed:
•Source code: partitioning by function or Code Property
Graph nodes to capture logical code blocks [78], [84].

20
•Knowledge graphs: aggregating graph triples into textual
statements for embedding [69].
•Legal documents: breaking cases into (question,
snippet,entity,answer) tuples [102].
•Biomedical texts: micro-chunking into fixed five-token
units to capture fine-grained concepts [132].
•Multimodal inputs: splitting image–text pairs into aligned
patches or entries for vision–language RAG [61].
These tailored pipelines yield superior performance within
their target domains, but require manual configuration and do
not generalise easily across new data types.
Adaptive dynamic chunking.The most recent research line
seeks to automate chunk-size and overlap selection based on
query characteristics or retrieval performance. Representative
techniques include sliding windows (for example, 1000-token
windows with 200-token overlaps in LangChain [107], fixed-
size 1200-token chunks with dynamic overlap [31]), automated
parameter search for domain-specific corpora (e.g., clinical
notes [88]), and half-stride overlapping to balance novelty and
context continuity [81]. Adaptive methods aim to integrate the
benefits of static, semantic, and domain-specific approaches,
yet remain largely experimental, facing challenges in hyper-
parameter optimisation, runtime overhead, and cross-domain
robustness.
Over time, RAG document chunking has evolved from
simple one-size-fits-all splits to sophisticated, context and
domain-aware pipelines. Static segmentation offers scalability
but suffers semantic fragmentation; semantic boundary meth-
ods enhance coherence but add preprocessing costs; domain-
specific chunkers exploit structural priors at the expense of
generality; and adaptive strategies promise end-to-end au-
tomation but require further validation. Future work should
establish standardised coherence benchmarks, develop unified
frameworks that dynamically leverage linguistic and domain
signals, and evaluate scalability in large-scale RAG deploy-
ments.
4) Vector encoders:In RAG systems, thevector space
encoderprojects both user queries and document chunks into
a shared high-dimensional embedding space, enabling efficient
similarity-based retrieval. Influential RAG studies fall into
three principal paradigms:
Sparse encoders.Traditional IR techniques convert text into
high-dimensional sparse vectors of term weights (e.g. TF-IDF,
BM25), scoring relevance via inverted indices. BM25 remains
a robust baseline, often combined with dense methods to
increase recall in open-domain QA and hybrid pipelines [30],
[56]. In specialised settings such as code and graph retrieval,
additional sparse schemes are common. For example, one can
measure the overlap between the query and a language-specific
token inventory (e.g., Java identifiers, keywords, and API
names), or employ weighted n-gram counts to capture local
lexical structure. These approaches deliver low latency and
scale efficiently but provide limited deep semantic modelling;
in RAG, they are therefore used primarily as recall-orientated
components that are complemented by dense retrieval and/or
learnt re-ranking [81], [86].
Dense encoders.Deep learning–based dense encoders map
inputs to continuous embeddings that capture contextual andsemantic nuances:
•Transformer-based bi-encoders.Frameworks such as
DPR, ANCE, REALM, ORQA, and dual encoder BERT
variants embed queries and passages separately, optimis-
ing retrieval metrics (Recall@k, MRR) through end-to-
end fine-tuning [122], [127].
•Sentence and paragraph embeddings.Models such as
Sentence-BERT, MPNet, paraphrase-mpnet-base-v2 and
Contriever produce fixed-length vectors for larger text
spans, improving semantic similarity on standard bench-
marks [65], [106], [142].
•Foundation & specialised models.API-driven encoders
(e.g., text-embedding-ada-002, text-embedding-3-small /
large) and proprietary systems (Dragon, E5, BGE) de-
liver broad coverage with minimal tuning [39], [127],
[135]. Domain-adapted variants, MedLLaMA-13B for
biomedicine [132], PubMedBERT for clinical language
[134], CodeBERT / CodeT5 for source code, demonstrate
versatility in specialised vocabularies [79], [84].
Hybrid & multi-modal encoders.To retrieve across het-
erogeneous sources, modern RAG systems fuse sparse and
dense signals or jointly encode multiple modalities.
•Sparse–dense hybrids.Elastic Learnt Sparse Encoder
(ELSER) integrates learnt sparse representations with
dense sentence embeddings, balancing latency and recall
[60].
•Vision–language models.CLIP (text + image),
LXMERT, ALBEF, and temporal deformable
convolutional encoders support multimodal retrieval
for visual QA and image-based generation [115], [116],
[118].
•Graph and sequence models.Graph Transformers and
Graph Attention Networks embed structured data (knowl-
edge graphs, ASTs) into vector spaces for retrieval-
augmented reasoning [38], [52].
The selection of encoders in RAG reflects a trade-off among
retrieval accuracy, computational efficiency, and domain adapt-
ability. Future work should target out-of-domain robustness,
real-time index updates, and unified frameworks that seam-
lessly integrate sparse, dense, and multimodal representations.
5) Training:Training of Retrieval-Augmented Generation
(RAG) models has coalesced into five interrelated paradigms,
each addressing distinct trade-offs in performance, efficiency,
and domain applicability.
Joint end-to-endtraining optimises retriever and generator
components simultaneously by minimising a combined neg-
ative marginal log-likelihood loss, often through expectation-
maximisation loops that alternate reader and retriever updates
[1], [72]. Although this yields cohesive alignment of retrieval-
generation and can leverage implicit retrieval supervision, it
incurs a high computational cost due to frequent document-
encoder refreshes and requires careful weighting between
retrieval versus generation gradients to avoid collapse of one
component [47], [119].
Modular two-stageapproaches decouple training—pre-
fine-tuning dense retrievers (e.g., DPR) before generator tun-
ing—trading end-to-end optimality for pipeline stability and

21
simplified hyperparameter search [44], [57]. Although this
separation can ease convergence and allow retrieval-specific
objective design, it may lead to suboptimal global coordination
and requires additional engineering to integrate retrieval scores
during generation.
Parameter-efficient fine-tuning(PEFT) andinstruction
tuningtechniques update only a small subset of model pa-
rameters, via low-rank adapters (LoRA), prefix-tuning, or
lightweight mapper modules, dramatically reducing GPU
memory requirements while preserving downstream perfor-
mance [23], [52]. These methods have been successfully ap-
plied in financial forecasting (e.g., StockGPT) and clinical QA,
yet remain sensitive to adapter rank, learning-rate schedules,
and the diversity of instruction data used [95], [143].
Specialized training objectivesincrease standard cross
entropy with contrastive losses (to distinguish relevant from
irrelevant documents), self-critical sequence training (SCST)
for sequence-level rewards and analogy or style-aware losses
to capture higher-order relations or lexical emphasis [61],
[117]. Such multiobjective schemes can yield significant gains
in task-specific metrics (BLEU, CIDEr, accuracy) but intro-
duce additional hyperparameter tuning complexity and obscure
training dynamics.
Domain and modality specific adaptationtailors RAG
pipelines to code (e.g., RepoCoder in large codebases [86]),
vision-language (e.g., ReViLM’s gated cross-attention for ra-
diology reports [116]), and specialised legal or biomedical
corpora [102], [132]. Although these systems achieve state-
of-the-art benchmark results, they face challenges in data
scarcity, overfitting, modality alignment, and cross-domain
generalisation.
Collectively, these training paradigms illustrate the field’s
evolution from monolithic joint optimisation to modular,
resource-aware and domain-focused strategies, each of which
presents open problems in objective balance, compute effi-
ciency, and transferability that continue to drive RAG research
forward.
6) Generation Model:Since its introduction, retrieval-
augmented generation (RAG) has evolved from a proof-
of-concept dual-encoder retriever paired with an en-
coder–decoder backbone to a rich landscape of end-to-end
retrieval–generation pipelines. The original RAG framework
demonstrated that the addition of a T5-style encoder-decoder
with an open-domain retriever markedly improved question
answers over purely generative baselines [62]. Fusion-in-
Decoder subsequently refined this approach by fusing multiple
retrieved passages via late-stage cross-attention, yielding more
coherent multidocument summaries [145]. In parallel, models
with decoder only, such as RETRO, showed that fragment level
retrieval could be interleaved within autoregressive decoding,
laying the groundwork for lightweight, scalable RAG in con-
versational settings [146]. More recent work like Self-RAG
has pushed toward self-supervised alignment of latent retrieval
signals, bypassing external supervision, and underscoring a
trajectory from loosely coupled retriever–generator pairs to
fully integrated systems [33].
Beyond open-domain QA and summarization, highly cited
studies have extended RAG to specialised and multimodaltasks. In biomedicine, BioGPT applied retrieval-augmented
generation to clinical question answering, demonstrating im-
proved factuality on medical benchmarks [147]. Legal research
platforms such as Lexis+ AI and Ask Practical Law AI have
tailored retrievers to statutory and case law corpora, helping
practitioners with contextually grounded legal drafting [111].
Code-centric work, using models like CodeT5 and Codex, has
recovered API documentation at generation time, enhancing
code synthesis and reducing syntactic errors [148], [149].
More recent multimodal RAG approaches (e.g. MiniGPT-4,
LLaV A, Qwen2-VL) incorporate image retrieval to support vi-
sually grounded question answering, pointing to an expanding
modality scope within the RAG paradigm [150]–[152].
7) Generative Model Families:Since the original RAG
paper, model families have proliferated, each contributing
distinct architectures, scale points, and fine-tuning strategies
that shape retrieval integration:
Anthropic (decoder-only).Claude-3-Opus and Claude-
3.5-Sonnet (2024) interleave retrieved context with safety-
orientated controls to mitigate hallucinations in conversational
QA [153].
BigScience (encoder–decoder).Bloom (2022) provided a
multilingual, multisize foundation; RAG adapters later enabled
domain-agnostic retrieval experiments atop this family [154].
DeepSeek (decoder-only).DeepSeek-V2-Chat (2024) em-
beds a lightweight retriever within a proprietary autoregressive
backbone, optimising low-latency RAG for chatbots [155].
EleutherAI (decoder-only).GPT-J (2021) and GPT-Neo
variants served as open alternatives to evaluate the impact of
retrieval on QA without instruction tuning [156].
Google (encoder–decoder & decoder-only).Flan-T5 (base
to XXL, 2022) set the standard for cross-attention fusion in
summarization and QA [145], [157], while PaLM-2 (XXS
to 540B, Text-Bison) and the Gemini/Gemma chat series
(2023–24) explore retrieval adapters in massive decoder-only
contexts [158].
Meta AI (encoder–decoder & decoder-only).BART
(2020) pioneered the integration of retrieval through cross-
attention [159]. The Llama family (2023-2025)—Llama-1/2/3
in sizes 7B to 70B, with LoRA and quantised variants—
illustrates how scale and parameter-efficient fine-tuning affect
RAG on conversational and QA tasks [160]–[164].
Mistral AI (decoder-only).Mistral-7B (2023) and its In-
struct, quantised, and Mixtral-8×7B ensemble (2024) probe
the trade-offs between open source accessibility, instruction
alignment, and retrieval fluency [165], [166].
Nomic AI & NVIDIA.GPT4All (2025) offers on-device
prototyping for lightweight RAG [167]. NVIDIA’s NeMo
GPT-43B (2023) and Llama3-ChatQA (8B/70B, 2024) com-
bine large-scale proprietary pre-training with retrieval-aware
objectives for enterprise applications [29], [121].
OpenAI (decoder-only).From GPT-2 (2019) through GPT-
3/3.5 (2020), ChatGPT/3.5-turbo (2022), to GPT-4/GPT-4o
(2023–24), OpenAI has incrementally embedded retrieval:
early work pre-generated snippets to GPT-2 input [168], while
GPT-4-turbo (2024) dynamically issues retrieval calls via
system prompts [169]–[171].

22
Qwen-1.5 (decoder-only).The Qwen-1.5 lineup (0.5B to
72B, chat variant) explores multilingual retrieval for both text
and code generation [172].
Despite this rich diversity, two broad patterns emerge. En-
coder–decoder models (Flan-T5, BART, Bloom) excel at mul-
tipassage fusion via cross attention, making them well suited
for tasks demanding precise grounding (e.g., summarization,
QA). Decoder-only families (GPT-J to GPT-4, Mistral, Claude)
leverage token-insertion or adapter-based retrieval, trading
architectural simplicity, and inference speed for conversa-
tional flexibility. Open challenges persist: the absence of a
unified, modality-spanning RAG benchmark suite; systematic
evaluation of retrieval noise versus generation fluency; and
thorough study of parameter-efficient fine-tuning (e.g. LoRA,
quantisation) on RAG outcomes. Addressing these gaps will
be critical to guide the next wave of retrieval-augmented
generation research.
B. What are the innovative methods and approaches compared
to the standard retrieval augmented generation?
The record-breaking pace of work on RAG is no longer
about proving that some retrieval helps large language models,
but about how we can make retrieval more adaptive, con-
trollable, trustworthy, and efficient. We synthesise the main
messages that emerge when we contrast their contributions
with the canonical DPR + seq-to-seq pipeline (one-shot top-k
retrieval concatenate passages). We organise this section along
the RAG data flow: preparation, retrieval, control, memory, or-
chestration, optimisation, and emerging multimodal frontiers.
1) Pre-retrieval & Post-retrieval Stages – the plumbing that
keeps RAG watertight:When a clinical chatbot invents a drug
dosage, the root cause is often not the language model but
a silent pre-processing step that mangled the source PDF.
The unglamorous work that happensbeforethe first similarity
search andafterthe hit list come back, therefore, deserves as
much care as fancy retrievers or generators.
a) Pre-retrieval: how we feed the index:Structure-
aware chunking.Pipelines now segment along headings,
tables and coherent narrative blocks detected by multi-
modal (vision-text) encoders; on FinanceBench, element-
aware chunking achieved 84.4% page-level retrieval accuracy
and 53.19% manual Q&A accuracy, outperforming token-only
baselines [144].
Metadata enrichment at chunk time.Generate keywords
and micro-summaries for each chunk automatically (e.g. with
GPT-4) to aid retrieval and avoid manual labelling; element-
aware pipelines use these metadata during indexing [144], and
retrieval augmentation has substantially increased accuracy in
clinical deployments (e.g., GPT-4 from 80.1% to 91.4%) [88].
Curated corpus construction.Restrict retrieval to
sentence-level snippets from authoritative clinical guidelines
and other public sources; by indexing only such content,
domain assistants avoid introducing protected health informa-
tion and curb hallucinations by grounding answers in vetted
guidance [89], [98].
Longer retrieval units/chunks.Treat each PDF or cluster
of interlinked pages as a long “retrieval unit” (≈4k tokens).This 30-fold reduction in retrieval units (for example, from 22
million to 600 thousand) dramatically lowers the retriever’s
workload while preserving or even improving recall, for
example, answer-recall @ 1 increases from 52% to 71% in
Natural Questions and answer-recall@2 from 47% to 72% on
HotpotQA [66]. LongRAG achieves comparable exact-match
performance, EM of 62.7% on NQ and 64.3% on HotpotQA,
without additional training [66].
Security at the retrieval interface.Obfuscated code IDs,
L2-normalised embeddings and poison filters remind us that
the retriever, not the LLM, is the outer security wall [59], [82],
[84].Defend the entry point.By obfuscating code identi-
fiers, applying L2-normalisation to embeddings, and filtering
poisoned content, the retriever serves as the primary line of
defence in retrieval-augmented systems [59], [82], [84].
b) Post-retrieval: what we pass to the model:Re-
ranking of retrieved evidence.Employ reciprocal rank fu-
sion or listwise autoregressive rankers to reshuffle retrieved
evidence so that the most relevant passage appears first;
this yields steady, low-cost improvements in accuracy and
comprehensiveness [36], [44], [68].
Context reduction and token budgeting.Apply sentence-
level context filtering (e.g. FILCO), one-line hints, or fast
extractive summaries to reduce token usage while preserving
factual accuracy and coherence [43], [45], [64].
Utility-based passage selection.Employ lightweight utility
scorers that decide whether to drop, keep, or evenrepeatpas-
sages; a learnt “bridge” model edits passage IDs dynamically,
keeping the prompt short while adapting to LLM preferences
[27], [46].
Noise-aware inclusion of unrelated passages.When the
context allows, inserting a small number of unrelated passages
can improve the accuracy of the answer in RAG; one study
reports gains of up to 35% when random documents are added
to the prompt, with the effect depending on position and count
[70].
Early verification with local regeneration.A lightweight
verifier LM diagnoses whether errors stem from retrieval (ir-
relevant knowledge) or grounding (unfaithful use of retrieved
knowledge), and triggers only the needed correction - re-
retrieve or regenerate [67].
Adaptive context-window management.Use a budget-
aware consolidator to setkto the space remaining in
the prompt-trimming, merging or compressing passages as
needed-so that the pipeline works across small and large
context windows (for example, 4k-8k and beyond) [76], [119].
These plumbing stages create a token-efficient, high-recall
foundation that underpins adaptive, controllable, and cost-
effective RAG architectures. This groundwork addresses the
retriever directly on how to make retrieval more trustworthy
and efficient in practice. In the next section, we examine how
intelligent prompt and query strategies transform the front end
of RAG into an active programmable interface.
2) Prompting & Query Strategies-making the front-end
intelligent:Standard retrieval-augmented generation (RAG)
typically issues a single literal query, retrieves top-kpassages,
and concatenates them into a fixed prompt for generation. This
baseline often treats the prompt as a static container rather than

23
an instrument for steering retrieval and inference. In contrast,
recent prompting and query strategies reconceptualise the
prompt as an active control interface that selectively modulates
grounding, reformulates queries, and sequences reasoning with
tool use. Consider the question “How many valves does the
human heart have?” In RAG, performance is driven less by
model size than by two factors: how we frame the query
(which controls what is retrieved) and how strictly we require
the model to use the retrieved evidence.
Flexible grounding and structural prompting.RAG re-
duces hallucinations on knowledge-intensive tasks by condi-
tioning answers on retrieved passages and enabling provenance
attribution, yielding more factual outputs than parametric mod-
els alone. [1].Beyond prescriptive prompting, retrieval com-
position itself can regularise behaviour: deliberately adding
irrelevant documents (“noise”) to the context can improve
answer accuracy and robustness by counteracting misleading
high-scoring passages [70]. Domain scaffolds further formalise
evidence: workflow synthesis expressed in JSON [126], organ
label tags for radiology reports [137], or hybrid text-graph tem-
plates for multi-hop knowledge-graph reasoning [58]. Com-
pared to the free-form concatenation of standard RAG, these
wrappers restrict the output format, reduce cognitive load, and
improve faithfulness by aligning the generator’s attention with
well-written evidence.
Relative to baseline RAG, structural prompting improves
relevance and robustness by imposing schemas that suppress
spurious correlations, though it may add authoring overhead
and requires schema governance to avoid brittleness in open-
domain settings [1], [58], [70], [126], [137]. Future work
should quantify how schema granularity trades off against
generalisability across domains.
Query reformulation, expansion, and selective triggering
of queriesAllowing the model to expand or rewrite a user
query typically improves recall by surfacing semantically
diverse contexts; multi-query expansion bundles, as well as
merge evidence downstream [36]. However, issuing additional
queries indiscriminately increases latency and noise. To ad-
dress this, uncertainty-aware controllers such as FLARE and
RIND+QFS trigger retrieval only when token-level entropy
spikes, thus avoiding unnecessary index lookups and focusing
retrieval on genuinely uncertain spans [25], [50]. In specialised
settings, lightweight agents first extract salient entities (for
example, disease names) and then query structured stores to
reduce vocabulary mismatch and improve precision [134]. For
streaming code completion, continuous query updates track the
evolving context so that cross-file references remain current,
a capability that standard single-shot RAG lacks [87].
Compared to baseline, reformulation and entropy-triggered
querying improve recall-precision balance and control latency,
but they rely on robust fusion or re-ranking to prevent evidence
dilution when multiple queries are issued [25], [36], [50],
[87], [134]. Open questions include how to calibrate entropy
thresholds across domains and how to amortise multi-query
costs under tight latency budgets.
Example-augmented prompting (retrieval-augmented
in-context learning).Retrieval-augmented in-context learning
dynamically inserts near-neighbour exemplars while assem-bling the prompt. Systems such as R-GQA and MolReGPT
incorporate similar question–answer pairs, improving accuracy
at a modest token cost [37], [103]. Time-aware variants add
hard negative examples so that the model learns when not to
retrieve, mitigating over-reliance on stale or irrelevant context
[57]. Confidence-conditioned prefixes further allow the gener-
ator to modulate trust in retrieved snippets by signalling low
certainty, which reduces the risk of over-fitting to misleading
passages [116]. Relative to standard RAG, which typically
lacks task-specific exemplars, these strategies better align the
prompt distribution with the current query manifold.
Example-augmented prompting enhances relevance and ro-
bustness, particularly for specialised or temporally sensitive
queries, but raises curation questions (which exemplars, how
many and how to manage drift) and requires careful token
budget management to avoid context saturation [37], [57],
[103], [116]. Promising directions include adaptive exemplar
selection driven by utility estimates rather than fixedk.
Deliberate reasoning before retrieval.ReAct-style
prompts interleave THOUGHT, ACTION, and OBSERVATION,
allowing the model to plan tool calls, execute retrieval, and
revise its plan iteratively [85]. Graph-of-Thought extends this
idea by decomposing the question into sub-problems, each
with a targeted retrieval hop, before composing a final answer
[133]. These patterns depart from the standard RAG one-
pass pipeline by explicitly sequencing reasoning and evidence
gathering. However, such scaffolds can accidentally expose
sensitive content if intermediate thoughts are logged or re-
flected back to the user, underscoring the need for strict access
control and privacy-aware prompt design [24].
Reason-first strategies improve multi-step fidelity and re-
duce retrieval of irrelevant context by aligning evidence to
sub goals, at the cost of additional control complexity and
potential privacy risks if traces are not properly contained [24],
[85], [133]. Future research should formalise safety-preserving
variants that preserve trace benefits without leaking private
artefacts.
Operational policy, fusion, and safety.Empirically, ex-
plicit prompt policies, such as clear grounding clauses, zero-
temperature reasoning steps and domain-specific wrappers,
often match or exceed the benefits of introducing a new
retriever [97]. However, query expansion must be paired with
fast fusion or re-ranking to curb latency and maintain precision
as the number of evidence candidates grows [36]. Field-
specific schemas (for example, ECG JSON blocks in cardiol-
ogy) improve reliability in safety-critical applications relative
to open completions [90]. Finally, the prompt itself is an attack
surface; sanitising complex instructions and constraining tool
outputs are, therefore, mandatory operational controls.
Overall implications for the research question.Across
these categories, innovative prompting and query strategies
advance, challenge, and in some cases redefine standard RAG
by (i) making grounding adaptive and schema-aware, (ii)
coupling query reformulation with uncertainty-aware trigger-
ing, (iii) leveraging exemplar retrieval to shape the prompt
distribution, and (iv) sequencing reasoning to target retrieval
more precisely. In general, these methods often yield larger
improvements per unit cost than architectural changes, partic-

24
ularly when prompts are treated as versioned, testable artefacts,
as code would treat, so that RAG systems become more
controllable, economical, and safe to deploy [36], [90], [97].
3) Hybrid and Specialised Retrievers: No single needle-
finder:Early RAG systems typically rely on asingle, dense
passage retriever whose top-kchunks are appended, wholesale,
to the generator input. A striking commonality in the more
recent literature is the rejection of this monolithic design in
favour ofhybrid retrieval: lexical and dense signals are
combined, cascaded or adaptively weighted, often alongside
domain-specific similarity functions or graph indices. The
consensus that emerges is clear: no single similarity metric
can surface every useful evidence fragment.
Work in the clinical domain illustrates the value of score-
level fusion: MEDRAG aggregates BM25 with up to three
dense retrievers by Reciprocal Rank Fusion and records 3 to
6 percentage points gains in top-5 recalls for medical QA [97].
A more generalisable variant is theBlended Retriever, which
stitches together BM25, KNN-dense, and sparse-encoder in-
dices behind a unified API; exhaustive sweeps over six query
formulations reveal that the fused output consistently outper-
forms the best individual index, without task-specific fine-
tuning [60]. Similar ideas appear in open-source toolkits such
as Auto-RAG, which expose multiple indices at runtime and
leave the choice to a lightweight policy learner or the user [26].
These studies collectively suggest that recall drops caused by
the “long tail” of lexical variability can be mitigated without
costly supervision, provided that one is willing to maintain
multiple indices.
Several papers move beyond static mixtures and train the
system toadaptivelydecide where to sample evidence. In
event argument extraction, Adaptive Hybrid Retrieval sam-
ples pseudo-demonstrations from continuous semantic regions
defined jointly over document and schema embeddings, de-
livering a five-point F 1improvement over nearest-neighbour
baselines [65]. A complementary strategy, introduced for legal
case reasoning, learns dual embeddings: one space captures the
similarity between questions, the other captures the affinity
between questions and support. Then optimises a weighting
scheme that can privilege either signal depending on the input
[102]. Such results hint at a future in which hybrid retrieval
islearnedrather than manual operation.
Hybridisation is especially powerful when it exploits struc-
ture that generic dense vectors cannot easily encode. For
knowledge-graph question answering, a dual-level pipeline
first retrieves entity neighbours or thematic nodes using key-
word matching, then refines the candidate set with vector
similarity; this combination captures both symbolic locality
and semantic relatedness and proves markedly more accurate
than flat chunk retrieval [31]. In code intelligence, lexical
overlap remains a robust signal of syntactic similarity, whereas
a fine-tuned dense retriever better captures semantics; a two-
stage hybrid first filters with BM25 and then re-ranks with
a CodeT5-based encoder, cutting irrelevant patches by more
than one third [79]. Multimodal cascades follow the same
philosophy: an image-to-text system uses CLIP similarity to
shortlist images whose titles match a visual prompt, then
applies a text encoder to retrieve the precise passages requiredfor answer generation [63].
Hybridisation also affectshowevidence is consumed.
RETRO++ routes the single most relevant chunk directly to
the decoder, where it can influence every token, while sending
additional passages to the encoder as background context,
yielding significant gains on open-domain QA without increas-
ing sequence length [20]. Such architectural nuances reinforce
the broader lesson that retrieval and generation cannot be
optimised in isolation.
Although the quality gains are unambiguous, hybrid de-
signs are not free. Maintaining several indices requires more
memory and imposes separate refresh cycles; empirical studies
report end-to-end latency increases of 5-50 ms per query on
commodity GPUs. Where low latency is mandatory, selective
trigger policies, e.g., avoiding dense retrieval for purely factual
lexical queries, recover much of the benefit at a fraction of the
cost [45]. However, very few papers measure index update
overhead or the engineering effort needed to keep blended
systems in sync with evolving corpora.
Two methodological gaps remain. First, cross-domain ro-
bustness is largely untested: most hybrids are tuned and
evaluated on the same corpus, leaving questions about their be-
haviour when the domain shifts. Second, security aspects, how
fusion strategies cope with poisoned subindices or adversarial
trigger documents, are almost entirely unexplored. Bridging
these gaps will require shared benchmarks that couple quality
metrics with latency, energy, and robustness reporting.
The evidence base demonstrates that retrieval heterogeneity
is a virtue: lexical scoring anchors precision, dense vectors
widen semantic recall, structure-aware indices inject domain
priors, and increasingly, learnt policies decide which mixture
to trust. Treating retrieval composition as a first-class, config-
urable module, rather than a line in the appendix, appears to
be essential for the next generation of reliable and efficient
RAG systems.
4) Structure-aware & Graph-based RAG: "Talk to me in
triples, not tokens":A growing strand of work argues that
retrieval-augmented generation should reason overrelations
rather than over flat passages. By turning documents, captions
or code into nodes and edges, these systems place LLMs in
environments where neighbourhood, path and provenance are
explicit. The result is a family ofstructure-awareorgraph-
basedRAG pipelines that differ from the canonical baseline
DPR + seq2seq at every stage, from indexing to decoding.
The first departure is at retrieval time. Instead of rank-
ing passages in isolation, systems such as G-RETRIEVER
construct a minimal connected sub-graph that already en-
codes multi-hop context before it is shown to the LLM
[52]. Knowledge-Graph Prompting extends the idea to ad hoc
graphs built on whole document collections, thereby recover-
ing passages that are jointly rather than individually relevant
[56]. Biomedical variants prune domain KGs aggressively:
KG-RAG selects only the “prompt-aware” neighbourhood of
SPOKE, halving token expenditure without loss of precision
[134]. Across these studies, the lesson is consistent: a few
well-chosen triples beat many loosely related sentences.
Once a graph has been selected, it must align with the token
world of the generator. Two strategies dominate.Soft-prompt

25
projectionfeeds the LLM a dense prefix derived from a Graph
Neural Network encoder; Graph Neural Prompting shows that
a learned projector lets the language model attend to sub-
graph semantics without retokenising long edge lists [54]. In
contrast, mixed-modal encoders treat each document embed-
ding as a latent token. The xRAG architecture concatenates
a textual view with such projected embeddings, while RAG-
Token marginalises over latent documents so that each gener-
ated word may be conditional on a different evidence source
[78]. These designs blur the boundary between retrieval and
generation, but they also introduce computational overhead:
fast approximate decoding is now an open system challenge.
Manual curation of graphs is untenable, so recent work
automates their creation. A graph-based text indexer seg-
ments documents, extracts entities and relations with an LLM,
then maintains the structure as a hybrid keyword and vector
store that supports both lexically exact and semantic queries
[31]. Customer-service pipelines construct dual-level graphs in
which intra-ticket trees are inter-linked via clone or reference
relations; a two-step process retrieves a sub-graph and then
issues Cypher queries for precise answer extraction [139].
In code intelligence, static analysis graphs are fused with
retrieved exemplars so that program repair models reason
simultaneously over abstract syntax and concrete fixes [41],
[79]. Across these domains, the “graph first, dense fallback”
has become the pragmatic recipe: traversal is attempted, but
vector similarity remains a safety net.
Structure-aware RAG is also proving its worth beyond text.
Vision language pipelines ground image regions in Wikidata
entities using a CLIP-based retriever, allowing captioning
models to cite explicit facts rather than hallucinating [74].
Multimodal captioning systems encode images, retrieved cap-
tions, and their cross-caption relations in a single transformer,
improving rare-concept coverage and faithfulness [104], [140].
These studies confirm that the graph perspective can bridge
modality gaps as well as logical ones.
The empirical gains are grouped around three themes.
First, the answerfaithfulnessrises when the model can quote
paths or node identifiers, giving analysts concrete error traces
[52], [75]. Second,token efficiencyimproves because graph
neighbourhoods are far denser information carriers than flat
chunks; prompt length drops by 40-60% in biomedical QA
[134]. Third, graphs offer natural hooks forexplainability:
Users can inspect which edge or entity grounded a statement,
an impossibility when evidence is a text passage spanning
many pages.
However, significant obstacles remain. Current pipelines are
based on the linkage of weak entities and the cost of stale
or mislinked nodes. Incremental update algorithms exist [31],
but their impact on answer drift over months is unknown.
Finally, evaluation practices lag: while factual QA has BLEU
and EM, graph RAG lacks agreed metrics for edge coverage
or topological correctness, hindering cross-paper comparison.
We expect structure-aware RAG to converge on three design
principles: lightweight on-the-fly KG construction; learned
policies that choose pragmatically between graph traversal and
vector search; and plug-in projection layers that make any
LLM “graph-ready” without bespoke retraining. As modalitiesproliferate—tables, time series, 3-D scenes—the foundational
insight stays the same: represent knowledge in the form that
preserves its relations, then let the language model converse
in that richer vocabulary.
5) Iterative & Active Retrieval Loops: From Static Context
to Conversational Search:Work published during the past
two years reveals a decisive migration from the traditional
“retrieve-then-generate” pipeline towardsclosed-loopsystems
in which LLMs continually query external knowledge, inspect
their own draughts, and revise both the retrieval context and
the answer. These approaches treat the retriever not as a one-
off helper but as a conversational partner that can be invoked,
ignored, or re-invoked in response to model uncertainty, veri-
fication feedback, or evolving sub-goals.
A first line of work equips the generator withuncertainty
trigger. In FLARE, the model examines each newly generated
sentence for high-entropy spans; when token-level uncertainty
exceeds a threshold, it halts generation, masks those spans,
emits a focused search string to the retriever, and then regen-
erates the sentence [50]. A related attention-based mechanism,
RIND+QFS, similarly uses uncertainty triggers to decide when
and which tokens should form subsequent queries, improv-
ing recall without compromising precision [25]. Real-time
Information Needs Detection (RIND) combined with Query
Formulation by Self-attention (QFS) generalises this idea by
blending token-level entropy with self-attention salience to
decide when to retrieve and which tokens should form the
query [25]. In these designs, the model literallyemits a search
string token(e.g. <SEARCH> how many valves in the human
heart?), which the orchestration layer interprets as a call to the
retriever, giving the loop an explicit and inspectable hand-off
point.
SELF-RAG uses reflection tokens (Retrieve,ISREL,
ISSUP,ISUSE) to trigger retrieval, assess evidence and cri-
tique outputs, giving segment-level control [33]. The biomed-
ical Self-RAG further extends the mechanism by training a
domain-specific critic language model whose reflection tokens
signal both the need for retrieval and the subsequent relevance
of the evidence [91]. Collectively, these studies demonstrate
that trigger on model uncertainty recovers the majority of
the accuracy gains of full iterative pipelines while invoking
the retriever only when it is genuinely useful. Parallel work
on agentic systems confirms this principle: SELF-RAG [33],
DRAGIN [25] and TA-ARE [57] introduce explicit decision
tokens, entropy thresholds and veto classifiers that suppress
unnecessary searches, trimming 15–45% of context tokens
with negligible loss in fidelity.
A second group of research emphasisesiterative refinement.
The CHAIN-OF-NOTE (CON) framework obliges the LLM
to write concise “reading notes” for each retrieved document,
thereby exposing document reliability and reducing hallucina-
tion before synthesis of the final answer [71]. Batch grounding
strategies process evidence in successive mini-batches, stop-
ping as soon as adequate justification is found and injecting
the progressively revised answer back into the context, a tactic
that curbs noise and token bloat [38]. RAT performs a stepwise
revision of an explicit chain of thought, generating a new query
for each reasoning step and localising corrections instead of

26
rewriting entire explanations [22]. Verification-driven loops
such as KALMV enact automatic error rectification: if a
verifier flags a retrieval or grounding fault, the pipeline re-
retrieves new passages or re-generates the answer until the
verifier is satisfied, closing the loop on both failure points
[67]. Agentic pipelines strengthen this pattern by exposing
each stage—retrieval, reranking, refinement and generation as
discrete—inspectable actions inside modular toolchains such
as RALLE [30] and MEDRAG [26], making revision steps
debuggable and reusable.
When the original query is too sparse or ambiguous for
high-recall retrieval,generation-augmented loopsbecome ef-
fective. ITER-RETGEN feeds the intermediate draught of the
model back to the retriever, providing increasingly informative
queries at each turn [47]. ITRG offers two complementary
modes.Refine, which updates an existing draft with only newly
retrieved documents.Refresh, which starts afresh from the lat-
est evidence. This shows that alternating between these modes
improves long-form document generation [117]. RepoCoder
adopts the same principle for code completion, appending the
most recent code continuation to the retrieval query so that
cross-file context converges towards the intended target snippet
[87].
A fourth strand decomposes the original task into smaller
sub-problems and retrieves evidence in amulti-hopfashion.
RA-ISF first checks whether the LLM already knows the
answer, then filters irrelevant passages, and finally decomposes
unanswered questions into simpler subquestions, recursing
until each leaf is resolved [40]. SearChain externalises the
reasoning trajectory as aChain-of-Querytree, allowing the
IR engine to verify or veto each hop and permitting back-
tracking when evidence contradicts prior steps [49]. Graph-
oriented systems traverse knowledge graphs node by node,
either through an LLM-guided agent [56] or via a divide-and-
conquer ego-graph search with learnable pruning [58], thereby
combining symbolic relational structure with neural retrieval.
Reason–act loops in the agentic literature echo this multi-hop
spirit, alternating between planning, external tools and answer
revision to accumulate evidence from diverse sources—for
instance a ReAct-style clinical assistant [118] or the Retrieval-
augmented Recommender System [120].
Finally, several papers exploitself-consistency or memory.
SelfMem alternates between producing multiple candidate
memories and selecting the best one to seed the next round of
generation, enabling the model to bootstrap its own knowledge
without external corpora [35]. A related idea is used in
activity-pattern generation, where multiple hypothetical tra-
jectories are rated for alignment with historical data before
the most self-consistent plan is chosen [51]. The Knowledge-
to-Response architecture separates knowledge prediction from
response generation, gives an explicit checkpoint that can be
inspected or re-executed if downstream verification fails [116].
Across these diverse implementations, a set of common
lessons emerges. First, retrieval should bepolicy-driven: sys-
tems that fire the retriever only under measured uncertainty or
verified need to gain most of the quality benefits at a fraction
of the computational cost. Second,localrevision—editing one
thought, sentence, or document at a time—prevents promptlengths from exploding and keeps provenance transparent.
Third, closed loops demand fail-safes: lightweight critic LMs
or verifiers effectively halt divergence when early retrieval
or generation steps go wrong. Lastly, latency and energy
budgets vary dramatically between designs; rigorous reporting
of retrievals-per-answer, wall-clock delay, and GPU minutes is
essential if future work is to compare accuracy improvements
on an equal footing.
These iterative and active retrieval loops recast RAG as
aninteractive search companion. By recognising their own
knowledge gaps, gathering fresh evidence on demand, and
continuously revising their reasoning, modern RAG systems
approach the discipline of a human researcher. The next
frontier is to make these loopsbudget-awareand embed them
in evaluation frameworks that reward knowledge fidelity and
resource efficiency.
6) Memory-augmented RAG: Personalisation and Long-
Horizon Context:Early retrieval-augmented systems were
stateless: each turn re-embedded the user’s query, retrieved
passages, concatenated them, and produced an answer. How-
ever, domains like education, clinical care and personal assis-
tance benefit from knowledge that accumulates and varies by
user. Thus, a family ofmemory-augmentedRAG architectures
has emerged, persisting dialogue turns, sensor readings, search
history or model-generated thoughts beyond a single query.
One line of work introducesshort-horizon conversational
buffers. In education, MoodleBot allocates a vector store
per course and rewrites follow-ups into standalone queries
that include recent turns; students rate its coherence far
above a buffer-free baseline [136]. Likewise, LangChain’s
ConversationBufferMemoryretains the chat transcript
for retriever and generator use, boosting F 1by over eight
percentage points in follow-up QA benchmarks, largely across
domains [138].
Beyond fleeting context, some systems maintainpersistent,
user-specific memories. LiVersa’s hepatology assistant sepa-
rates long-term documents (e.g. discharge summaries), short-
term signals and a dynamic slot of the fifteen latest queries.
Selective retrieval from these stores cuts hallucinations by∼25
per cent and halves prompt length [96]. The entity-centric
storeK Etimestamps canonicalised entities from browsing
history, storing compact IDs rather than raw text; this achieves
personalisation with strong privacy and mere megabytes per
user [99]. Similarly, the agentic BRAINlogs every percep-
tion–thought–action tuple and recalls them to aid planning in
complex optimisation tasks [107].
Another approach embeds memory within the model. Re-
trieval Augmentation Mechanism (RAM) for video captioning
initialises a key–value store with hidden states from teacher-
forcing; at inference the decoder attends this store, injecting
linguistic and visual cues and raising CIDEr by nearly 10
per cent on MSR-VTT [77]. SELFMEMappends its own
generations to a growing memory pool, lowering retrieval
latency over time while BLEU keeps improving [35]. A
clustered memory module groups millions of examples into
centroids, allowing soft interpolation or hard selection so the
generator exploits abstracted task knowledge rather than a few
nearest neighbours [125].

27
Despite gains in coherence, relevance and efficiency, open
challenges remain. Few works addressmemory governance:
LiVersa encrypts clinical memories at rest andK Eavoids raw
text, yet no standards exist for retention, revocation or audit.
The second issue isforgetting: none of these works imple-
ment principled eviction or decay, despite stale or erroneous
memories causing model drift. Finally, evaluation stays nar-
row—accuracy metrics dominate, while longitudinal measures
(trust calibration, drift detection, catastrophic memory errors)
are seldom reported.
The memory-augmented RAG shifts from “answering the
current question” to “accompanying the user over time”.
Whether through lightweight buffers, structured personal
knowledge graphs, or train-time key value abstractions, in-
tegrating memory with retrieval and generation paves the way
for truly adaptive, user-centred assistants. To move beyond
prototypes, these systems must tackle privacy, life-cycle man-
agement, and long-term robustness.
7) Agentic & Multi-tool Pipelines: Orchestrating Reason-
ing, Tools and Memory:Where the previous sections zoom in
onwhatto retrieve (hybrid indices, structure-aware graphs),
whento retrieve (uncertainty-driven loops) andwhereto store
past context (memory buffers and personal knowledge bases),
the emerging notion of anagentasks a broader systems
question:How can a language-model controller weave all
of these capabilities, including retrievers, memories, external
APIs, calculators, and even other LLMs, into a single adaptive
execution plan?
Under the hood, each agent exposes a toolbox of hetero-
geneous capabilities. Hybrid retrievers supply both lexical
and dense evidence; structure-aware traversals explore knowl-
edge graphs; memory stores cache past interactions; and do-
main plugins execute arbitrary APIs—from code compilers to
database queries. For example, MEDRAG’s laboratory orches-
trates five discrete steps (Judger,Retriever,Reranker,
Refiner,Generator) in a fixed graph, while RALLE
provides practitioners with a drag-and-drop canvas to create
custom pipelines in real time [26], [30].
How does the controller decide its next move? Research
is grouped around three design patterns. In static graphs, the
flow is scripted (for example retrieve - rerank - generate), but
nodes can be toggled at runtime (for instance, switching from a
general-purpose index to a proprietary one when domain drift
is detected). Dynamic planning agents interleave THOUGHT,
ACTIONand OBSERVATIONtokens, letting the model plan
each step—should it consult the calculator or dive into long-
term memory next? And learned controllers treat tool selection
as a reinforcement-learning problem, optimising for latency,
cost and accuracy under real-world constraints [118], [120],
[139].
Memory is not an afterthought but a peer of retrieval.
Short buffers prevent conversational dead ends, but true agency
emerges when the system logs every perception-thought-action
tuple for hours—or even days. LiVersa’s hepatology assistant
splits data into long-term documents, streaming vitals and a
sliding window of recent queries; the result is a 50% reduction
in hallucinations and half the prompt length [96]. The BRAIN
architecture goes further, treating each memory as an explicitaction token that the agent can revisit when planning complex
optimisation tasks [107].
Orchestration unlocks tangible benefits and magnifies new
risks. On the upside, agents can superintend long-horizon
workflows (from syllabus design to lab automation), hot-swap
tools when one fails, and gracefully fall back on alternative
evidence sources. However, this flexibility invites debugging
nightmares: tracing a misstep through a branched execution
graph is much harder than inspecting a single “retrieve-
then-generate” call. Credit assignment across cascaded tools
remains unresolved, and persistent memories demand rigorous
governance for retention, revocation and audit [99].
Looking ahead, agentic RAG must mature from ad hoc
scripts to dependable infrastructure. We need vendor-neutral
DSLs to describe tool graphs, unified dashboards that report
accuracy alongside latency, energy consumption and privacy
metrics, and formal memory policies that prevent drift and
data leakage. Once these scaffolds are in place, controllers will
be free to juggle dozens of modules—truly turning retrieval-
augmented models into retrieval-augmented systems.
8) Efficiency & Compression—token budgets still matter:
The first time a production team wired a 32 K-token model
into its help-desk bot, the GPU bill doubled overnight. The
lesson landed quickly: long contexts feel free, but every extra
symbol still burns memory, latency, and cash. Recent papers
therefore chase leaner recipes that keep answers faithful whilst
maintaining efficiency [21], [32], [83].
Why carry an entire document when a single learned vector
will suffice? xRAG maps each retrieved passage to a single
document token, reducing the retrieved context from roughly
175 tokens to one and delivering task performance comparable
to uncompressed RAG, while also lowering compute (a3.53×
reduction in GFLOPs) and improving speed (a1.64×speed-up
in CUDA time) [21]. Biomedical variants prune entire graph
branches; Cypher-RAG++ restricts its prompt to “prompt-
aware” triples and nevertheless improves robustness [134].
Even simple prompt engineering helps: RAPT stores most
tunable weights in a global prefix and keeps per-example
infixes small [106].
A bloated index slows everything downstream. One group
runs an asynchronous re-encoder that refreshes FAISS shards
while the system is online, so nightly jobs never block training
[62]. Another treats megabyte-scale PDFs as single “long
retrieval units”, resulting in thirty-fold smaller indices but
the same recall [66]. Toolkits such as Parrot and Auto-RAG
now expose multiple vector stores and show that picking the
right dimensionality can improve speed better than another
hardware upgrade [23], [88], [97].
PipeRAG drags passages from the CPU while the GPU
is already decoding, roughly cutting a third off end-to-end
latency [32]. RAGCache predicts which passages are likely to
be reused, warms the key-value cache, and initiates speculative
decoding before the retriever responds. In a production trace,
this approach reportedly halved the US dollar cost and reduced
the 95th-percentile latency by 200 ms [39].
RETRO++ adjusts retrieval cadence analogously to adaptive
bitrate streaming: fetch every token for maximum quality,
or every few hundred for speed; quality degrades smoothly

28
rather than collapsing [20]. PipeRAG pushes adaptivity further,
tuning its cadence at runtime to respect a global latency budget
[32]. Other teams precompute dense knowledge stores offline,
shifting the heaviest computation away from the critical path
[104], [124].
In these approaches, compression is no longer a lossy
compromise; it is a design posture. Whether by projecting
documents into single embeddings, refreshing indices on the
fly, overlapping compute, or throttling retrieval frequency,
modern RAG systems show that frugality can coexist with
accuracy. Future benchmarks should report energy (joules)
and monetary cost alongside EM and BLEU; otherwise, we
will continue to top leaderboards whilst exceeding budget
constraints.
9) Modality Expansion – RAG Beyond Plain Text:Early
RAG systems treated all knowledge as text—until researchers
discovered that a single X-ray caption or table row can
transform a dry answer into a vivid insight. Imagine a disaster-
response chatbot that not only quotes tweets but overlays them
on live satellite imagery. This fusion is now within reach
thanks to unified multimodal backbones. MuRAG, for exam-
ple, couples a Vision Transformer with a T5 encoder–decoder
so that images and text share the same embedding space,
letting a prompt about “the mysterious lesion” fetch both
radiology reports and the relevant chest X-ray as a single
learned token projection—and it works without retraining the
language model for each modality [21], [61]. Meanwhile,
xRAG shows that whole documents—whether PDF, PNG or
CSV—can collapse into one compact token, greatly reducing
context length and memory use without sacrificing answer
quality [21].
Beyond model-level tweaks, contemporary orchestration
frameworks expose pluggable components: engineers can con-
figure CLIP-style embedding models for image/text retrieval,
Whisper-based audio transcription and HTML/CSV/Excel
loaders with minimal code changes, and then index outputs in
interchangeable vector stores. In practice, frameworks such as
LangChain provide loaders for web pages and YouTube tran-
scripts, Whisper parsers, Pandas/CSV tooling and a common
vector-store interface; this allows a single workflow to draw on
web pages, video transcripts and tabular datasets, with retrieval
improving grounding in downstream generation [23], [141].
In clinical imaging, one line of work retrieves text us-
ing contrastively pre-trained vision–language encoders (e.g.,
ALBEF) and then prompts general-purpose language models
(including GPT-4) to draft radiology findings; a separate line
develops grounded report generation that links textual findings
to specific image regions, improving traceability beyond text-
only outputs. Beyond imaging, retrieval augmentation has also
been explored for lay-language clinical communication and
explanation [137].
Yet challenges linger: CLIP-style joint spaces work well for
vision and language but falter on tables or code snippets; scale-
up strains storage budgets when every video frame becomes
an index entry; and privacy controls for sensitive modalities,
from medical scans to CAD files, have no industry standard.
Addressing these gaps will make multimodal RAG not just
possible, but dependable.10) Synthesis & Outlook:The evidence in this review indi-
cates a clear shift from the canonicalDPR + seq2seqpipeline
towards modular, policy-driven architectures. Hybrid indices
broaden coverage; structure-aware retrievers identify relations
that are otherwise difficult to detect; and uncertainty-triggered
loops request additional evidence only when model uncertainty
is high [50], [52], [60]. The combined effect is higher top-
krecall without overloading the generator with unnecessary
tokens.
Closed-loop control and lightweight critics have trans-
formed retrieval from a static pre-retrieval step into a dynamic,
in-generation process. Verifiers can filter low-information snip-
pets during generation, and memory buffers retain relevant
prior context. Early deployments in medicine and education
report reduced hallucination and improved personalisation
[96], [138]. Efficiency techniques—document projection, spec-
ulative decoding, cache-aware scheduling—demonstrate that
speed need not be sacrificed for rigour [21], [32]. Token
budgets remain a constraint; the most efficient token is the
one the generator never processes.
Despite these advances, the field continues to rely on
incomplete quality signals. Benchmarks often prioritise ac-
curacy and rarely report cost. Few studies record retrievals
per answer, GPU minutes or carbon emissions, and even
fewer analyse how compromised sub-indices may influence
agentic planning. Memory governance—retention, revocation,
audit and related controls—is seldom emphasised in system
evaluations. Without shared yardsticks, reported gains are not
readily comparable.
Future work should prioritise three directions to support
the transition of RAG systems from prototypes to dependable
infrastructure: developing holistic benchmarks that report not
only accuracy but also retrieval latency, energy consump-
tion and privacy guarantees; treating retrieval strategy as a
resource-allocation problem, with policies that respect time,
token and compute budgets rather than fetching evidence
indiscriminately; and defining open, vendor-agnostic interfaces
for heterogeneous indices (graphs, tables, images, streams) to
enable drop-in retrievers without extensive pipeline refactor-
ing.
C. What are the most frequently used metrics for evaluating
the effectiveness of retrieval-augmented generation systems?
Evaluating such hybrid architectures requires more than
standard natural language generation metrics: it requires a suite
of measures that capture both the retriever’s ability to surface
relevant evidence and the generator’s ability to weave that
evidence into factually accurate and contextually appropriate
responses. In the paragraphs that follow, we survey the most
widely adopted metrics, ranging from low-cost, repeatable
automated scores (e.g. EM, F1, BLEU/ROUGE, perplex-
ity, recall@k) to resource-intensive human judgements and
emerging LLM-as-judge protocols, and discuss their respective
strengths, blind spots, and complementarities. By mapping
out this evaluation landscape, we highlight best practices for
assembling a balanced metric rubric and pinpoint enduring
gaps that future research must address.

29
1) Overview of the Evaluation Landscape:Across our set
of RAG evaluations, metrics are grouped into three broad
types: automated, human, and LLM-as-judge, with a pro-
nounced skew toward automated measures.
Automated metrics dominate: by far the most frequent
single metric is accuracy (e.g. [27], [65], [70]), appearing
in diverse contexts from biomedical QA to commonsense
reasoning. The exact match (EM) and the F1 score are likewise
ubiquitous, serving as strict (EM) versus soft (F1) string
overlap measures in QA, summarization, code generation, and
information extraction tasks. Lexical similarity metrics such
as BLEU, ROUGE-L, are also common, while perplexity
and diversity measures (e.g. Distinct-1/2, Self-BLEU) appear
more sporadically. Automated measures are prized for their
reproducibility and low cost, but they largely capture surface
overlap or retrieval success, not deeper semantic fidelity.
Human-judged metrics appear less often, but remain crit-
ical for qualitative aspects. Approximately one third of the
articles we survey report some form of expert or crowd-rated
accuracy [94], hallucination counts [23], [141], completeness,
consistency, or user satisfaction. These metrics provide insight
into factuality, fluency, and user experience, but at the expense
of higher annotation cost and interannotator variability.
LLM-as-judge approaches are an emerging third pillar: a
handful of recent studies (e.g., [47], [76]) prompt powerful
models like GPT-4 or text-davinci-003 to score correctness,
fluency, or safety. These surrogate evaluators combine se-
mantic evaluation with automation, ideally offering a strong
correlation with human judgements, although with risks of
model bias and prompt sensitivity.
This landscape shows a clear tension: scalable, repeatable
automated metrics versus nuanced, costly, human assessments,
and with LLM-based evaluators positioned to bridge the gap.
Therefore, any comprehensive RAG evaluation should com-
bine at least one high-level retrieval or overlap metric (e.g.
recall@k, EM/F1), one semantic or embedding-based score
(e.g. BERTScore [139] or BLEURT [139]), and either a human
or LLM-mediated judgement to ensure both rigour and depth.
2) Automatic Generation Metrics:Automatic generation
metrics quantify the fidelity, fluency, and informativeness of
RAG outputs without human intervention. They fall into four
broad categories: (1) classification-based metrics, (2) overlap-
based n-gram metrics, (3) probabilistic metrics, and (4) spe-
cialised diversity and grounding metrics. Each offers unique
insight and carries distinct limitations in the evaluation of
retrieval-augmented generation.
Accuracymeasures the proportion of responses generated
that are correct in the total number of outputs. It provides
a straightforward gauge of answer correctness, although it
ignores partial matches or semantic equivalence [27], [70].
Exact Match (EM)is a stricter binary metric: it reports
the fraction of outputs that coincide exactly (character-for-
character) with one of the reference answers [1], [62]. EM
is essential in tasks demanding verbatim precision, such as
code generation or fact retrieval, but does not give credit for
near-correct paraphrases.
F1 scoreis the harmonic mean of the precision and recallof the token level:
F1= 2×Precision×Recall
Precision + Recall
Precision is the fraction of overlapping tokens in the generated
output that also appear in the reference; recall is the fraction
of reference tokens recovered in the output. F1 allows partial
credit for overlap and is widely used in QA and summarization
benchmarks (e.g., SQuAD, WebQSP) [54], [62].
BLEU(Bilingual Evaluation Understudy) measures n-gram
precision relative to one or more references and applies a
brevity penalty to discourage overly short outputs:
BLEU = BP×expNX
n=1wnlogp n
wherep nis the n-gram precision fornup to typically 4 [37],
[137]. Despite its ubiquity, the reliance of BLEU on exact
n-gram matches leads to poor sensitivity to synonymy and
paraphrase [26], [103].
ROUGE(Recall-Oriented Understudy for Gisting Evalu-
ation) emphasises recall of n-gram matches; the ROUGE-
L variant measures the longest common subsequence (LCS)
between candidate and reference:
ROUGE-L =LCS
length(reference)
ROUGE-L captures sequence-level cohesion and is especially
prevalent in summarization and long-form QA [1], [24]. How-
ever, like BLEU, it fails to capture semantic similarity beyond
surface overlap.
METEOR(Metric for Evaluation of Translation with Ex-
plicit ORdering) extends n-gram overlap by incorporating
stemming, synonym matching, and a fragmentation penalty.
Calculates a weighted F-mean of unigram matches, typi-
cally showing higher correlation with human judgements than
BLEU or ROUGE at the cost of increased complexity [37],
[137].
BERTScoremeasures semantic similarity by comparing
contextual token embeddings (e.g. RoBERTa base) between
the generated text and the reference. It computes cosine
similarities at the token level and aggregates them to produce
a single score that better captures paraphrase and meaning
overlap than surface n-gram metrics [92], [106], [137].
Perplexityquantifies a model’s uncertainty by exponen-
tiating the negative logarithmic likelihood of the generated
sequence:
PPL = exp
−1
NNX
i=1logp(w i)
Lower perplexity indicates that the model predicts the next
token with greater confidence [27], [32]. Although useful for
assessing fluency and coherence, perplexity does not directly
measure alignment with retrieved evidence or task-specific
correctness.

30
a) Specialized Diversity & Grounding Metrics:Self-
BLEUcomputes BLEU of each generation against its peers to
quantify diversity (lower Self-BLEU to higher diversity) [20],
[101].
chrF++evaluates character-level F-measure over character
n-grams, capturing fine-grained similarity in morphologically
rich settings [35].
Self-TER(Translation Edit Rate) measures the average edit
distance between multiple outputs, thus quantifying novelty
[106].
Supportlabels each generated claim fully, partially, or
not supported by the retrieved evidence, ensuring factual
grounding [33].
Rare F1andPredicted Knowledge F1 (PKF1)focus on
specialised tasks: Rare F1 emphasises performance on low-
frequency tokens, while PKF1 gauges the model’s ability to
recover explicit knowledge sentences [126].
3) Automatic Retrieval Metrics:Effective retrieval is a
prerequisite for high-fidelity generation in retrieval-augmented
generation (RAG) systems. Automatic retrieval metrics quan-
titatively assess how well the retriever component selects
and ranks relevant documents from a large corpus for a
given query. In general, these metrics fall into (1) set-based
measures, which evaluate the accuracy and completeness of
the retrieved set, (2) ranking-based measures, which assess
the ordered quality of the retrieval, and (3) hit-based measures,
which capture the presence of any relevant document within
a specified cut-off point.
Retrieval Accuracy.computes the proportion of queries
for which all retrieved documents are relevant, relative to the
gold standard for relevance judgements. By directly evaluating
whether the retriever selects exclusively pertinent documents,
the accuracy of document retrieval gauges the binary cor-
rectness of the retrieval set, a fundamental prerequisite for
downstream generation [41].
Precision@kis defined as the fraction of the top k retrieved
documents that are relevant. Measures the system’s ability
to avoid including irrelevant items among its highest-ranked
results [26], [173].Recall@kis the fraction of all relevant
documents that appear within the top k positions, thereby
capturing the completeness of the retrieval [26], [63]. Together,
they offer complementary views: precision penalises false
positives at high ranks, while recall penalizes false negatives
within the cutoff.
F1@kis the harmonic mean of Precision@k and Recall@k,
defined as
F1@k= 2×Precision@k×Recall@k
Precision@k+ Recall@k.
This balanced metric mitigates trade-offs between precision
and recall, providing a single score that reflects both accuracy
and completeness of the top-k retrieval [26].
Mean Average Precision (MAP@k)averages the precision
scores computed at each rank position where a relevant doc-
ument occurs, then aggregates over all queries. Formally, for
each query q,
AP@k(q) =1
NqkX
i=1P(i)1{doc iis relevant},whereN qis the number of relevant documents for q andP(i)
is precision at rank i, and MAP@k is the mean of AP@k
over q [112], [173]. MAP@k rewards retrieval sets that place
relevant documents early and penalises late retrievals.
Mean Reciprocal Rank (MRR@k)focusses solely on the
rank of the first relevant document. For each query, it computes
the reciprocal of the rank position of the first relevant hit
(capped at k) and then averages over queries:
MRR@k=1
|Q|X
q∈Q1
min(rank q, k).
It is particularly informative when downstream tasks depend
critically on the earliest relevant context, as in ODQA [37],
[112].
Normalized Discounted Cumulative Gain (nDCG@k)
accommodates graded relevance by weighting each retrieved
document gain by a logarithmic discount based on its position,
then normalising by the ideal DCG. It is defined as
nDCG@k=Pk
i=12reli−1
log2(i+1)
IDCG@k,
whererel iis the relevance grade of theith document and
IDCG@k is the maximum possible DCG@k [49], [60].
nDCG@k is well suited to scenarios with multiple relevance
levels or varying document importance.
R-Precisionsets the cutoff R equal to the total number of
relevant documents for a query and computes precision at that
rank:
R-Precision = Precision@R.
Adapting the cut-off to the relevance count of each query, R-
Precision offers a query-specific summary of ranking quality.
It forms a core component of composite benchmarks (e.g.,
KILT) that jointly evaluate retrieval and generation [30], [44].
Hit@Kis a binary metric that indicates whether at least
one relevant document appears within the top K positions;
it is averaged over queries to produce a success rate [68].
Hit Success Ratio (HSR)similarly counts the proportion of
queries that require external knowledge for which the retriever
provides supporting evidence, highlighting the dependence of
the model on the retrieved context [119].
Beyond standard relevance metrics, some studies measure
the model’s ability to decide whether retrieval is necessary
(that is, the accuracy of retrieval abstention) or to withstand
adversarial passages (adversarial success rate) [65], [130].
These metrics inform selective retrieval policies and robustness
evaluations.
Using a combination of these metrics, set-based, ranking-
based, and hit-based, researchers obtain a multifaceted under-
standing of retrieval effectiveness. This rigour in evaluating the
retriever component is critical to ensuring that RAG systems
have reliable and comprehensive access to external knowledge.
4) Other Automated metrics:In addition to standard met-
rics, a diverse set ofother automated metricshas emerged to
target specific facets of RAG that are not captured by general
purpose measures. These include computational efficiency,
robustness, bias, and domain- or task-specific criteria. Because
each metric addresses a narrow aspect of system behaviour

31
or relies on specialised evaluation procedures, they appear
only occasionally in the RAG literature, typically in studies
with unique experimental setups or domain constraints. Their
limited adoption reflects both the implementation overhead and
the context-specific validity of the measures.
a) Computational Efficiency:Latencyquantifies the
time to retrieve documents and generate text, often decom-
posed into retrieval time (T r), decision time (T d), and gen-
eration time (T g), with speedup (SU) defined as the relative
reduction in total latency compared to a baseline of always
retrieving [49], [81].Response Timemeasures the end-to-
end delay from query submission to first token output, a
critical factor in interactive and clinical settings [89], [98].
These metrics are crucial for real-time applications, where
user experience and operational feasibility depend on prompt
responses. However, their computation depends on controlled
hardware environments and precise logging, which limits
cross-study comparability.
b) Robustness & Error Handling:Hallucination Rate
tracks the frequency or density of fabricated content in gener-
ated responses, either as hallucinations per 100 words or as the
proportion of faulty outputs [40], [88], [89], [111].Rejection
Rate(Reject Rate) measures the system’s ability to refuse
answers when the knowledge base is insufficient, thus avoiding
hallucinations [71], [113], [130].Success Rateevaluates the
success of adversarial jailbreak attempts, reflecting the vul-
nerability under malicious prompts [131]. These metrics are
indispensable for safety-critical domains (e.g., medicine, law),
yet they demand rigorous annotation protocols or adversarial
testing frameworks, constraining their routine use.
c) Contextual Bias:Contextual Biasmeasures the ten-
dency of a model to adopt incorrect assumptions from a
misleading context, even when its internal knowledge would
suggest a correct response [89], [109]. This metric surfaces
subtle failure modes of RAG pipelines, particularly when
retrieval yields noisy passages, but requires carefully crafted
bias scenarios, which are rarely standardised.
d) Image- and Code-Specific Metrics:CIDEr & SPICE
evaluates generated image captions by assessing consensus-
based textual agreement or semantic propositional fidelity
against human references [83], [117], [118].Edit Similarity
(ES)computes1−Lev(bY ,Y)
max(| bY|,|Y|), whereLevis Levenshtein
Distance, to quantify token-level similarity of code snippets
[80], [81].Pass@kmeasures the proportion of code generation
attempts that pass automated test suites withinktrials [85],
[86].CodeBLEUextends BLEU by incorporating abstract
syntax tree and data flow comparisons, capturing both syn-
tactic and semantic correctness of code [77], [80]. These task-
specific metrics yield deep insights in their respective domains
but lack generalisability: captioning and code generation each
demand bespoke reference datasets, execution environments,
or parser toolchains.
e) Performance Comparison:Comparative Metrics
quantify improvements over baseline systems (e.g., KRAGEN
vs. BioGPT / OpenChat in biomedical QA) by aggregating
multiple performance indicators into a single comparative
score [107], [133]. Although succinct, such composite mea-
sures often obscure which individual components drive gainsand presuppose the availability of strong baselines in the target
domain.
f) Discussion & Recommendations:Theseother auto-
mated metrics, while rarely applied in general RAG research,
play a pivotal role in specialised studies by illuminating
efficiency, safety and domain-specific quality attributes. Their
sporadic use stems from (1) the high cost of bespoke dataset
creation or annotation; (2) dependencies on hardware and ex-
ecution environments; and (3) the lack of universally accepted
standards for task-specific evaluation. To enhance compara-
bility and encourage broader adoption, we recommend the
following.
•Modular Reporting:Package each specialised metric
within containerised pipelines to facilitate deployment.
•Benchmark Extensions:Propose extensions to popular
RAG benchmarks (e.g., adding hallucination annotations
to QA datasets).
•Open-Source Toolkits:Contribute wrappers for less
common metrics, such as ES and contextual bias, to
public evaluation libraries.
By situating these metrics alongside standard automated mea-
sures in future studies, researchers can achieve a more holistic
assessment of RAG systems without imposing prohibitive
setup costs.
5) Human Evaluation Metrics:Human evaluation remains
indispensable for assessing aspects of RAG that escape purely
automatic measures. By soliciting judgments on dimensions
such as correctness, relevance, fluency, and factuality, re-
searchers gain insight into real-world performance and user
impact [55], [137].
a) Correctness & Accuracy.:Accuracy gauges the de-
gree to which the generated outputs match the expert-validated
answers. In the clinical settings of RAG, evaluators verify
whether the responses of the model reflect consensus recom-
mendations [94]. Legal RAG evaluations similarly require that
each response be both factually correct and properly grounded
in authoritative sources [111]. Educational chatbots assess
’correctness’ using multipoint rating scales applied by subject
matter experts [58], [138].
b) Relevance.:Relevance measures how well the context
retrieved or the generated text aligns with the user’s query.
Human raters typically score summaries or answers on a
binary or Likert scale for topical pertinence, grammatical
coherence, and external information appropriateness [137].
In personalised RAG frameworks, relevance judgements of
retrieved passages ensure that augmentation truly addresses
user intent [99].
c) Hallucination & Groundedness.:Hallucination met-
rics capture instances of fabricated or misattributed content.
Annotators label responses as ’Extrinsic’ (not supported by
any input), ’Intrinsic’ (incorrectly synthesised from input) or
’Misgrounded’ (false citation) [111], [141]. Human evaluation
thus directly quantifies the tendency of the model to invent
facts, a critical safety concern in high-stakes domains such as
healthcare care and law [23], [126].
d) Factual Correctness & Consistency.:Beyond binary
correctness, human judges assess whether a response main-
tains internal consistency, avoids contradictions, and remains

32
factually accurate throughout longer interactions [122], [126].
This qualitative lens captures subtle semantic errors that are
not detected by overlap metrics.
e) Comprehensiveness & Quality.:Comprehensiveness
evaluates depth of coverage: whether the generated text ad-
dresses all aspects of a query [36]. General quality scales
(for example, 1 to 5 points) combine relevance, coherence,
and absence of typos, resulting in a single interpretable score
[123], [125].
f) User-Centric Metrics:User Satisfactionis measured
via post-interaction surveys; satisfaction scores reflect per-
ceived usefulness and clarity [89], [98].
System Usability (SUS): a standardised 5-point question-
naire assesses accuracy, clarity, relevance, and ease of under-
standing [93].
Technology Acceptance (TAM):structures such as per-
ceived usefulness and ease of use are quantified through val-
idated survey instruments, offering insight into the likelihood
of adoption [136].
g) Annotation Protocols & Reliability:Most studies use
three to five human annotators to rate system outputs against
predefined criteria. Common protocols include Likert scales
(3–5 points) to assess relevance, fluency, and factuality [55],
[137]; binary judgements (yes/no), particularly for retrieval rel-
evance or groundedness [99], [111]; comparative judgements
(win/tie/loss) for head-to-head model comparisons [121]; and
error classification, in which incorrect outputs are sampled
and error types are categorised (e.g. reasoning versus retrieval
failures) [69]. To support reliable annotation, studies typically
provide clear guidelines with worked examples for each rating
level, pilot the scheme on a small subset and refine the instruc-
tions, and report inter-annotator agreement (e.g. Cohen’sκ),
including both raw agreement and chance-corrected statistics
[111].
h) Strengths, Limitations & Recommendations:Human-
judged metrics capture nuanced aspects of RAG output, such
as hallucination, conversational coherence, and user trust,
that automated measures often miss. However, they are time-
intensive, costly, and susceptible to annotator bias, with inter-
annotator agreement frequently belowκ= 0.7, reflecting
subjectivity in complex judgements [111].
To maximise rigour and reproducibility, evaluations should
combine measures spanning core dimensions (e.g., accuracy,
relevance, hallucination, comprehensiveness, and satisfaction),
report annotation scales, rater qualifications, and agreement
statistics transparently, and consider hybrid designs that sup-
plement expert judgements with carefully prompted LLM-as-
judge procedures to increase scale while retaining depth. Mak-
ing annotation guidelines and code openly available further
facilitates external replication and community benchmarking.
When protocols are defineda priori, each metric is
grounded in previous work and reliability is reported, the
human evaluation section can more convincingly demonstrate
both the real-world viability and the limitations of an RAG
system.
6) LLM-as-Judge Metrics:Recent advances in evaluation
methodologies have shifted toward the use of LLMs them-
selves as automated judges of generated content. Rather thanrelying solely on surface-level overlap or costly human annota-
tion, LLM-as-judge approaches prompt a high-capacity model,
such as GPT-4, to assess outputs along dimensions such as
correctness, relevance, coherence, and safety.
a) Accuracy via Advanced LLM Verification:One com-
mon formulation applies an LLM (e.g. text-davinci-003) to
re-evaluate model outputs against ground-truth answers, flag-
ging semantically correct yet lexically divergent generations
as accurate [47]. This “LLM-verified accuracy” provides a
more robust correctness estimate than exact-match metrics,
particularly in question-answering settings where paraphrase
is common.
b) GPT-Based Correctness and Quality Ratings:A suite
of studies instruct ChatGPT or GPT-4 to assign binary or scalar
judgements to outputs:
Binary correctness:ChatGPT classifies each response as
correct or incorrect, yielding a proportion-correct score [130].
Quality scales:Responses are rated on a 1–10 scale
for overall quality—including helpfulness, relevance, and
depth—by ChatGPT [130], and similarly by GPT-4 across
multiple facets (relevance, clarity, depth) in fully automated
scoring systems [138].
Sentiment assessment:ChatGPT assesses the polarity of
model outputs (positive vs. negative) to gauge tone and user
experience [130].
c) Benchmarking Against GPT-4 judgements:To validate
internal model evaluations, some works compare their own
LLM’s judgements with those of GPT-4. For example, GPT-
4 is used as a reference judge for self-knowledge, passage
relevance, and question-decomposition tasks, establishing a
reliability benchmark [55].
d) Harmfulness and Safety Classification:Ensuring eth-
ical outputs, researchers prompt GPT-4 to detect and classify
harmful or toxic content, computing the proportion of harmful
responses or the worst-case toxicity over multiple samples
[59]. This approach complements traditional toxicity metrics
by leveraging the LLM’s contextual understanding of offen-
siveness.
e) LLM-Fact-Checker Chains:Leveraging frameworks
such as LangChain, an LLM (e.g., gpt-3.5-turbo) is embedded
in a fact-checking pipeline: it cross-verifies chatbot responses
against course content or reference materials and generates
confusion-matrix statistics (accuracy, precision, sensitivity,
specificity) to automate what was formerly manual evaluation
[136].
f) G-EVAL: Comprehensive LLM-Judged Evaluation:
G-EV AL uses GPT-4 to score generated text on coherence,
consistency and fluency using a 1–5 rubric, outperforming
traditional overlap metrics in correlating with human judge-
ments [100]. It has been used to evaluate the generation of
domain-specific reports, such as flood incident summaries,
demonstrating superior alignment with expert evaluators [100].
g) Semantic Accuracy via LLM Instruction Models:
By prompting gpt-3.5-turbo-instruct to compare generated an-
swers semantically against ground truths, “semantic accuracy”
metrics capture meaning preservation beyond exact tokens,
addressing limitations of classical exact-match scores [58].

33
h) Discussion & Recommendations:LLM-as-judge met-
rics offer scalable, semantically rich evaluation but inherit po-
tential biases and prompt-sensitivity from their host models. To
mitigate these issues, we recommend calibrating LLM prompts
against a small human-annotated validation set, reporting
multiple perspectives (e.g. combining binary correctness with
a scalar quality score) and disclosing prompt templates and
model versions to ensure reproducibility. Adopting these prac-
tices can harness the efficiency of LLM-judged evaluation
while maintaining rigorous, transparent assessment standards.
7)Automated Frameworks:Automated evaluation frame-
works are pivotal for assessing RAG systems by mitigating
subjectivity, scalability issues, and bias. Two notable systems,
ARES [110] and RAGAS [114], concentrate on three core
metrics: context relevance, answer faithfulness, and answer
relevance.
ARES adopts a quantitative approach, using fine-tuned
language models and Kendall’sτto align automated scores
with human judgements [110]. This method delivers high pre-
cision and nuanced insights into response fidelity; however, its
reliance on extensive annotated data may restrict scalability. In
contrast, RAGAS employs a reference-free strategy that uses
cosine similarity to measure semantic relationships between
queries, retrieved contexts, and generated responses [114].
Although this technique improves objectivity and accelerates
evaluations, it is more sensitive to prompt variations, which
can reduce consistency.
ARES and RAGAS thus represent two contrasting yet com-
plementary approaches to RAG system evaluation. ARES of-
fers detailed, human-aligned assessment but can be hampered
by scalability issues due to its dependency on annotated data.
Conversely, RAGAS provides operational efficiency through
automated semantic similarity measurements, albeit with po-
tential variability due to prompt sensitivity. This juxtaposition
highlights the trade-off between detailed, qualitative insights
and streamlined quantitative evaluation, prompting critical
questions about whether future frameworks might integrate
the strengths of both methods to achieve a balanced, robust
evaluation strategy.
It is important to note that automated evaluation frameworks
relying on large language models are not immune to inherent
biases, which can subtly skew outcomes and misrepresent true
system performance. To mitigate these issues, future evaluation
strategies could benefit from hybrid approaches that integrate
LLM-based assessments with calibrated human oversight, bal-
ancing the objectivity and scalability of automated methods
with the nuanced insights of human evaluators.
Practical implications of these frameworks include guiding
the design of adaptable RAG systems that lower annotation
costs while upholding rigorous evaluation standards. Future
research may further integrate qualitative elements and re-
fine metrics to address emerging concerns, as discussed in
Section IV-C5. Ultimately, merging ARES’s detailed human
insight with RAGAS’s operational efficiency may offer the
most balanced strategy to advance the evaluations of the RAG
system.
8) Holistic Evaluation of RAG Benchmarks:RAG bench-
marks, although diverse in their origins and target domains,collectively map out a multidimensional landscape of model
performance. At the core, each benchmark isolates particular
capabilities—be it resilience to noise, financial forecasting
acuity, medical question precision, multi-hop reasoning, or
CRUD-style text operations—and in doing so, they both
complement and challenge one another.
a) Connecting the Four Pillars of RGB to Broader RAG
Metrics:The Retrieval-Augmented Generation Benchmark
(RGB) explicitly dissects RAG capability into noise robust-
ness, negative rejection, information integration, and counter-
factual robustness [113]. These four axes are not arbitrary:
they represent the basic dilemma of “when and how to
trust retrieved context.” Noise robustness measures whether a
model can sift signal from distractors—a requirement shared
by nearly all other RAG tasks, since any retrieval pipeline
may surface irrelevant documents [113]. Negative rejection,
on the other hand, examines the model’s restraint: ability to
say “I don’t know” rather than hallucinate. This restraint is
critical in high-stakes domains such as medicine, where wrong
answers can mislead practitioners [97]. Information integration
overlaps naturally with multi-hop retrieval and summarization:
it probes the model’s capacity to aggregate evidence from mul-
tiple sources, akin to what MultiHop-RAG quantifies through
MAP@K and answer accuracy [112]. Finally, counterfactual
robustness examines error detection and correction—an echo
of CRUD-RAG’s “Update” task, which tests factual revision
in generated text [174].
b) Quantitative Meets Qualitative: Trade-offs in evalua-
tion:While RGB relies primarily on exact match accuracy and
rejection / error rates, AlphaFin extends evaluation to financial
performance metrics: annualized rate of return (ARR), Sharpe
ratio, drawdowns, along with traditional language metrics like
ROUGE and human preferences [143]. This duality high-
lights a fundamental trade-off: quantitative metrics (ARR,
MAP@K, accuracy) offer objectivity and comparability, yet
may miss subtleties of fluency, coherence, or interpretability
that qualitative human studies and chain-of-thought evalua-
tions capture. For example, a model that achieves high ARR by
blindly following market trends may still produce explanations
that fail regulatory standards or mislead users; here, GPT-4
preference judgements in financial Q&A illuminate whether
the model’s reasoning is human-aligned [143]. In contrast,
purely qualitative assessments can be subjective and difficult
to standardise in large test beds such as the 7,663 medical
questions of MIRAGE [97].
c) Domain-Specific Demands and Broader Trends:The
emergence of domain-tailored benchmarks—AlphaFin in fi-
nance, MIRAGE in medicine—reflects a broader shift in RAG
research: One-size-fits-all evaluation is giving way to spe-
cialised suites that capture domain nuances. In medicine, zero-
shot versus retrieval-augmented evaluations in MIRAGE reveal
that RAG can increase accuracy by up to 18%, but also surface
’lost in the middle’ issues when too much context overwhelms
the model [97]. MultiHop-RAG similarly shows that retrieval
itself remains a bottleneck: even GPT-4 reaches only 56%
accuracy with real retrieval versus 89% with ground-truth
contexts [112]. These findings spark questions:How might
improvements in retriever architectures reorder the current

34
performance hierarchy?Andcan domain-agnostic LLMs ever
match domain-specific ones once retrieval pipelines are fully
optimised?
d) Methodological Reflections: Why These Metrics?:
Each benchmark’s metric choices go back to its core use case.
The rejection rate metric of RGB emerges from the need to
induce hallucinations in open domain QA, while the AlphaFin
ARR and Sharpe ratio base the evaluation on financial risk-
reward trade-offs [113], [143]. The reliance of MIRAGE on
established medical QA datasets (MMLU-Med, MedQA-US,
BioASQ) ensures comparability with previous work, but by
layering the retrieval into zero-shot and multichoice settings,
it exposes where medical LLMs overuse or underuse external
evidence [97]. The combination of retrieval metrics (MAP@K,
MRR@K) and generation (accuracy) of MultiHop-RAG mir-
rors the two-stage reality of the RAG pipelines, allowing
separate diagnostics for the retriever and the generator [112].
The taxonomy of CRUD-RAG in the Create, Read, Update,
Delete tasks underscores the need for full-lifecycle assessment
of text operations, not just answering questions [174].
e) Practical Implications and Future Directions:In
practice, these benchmarks guide system design: a retrieval
pipeline optimised for MAP@10 may not yield the best error
correction performance in counterfactual settings; a model
fine-tuned for ROUGE in financial summaries could underper-
form in drawdown mitigation metrics. Thus, practitioners face
calibration challenges:Which trade-off between retrieval depth
and generative precision aligns best with their application’s
risk profile?
Looking ahead, several avenues merit exploration. First,
integrating qualitative fluency measures directly into quantita-
tive benchmarkscould bridge the gap between human-centric
evaluation and automated metrics. Second,extending bench-
marks to multilingual or cross-modal contexts—combining
text with tables, charts, or code—would reflect real-world uses.
Finally, as interactive RAG agents grow,dynamic benchmarks
that simulate user feedback loopswill be critical to measure
adaptability and continuous learning.
RGB, AlphaFin, MIRAGE, MultiHop-RAG, and CRUD-
RAG form a tapestry of complementary benchmarks: each
covers a slice of the performance spectrum: signal fil-
tering, domain-specific reasoning, error detection, evidence
synthesis, and text lifecycle operations. Their varied met-
rics—accuracy, rejection rates, ARR, ROUGE, MAP@K,
Sharpe ratios—highlight that no single number suffices. A
holistic evaluation demands a suite of metrics that reflect both
quantitative rigour and qualitative nuance. As RAG systems
advance, our benchmarks must evolve in tandem, posing ever
more challenging questions:Can we craft unified metrics that
capture trustworthiness, utility, and user alignment in one
framework?Only through such integrative efforts can the next
generation of RAG applications realise their full potential.
9) Datasets:In our systematic survey, we find that re-
searchers have used a large array of approximately 343 unique
datasets to evaluate RAG systems, illustrating the multifaceted
nature of performance assessment. Open-domain resources
such as Wikipedia [1], Natural Questions [175], and MS
MARCO [176] provide a baseline, particularly for question-answer tasks. These datasets excel in benchmarking fluency
and general comprehension, but may not fully represent spe-
cialised applications. In contrast, domain-specific collections,
ranging from legal (e.g. ALQA [177], LEDGAR [178]) to
biomedical sources (e.g. CORD-19 [179], KGRAGQA [180]),
offer in-depth evaluation in high-stakes contexts, although they
often suffer from inconsistent preprocessing and versioning
practices. Table IV summarises the content description and
intended use of these datasets.
Multi-hop QA sets, including HotPotQA [181] and 2Wiki-
MultihopQA [182], challenge systems with complex reasoning
tasks, highlighting strengths in multistep inference, while
also revealing limitations in current methodologies. Similarly,
multimodal and code-centric corpora, such as COCO [183]
for image-text pairs and CodeSearchNet [184] for code-centric
evaluations, extend performance evaluation beyond traditional
text, addressing broader application domains, yet introducing
variability due to differences in data segmentation and anno-
tation standards.
This diversity reflects both advantages and trade-offs: Al-
though open domain datasets support benchmark consis-
tency, specialised datasets provide critical insight into domain-
specific challenges [1], [175], [176], [178]. The absence of
standardised dataset preparation, ranging from segmentation
to versioning, poses a significant methodological challenge
and raises questions about the reproducibility and compara-
bility of RAG evaluations. For example, how might emerging
frameworks for dataset processing and standardised evaluation
metrics improve consistency between studies?
The interplay among these datasets underscores a broader
trend toward holistic, multidimensional evaluation strategies in
the development of the RAG system. By integrating both quan-
titative benchmarks and qualitative assessments, researchers
can better capture the strengths and limitations of current
models, ultimately guiding future innovations and establishing
more robust operational standards.
D. What are the key challenges and limitations associated
with retrieval-augmented generation techniques?
As detailed in Section IV-B, recent RAG systems have
been propelled by dynamic query generation, universal-scheme
frameworks, multimodal fusion, and iterative refinement.
These innovations revolutionise data transformation and con-
text preservation [75], [78], [119], [122]. However, their very
integration reveals a set of stubborn obstacles that constrain
performance, scalability, and adaptability. The remainder of
this section therefore organises these obstacles into six the-
matic challenges, tracing how each one limits today’s retrieval-
augmented generation pipelines.
1) Computational and Resource Trade-offs:The first ob-
stacle is the raw cost, both in time and in hardware. Dy-
namic query rewriting, iterative retrieve-and-refine loops, and
extended context attention improve relevance, but each extra
pass increases the wall clock latency and memory footprint
[45], [47], [122], [140], [185]. Universal-schema frameworks
compress representations and trim indices, yet they handle
novel patterns brittlely and still struggle to keep edge devices

35
within real-time budgets [75]. Training compounds the strain:
full end-to-end tuning of large retriever–generator stacks can
consume days of multi-GPU time and hundreds of gigabytes of
RAM [23], [117], [140]. Lighter alternatives, such as adapter
layers, retrieval-only updates, or prompt tuning, reduce cost
but usually eliminate domain specificity [28], [119].
Even at inference, resources rarely align neatly. CPU-bound
vector search typically precedes GPU-bound decoding, so one
processor idles while the other works; bespoke schedulers
such as PipeRAG and RAGCache attempt to hide lookup
time by overlapping ANN search with generation, yet they
demand careful profiling and remain sensitive to corpus size
[30], [32], [39]. Approximate-nearest-neighbour indices halve
retrieval latency but lower recall, whereas exhaustive search
inverts the trade-off. Progress therefore hinges on adaptive
scheduling policies that co-optimise ANN depth, speculative
decoding, and device utilisation, plus resource-efficient joint
objectives that align retriever and generator without prohibitive
fine-tuning [132].
2) Noise, Heterogeneity, and Multimodal Alignment:RAG
pipelines are only as good as their inputs, yet most inputs
are noisy and heterogeneous. Vision-to-Language transformers
compress complex scenes into terse captions, suppressing
spatial clues such as gaze or depth [119]. Code-Property
graphs balloon super-linearly with project size, so aggres-
sive pruning saves space but can excise rare, security-critical
constructs [78]. Selective densification, reinjecting previously
filtered snippets when retrieval confidence dips, offers a middle
ground, although it still inflates indices [75], [144].
Noise also lurks in hybrid retrieval itself. Dense vectors,
sparse keywords and rule filters score on incompatible scales;
naive normalisation swings between overrecall and under-
recall, while cross-encoders that fix the problem add 2–5
times the latency [101], [186]. Learnable weighting gates are
promising but lack cross-domain evidence [54]. Multimodal
encoders introduce another layer of fragility: CLIP-style mod-
els often suffer “semantic bleeding”, where irrelevant visual
regions influence text similarity, a serious risk in radiology
and surgical robot logs [92], [118], [120], [144]. Fine-grained
alignment losses mitigate leakage but add both milliseconds
and supervision cost. Lightweight validation schemes, such as
attention entropy checks or cross-view consistency regularis-
ers, offer a protection at marginal run-time cost and do not
require dense pixel labels [144].
Finally, knowledge graph retrieval excels in multi-hop rea-
soning, yet depends on noisy entity linking and heuristic
pruning; over-pruning deletes long-tail nodes, under-pruning
explodes memory, and classic graph metrics correlate only
weakly with downstream QA [37], [54], [134]. What the field
needs are learnable fusion frameworks that expose per-channel
uncertainty and graph-aware benchmarks that reveal the real
cost-benefit envelope of noise mitigation strategies.
3) Domain Shift, Dataset Alignment, and Generalisation:
Our focus now shifts to distributional robustness—RAG mod-
els that shine in one domain often stumble in another. Systems
tuned to PubMed outperform BM25 on biomedical queries
but falter in legal corpora without costly retraining [43], [62].
Hybrid pipelines that anchor language-agnostic schemas withthin domain-specific rules travel better, but add engineering
overhead and still require careful calibration when knowledge
is fragmented across disconnected sources [55].
Repository freshness compounds the problem. Stale or erro-
neous material propagates directly into answers, a high-stakes
liability in finance and medicine [22], [92]. Index refreshes
mitigate drift but demand labour-intensive validation pipelines
and may introduce their own lag [107]. Worse, most evaluation
sets lean heavily on English-language Wikipedia, masking
specialist failure modes and inflating scores through train–test
overlap [132], [186]. Corpus choice is thus decisive: biomedi-
cal encoders dominate on PubMed but misfire elsewhere [97],
and multilingual retrieval remains hamstrung by scarce aligned
data and inconsistent terminology [102]. Adaptive “retrieval
triggers” that fire the retriever only when the generator signals
high uncertainty appear attractive; yet, when they misfire, they
either waste compute or omit indispensable evidence [91].
Seemingly mundane hyperparameters—chunk size, hierar-
chical fragmentation strategy, the number of documentskto
return, and undocumented caching policies—can shift accu-
racy–latency curves by double-digit margins: small windows
fracture discourse; large ones bloat latency; and inconsistent
choices ofkthwart reproducibility [34], [39], [54], [66], [125].
Closing these gaps will require continuous validation pipelines
and unified cross-domain, multilingual testbeds that expose
real-world brittleness while tracking accuracy–latency trade-
offs.
4) Modular Pipelines and Error Cascades:Even when
knowledge is fresh and well aligned, architectural glue can
fail. In this section, we focus on interfaces that link retrieval
to generation. Splitting retrieval, reranking, and generation
kerbs hallucination but creates brittle processing chains. A
misranked passage in the first stage can irreversibly bias
the generator, and although deep cross-encoders lift ranking
fidelity, their compute cost still forces approximate first-pass
filters whose scores are tuned ad hoc [35], [42], [44], [126].
Iterative and memory-augmented pipelines add another
wrinkle. External memories curb repetition but introduce stal-
eness and snowballing: cached errors are re-retrieved in later
turns [107]. Content-based decay, which weights cache entries
by both recency and reuse, cuts latency by up to 40% without
hurting precision, yet evidence remains limited to small-scale
experiments [34]. Ultimately, combining interface patterns that
expose calibrated model confidence with uncertainty-triggered
safeguards, for example, probability/entropy thresholds that
proactively invoke retrieval, verification, abstention, or roll-
back, can prevent error cascades from taking hold [122], [187].
5) Large-Language-Model Constraints and Safety Risks:
Next, we examine the constraints of the generator (the LLM)
that produces user-facing text. Commercial LLM APIs deliver
strong performance but impose per-token fees, usage limits
and a requirement for internet connectivity. Open-weight mod-
els avoid vendor lock-in and can run locally, however require
substantial hardware and usually offer fewer tuning options
[47], [101]. Fixed context windows—often four thousand
tokens or fewer—truncate multi-document evidence, forcing
lossy chunking that undercuts retrieval depth; long-context
variants help, but do not fully restore cross-passage reasoning

36
[83], [124].
Bias, toxicity, and hallucinations remain endemic. Encoding
a user’s information need in only a few tokens is brittle, and
attempts to map that intent into structured formats (e.g. JSON)
often break under domain drift [25], [73], [87]. Automatically
generated search strings show the same fragility: ill-formed
queries invite off-topic retrieval and can launch an irrelevant
evidence cascade [122]. Skewed pre-training corpora, mean-
while, inject demographic bias and toxic completions; retrieval
softens but does not eliminate hallucination, and lapses are
especially hazardous in medicine [23], [90], [135]. Prompt
design does not offer a silver bullet: minor syntactic edits shift
coherence and factuality, while adversarial prompts can bypass
guardrails or surface-corrupted evidence [30], [131], [134],
[139]. Progress therefore hinges on bias- and hallucination-
aware losses, adversarial-prompt test suites, and extended-
context architectures that enlarge windows at sustainable cost.
Skewed pre-training corpora drive demographic bias and
toxic completions; retrieval dampens but does not eliminate
hallucination, which is especially problematic in medicine,
where factuality lapses carry real harm [23], [90], [135].
6) Security Threats in Retrieval-Augmented Generation:
Even the best-engineered and safest pipelines remain vulner-
able to deliberate attack, so finally we consider the RAG
security landscape. The same external knowledge that makes
RAG systems powerful also opens up a new attack surface:the
retrieval corpus itself. Because the language model is trained
to trust whatever the retriever returns, even asinglepoisoned
document or a carefully crafted query can steer generation,
violate safety policies, or leak private data. Recent work
exposes three broad threat families—(i) corpus-poisoning
back-doors,(ii) data-exfiltration and privacy attacks, and
(iii) jailbreak and policy-evasion triggers—all of which
exploit the loose coupling between the retriever and generator.
AGENTPOISON [127] and Phantom [128] show that an
attacker needs to tamper withfewer than 0.1 %of corpus
items—sometimes only one passage—to create a back-door
that fires when a secret trigger word appears. A constrained
trigger optimisation maps those queries to a compact, unique
region of the embedding space, guaranteeing retrieval while
remaining stealthy (low perplexity, robust to paraphrase). The
result is alarming: across six dense retrievers and multiple
LLMs, retrieval success exceeds 80%, and end-to-end mali-
cious action rates sit around 60% with virtually no drop in be-
nign accuracy. These findings underline a systemic weakness:
current RAG deployments rarely authenticate or provenance-
stamp the documents they ingest, so “sleeper” passages can
lie dormant until the attacker issues the right query.
BadRAG [130] extends the idea tocontent-only poisoning:
its COP / ACOP / MCOP techniques craft passages that are
onlyretrieved under specific trigger conditions and then bias
the generated output (Alignment-as-an-Attack, Selective-Fact-
as-an-Attack). With as few as ten poisoned passages, the
framework achieves a 98% trigger-retrieval rate and slashes
GPT-4 accuracy from 92% to 19%. Crucially, attacks bypass
naive defences such as perplexity filters or keyword blacklists
and can even nudge sentiment or political stance without
overtly toxic text, highlighting how difficult covert bias de-tection will be once adversaries understand retrieval scoring.
A different axis of vulnerability is privacy. “Follow My
Instruction and Spill the Beans” [129] demonstrates that sim-
ply appending a malicious system or user prompt can coerce
instruction-tuned models to copy verbatim from their private
datastores. Across nine open-source LLMs and 25 production
GPTs, the leakage success hit 100% in at most two queries;
larger models leaked >70 BLEU points of text. Leakage
worsens with coarse, semantically coherent chunks and when
prompts are injected at the start or end of the context, painting
a clear blueprint for would-be attackers. Mitigations such
as PINE (Position-bias Elimination) and safety-aware system
prompts halve—yet do not eliminate—the reconstruction rate,
signalling that stronger retrieval-side controls are required.
Pandora [131] and Phantom [128] move beyond bias or
leakage to fullpolicy evasion. By injecting adversarial content
that the retriever dutifully surfaces, the attacker sidesteps the
usual guard-rail prompt hierarchy; GPT-4, normally resilient
to direct jailbreaks, yields prohibited outputs in 35% of
cases once the supporting evidence comes from a poisoned
corpus. Because the unsafe text reaches the generator as
“ground truth”, refusal classifiers often let it pass. These
results expose an uncomfortable asymmetry: alignment layers
superviseprompts, yet poisoned retrieval arrives as “context”
and therefore inherits implicit trust.
In practice, commonly deployed defences provide only lim-
ited protection. Perplexity-based filtering and query rephrasing
reduce AgentPoison’s end-to-end success by at most single-
digit percentage points in some tasks (e.g., 9.6 percentage
points and 6.8 percentage points in Agent-Driver), but do
not produce any reduction - and sometimes an increase - in
others (ReAct-StrategyQA). Moreover, AgentPoison’s triggers
remain low-perplexity and thus difficult to flag [127], [130].
Query rephrasing or majority vote reranking is similarly inef-
fective because trigger optimisation tends to cluster poisoned
queries tightly in embedding space; paraphrases remain within
the backdoor region. Safety prompting and refusal classifiers
cannot, in general, distinguish benign evidence from adversar-
ially retrieved content, and therefore authorise harmful com-
pletions [131]. Blacklisting triggers is also brittle: Phantom
shows that an unseen synonym can reactivate the attack, and
adversaries can optimise entirely new token sequences that
were not present at the defence time [128].
These limitations motivate a set of complementary direc-
tions. Strengthening corpus provenance and attestation is a
priority: practical mechanisms to sign, version, and audit
documents in large-scale vector stores remain scarce, but
append-only logs based on Merkle trees, together with proofs
from trusted execution environments, could make retroactive
poisoning detectable. Retrieval-time anomaly detection also
merits attention; distance-based or density-ratio detectors in
embedding space may identify outlier triggers, provided they
operate at millisecond latency and resist adaptive manipula-
tion. A further avenue is joint retriever–generator training:
current pipelines typically “freeze” the retriever at deployment,
coupling the retriever’s gradients to downstream safety losses
may instead lead the system tounlearnreliance on poisoned
sources. In parallel, refusal mechanisms should assess the

37
provenance of retrieved spans—not only the prompt—so that
unsafe evidence is withheld before it reaches the LLM.
Continued progress will depend on rigorous benchmarking,
since most leaderboards emphasise hallucination and factuality
rather than integrity; a standard suite that measures attack-
specific metrics—retrieval attack success rate, end-to-end ASR
under transfer (ASR-t), and drift in benign accuracy—would
enable systematic evaluation.
Security threats in RAG are no longer theoretical. With a
handful of poisoned passages or a single prompt injection,
adversaries can bias, leak, or jailbreak state-of-the-art systems
while evading current defences. The community must therefore
treat the retrieval pipeline—and, by extension, the knowledge
base—as a first-class security boundary, on a par with the
language model itself.
7) Synthesis and Outlook:The six challenges exam-
ined above form an interlocking system rather than a
menu of orthogonal pain points. Compute budgets shape
how much noise can be tolerated (§IV-D1↔§IV-D2);
data cleanliness conditions the severity of domain shift
(§IV-D2↔§IV-D3); imperfect domain coverage magnifies
error cascades (§IV-D3↔§IV-D4); architectural fragility
limits the safe operating range of large-language models
(§IV-D4↔§IV-D5); and every residual weakness enlarges
the attack surface (§IV-D5↔§IV-D6).
Progress hinges onco-design: latency-aware scheduling
across retrieval and generation; benchmarks that jointly score
robustness to noise, distribution shift, and security; extended-
context models balanced by adaptive retrieval depth; and
probabilistic defences that propagate calibrated uncertainty
end-to-end. Tackling these dependencies together will yield
RAG systems that are efficient, reliable, and resilient amid
rapidly evolving knowledge and threats.
V. LIMITATIONS OF THESYSTEMATICREVIEW
The methodological choices we made in selecting the lit-
erature for our systematic review were intended to maximise
focus, transparency, and reproducibility, but they entail con-
straints that should be acknowledged. The citation thresholds
(§II-C) foreground influential contributions and kept screening
tractable, however, they also risk citation-lag bias, under-
representing very recent breakthroughs and niche, domain-
specific work that has not had time to accrue citations. Fu-
ture updates could mitigate this by adopting time-normalised
criteria (for instance, citations per month since publication),
reporting a short sensitivity analysis with relaxed cut-offs, and
optionally tracking an expert-curated “emergent” set alongside
the main corpus.
Coverage was restricted to five major digital libraries plus
DBLP, and to English-language publications. This scope im-
proves deduplication and comparability, but probably under-
samples grey literature, non-indexed preprints, and research
disseminated in other languages. Terminological inconsistency
in the field (“RAG”, “retriever-reader”, “retrieval-augmented
LLMs”) further complicates study selection; a component-
based inclusion rule was applied to reduce misclassification,
although borderline cases may remain.Screening and extraction procedures also introduce potential
bias. Titles and abstracts were double-screened, while full-
text data extraction was performed by a single reviewer with
verification; In addition, suggestions assisted by LLM were
used to support, not replace, human judgement. These steps
accelerated the workflow but may still allow selection or
extraction bias despite audit trails. Given the pace of the
area and our search window ending in May 2025, temporal
generalisability is limited. Periodic updates, a broader database
and language coverage, preregistered protocols, and a brief
sensitivity analysis in future iterations would strengthen ro-
bustness.
VI. CONCLUSION
In this systematic review, we synthesised a comprehensive
picture of the RAG research landscape through the lens of
128 highly cited studies on retrieval-augmented generation
(2020–May 2025) using a citation-weighted PRISMA proto-
col. Although DPR + seq-to-seq remains a strong baseline, re-
cent progress centres on hybrid retrieval, structure- and graph-
aware indexing, uncertainty-triggered and iterative retrieval
loops, efficiency-orientated compression, and the integration
of memory and multimodality. Evaluation practice is broad
but uneven: overlap metrics still dominate, with growing use
of retrieval quality measures (e.g., Recall@k, MAP), human
judgements, and LLM-as-judge protocols.
However, important gaps remain. Reporting seldom couples
accuracy with cost and latency; systems generalise brittlely
under domain shift and evolving corpora; and defences against
retrieval-side poisoning and prompt-in-context attacks are
still immature. We therefore recommend holistic benchmarks
that combine quality, efficiency, and safety; treating retrieval
depth and tool use as budget-aware policy decisions; and
provenance-aware retrieval pipelines that surface uncertainty
and provide traceable evidence. Looking ahead, modular adap-
tive retrieval generation stacks that allocate compute based on
uncertainty, unified multilingual and multimodal benchmarks,
and end-to-end security and privacy frameworks will be key
to moving RAG from promising prototypes to reliable and
scalable systems.

38APPENDIX
TABLE IV: Summary of datasets utilised in the studies included in this systematic literature review of RAG. It outlines the key characteristics and
origins of each dataset, offering an overview that enhances understanding of the data employed across the reviewed research articles. This summary
supports an analysis of the trends and methodologies specific to RAG, showcasing the variety and scope of datasets applied in this area of research.
Dataset Name Content Description Intended Use Citation
Fre-
quency
Natural Questions (NQ) [175] 323,045 QA examples across train/dev/test splits. Train and evaluate open-domain QA systems. 27
HotPotQA [181] 113,000 multi-hop QA pairs. Train/test QA with multi-hop reasoning and explanations. 26
Wikipedia [1] 6 million articles of text and metadata. General corpus of Wikipedia text for NLP tasks. 19
TriviaQA (TQA) [188] 96,000 QA pairs with six supporting documents each. Develop comprehension models requiring complex inference. 18
2WikiMultihopQA (2WikiMQA) [182] 192,606 multi-hop QA pairs from Wiki data. Multi-hop QA using structured and unstructured sources. 11
Multihop Questions via Single-hop Question Composition
(MuSiQue) [189]25,000 2–4-hop questions (50,000 with contrast). Multi-hop QA by composing single-hop questions. 9
Fact Extraction and VERification (FEVER ) [190] 185,445 claims annotated with evidence. Designed for verifying claims using Wikipedia as the textual
source8
Microsoft MAchine Reading COmprehension (MS MARCO)
[176]100,000 questions and 1 M passages from web docs. Reading comprehension and QA from real web data. 8
StrategyQA [191] 2,780 yes/no questions with step-by-step reasoning. Benchmark Boolean QA needing implicit multi-hop reasoning. 8
Wizard of Wikipedia (WoW) [192] 22,311 dialogues (202K utterances) using Wiki info. Dialogue with a “wizard” answering via Wikipedia. 8
WebQuestions (WebQ) [193] 6,642 QA pairs from real user web queries. Semantic parsers using Freebase KG. 7
Arc-Challenge [194] 2,590 science multiple-choice questions. Benchmark deep-reasoning QA systems. 5
Explain Like I’m Five (ELI5) [195] 72k QA pairs with supporting web documents. Long-form QA understandable by five-year-olds. 5
Massive Multitask Language Understanding (MMLU) [196] 57 task-specific single-sentence summaries. Benchmark broad knowledge and reasoning coverage. 5
NarrativeQA [197] 1,572 narratives, 46,765 QA pairs. QA over long narratives and summaries. 5
PopQA [198] 14,000 Wikipedia QA pairs across 16 relations. QA focusing on Wikidata relationship types. 5
WebQuestions Semantic Parses (WebQSP) [199] SPARQL queries for 4,737 questions, 1,073 partial. KB-QA research using Freebase semantic parses. 5
Wikipedia English (December 2018) [200] 21 Million passages from December 2018 English Wikipedia. Passage corpus for retrieval and QA tasks. 5
Answer Summaries for Questions which are Ambiguous
(ASQA) [201]12,632 ambiguous QA annotations. Long-form QA for ambiguous factoid questions. 4
OpenbookQA (OBQA) [202] 6k science MCQs with 1,326 core facts. Multi-hop science QA using core facts. 4
Stanford Question Answering Dataset (SQuAD) [203] 23k passages, 108k questions (span answers). Reading comprehension with span answers. 4
Triple-based Relation Extraction (TREx) [204] 3.09 Million abstracts with 11 Million triples. Relation extraction and KB population tasks. 4
TruthfulQA [205] 817 questions across 38 categories. Evaluate factual consistency in QA. 4
Zero Shot RE (zsRE) [206] Over 30 M QA examples for relation extraction. Zero-shot relation extraction without examples. 4
Conversational Question Answering (CoQA ) [207] 127k questions from 8k multi-turn dialogues. Build conversational QA systems. 3
MultifieldQA-en (MFQA) [208] 150 docs, 150 cases, 4.6k words each Single-document long-context QA. 3
Physical Interaction: Question Answering (PIQA) [209] 16,000 physical commonsense MCQs. Reason about everyday physical tasks. 3
PubMedQA [180] PubMed abstracts QA (yes/no/maybe) Biomedical QA benchmarking. 3
Qasper (QASP) [210] 416 papers, 371 cases, 4.7k tokens per doc Academic QA over research papers. 3
Unified Medical Language System (UMLS) [211] Integrated biomedical vocabularies. Standardize medical terminologies. 3
Wikipedia Aspect-based summarization (WikiAsp) [212] 320,272 docs with section-title aspects. Aspect-based summarization of Wikipedia articles. 3
WikiQA [213] 3,047 questions with Wikipedia candidate sentences. Evaluate answer-sentence selection in QA. 3
Bamboogle [214] 125 handcrafted 2-hop reasoning questions. Evaluate compositional reasoning capabilities. 2
BioASQ [215] 4k+ PDFs and 1k domain-specific questions Biomedical retrieval & QA tasks. 2
BoolQ [216] 16,000 yes/no questions with passages. Boolean question answering 2
C Code Summarization Dataset (CCSD) [217] 95k function–summary pairs. Source code summarization. 2
CNN/Daily Mail [218] News articles paired with human-written summaries. Summarization and hallucination benchmarking. 2
Code mixed-language GLUE (General Language Understanding
Evaluation) (CodeXGLUE) [219]Millions of code - NL pairs across tasks. Code understanding and generation. 2
CodeSearchNet (CSNet) [184] 6M functions, 2M docstring pairs in six langs. Semantic code search evaluation. 2
Continued on next page

39Dataset Name Content Description Intended Use Citation
Fre-
quency
Colossal Clean Crawled Corpus (C4) [220] Billions of English tokens from web. Unsupervised pre-training for NLP models. 2
Common Crawl dump of the internet (CCNet) [221] 1.5B documents, 532B tokens across 174 langs. Pre-training large-scale language models. 2
Common Objects in Context (COCO) [183] 330k images, 1.5M captions. Object recognition and image captioning. 2
CommonsenseQA [222] 12,247 MCQs from ConceptNet subgraphs. Evaluate commonsense question answering. 2
Conceptual Caption (CC) [223] 3.3M image-text pairs. Pretrain vision-language models. 2
Dolly [224] 15k human-crafted instruction–response pairs Instruction-following model training. 2
Enron Email [77] 500k corporate emails for PII extraction tasks Evaluate PII detection and removal 2
ExplaGraphs [225] 3,166 belief-argument-explanation graphs. Commonsense reasoning via explanation graphs. 2
Flickr30k [226] 30k images with five captions each. Image captioning research. 2
Google Search corpus (GSfull) [227] 280k sentences from Google Search snippets. Visual QA (OK-VQA) supporting data. 2
HellaSwag [228] 70k multiple-choice from ActivityNet/WikiHow Commonsense reasoning evaluation. 2
Incomplete Information Reading Comprehension Questions
(IIRC) [229]13,441 questions, 5,698 paragraphs. Challenging reading comprehension. 2
LAION [230] Billions of image-text pairs. Train multi-modal language-vision models. 2
MultimodalQA [231] 30k questions, 58k images, text, tables. Multi-modal QA requiring joint reasoning. 2
Outside-Knowledge Visual Question Answering (OKVQA)
[232]14k visual questions needing external knowledge. Visual QA with outside knowledge. 2
PubHealth [233] True/false health-claim questions. Health-claim verification. 2
PubMed Clinical Papers [234] Millions of biomedical abstracts. Biomedical literature retrieval. 2
QMSum [235] Meeting transcripts with query-based summaries. Query-focused dialogue summarization. 2
RealNews [236] 120 GB news articles from Common Crawl. News summarization benchmark. 2
RealTimeQA [65] Weekly news quizzes on politics, business, entertainment. Evaluate QA on current events requiring retrieval. 2
RepoEval [237] Curated GitHub repos for code completion benchmarks. Evaluate repository-level code completion. 2
WikiData [238] Structured knowledge graph for Wikipedia. Knowledge-base for various QA tasks. 2
Wikipedia (December 2021) [239] 37M passages, 78-word average. Updated Wikipedia text corpus. 2
Wikipedia Event (WikiEvent) [240] 246 docs, 6,132 sentences, 3,951 events. Event extraction and coreference analysis. 2
WikiText (WikiText) [241] 103M words (103); 2M words (2). Evaluate long-context language modeling. 2
1,000-User Benchmark Subset [242] 1,000 user-session sample with 493 queries avg. Train and evaluate personalized query prediction. 1
14 De-identified Clinical Scenarios [243] 14 anonymized patient scenarios with structured data. Evaluate clinical query handling. 1
2019 TREC Deep Learning track (TREC DL19) [244] 2019 deep-learning track for passage ranking. Benchmark passage ranking in IR. 1
2020 TREC Deep Learning track (TREC DL20) [245] 2020 deep-learning track for passage ranking. Benchmark passage ranking in IR. 1
35 Preoperative Guidelines [243] 35 guidelines on preoperative assessment and care. RAG knowledge for pre-op instructions. 1
ACE04 [246] 300k words train, 50k words evaluation Entity/relation extraction. 1
ActivityNet Captions [247] Consists of 20,000 YouTube videos with 100k localized sen-
tences.Dense video event description modeling. 1
ade-corpus-v2 [248] Sentences labeled for adverse drug reactions. Text classification focus on ADE detection in biomedical texts 1
Adversarial Benchmark (AdvBench ) [249] 520 harmful queries simulating jailbreak attacks. Support defense against adversarial prompts. 1
Adversarial NLI (ANLI) [250] Adversarial inference examples. Evaluating the inference and reasoning robustness of language
models.1
Adverse Drug Effect (ADE) [251] 2,972 documents on adverse drug effects Train ADE extraction models. 1
Agent-Driver [252] 23,000 driving episodes with states, objects, reasoning chains,
actions.Retrieval-based memory for safe driving planning. 1
Aggregated flood event listings from EMSR, GDACS, and
ReliefWeb [100]Curated list of major global flood disasters. Provide event codes for UI. 1
AGNews [253] 496k news articles in four topics. Topic classification in news. 1
AI Tutor [254] Course PDFs, HTML, and video transcripts. Retrieve source-based answers for students. 1
AIDA CoNLL-YAGO [255] CoNLL03 news articles linked to YAGO entities. Named entity disambiguation tasks. 1
Alzheimer’s Disease Interventions (ADInt) [256] Pharmaceutical interventions entries. Advance AD intervention knowledge extraction. 1
Alzheimer’s knowledge graph (AlzKB) [257] Neo4j dump of genes, diseases, drugs with NL statements and
embeddings.Drive precise biomedical RAG for Alzheimer’s queries. 1
Amazon Book Reviews [258] Reviews with user, product IDs, ratings. Analyze book recommendation and sentiment. 1
Continued on next page

40Dataset Name Content Description Intended Use Citation
Fre-
quency
Amazon Movie Reviews [259] 42M reviews, 10M users, 3M items. Recommender-system and sentiment analysis 1
AmbigQA [260] 14,042 ambiguous open-domain questions with rewrites. Benchmark QA systems’ disambiguation ability. 1
American Association for the Study of Liver Diseases (AASLD)
[261]30 liver disease clinical practice guidelines Reference for hepatology QA tasks 1
Apnea-ECG Dataset (Sleep Apnoea Detection) [262] 70 long ECG recordings with minute-wise apnea labels. Detect sleep apnoea via ECG variability. 1
Arc-Easy [194] 5,197 easy science multiple-choice questions Benchmark simple science QA 1
Australian Open Legal QA (ALQA) [177] 232K legal docs, 69.5M lines, 1.47B tokens. Legal AI research on Australian law. 1
Automatic Content Extraction 2005 (ACE 2005) [246] 625k annotated words in English, Arabic, Chinese Train entity, relation, event extraction. 1
Avocado Research Email Collection [263] Corporate email archive with threads and metadata. Retrieval-augmented personalized email drafting. 1
Bias Benchmark for Question Answering (BBQ) [264] Multiple-choice QA testing nine social bias categories. Diagnose representational harms in QA. 1
BigPatent [265] 1.34M patent documents Abstractive text summarization. 1
Bing Search Logs [266] Three months of anonymized Bing queries and clicks. Build search-history memory for query suggestion. 1
BioChatter Continuous-Monitoring Benchmark Suite [53] Growing suite of biomedical LLM workflow tasks. Track performance over evolving system features. 1
BioChatter Knowledge-Graph Query-Generation Benchmark
[53]QA pairs with correct BioCypher graph queries. Evaluate LLM-to-KG query translation accuracy. 1
Biography [267] Long-form biographical narratives of various entities. Test biographical text generation. 1
Biomedical Instructions [91] 18k generated biomedical and clinical instruction sets. Fine-tune models on diverse biomedical tasks. 1
Biomedical Multiple Choice Questions (MCQ) [268] Biomedical MCQs with five answer options. Evaluate biomedical multiple-choice QA. 1
CaseHOLD [269] 846K contract provisions with 12.6K refined labels. Benchmark legal question-answering systems. 1
Census/projection-disaggregated gridded population datasets
[270]2020 global population grid disaggregated by census. Quantify populations in flood zones. 1
Chain-of-thought [271] Explicit multi-step reasoning demonstrations Foster coherent stepwise reasoning 1
ChEBI-20 [272] 33,010 molecule-caption pairs Chemical image captioning models 1
Chemical Protein Interaction Corpus (ChemProt) [273] 2,432 PubMed abstracts annotated with interactions. Chemical-protein relationships and advancing the performance
of biomedical relation extraction algorithms1
ClashEval Drug Dosage [109] 249 QA pairs on drug dosages with perturbed contexts. Benchmark precise dosage retrieval from text. 1
ClashEval Locations [109] 200 QA pairs asking for place names from entries. Test place-name retrieval under context errors. 1
ClashEval Names [109] 200 QA pairs querying two-word proper names. Benchmark proper-noun retrieval against noise. 1
ClashEval News [109] 238 numeric QA pairs from AP headline excerpts. Assess numerical answer extraction under noise. 1
ClashEval Sports Records [109] 191 QA pairs on Olympic-record tables with perturbations. Evaluate correct sports record retrieval. 1
ClashEval Wikipedia Dates [109] 200 QA pairs asking for four-digit years from text. Test year retrieval robustness under corruption. 1
Clinical Practice Guidelines [274] Curated guideline articles from MEDITRON. Support clinical decision-making tasks. 1
Code Refinement Dataset (CRD) [275] 2.3M bug-fix function pairs. Code repair and refinement. 1
CodeMatcher [276] 10.5M Java methods paired with first doc sentence. Retrieve exemplar code snippets for generation. 1
codeparrot/github-jupyter [277] 165k Jupyter notebooks with metadata Train code exemplar retrieval 1
Cognitive Reviewer [254] Research PDFs analyzed and ranked for reviews. Facilitate literature reviews via RAG. 1
ConceptNet [278] Multilingual commonsense KG with everyday concept triples. Augment LLM QA with retrieved commonsense subgraphs. 1
Conceptual 12M (CC12M) [279] 12M image-text pairs from the web. Pretrain vision-and-language models. 1
Concode [280] 100k train, 2k val/test of NL-to-Java examples. Generate code from natural language. 1
Conference on Natural Language Learning 2003 (CoNLL03)
[281]301k English/German tokens for NER. Named-entity recognition benchmark. 1
Conference on Natural Language Learning 2004 (CoNLL04)
[282]2k sentences for NER and SRL. Joint NER and semantic-role labeling. 1
Conversation QA (QAConv) [283] 10,259 conversations; 34,608 QA pairs. QA from informative multi-turn conversations. 1
ConvFinQA (CFQA) [284] Financial QA grounded in tables and text, requiring math. Table comprehension and arithmetic in dialogues. 1
Corpus for Enhancement of Lay Language Synthesis (CELLS)
[137]62,886 abstract - lay summary pairs from biomedical journals. Simplify scientific text. 1
COVID-19 Open Research Dataset (CORD19) [179] >140k articles on COVID-19, SARS, MERS (72k full-text). COVID-19 literature retrieval & QA. 1
COYO-700M (COYO) [285] 747M image-text pairs with metadata. Support robust vision-language models. 1
CREAK [286] Human-authored true/false entity claims. Fact-checking and commonsense reasoning. 1
CrossCodeEval [287] Multilingual code completion benchmarks in four langs. Assess cross-language code completion generalization. 1
Continued on next page

41Dataset Name Content Description Intended Use Citation
Fre-
quency
CrossCodeLongEval [81] 5k chunk + 5k function completions from 1500 repos. Evaluate large-span code completion. 1
CSQA2.0 [288] Multiple-choice commonsense QA questions. Evaluate advanced commonsense reasoning. 1
Curated Golden Evaluation [37] Standard queries with tickets and authoritative solutions. Benchmark retrieval and answer accuracy. 1
CuratedTrec (CT) [289] 867 open-domain factoid questions. Benchmark factoid QA systems. 1
Current Events [48] 910 multiple-choice questions from Aug–Nov 2023 U.S. news
articles.Test LLM’s ability to learn new facts via fine-tuning/RAG. 1
CXR-PRO [290] 248,236 chest X-ray images with de-identified metadata. Support thoracic disease detection models. 1
CyberAttack Sensing and Information Extraction (CASIE) [291] 1,000 English news articles on cybersecurity events. Extract cybersecurity event information. 1
DailyDialog [292] 13,118 daily-life multi-turn dialogues. Develop human-like conversational agents. 1
Data Mining and Text Analytics Course Materials Corpus [138] 500 pages of course textbooks, transcripts, figures. RAG-enabled Q&A and knowledge retrieval for course. 1
De-identified electronic health records [293] 2,278 malnutrition-related clinical notes Validate summarization and extraction 1
Defects for Java version 1.2 (Defects4J (v1.2)) [294] 20,109 KLOC of Java code & tests with real bugs. Evaluate automated bug repair models. 1
DialogSum [21] 13k multi-speaker dialogues with human summaries. Evaluate conversational summarization. 1
DigMinecraft [295] Images and step-by-step task instructions Minecraft planning retrieval 1
Discrete Reasoning Over Paragraphs (DROP) [296] 96k questions requiring numeric and logical reasoning. Benchmark discrete reasoning in QA. 1
Django [297] NL descriptions and Django implementation code. Evaluate NL-to-code generation on Django framework. 1
Doc2Dial (D2D) [298] Document-grounded QA across four domains with long texts. Benchmark passage retrieval in conversational QA. 1
DomainRAG [299] Multiple RAG sub-datasets (extractive, noisy, etc.). Benchmark domain-specific retrieval-augmented generation. 1
DoQA [300] Conversational QA over cooking, travel, movie forums. Domain-specific dialogue QA with unanswerables. 1
Drug-Drug Interactions (DDI) [301] 1,025 texts from Medline and DrugBank. Identify and classify drug interactions. 1
Dynamed [302] Clinically organized summaries on 3,200+ topics. Point-of-care clinical reference tool. 1
EHRAgent [303] Four exemplar EHR cases + 700 patient “experience” records. Complex reasoning over EHR-based patient scenarios. 1
Emotion-Specific Dialogue [304] Chinese dialogues annotated for five emotions. Train emotion-conditioned dialogue agents. 1
EN.MC [305] 229 multiple-choice QAs on novel contexts. Benchmark novel-based MCQA. 1
En.QA [305] 351 QAs on long novels ( 150k words context). Test QA over very long texts. 1
Encyclopedic-VQA [306] 221k image QA pairs linked to 16.7k entities. Knowledge-based visual question answering. 1
EntityQuestion (EQ) [307] 17,300 QA pairs on 24 relation types Assess entity-centric knowledge retrieval 1
European Association for the Study of the Liver Guidelines
(EASL) [308]HCV screening, diagnosis, and treatment guidelines. Hepatology clinical decision support. 1
Extreme Summarization (XSum) [309] 226,711 news articles for single-sentence summaries. Support abstractive summarization models. 1
Facebook Books [310] User–book interactions data. Research book recommendation systems. 1
Fact Extraction and VERification Over Unstructured and Struc-
tured information (FEVEROUS) [311]87,026 claims with text and table evidence. Automate claim verification using text/tables. 1
FAct Verification from Information-seeking Questions (FaV-
IQAmbig) [312]188,000 true/false claims from info-seeking queries. Generate and assess factual QA claims. 1
FactKG [313] Claims aligned with knowledge-graph triples. Assess verification over structured KG. 1
Factual Recall Questions [111] 30 metadata-style queries (author, decision year, citation, etc.). Assess factual recall accuracy in legal RAG. 1
FACTUALITYPROMPTS [314] Prompts targeting factual accuracy and entity hallucinations Evaluate factual consistency in generation 1
False Premise Questions [111] 22 queries embedding legally incorrect assumptions. Probe AI’s handling of contrafactual legal prompts. 1
Fermi [315] Estimation “Fermi problems.” Reason about numeric magnitudes and estimates. 1
Fifty-Four Question-Answer Pairs for Few-Shot Learning [94] 54 hepatologist-crafted QA examples. Evaluate few-shot learning in clinical scenarios. 1
FinanceBench [316] 80 docs, 141 finance QA questions Open-book financial QA 1
Financial News [143] 79k Chinese news articles with ChatGPT summaries. Improve summarization and market context knowledge. 1
Financial Reports [143] 120k equity research reports with same-day price data. Teach LLMs technical analysis and trend prediction. 1
Financial Reports CoT [143] 200 CoT annotations on financial report predictions. Teach rationale-rich stock movement predictions. 1
FLAN [317] Natural language instructions for zero-shot learning Boost zero-shot performance and generalization 1
FloodBrain ablation study dataset [100] 26 paired human and FloodBrain flood reports Evaluate pipeline component impact. 1
FloodBrain evaluation dataset [100] 10 human vs 10 FloodBrain-generated flood reports. Compare generated vs human summaries. 1
FreebaseQA [318] 28k trivia-style QAs mapped to Freebase entities. KB-grounded question answering. 1
FreshQA [319] 600 questions with rapidly changing answers. Test QA on dynamic answers needing external search. 1
Gaokao-MM [320] 646 MCQs across 8 subjects with 897 images. Test multimodal perception and reasoning. 1
Continued on next page

42Dataset Name Content Description Intended Use Citation
Fre-
quency
Gender-Specific Dialogue [321] Chinese dialogues labeled by speaker gender. Model gendered linguistic features. 1
General Legal Research [111] 80 open-ended legal research questions (common-law, bar ex-
ams, doctrine).Benchmark legal-AI retrieval for practicing attorneys. 1
GIT [322] Biomedical triple-extraction dataset for non-drug therapies. Support biomedical relation extraction models. 1
GIT Relation Extraction (GITRE) [322] Sentences with head/tail entities and relations. Predict relationships between biomedical entities. 1
GPT-Generated Answer Evaluation Corpus [136] 100 answers with TA and automated correctness labels. Quantify model factual accuracy metrics. 1
GraphQA [323] ntegrates ExplaGraphs, SceneGraphs, WebQSP into QA. Graph-based QA benchmark. 1
GSM-HARD [324] GSM8K variant with larger numeric values Test arithmetic robustness 1
GSM8K [325] 8.5k grade-school math word problems Benchmark multi-step math reasoning 1
HANS [326] Heuristic-bias evaluation for NLI. Test NLI heuristic vulnerability. 1
Harry Potter Series (Books3 subset) [327] Full text of seven books ( 1 M words). Study model memorization and extraction from training. 1
Harvard Law Case Corpus [328] Extensive collection of Harvard Law case texts. Pretrain/fine-tune legal language models. 1
Harvard-FairVLMed [329] Multimodal fundus images with associated textual data. Fairness evaluation in ophthalmic vision-language. 1
HealthcareMagic-101 [330] 200k doctor-patient medical dialogues Model sensitive medical conversational contexts 1
Hearthstone [331] Game-card logic code paired with card names. Benchmark NL-to-code on game logic generation. 1
Historical Issue Tickets [37] Customer service tickets parsed into hierarchical trees. Improve retrieval/QA over support tickets. 1
Hospital Neurology Discharge Summaries [96] 100 anonymized neurology discharge summaries. Personalize advice and track recovery via memory. 1
Human-Edited Counterfactuals Subset of IMDb [101] 1.7K movie reviews manually sentiment-inverted. Augment data via sentiment counterfactuals. 1
Human-Generated Responses [243] Free-text pre-op instructions by junior doctors Baseline pre-op instruction generation 1
HumanEval [332] 164 Python programming tasks with unit tests. Evaluate code generation correctness. 1
HumanEval+ [333] 164 tasks with 80× more test cases Robustness evaluation for code generation 1
HybriDialogue (HDial) [334] QA on hybrid pages (text + tables) in conversation. Mixed-modal conversational reasoning. 1
IMDB (Internet Movie Database) [335] Subsets of movie reviews and associated metadata. Sentiment analysis and recommendation tasks. 1
InferredBugs [82] 6,200 repos; 8,280 bug-fix patches. Suppoty models on static-analysis bug fixes. 1
Infineon Developer Community Forum Questions [336] Technical Q&A with expert answers. Benchmark chatbot against forum solutions. 1
Infineon Product Documents [337] Datasheets and product guides. Retrieval for technical RAG systems. 1
InfoSeek [338] 1.3M image-QA triplets for 11k entities. Assess external knowledge integration in VQA. 1
INSCIT [339] Under-specified Wikipedia QA requiring clarification. Test clarification question generation. 1
IU-Xray [340] Chest X-rays paired with detailed diagnostic reports. Support medical image-reporting systems. 1
Joint Research Centre Acquis (JRCAcquis) [341] 8,000 legal docs per language, 20+ EU languages. Multilingual legal parallel corpus. 1
Jurisdiction or Time-Specific Research [111] 70 questions on jurisdictional splits or overturned precedents. Test RAG on time-sensitive legal rule retrieval. 1
Knowledge Intensive Language Tasks (KILT) [342] 11 datasets for fact checking, QA, entity linking. Unified evaluation of knowledge-intensive tasks. 1
Labeled EDGAR (LEDGAR) [178] 846K contract provisions with 12.6K refined labels. Contract clause classification. 1
Lambada [343] Cloze tasks requiring broad discourse context. Test long-range dependency in LMs. 1
Language Model Personalization (LaMP) [344] Seven classification and generation tasks. Benchmark personalized model outputs. 1
Lecture-Material [136] Lecture notes, slides, exercise sheets corpus. RAG retrieval for course-related queries. 1
LegalBench Collection [345] 50 manual legal QA pairs. Small-scale legal QA benchmarking. 1
LightQA [126] QA from role-playing dialogues with final utterance. Evaluate factual QA in game dialogue contexts. 1
LightWild [346] 462K utterances across 41K RPG episodes. Support dialogue agents in fantasy settings. 1
LiveQA [347] Real medical questions with long-form answers. Evaluate clinical long-answer generation. 1
LLaV A-Instruct [151] 158k image–instruction training pairs. Visual instruction tuning for MLLMs. 1
Lumos-QG-Generated QA Dataset (9 000 Pairs) [138] 9,000 auto-generated QA pairs from course materials. Expand knowledge base for Alexa skill and evaluation. 1
lyft_2021 [348] Lyft 2021 document used for chunking benchmark queries. Benchmark document-chunking techniques. 1
Massive Multi-discipline Multimodal Understanding (MMMU)
[349]11.5k college-level multimodal exam questions. Expert-level multimodal reasoning evaluation. 1
Math Nation Queries [139] 51 factual/conceptual math questions from forum. Benchmark math QA from student discussions. 1
MathVista [350] 6,141 math problems with diagrams, charts, plots. Evaluate multimodal math reasoning. 1
Medical Transcription Samples (MTsample) [351] Transcriptions across 40+ clinical specialties. Research clinical text classification patterns. 1
MedicationQA [352] Long-form QA focused on medication queries. Test medication-related answer accuracy. 1
MedInstruct [353] Biomedical instructions: QA, summarization, MCQs. Fine-tune models on diverse clinical tasks. 1
MedMCQA [354] Multiple-choice biomedical questions Benchmark biomedical QA systems. 1
Continued on next page

43Dataset Name Content Description Intended Use Citation
Fre-
quency
MedQA [355] Multiple-choice medical exam questions Evaluate medical QA models. 1
MetaQA [356] 400k questions covering single- and multi-hop reasoning. Test end-to-end KG QA systems. 1
Microsoft COCO (MSCOCO) [357] 328K images, 2.5M labeled object instances. Scene understanding and object detection. 1
Microsoft Research Paraphrase Corpus (MSRPC) [358] 2.2k train, 550 val, 1.1k test paraphrase pairs. Evaluate paraphrase detection. 1
Microsoft Research Video Description Corpus (MSVD) [359] 1,970 YouTube clips with 80k English descriptions. Benchmark video captioning models. 1
Microsoft Research Video to Text (MSRVTT) [360] 10,000 videos with 200k captions. Video captioning evaluation across domains. 1
MIMIC-CXR [361] Large public CXR images with radiology reports. Develop chest X-ray interpretation models. 1
Minecraft Wiki [362] Thousands of community-curated Minecraft articles Retrieval for planning tasks 1
Mintaka [363] Knowledge graph QA with complex, diverse questions. Knowledge graph QA benchmark. 1
MMBench (MMB) [364] 3k multiple-choice questions covering 20 abilities. Benchmark fine-grained multimodal capabilities. 1
Mol-Instructions [365] Off-the-shelf biomedical instruction tasks. Instruction-tuning biomedical models. 1
MongoDB-Logs (Chat & Cost) [136] Conversation logs and token-cost data. The logs underpin post-hoc accuracy checks, cost calculations
and support future optimisation of the chatbot service.1
MongoDB-QA (Question Answer Pairs) [136] 170 validated course QA pairs. It is sampled by the QAGeneration-Chain to generate quick
practice exercises for students.1
Mostly Basic Programming Problems (MBPP) [366] 974 beginner Python problems with tests Evaluate beginner-level code models 1
Mostly Basic Programming Problems+ (MBPP+) [333] MBPP tasks with added test cases Enhanced MBPP evaluation coverage 1
MovieLens100K [367] 100k movie ratings by various users. Benchmark recommendation algorithms. 1
MS-CXR [368] 1,153 chest X-rays with paired radiology reports. Evaluation CXR interpretation and report models. 1
Multi-Domain Wizard-of-Oz version 2.1 (MultiWOZ 2.1) [369] 10,438 dialogs across seven domains with slots. Develop and benchmark multi-domain dialogue. 1
Multi-Genre Natural Language Inference (MNLI) [370] 433k sentence pairs labeled entailment/contradiction/neutrality. Evaluate natural language inference models. 1
Multi-programming Language Commit Message (MCMD) [371] 2.25M commit messages across five programming languages. Evaluate semantic code search capabilities. 1
Multi-Sentence Reading Comprehension (MultiRC) [372] 800 paragraphs with 6,000 multi-sentence questions. Evaluation comprehension over multi-sentence contexts. 1
Multimodal Evaluation (MME) [373] 14 tasks in cognition and perception categories. Standardized benchmark for multimodal LLMs. 1
Natural Language to Bash (NL2Bash) [374] 9,000+ English descriptions paired with Bash commands. Translate natural language to shell commands. 1
Natural Language to Command Line (NLC2CMD) [375] 100 NL-to-command evaluation examples. Build NL-to-command translation systems. 1
New York Times (NYT) [376] 1.8M articles published between 1987–2007. News summarization. 1
NewsQA [377] 119k QA pairs from 12.7k CNN news articles. Human-generated question-answer pairs developed from news
articles from CNN1
NoCaps [378] 15k images of novel objects without MSCOCO overlap. Evaluate novel-object captioning. 1
North American HCV Guidelines [379] AASLD-IDSA supplemental HCV practice guidelines. Supplementary HCV clinical reference. 1
Online Sources Nursing Knowledge JSON [96] Scraped nursing instructions and academic papers JSON. Supply RAG pipeline with clinical knowledge. 1
OpenQA-NQ (subset of Natural Questions) [380] 13M evidence blocks from Wikipedia for QA retrieval. Open-retrieval question answering. 1
OpenStax Prealgebra Textbook [381] Textbook sections on prealgebra The content from the math textbook is used to generate responses
to real student questions.1
OpenStreetMap Planet dump [382] Global vector map data: roads, buildings, POIs. Enrich flood maps with geographic data. 1
Osaka Personal Activity Trajectory [51] 2,102 daily check-in trajectories, 537 synthetic samples. Evaluate mobility framework’s city generalization. 1
ParaSCI-ACL [383] 28,883 scientific paraphrase training examples. Scientific-domain paraphrase generation. 1
Patient Inquiry Dataset [96] Timestamped patient questions during system testing. Evaluate conversational performance and short-term memory. 1
Patient Symptom Record Dataset [96] Daily self-reported vital signs and symptom notes. Monitor condition changes and trigger alerts. 1
PDFTriage (PDFT) Questions on PDF document structures. Benchmark document-structure QA tasks. 1
PMC Full-text [384] Full-text articles from PubMed Central. Enable retrieval for biomedical question answering. 1
Polling-based Object Probing Evaluation (POPE) [385] Binary yes/no questions from ground truth objects/negatives. Assess object hallucination in V-L models. 1
Pre-training Corpus [386] 330 B tokens from 15 high-quality sources. Pretrain RETRO and GPT language models. 1
Probably-Asked Questions (PAQ) [387] 65 M auto-generated QA pairs Semi-structured KB QA knowledge base. 1
PTB-XL [388] 21,837 12-lead ECG records with cardiologist annotations. Arrhythmia diagnosis and zero-shot eval. 1
PTB-XL+ [389] Adds algorithm-extracted ECG features for each record. Detailed ECG feature analysis for diagnosis. 1
PubMed Abstract [390] Corpus of PubMed abstracts. Provide domain evidence for QA retrieval. 1
PwC Reading-Comprehension Corpus [391] 241k passage-question-answer triples. Research on large-context compression. 1
Python Code Summarization Dataset (PCSD) [392] 150 k function–docstring pairs Code summarization. 1
PyTorrent [393] 2M Python methods from PyPI/Anaconda packages. Code exemplar retrieval for Python generation. 1
Continued on next page

44Dataset Name Content Description Intended Use Citation
Fre-
quency
QReCC [394] Open-domain conversational QA over web docs (avg 5K words). Zero-shot conversational retrieval and QA. 1
QuAIL [395] 15k multiple-choice questions across varied texts. Evaluate adaptive QA across question types. 1
QuALITY [396] MCQs from stories/articles (multiple-choice). Narrative comprehension evaluation. 1
QuaRTz [397] 3,864 MCQs on qualitative relationships. Semantic and linguistic reasoning in QA. 1
Question Answering in Context (QuAC) [398] Multi-turn dialogues over Wikipedia with answerable turns. Conversational QA with linked long contexts. 1
Quora Question Pairs 140K (QQP) [399] 134k train, 5k val, 5k test paraphrase pairs. Paraphrase detection and generation. 1
Quora Question Pairs 50K (QQP) [400] 50k paraphrase question pairs. Paraphrase detection and generation. 1
RACE [401] Exams-derived reading comprehension dataset. Benchmark multi-paragraph comprehension. 1
RAG Comparison (Derived from the SPOKE KG) [134] Biomedical questions from SPOKE KG entity associations. Compare RAG: KG, Cypher, full-text methods. 1
RAG-Fusion Query Set [36] Dynamically generated multi-query sets. Enhance retrieval via rank fusion. 1
RAGTruth [402] 18,000 LLM-generated responses with quality labels. Benchmark hallucination detection in RAG. 1
Reading Comprehension with Commonsense Reasoning Dataset
(ReCoRD) [403]70 k passages, 120 k queries Commonsense reading comprehension 1
REALTOXICITYPROMPTS [404] Prompts engineered to elicit toxic language. Evaluate worst-case toxicity in outputs. 1
Reddit Webis-TLDR-17 [405] Reddit posts paired with short summaries Test summarization with varied tones 1
ReliefWeb flood reports [100] Human-authored situational flood event reports. Benchmark report factual accuracy. 1
Research Dataset [143] 42k finance texts merging sentiment, numeric, headline tasks. Pretrain/fine-tune LLMs on financial language. 1
Retrieval-Augmented Generation Benchmark (RGB ) [113] 1,000 English & Chinese QA Evaluate retrieval-augmented generation 1
RiddleSense [406] 5,000 riddles with answer options requiring creative reasoning. Challenge models on linguistic creativity and commonsense. 1
Roles Across Multiple Sentences (RAMS) [407] 3,993 docs, 9,124 event annotations Multi-sentence semantic role labeling 1
RTLLM [408] RTL generation benchmark tasks. Evaluate LLM-based RTL design generation. 1
SamSum [409] 16k messenger-style dialogues with abstractive summaries. Train dialogue summarization systems. 1
SBU Captions (SBU) [410] 1M Flickr-based image–caption pairs. Large-scale image captioning research. 1
SceneGraphs (from GQA) [411] 100k scene graphs of images for visual reasoning. Support spatial and visual inference tasks. 1
Scoliosis Research Society (SRS) [412] Educational, research, patient resources Support spinal deformity care 1
SearchQA [413] 140 k QA pairs, 6.9 M snippets QA simulating real web search 1
Self-Instruct [414] LM-generated instruction examples Support models on diverse self-generated directives 1
Sentiment-Specific Dialogue [123] English dialogues labeled by sentiment. Generate sentiment-controlled responses. 1
ServiceNow Internal Data [87] Annotated queries with structured workflow JSON. Translate NL requests into workflows. 1
SocialIQA (SIQA) [415] 38,000 social-context multiple-choice QA pairs. Test commonsense reasoning in social contexts. 1
SODA [416] High-quality social dialogue examples Enhance conversational fine-tuning 1
SQA [417] Conversational QA over single Wikipedia tables Compositional multi-column table QA. 1
SQuAD v2 [203] 150k QAs plus 50k unanswerable questions on Wikipedia. QA with answer/no-answer classification. 1
Stanford Sentiment Treebank (SST2) [418] 215k phrases labeled for fine-grained sentiment. Benchmark sentiment classification 1
StockQA [143] 21k Chinese QA pairs from real stock-price sequences. Train time-series reasoning for investor queries. 1
TACRED [419] Adapted TACRED for zero/few-shot slot filling (41 types). Benchmark relation extraction and slot filling. 1
TAM Questionnaire Response Set [136] 30 students’ Likert-scale survey responses. Evaluate user acceptance via factor/regression. 1
TFix [420] 100 k code error-fix pairs Evaluate code repair models 1
The human cost of disasters (2000-2019) [421] Global disaster human-impact records 2000–2019. Analyze flood impacts for planning. 1
The Pile [422] 825 GiB text from 22 sources Pretrain diverse language models 1
The Stack [423] 3 TB public source code from GitHub. Pretrain and fine-tune code language models. 1
Tokyo Personal Activity Trajectory [51] 100 users’ time-ordered GPS check-ins (2019–2022). Model realistic human mobility patterns. 1
ToolQA [424] Personal-agenda questions assessing external tool use. Measure LLM integration of external tools in QA. 1
TopiOCQA (TCQA) [425] QA over full Wikipedia with topic shifts. Evaluate topic-transition conversational QA. 1
TREC-COVID [426] Dynamic COVID-19 docs with topics and relevance labels. Pandemic literature retrieval evaluation. 1
True/False dataset [134] True/false statements on gene-disease and drug-disease. Benchmark biomedical assertion verification. 1
UltraDomain - Agriculture [427] 2,017,886 tokens from 12 college-agriculture texts Evaluate RAG’s sense-making in agriculture domain 1
UltraDomain - CS [427] 2,306,535 tokens from 10 computer-science texts Test RAG on technical computer-science content 1
UltraDomain - Legal [427] 5,081,069 tokens from 94 legal textbook documents Benchmark RAG on complex legal language and reasoning 1
UltraDomain - Mixed [427] 619,009 tokens across 61 humanities texts Challenge RAG with heterogeneous humanities content 1
Unnatural Instructions [428] Minimally human-curated challenging instructions Augment instruction tuning diversity 1
Continued on next page

45Dataset Name Content Description Intended Use Citation
Fre-
quency
UpToDate Clinical decision support content by Wolters Kluwer. Point-of-care medical reference. 1
V ATEX [429] 25,991 train, 9k val/test English video captions. Multilingual and multi-modal captioning. 1
VerilogEval [430] Verilog code generation tasks. Assess LLM Verilog functional correctness. 1
VerilogEval-syntax [85] 200+ clustered Verilog syntax error examples. Test syntax-error correction in Verilog. 1
Visual Question Answering (VQA) [431] 254,721 images with 760k questions and 10M answers. Visual QA tasks combining vision and language. 1
W3C-Email Emails similar to GPT-Neo’s training distribution Study retrieval-augmented memorization effects 1
Web Search - - 1
WebQA [432] 34,200 train, 5,000 val, 7,500 test QA pairs; 390k images. Multimodal web-based QA benchmarking. 1
Weibo [433] 4.4M post-response pairs from Sina Weibo. Support short-text conversation models. 1
WikiPassageQA [434] 4,165 QA with long answer passages. Reading comprehension with long answers. 1
Wikipedia (October 2017) [181] Snapshot of English Wikipedia articles. Historic Wikipedia text for NLP. 1
Wikipedia Evaluation (WikiEval) [435] 50 Wikipedia pages covering diverse topics. Evaluate retrieval-augmented systems. 1
Wikipedia Passages [436] 6M+ articles, 3.8B words across languages (as of 2021). Large-scale text corpus for NLP. 1
WinoGrande [437] Pronoun-resolution tasks in complex contexts. Assess coreference resolution capability. 1
WitQA [438] 14k factual QA pairs on 32 relation types Evaluate factual QA across relations 1
Wizard of the Internet (WizInt) [122] 9,633 dialogues, 93,665 utterances, 29,500 URLs. Dialogue with live internet search. 1
WNED [439] 320 documents with 6,821 linkable mentions. Evaluate entity linking systems. 1
Word-in-Context (WiC) [440] Word-in-context disambiguation pairs. Evaluate word sense disambiguation. 1
Worker and AI Collaboration for Natural Language Inference
(WaNLI) [441]107,885 NLI examples combining human and GPT-3 data. Natural language inference with AI mix. 1
Yelp Reviews [442] 1.1M+ reviews, 42k businesses, 400k tips, check-ins. Recommendation and sentiment analysis. 1
Yelp. 2021 [443] Business attributes and reviews with detailed schema. Data-to-text generation and hallucination tests. 1
ZINC-15 [444] 1.54B filtered SMILES strings Virtual screening compound datasets 1

46
ACKNOWLEDGMENT
This research is supported by the Advanced Research and
Engineering Centre (ARC) in Northern Ireland, funded by
PwC and Invest NI. The views expressed are those of the
authors and do not necessarily represent those of ARC or the
funding organisations.
The authors appreciate the use of the Kelvin2High Perfor-
mance Computing cluster at Queen’s University Belfast for
computational work.
REFERENCES
[1] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,
H. Kuttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela,
“Retrieval-augmented generation for knowledge-intensive nlp tasks,”
ArXiv, vol. abs/2005.11401, 2020.
[2] M. J. Page, J. E. McKenzie, P. M. Bossuyt, I. Boutron, T. C.
Hoffmann, C. D. Mulrow, L. Shamseer, J. M. Tetzlaff, E. A.
Akl, S. E. Brennan, R. Chou, J. Glanville, J. M. Grimshaw,
A. Hróbjartsson, M. M. Lalu, T. Li, E. W. Loder, E. Mayo-Wilson,
S. McDonald, L. A. McGuinness, L. A. Stewart, J. Thomas, A. C.
Tricco, V . A. Welch, P. Whiting, and D. Moher, “The prisma 2020
statement: an updated guideline for reporting systematic reviews,”
Systematic Reviews, vol. 10, no. 1, p. 89, 2021. [Online]. Available:
https://doi.org/10.1186/s13643-021-01626-4
[3] B. Kitchenham and S. Charters, “Guidelines for performing systematic
literature reviews in software engineering,” vol. 2, 2007.
[4] G. Sidiropoulos and E. Kanoulas, “Analysing the robustness
of dual encoders for dense retrieval against misspellings,” pp.
2132–2136 , numpages = 5, 2022. [Online]. Available: https:
//doi.org/10.1145/3477495.3531818
[5] Y . Kuratov, A. Bulatov, P. Anokhin, I. Rodkin, D. Sorokin,
A. Sorokin, and M. Burtsev, “Babilong: Testing the limits of llms
with long context reasoning-in-a-haystack,” p. arXiv:2406.10149, June
01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240610149K
[6] M. Alaofi, N. Arabzadeh, C. L. A. Clarke, and M. Sanderson,
“Generative information retrieval evaluation,” p. arXiv:2404.08137,
April 01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2024arXiv240408137A
[7] Y . Kumar and P. Marttinen, “Improving medical multi-modal con-
trastive learning with expert annotations,” inComputer Vision – ECCV
2024, A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler,
and G. Varol, Eds. Springer Nature Switzerland, 2025, Conference
Proceedings, pp. 468–486.
[8] M. Wang, L. Chen, F. Cheng, S. Liao, X. Zhang, B. Wu, H. Yu,
N. Xu, L. Zhang, R. Luo, Y . Li, M. Yang, F. Huang, and Y . Li,
“Leave no document behind: Benchmarking long-context llms with
extended multi-doc qa,” ser. Proceedings of the 2024 Conference on
Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2024, Conference Proceedings, pp. 5627–
5646. [Online]. Available: https://aclanthology.org/2024.emnlp-main.
322/https://doi.org/10.18653/v1/2024.emnlp-main.322
[9] J. Wu, J. Zhu, Y . Qi, J. Chen, M. Xu, F. Menolascina,
and V . Grau, “Medical graph rag: Towards safe medical large
language model via graph retrieval-augmented generation,” p.
arXiv:2408.04187, August 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240804187W
[10] L. Zheng, L. Yin, Z. Xie, C. Sun, J. Huang, C. Hao Yu, S. Cao,
C. Kozyrakis, I. Stoica, J. E. Gonzalez, C. Barrett, and Y . Sheng,
“Sglang: Efficient execution of structured language model programs,”
p. arXiv:2312.07104, December 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv231207104Z
[11] N. Arora, I. Chakraborty, and Y . Nishimura, “Ai–human hybrids
for marketing research: Leveraging large language models (llms)
as collaborators,”Journal of Marketing, vol. 89, no. 2, pp. 43–
70, 2025. [Online]. Available: https://journals.sagepub.com/doi/abs/10.
1177/00222429241276529
[12] R. K. Luu and M. J. Buehler, “Bioinspiredllm: Conversational
large language model for the mechanics of biological and bio-
inspired materials,”Advanced Science, vol. n/a, no. n/a, p. 2306724.
[Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/
advs.202306724[13] B. Zhang and H. Soh, “Extract, define, canonicalize: An
llm-based framework for knowledge graph construction,” ser.
Proceedings of the 2024 Conference on Empirical Methods
in Natural Language Processing. Association for Computational
Linguistics, 2024, Conference Proceedings, pp. 9820–9836.
[Online]. Available: https://aclanthology.org/2024.emnlp-main.548/
https://doi.org/10.18653/v1/2024.emnlp-main.548
[14] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou,
J. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-
plus: Learning to use tools for creating multimodal agents,” p.
arXiv:2311.05437, November 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv231105437L
[15] S. A. Gebreab, K. Salah, R. Jayaraman, M. H. u. Rehman, and S. El-
laham, “Llm-based framework for administrative task automation in
healthcare,” in2024 12th International Symposium on Digital Forensics
and Security (ISDFS), 2024, Conference Proceedings, pp. 1–7.
[16] L. Loukas, I. Stogiannidis, O. Diamantopoulos, P. Malakasiotis, and
S. Vassos, “Making llms worth every penny: Resource-limited text
classification in banking,” pp. 392–400 , numpages = 9, 2023.
[Online]. Available: https://doi.org/10.1145/3604237.3626891
[17] M. J. Buehler, “Mechgpt, a language-based strategy
for mechanics and materials modeling that connects
knowledge across scales, disciplines, and modalities,”
Applied Mechanics Reviews, vol. 76, no. 2, 2024. [Online].
Available: https://www.scopus.com/inward/record.uri?eid=2-s2.
0-85184374431&doi=10.1115%2f1.4063843&partnerID=40&md5=
084eb60d2696016fb425056f373995e0https://asmedigitalcollection.
asme.org/appliedmechanicsreviews/article-abstract/76/2/021001/
1169582/MechGPT-a-Language-Based-Strategy-for-Mechanics?
redirectedFrom=fulltext
[18] J. Chen, R. Zhang, J. Guo, M. de Rijke, W. Chen, Y . Fan, and
X. Cheng, “Continual learning for generative retrieval over dynamic
corpora,” pp. 306–315 , numpages = 10, 2023. [Online]. Available:
https://doi.org/10.1145/3583780.3614821
[19] Y . Mao, P. He, X. Liu, Y . Shen, J. Gao, J. Han, and
W. Chen, “Generation-augmented retrieval for open-domain question
answering,” inACL-IJCNLP 2021 - 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing, Proceedings of
the Conference. Association for Computational Linguistics (ACL),
2021, Conference Proceedings, pp. 4089–4100. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117763005&
partnerID=40&md5=b066a0e1a38949f470b9e34d6d825db9
[20] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi,
Y . Dong, O. Kuchaiev, B. Li, C. Xiao, A. Anandkumar, and
B. Catanzaro, “Shall we pretrain autoregressive language models
with retrieval? a comprehensive study,” inEMNLP 2023 -
2023 Conference on Empirical Methods in Natural Language
Processing, Proceedings, H. Bouamor, J. Pino, and K. Bali,
Eds. Association for Computational Linguistics (ACL), 2023,
Conference Proceedings, pp. 7763–7786. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184809925&
partnerID=40&md5=f21c337f908c533a46e32c6cd9808d92
[21] X. Cheng, X. Wang, X. Zhang, T. Ge, S.-Q. Chen, F. Wei,
H. Zhang, and D. Zhao, “xrag: Extreme context compression for
retrieval-augmented generation with one token,” p. arXiv:2405.13792,
May 01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/
abs/2024arXiv240513792C
[22] Z. Wang, A. Liu, H. Lin, J. Li, X. Ma, and Y . Liang, “Rat: Retrieval
augmented thoughts elicit context-aware reasoning in long-horizon
generation,” p. arXiv:2403.05313, March 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240305313W
[23] C. Jeong, “A study on the implementation of generative ai services
using an enterprise data-based llm application architecture,” p.
arXiv:2309.01105, September 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230901105J
[24] S. Zeng, J. Zhang, P. He, Y . Liu, Y . Xing, H. Xu, J. Ren, Y . Chang,
S. Wang, D. Yin, and J. Tang, “The good and the bad: Exploring privacy
issues in retrieval-augmented generation (rag),” ser. Findings of the
Association for Computational Linguistics: ACL 2024. Association for
Computational Linguistics, 2024, Conference Proceedings, pp. 4505–
4524. [Online]. Available: https://aclanthology.org/2024.findings-acl.
267/https://doi.org/10.18653/v1/2024.findings-acl.267
[25] W. Su, Y . Tang, Q. Ai, Z. Wu, and Y . Liu, “Dragin: Dynamic retrieval
augmented generation based on the information needs of large language
models,” p. arXiv:2403.10081, March 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240310081S

47
[26] J. Jin, Y . Zhu, G. Dong, Y . Zhang, X. Yang, C. Zhang,
T. Zhao, Z. Yang, Z. Dou, and J.-R. Wen, “Flashrag: A
modular toolkit for efficient retrieval-augmented generation research,”
p. arXiv:2405.13576, May 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240513576J
[27] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, “Corrective
retrieval augmented generation,” p. arXiv:2401.15884, January 01,
2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240115884Yhttp://arxiv.org/pdf/2401.15884
[28] M. Glass, G. Rossiello, M. F. M. Chowdhury, and A. Gliozzo,
“Robust retrieval augmented generation for zero-shot slot
filling,” inEMNLP 2021 - 2021 Conference on Empirical
Methods in Natural Language Processing, Proceedings.
Association for Computational Linguistics (ACL), 2021,
Conference Proceedings, pp. 1939–1949. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121620598&
partnerID=40&md5=464214c8c940d3c69f1bd25bd77d6c12
[29] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,
E. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets
long context large language models,” p. arXiv:2310.03025, October
01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv231003025X
[30] Y . Hoshi, D. Miyashita, Y . Ng, K. Tatsuno, Y . Morioka, O. Torii,
and J. Deguchi, “Ralle: A framework for developing and evaluating
retrieval-augmented large language models,” inEMNLP 2023 -
2023 Conference on Empirical Methods in Natural Language
Processing, Proceedings of the System Demonstrations, Y . Feng and
E. Lefever, Eds. Association for Computational Linguistics (ACL),
2023, Conference Proceedings, pp. 52–69. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184658768&
partnerID=40&md5=4c8fd3b4def1911bcdb4e0744f889668
[31] Z. Guo, L. Xia, Y . Yu, T. Ao, and C. Huang, “Lightrag: Simple and
fast retrieval-augmented generation,” p. arXiv:2410.05779, October
01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv241005779G
[32] W. Jiang, S. Zhang, B. Han, J. Wang, B. Wang, and T. Kraska,
“Piperag: Fast retrieval-augmented generation via algorithm-system
co-design,” p. arXiv:2403.05676, March 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240305676J
[33] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, “Self-rag:
Learning to retrieve, generate, and critique through self-reflection,”
arXiv preprint arXiv:2310.11511, 2023.
[34] C.-M. Chan, C. Xu, R. Yuan, H. Luo, W. Xue, Y . Guo, and
J. Fu, “Rq-rag: Learning to refine queries for retrieval augmented
generation,” p. arXiv:2404.00610, March 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240400610C
[35] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao,
and R. Yan, “Lift yourself up: Retrieval-augmented text
generation with self memory,” p. arXiv:2305.02437, May 01,
2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv230502437Chttps://arxiv.org/pdf/2305.02437.pdf
[36] Z. Rackauckas, “Rag-fusion: a new take on retrieval-augmented
generation,” p. arXiv:2402.03367, January 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240203367R
[37] Z. Xu, M. Jerome Cruz, M. Guevara, T. Wang, M. Deshpande,
X. Wang, and Z. Li, “Retrieval-augmented generation with knowledge
graphs for customer service question answering,” p. arXiv:2404.17723,
April 01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2024arXiv240417723X
[38] X. He, Y . Tian, Y . Sun, N. V . Chawla, T. Laurent, Y . LeCun,
X. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation
for textual graph understanding and question answering,” p.
arXiv:2402.07630, February 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240207630H
[39] C. Jin, Z. Zhang, X. Jiang, F. Liu, X. Liu, X. Liu, and X. Jin,
“Ragcache: Efficient knowledge caching for retrieval-augmented
generation,” p. arXiv:2404.12457, April 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240412457J
[40] Y . Wu, J. Zhu, S. Xu, K. Shum, C. Niu, R. Zhong, J. Song, and
T. Zhang, “Ragtruth: A hallucination corpus for developing trustworthy
retrieval-augmented language models,” p. arXiv:2401.00396, December
01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240100396Whttp://arxiv.org/pdf/2401.00396.pdf
[41] W. Yu, “Retrieval-augmented generation across heterogeneous knowl-
edge,” inProceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: HumanLanguage Technologies: Student Research Workshop, 2022, Conference
Proceedings, pp. 52–58.
[42] A. Asai, M. Gardner, and H. Hajishirzi, “Evidentiality-guided
generation for knowledge-intensive nlp tasks,” inNAACL
2022 - 2022 Conference of the North American Chapter
of the Association for Computational Linguistics: Human
Language Technologies, Proceedings of the Conference, 2022,
Conference Proceedings, pp. 2226–2243. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134509909&
partnerID=40&md5=b0e80af190195c05f0fe2c3bf6e31c91
[43] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig,
“Learning to filter context for retrieval-augmented generation,”
p. arXiv:2311.08377, November 01, 2023 2023. [Online]. Avail-
able: https://ui.adsabs.harvard.edu/abs/2023arXiv231108377Whttps://
arxiv.org/pdf/2311.08377.pdf
[44] S. Hofst ˜tter, J. Chen, K. Raman, and H. Zamani, “Fid-light:
Efficient and effective retrieval-augmented text generation,” pp.
1437–1447 , numpages = 11, 2023. [Online]. Available: https:
//doi.org/10.1145/3539618.3591687
[45] S. Xu, L. Pang, H. Shen, X. Cheng, and T. S. Chua,
“Search-in-the-chain: Interactively enhancing large language models
with search for knowledge-intensive tasks,” inWWW 2024 -
Proceedings of the ACM Web Conference. Association for Computing
Machinery, Inc, 2024, Conference Proceedings, pp. 1362–1373.
[Online]. Available: https://www.scopus.com/inward/record.uri?eid=
2-s2.0-85194069617&doi=10.1145%2f3589334.3645363&partnerID=
40&md5=48abc699f6c20ebddd522b66a9a3f1ed
[46] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky, “Bridging
the preference gap between retrievers and llms,” ser. Proceedings
of the 62nd Annual Meeting of the Association for Computational
Linguistics (V olume 1: Long Papers). Association for Computational
Linguistics, 2024, Conference Proceedings, pp. 10 438–10 451.
[Online]. Available: https://aclanthology.org/2024.acl-long.562/https:
//doi.org/10.18653/v1/2024.acl-long.562
[47] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and
W. Chen, “Enhancing retrieval-augmented large language models
with iterative retrieval-generation synergy,” p. arXiv:2305.15294, May
01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv230515294Shttps://arxiv.org/pdf/2305.15294.pdf
[48] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or re-
trieval? comparing knowledge injection in llms,” p. arXiv:2312.05934,
December 01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2023arXiv231205934Ohttps://arxiv.org/pdf/2312.05934.pdf
[49] X. Wang, Z. Wang, X. Gao, F. Zhang, Y . Wu, Z. Xu, T. Shi,
Z. Wang, S. Li, Q. Qian, R. Yin, C. Lv, X. Zheng, and X. Huang,
“Searching for best practices in retrieval-augmented generation,”
p. arXiv:2407.01219, July 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240701219W
[50] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,
J. Callan, and G. Neubig, “Active retrieval augmented generation,”
p. arXiv:2305.06983, May 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230506983J
[51] J. Wang, R. Jiang, C. Yang, Z. Wu, M. Onizuka, R. Shibasaki,
N. Koshizuka, and C. Xiao, “Large language models as urban
residents: An llm agent framework for personal mobility generation,”
p. arXiv:2402.14744, February 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240214744W
[52] Y . Tian, H. Song, Z. Wang, H. Wang, Z. Hu, F. Wang,
N. V . Chawla, and P. Xu, “Graph neural prompting with large
language models,” inProceedings of the AAAI Conference on
Artificial Intelligence, M. Wooldridge, J. Dy, and S. Natarajan,
Eds., vol. 38. Association for the Advancement of Artificial
Intelligence, 2024, Conference Proceedings, pp. 19 080–19 088.
[Online]. Available: https://www.scopus.com/inward/record.uri?
eid=2-s2.0-85185803457&doi=10.1609%2faaai.v38i17.29875&
partnerID=40&md5=7db44360ec0406f99fd74f888e6343ffhttps:
//ojs.aaai.org/index.php/AAAI/article/download/29875/31526
[53] S. Lobentanzer, S. Feng, N. Bruderer, A. Maier, A. G. Díaz, A. Strange,
A. Ismail, A. Kulaga, A. Dugourd, B. Zdrazil, B. Chassagnol,
C. Pommier, D. Lucarelli, E. M. McDonagh, E. Verkinderen, F. M.
Delgado-Chaves, G. Fuellen, H. Sonntag, J. Menger, L. Christiaen,
L. Geistlinger, L. Z. Zetsche, M. Engelke, M. McNutt, M. Harrison,
M. Hizli, N. Usanov, P. Baracho, S. Beier, S. Boeing, T. A.
Muranen, T. T. Le, V . Dragan, X.-R. Zhou, Y . Nielsen-Tehranchian,
Y . Song, C. Wang, J. Baumbach, J. Abreu-Vicente, N. Krehl, Q. Ma,
T. Lemberger, J. Saez-Rodriguez, and C. The BioChatter, “A platform
for the biomedical application of large language models,”Nature

48
Biotechnology, vol. 43, no. 2, pp. 166–169, 2025. [Online]. Available:
https://doi.org/10.1038/s41587-024-02534-3
[54] Y . Hu, Z. Lei, Z. Zhang, B. Pan, C. Ling, and L. Zhao, “Grag:
Graph retrieval-augmented generation,” p. arXiv:2405.16506, May
01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240516506H
[55] Y . Liu, X. Peng, X. Zhang, W. Liu, J. Yin, J. Cao, and
T. Du, “Ra-isf: Learning to answer and understand from retrieval
augmentation via iterative self-feedback,” ser. Findings of the
Association for Computational Linguistics: ACL 2024. Association for
Computational Linguistics, 2024, Conference Proceedings, pp. 4730–
4749. [Online]. Available: https://aclanthology.org/2024.findings-acl.
281/https://doi.org/10.18653/v1/2024.findings-acl.281
[56] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and
T. Derr, “Knowledge graph prompting for multi-document
question answering,” inProceedings of the AAAI Conference on
Artificial Intelligence, M. Wooldridge, J. Dy, and S. Natarajan,
Eds., vol. 38. Association for the Advancement of Artificial
Intelligence, 2024, Conference Proceedings, pp. 19 206–19 214.
[Online]. Available: https://www.scopus.com/inward/record.uri?
eid=2-s2.0-85188263953&doi=10.1609%2faaai.v38i17.29889&
partnerID=40&md5=e78bdeb37abd7b343bbf4a79ceda6a83https:
//ojs.aaai.org/index.php/AAAI/article/download/29889/31552
[57] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-
generation synergy augmented large language models,”arXiv preprint
arXiv:2310.05149, 2023.
[58] Z. Shi, S. Zhang, W. Sun, S. Gao, P. Ren, Z. Chen, and Z. Ren,
“Generate-then-ground in retrieval-augmented generation for multi-hop
question answering,” ser. Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (V olume 1: Long
Papers). Association for Computational Linguistics, 2024, Conference
Proceedings, pp. 7339–7353. [Online]. Available: https://aclanthology.
org/2024.acl-long.397/https://doi.org/10.18653/v1/2024.acl-long.397
[59] P. Cheng, Y . Ding, T. Ju, Z. Wu, W. Du, P. Yi, Z. Zhang,
and G. Liu, “Trojanrag: Retrieval-augmented generation can be
backdoor driver in large language models,” p. arXiv:2405.13401, May
01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240513401C
[60] K. Sawarkar, A. Mangal, and S. R. Solanki, “Blended rag: Improving
rag (retriever-augmented generation) accuracy with semantic search and
hybrid query-based retrievers,” in2024 IEEE 7th International Con-
ference on Multimedia Information Processing and Retrieval (MIPR),
2024, Conference Proceedings, pp. 155–161.
[61] W. Chen, H. Hu, X. Chen, P. Verga, and W. W. Cohen,
“Murag: Multimodal retrieval-augmented generator for open question
answering over images and text,” p. arXiv:2210.02928, October
01, 2022 2022. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2022arXiv221002928Chttps://arxiv.org/pdf/2210.02928.pdf
[62] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi,
R. Rana, and S. Nanayakkara, “Improving the domain adaptation
of retrieval augmented generation (rag) models for open
domain question answering,” p. arXiv:2210.02627, October 01,
2022 2022. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2022arXiv221002627Shttp://arxiv.org/pdf/2210.02627.pdf
[63] D. Caffagni, F. Cocchi, N. Moratelli, S. Sarto, M. Cornia, L. Baraldi,
and R. Cucchiara, “Wiki-llava: Hierarchical retrieval-augmented gen-
eration for multimodal llms,” in2024 IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops (CVPRW), 2024,
Conference Proceedings, pp. 1818–1826.
[64] H. Soudani, E. Kanoulas, and F. Hasibi, “Fine tuning vs. retrieval
augmented generation for less popular knowledge,” p. 12–22, 2024.
[Online]. Available: https://doi.org/10.1145/3673791.3698415
[65] Z. Zhang, M. Fang, and L. Chen, “Retrievalqa: Assessing adaptive
retrieval-augmented generation for short-form open-domain question
answering,” p. arXiv:2402.16457, February 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240216457Z
[66] Z. Jiang, X. Ma, and W. Chen, “Longrag: Enhancing retrieval-
augmented generation with long-context llms,” p. arXiv:2406.15319,
June 01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/
abs/2024arXiv240615319J
[67] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,
“Knowledge-augmented language model verification,” inEMNLP
2023 - 2023 Conference on Empirical Methods in Natural
Language Processing, Proceedings, H. Bouamor, J. Pino, and
K. Bali, Eds. Association for Computational Linguistics (ACL),
2023, Conference Proceedings, pp. 1720–1736. [Online]. Available:https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184807859&
partnerID=40&md5=c68bf610bacee466173e6d81b587d4ad
[68] G. Dong, Y . Zhu, C. Zhang, Z. Wang, Z. Dou, and J.-R.
Wen, “Understand what llm needs: Dual preference alignment
for retrieval-augmented generation,” p. arXiv:2406.18676, June 01,
2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240618676D
[69] T. Guo, Q. Yang, C. Wang, Y . Liu, P. Li, J. Tang, D. Li, and Y . Wen,
“Knowledgenavigator: leveraging large language models for enhanced
reasoning over knowledge graph,”Complex and Intelligent Systems,
2024. [Online]. Available: https://www.scopus.com/inward/record.
uri?eid=2-s2.0-85197269337&doi=10.1007%2fs40747-024-01527-8&
partnerID=40&md5=97e6c8171dd93a547ec050c0fe99dfd9https:
//link.springer.com/content/pdf/10.1007/s40747-024-01527-8.pdf
[70] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,
Y . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:
Redefining retrieval for rag systems,” p. 719–729, 2024. [Online].
Available: https://doi.org/10.1145/3626772.3657834
[71] W. Yu, H. Zhang, X. Pan, P. Cao, K. Ma, J. Li, H. Wang, and
D. Yu, “Chain-of-note: Enhancing robustness in retrieval-augmented
language models,” ser. Proceedings of the 2024 Conference on
Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2024, Conference Proceedings, pp. 14 672–
14 685. [Online]. Available: https://aclanthology.org/2024.emnlp-main.
813/https://doi.org/10.18653/v1/2024.emnlp-main.813
[72] D. S. Sachan, S. Reddy, W. Hamilton, C. Dyer, and
D. Yogatama, “End-to-end training of multi-document reader
and retriever for open-domain question answering,” inAdvances
in Neural Information Processing Systems, vol. 31, 2021,
Conference Proceedings, pp. 25 968–25 981. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129541798&
partnerID=40&md5=807d42f555c81fb40cc73d8ae128b197
[73] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-
Brown, and Y . Shoham, “In-context retrieval-augmented language mod-
els,”Transactions of the Association for Computational Linguistics,
vol. 11, pp. 1316–1331, 2023.
[74] L. Gui, B. Wang, Q. Huang, A. Hauptmann, Y . Bisk,
and J. Gao, “Kat: A knowledge augmented transformer for
vision-and-language,” inNAACL 2022 - 2022 Conference of the
North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Proceedings of the
Conference. Association for Computational Linguistics (ACL),
2022, Conference Proceedings, pp. 956–968. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138386766&
partnerID=40&md5=639944a2f55ae03dd363963b7e98c523
[75] Y . Guo, Z. Li, X. Jin, Y . Liu, Y . Zeng, W. Liu, X. Li, P. Yang,
L. Bai, J. Guo, and X. Cheng, “Retrieval-augmented code generation
for universal information extraction,” p. arXiv:2311.02962, November
01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv231102962Ghttps://arxiv.org/pdf/2311.02962.pdf
[76] S. Barnett, S. Kurniawan, S. Thudumu, Z. Brannelly, and
M. Abdelrazek, “Seven failure points when engineering a retrieval
augmented generation system,” p. arXiv:2401.05856, January 01,
2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240105856Bhttps://arxiv.org/pdf/2401.05856.pdf
[77] M. R. Parvez, W. U. Ahmad, S. Chakraborty, B. Ray,
and K. W. Chang, “Retrieval augmented code generation
and summarization,” inFindings of the Association for
Computational Linguistics, Findings of ACL: EMNLP 2021, 2021,
Conference Proceedings, pp. 2719–2734. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127019086&
partnerID=40&md5=307e4040b2a99bca5daa20214fc1763b
[78] S. Liu, Y . Chen, X. Xie, J. Siow, and Y . Liu, “Retrieval-
augmented generation for code summarization via hybrid gnn,”
inICLR 2021 - 9th International Conference on Learning
Representations, 2021, Conference Proceedings. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121205001&
partnerID=40&md5=a4d297a0cd465773b6670095c2fff13e
[79] W. Wang, Y . Wang, S. Joty, and S. C. Hoi, “Rap-gen: Retrieval-
augmented patch generation with codet5 for automatic program repair,”
inProceedings of the 31st ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineer-
ing, 2023, Conference Proceedings, pp. 146–158.
[80] J. Chen, X. Hu, Z. Li, C. Gao, X. Xia, and D. Lo, “Code search is all
you need? improving code suggestions with code search,” p. Article 73,
2024. [Online]. Available: https://doi.org/10.1145/3597503.3639085

49
[81] D. Wu, W. U. Ahmad, D. Zhang, M. Krishna Ramanathan, and
X. Ma, “Repoformer: Selective retrieval for repository-level code
completion,” p. arXiv:2403.10059, March 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240310059W
[82] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, and
A. Svyatkovskiy, “Inferfix: End-to-end program repair with llms,”
inESEC/FSE 2023 - Proceedings of the 31st ACM Joint Meeting
European Software Engineering Conference and Symposium on the
Foundations of Software Engineering, 2023, Conference Proceedings,
pp. 1646–1656. [Online]. Available: https://www.scopus.com/inward/
record.uri?eid=2-s2.0-85180554634&doi=10.1145%2f3611643.
3613892&partnerID=40&md5=528a86559e550f7c698f3e2b189da3db
[83] E. Shi, Y . Wang, W. Tao, L. Du, H. Zhang, S. Han, D. Zhang, and
H. Sun, “Race: Retrieval-augmented commit message generation,”
ser. Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing. Association for Computational
Linguistics, 2022, Conference Proceedings, pp. 5520–5530. [Online].
Available: https://aclanthology.org/2022.emnlp-main.372https:
//doi.org/10.18653/v1/2022.emnlp-main.372https://aclanthology.org/
2022.emnlp-main.372.pdf
[84] C. Yu, G. Yang, X. Chen, K. Liu, and Y . Zhou, “Bashexplainer:
Retrieval-augmented bash code comment generation based on fine-
tuned codebert,” in2022 IEEE International Conference on Software
Maintenance and Evolution (ICSME). IEEE, 2022, Conference
Proceedings, pp. 82–93.
[85] Y . Tsai, M. Liu, and H. Ren, “Rtlfixer: Automatically fixing rtl syntax
errors with large language model,” p. Article 53, 2024. [Online].
Available: https://doi.org/10.1145/3649329.3657353
[86] F. Zhang, B. Chen, Y . Zhang, J. Keung, J. Liu, D. Zan,
Y . Mao, J. G. Lou, and W. Chen, “Repocoder: Repository-level
code completion through iterative retrieval and generation,” in
EMNLP 2023 - 2023 Conference on Empirical Methods in Natural
Language Processing, Proceedings, H. Bouamor, J. Pino, and
K. Bali, Eds. Association for Computational Linguistics (ACL),
2023, Conference Proceedings, pp. 2471–2484. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183361792&
partnerID=40&md5=1fe994a4c0a355b40dce07e3da4333fa
[87] P. Béchard and O. Marquez Ayala, “Reducing hallucination
in structured outputs via retrieval-augmented generation,” p.
arXiv:2404.08189, April 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240408189B
[88] Y . Ke, L. Jin, K. Elangovan, H. Rizal Abdullah, N. Liu,
A. T. H. Sia, C. R. Soh, J. Y . M. Tung, J. C. L. Ong, and
D. S. W. Ting, “Development and testing of retrieval augmented
generation in large language models – a case study report,” p.
arXiv:2402.01733, January 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240201733K
[89] J. Ge, S. Sun, J. Owens, V . Galvez, O. Gologorskaya, J. C. Lai, M. J.
Pletcher, and K. Lai, “Development of a liver disease-specific large
language model chat interface using retrieval augmented generation,”
medRxiv, 2023.
[90] H. Yu, P. Guo, and A. Sano, “Zero-shot ecg diagnosis with large
language models and retrieval-augmented generation,” inMachine
Learning for Health (ML4H). PMLR, 2023, Conference Proceedings,
pp. 650–663.
[91] M. Jeong, J. Sohn, M. Sung, and J. Kang, “Improving medical
reasoning through retrieval and self-reflection with retrieval-augmented
large language models,”Bioinformatics, vol. 40, pp. i119–i129, 2024.
[Online]. Available: https://www.scopus.com/inward/record.uri?eid=
2-s2.0-85197105929&doi=10.1093%2fbioinformatics%2fbtae238&
partnerID=40&md5=f1adb2f19c35ef97cf0e62185ccfcfa1https:
//www.ncbi.nlm.nih.gov/pmc/articles/PMC11211826/pdf/btae238.pdf
[92] M. Ranjit, G. Ganapathy, R. Manuel, and T. Ganu, “Retrieval
augmented chest x-ray report generation using openai gpt models,”
p. arXiv:2305.03660, May 01, 2023 2023. [Online]. Avail-
able: https://ui.adsabs.harvard.edu/abs/2023arXiv230503660Rhttps://
arxiv.org/pdf/2305.03660.pdf
[93] W. Shi, Y . Zhuang, Y . Zhu, H. Iwinski, M. Wattenbarger, and
M. D. Wang, “Retrieval-augmented large language models for
adolescent idiopathic scoliosis patients in shared decision-making,”
2023. [Online]. Available: https://doi.org/10.1145/3584371.3612956
[94] S. Kresevic, M. Giuffrè, M. Ajcevic, A. Accardo, L. S. Crocè,
and D. L. Shung, “Optimization of hepatological clinical guidelines
interpretation by large language models: a retrieval augmented
generation-based framework,”npj Digital Medicine, vol. 7, no. 1,
2024. [Online]. Available: https://www.scopus.com/inward/record.
uri?eid=2-s2.0-85191075594&doi=10.1038%2fs41746-024-01091-y&partnerID=40&md5=6d44109de8d3a3997cf2069604d69bedhttps:
//www.nature.com/articles/s41746-024-01091-y.pdf
[95] P. Xia, K. Zhu, H. Li, H. Zhu, Y . Li, G. Li, L. Zhang, and
H. Yao, “Rule: Reliable multimodal rag for factuality in medical
vision language models,” ser. Proceedings of the 2024 Conference on
Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2024, Conference Proceedings, pp. 1081–
1093. [Online]. Available: https://aclanthology.org/2024.emnlp-main.
62/https://doi.org/10.18653/v1/2024.emnlp-main.62
[96] Y . Yang, C. Xu, J. Guo, T. Feng, and C. Ruan, “Improving
the rag-based personalized discharge care system by introducing
the memory mechanism,” 2024/10/22 2024. [Online]. Available:
http://dx.doi.org/10.20944/preprints202410.1696.v1
[97] G. Xiong, Q. Jin, Z. Lu, and A. Zhang, “Benchmarking
retrieval-augmented generation for medicine,” ser. Findings of the
Association for Computational Linguistics: ACL 2024. Association for
Computational Linguistics, 2024, Conference Proceedings, pp. 6233–
6251. [Online]. Available: https://aclanthology.org/2024.findings-acl.
372/https://doi.org/10.18653/v1/2024.findings-acl.372
[98] J. Miao, C. Thongprayoon, S. Suppadungsuk, O. A. Garcia Valencia,
and W. Cheungpasitporn, “Integrating retrieval-augmented
generation with large language models in nephrology: Advancing
practical applications,”Medicina (Lithuania), vol. 60, no. 3,
2024. [Online]. Available: https://www.scopus.com/inward/record.
uri?eid=2-s2.0-85188954082&doi=10.3390%2fmedicina60030445&
partnerID=40&md5=c1e7d483b1c9621a773a1979a00eef1ehttps:
//mdpi-res.com/d_attachment/medicina/medicina-60-00445/article_
deploy/medicina-60-00445.pdf?version=1709877206
[99] J. Baek, N. Chandrasekaran, S. Cucerzan, A. Herring, and S. K.
Jauhar, “Knowledge-augmented large language models for personalized
contextual query suggestion,” p. 3355–3366, 2024. [Online]. Available:
https://doi.org/10.1145/3589334.3645404
[100] G. Colverd, P. Darm, L. Silverberg, and N. Kasmanoff, “Floodbrain:
Flood disaster reporting by web-based retrieval augmented generation
with an llm,” p. arXiv:2311.02597, November 01, 2023 2023. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2023arXiv231102597C
[101] T. Dixit, B. Paranjape, H. Hajishirzi, and L. Zettlemoyer,
“Core: A retrieve-then-edit framework for counterfactual
data generation,” inFindings of the Association
for Computational Linguistics: EMNLP 2022, 2022,
Conference Proceedings, pp. 2964–2984. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149904020&
partnerID=40&md5=d69654f7bc97eb5c3a17d15bc011d857
[102] N. Wiratunga, R. Abeyratne, L. Jayawardena, K. Martin, S. Massie,
I. Nkisi-Orji, R. Weerasinghe, A. Liret, and B. Fleisch, “Cbr-rag: Case-
based reasoning for retrieval augmented generation in llms for legal
question answering,” ser. Case-Based Reasoning Research and Devel-
opment. Springer Nature Switzerland, 2024, Conference Proceedings,
pp. 445–460.
[103] J. Li, Y . Liu, W. Fan, X.-Y . Wei, H. Liu, J. Tang, and
Q. Li, “Empowering molecule discovery for molecule-caption
translation with large language models: A chatgpt perspective,”
p. arXiv:2306.06615, June 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230606615L
[104] D. Di Palma, “Retrieval-augmented recommender system: Enhancing
recommender systems with large language models,” pp. 1369–1373
, numpages = 5, 2023. [Online]. Available: https://doi.org/10.1145/
3604915.3608889
[105] A. Salemi, S. Kallumadi, and H. Zamani, “Optimization methods for
personalizing large language models through retrieval augmentation,” p.
752–762, 2024. [Online]. Available: https://doi.org/10.1145/3626772.
3657783
[106] J. R. Chowdhury, Y . Zhuang, and S. Wang, “Novelty
controlled paraphrase generation with retrieval augmented
conditional prompt tuning,” inProceedings of the 36th AAAI
Conference on Artificial Intelligence, AAAI 2022, vol. 36.
Association for the Advancement of Artificial Intelligence, 2022,
Conference Proceedings, pp. 10 535–10 544. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137030948&
partnerID=40&md5=52a2718c348d88341a16b8ceb649dfb1
[107] R. Zhang, H. Du, Y . Liu, D. Niyato, J. Kang, S. Sun, X. Shen, and
H. V . Poor, “Interactive ai with retrieval-augmented generation for next
generation networking,”IEEE Network, vol. 38, no. 6, pp. 414–424,
2024.
[108] Z. Li, C. Li, M. Zhang, Q. Mei, and M. Bendersky, “Retrieval
augmented generation or long-context llms? a comprehensive study and

50
hybrid approach,” p. arXiv:2407.16833, July 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240716833L
[109] K. Wu, E. Wu, and J. Zou, “Clasheval: Quantifying the tug-
of-war between an llm’s internal prior and external evidence,”
p. arXiv:2404.10198, April 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240410198W
[110] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia,
“Ares: An automated evaluation framework for retrieval-
augmented generation systems,” p. arXiv:2311.09476, November
01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv231109476Shttps://arxiv.org/pdf/2311.09476.pdf
[111] V . Magesh, F. Surani, M. Dahl, M. Suzgun, C. D. Manning, and D. E.
Ho, “Hallucination-free? assessing the reliability of leading ai legal
research tools,” p. arXiv:2405.20362, May 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240520362M
[112] Y . Tang and Y . Yang, “Multihop-rag: Benchmarking retrieval-
augmented generation for multi-hop queries,” p. arXiv:2401.15391,
January 01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2024arXiv240115391Thttps://arxiv.org/pdf/2401.15391.pdf
[113] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language
models in retrieval-augmented generation,” inProceedings of the
AAAI Conference on Artificial Intelligence, M. Wooldridge, J. Dy,
and S. Natarajan, Eds., vol. 38. Association for the Advancement
of Artificial Intelligence, 2024, Conference Proceedings, pp. 17 754–
17 762. [Online]. Available: https://www.scopus.com/inward/record.
uri?eid=2-s2.0-85189613527&doi=10.1609%2faaai.v38i16.29728&
partnerID=40&md5=6f78ac42d80af63bf434a065136db443https:
//ojs.aaai.org/index.php/AAAI/article/download/29728/31250
[114] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas:
Automated evaluation of retrieval augmented generation,” p.
arXiv:2309.15217, September 01, 2023 2023. [Online]. Avail-
able: https://ui.adsabs.harvard.edu/abs/2023arXiv230915217Ehttps://
arxiv.org/pdf/2309.15217.pdf
[115] J. Chen, Y . Pan, Y . Li, T. Yao, H. Chao, and T. Mei,
“Retrieval augmented convolutional encoder-decoder networks for
video captioning,”ACM Trans. Multimedia Comput. Commun. Appl.,
vol. 19, no. 1s, 2023. [Online]. Available: https://doi.org/10.1145/
3539225
[116] Z. Yang, W. Ping, Z. Liu, V . Korthikanti, W. Nie, D. A. Huang, L. Fan,
Z. Yu, S. Lan, B. Li, M. Shoeybi, M. Y . Liu, Y . Zhu, B. Catanzaro,
C. Xiao, and A. Anandkumar, “Re-vilm: Retrieval-augmented
visual language model for zero and few-shot image captioning,” in
Findings of the Association for Computational Linguistics: EMNLP
2023. Association for Computational Linguistics (ACL), 2023,
Conference Proceedings, pp. 11 844–11 857. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179156882&
partnerID=40&md5=fd64b875cb5ee889b41cdebe351c6a2b
[117] S. Sarto, M. Cornia, L. Baraldi, and R. Cucchiara, “Retrieval-
augmented transformer for image captioning,” pp. 1–7 , numpages =
7, 2022. [Online]. Available: https://doi.org/10.1145/3549555.3549585
[118] R. Ramos, D. Elliott, and B. Martins, “Retrieval-augmented image
captioning,” ser. Proceedings of the 17th Conference of the European
Chapter of the Association for Computational Linguistics. Association
for Computational Linguistics, 2023, Conference Proceedings,
pp. 3666–3681. [Online]. Available: https://aclanthology.org/2023.
eacl-main.266https://doi.org/10.18653/v1/2023.eacl-main.266
[119] W. Lin and B. Byrne, “Retrieval augmented visual question answering
with outside knowledge,” inProceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2022,
2022, Conference Proceedings, pp. 11 238–11 254. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146879946&
partnerID=40&md5=c4222e451164e42b40b744f072900fd2
[120] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec,
P. Liang, M. Lewis, L. Zettlemoyer, and W. T. Yih,
“Retrieval-augmented multimodal language modeling,” in
Proceedings of Machine Learning Research, vol. 202, 2023,
Conference Proceedings, pp. 39 755–39 769. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174391879&
partnerID=40&md5=36def2f7a9f60998f9950aed47ef3901
[121] Z. Liu, W. Ping, R. Roy, P. Xu, C. Lee, M. Shoeybi, and
B. Catanzaro, “Chatqa: Surpassing gpt-4 on conversational qa and
rag,” p. arXiv:2401.10225, January 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240110225L
[122] M. Komeili, K. Shuster, and J. Weston, “Internet-augmented
dialogue generation,” inProceedings of the Annual Meeting
of the Association for Computational Linguistics, vol. 1, 2022,
Conference Proceedings, pp. 8460–8478. [Online]. Available:https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136224764&
partnerID=40&md5=a408abb09a7584e1105555f1db0fc27c
[123] Y . Su, Y . Wang, D. Cai, S. Baker, A. Korhonen, and N. Collier,
“Prototype-to-style: Dialogue generation with style-aware editing
on retrieval memory,”IEEE/ACM Transactions on Audio Speech
and Language Processing, vol. 29, pp. 2152–2161, 2021.
[Online]. Available: https://www.scopus.com/inward/record.uri?
eid=2-s2.0-85111034669&doi=10.1109%2fTASLP.2021.3087948&
partnerID=40&md5=8e9dd161bf019afab871294d93c7c4abhttps:
//ieeexplore.ieee.org/document/9449993/
[124] D. Thulke, N. Daheim, C. Dugast, and H. Ney, “Efficient
retrieval augmented generation from unstructured knowledge
for task-oriented dialog,” p. arXiv:2102.04643, February 01,
2021 2021. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2021arXiv210204643Thttps://arxiv.org/pdf/2102.04643.pdf
[125] Z. Tian, W. Bi, X. Li, and N. L. Zhang, “Learning
to abstract for memory-augmented conversational response
generation,” inACL 2019 - 57th Annual Meeting of the
Association for Computational Linguistics, Proceedings of the
Conference. Association for Computational Linguistics (ACL),
2020, Conference Proceedings, pp. 3816–3825. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084050639&
partnerID=40&md5=3d589e099583894e1d3a4cc1cdd9836b
[126] L. Adolphs, K. Shuster, J. Urbanek, A. Szlam, and
J. Weston, “Reason first, then respond: Modular generation
for knowledge-infused dialogue,” inFindings of the
Association for Computational Linguistics: EMNLP 2022,
2022, Conference Proceedings, pp. 7141–7161. [Online]. Available:
https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149851963&
partnerID=40&md5=bdffa62ce979e33238d03feb7e2a5d2c
[127] Z. Chen, Z. Xiang, C. Xiao, D. Song, and B. Li, “Agentpoison:
Red-teaming llm agents via poisoning memory or knowledge bases,”
p. arXiv:2407.12784, July 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240712784C
[128] H. Chaudhari, G. Severi, J. Abascal, M. Jagielski, C. A. Choquette-
Choo, M. Nasr, C. Nita-Rotaru, and A. Oprea, “Phantom: General
trigger attacks on retrieval augmented language generation,” p.
arXiv:2405.20485, May 01, 2024 2024. [Online]. Available: https:
//ui.adsabs.harvard.edu/abs/2024arXiv240520485C
[129] Z. Qi, H. Zhang, E. Xing, S. Kakade, and H. Lakkaraju,
“Follow my instruction and spill the beans: Scalable data extraction
from retrieval-augmented generation systems,” p. arXiv:2402.17840,
February 01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2024arXiv240217840Q
[130] J. Xue, M. Zheng, Y . Hu, F. Liu, X. Chen, and Q. Lou, “Badrag:
Identifying vulnerabilities in retrieval augmented generation of large
language models,” p. arXiv:2406.00083, June 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240600083X
[131] G. Deng, Y . Liu, K. Wang, Y . Li, T. Zhang, and Y . Liu, “Pandora:
Jailbreak gpts by retrieval augmented generation poisoning,” p.
arXiv:2402.08416, February 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240208416D
[132] M. Li, H. Kilicoglu, H. Xu, and R. Zhang, “Biomedrag: A
retrieval augmented large language model for biomedicine,”Journal of
Biomedical Informatics, vol. 162, p. 104769, 2025. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1532046424001874
[133] N. Matsumoto, J. Moran, H. Choi, M. E. Hernandez, M. Venkatesan,
P. Wang, and J. H. Moore, “Kragen: a knowledge graph-enhanced
rag framework for biomedical problem solving using large language
models,”Bioinformatics, vol. 40, no. 6, 2024. [Online]. Available:
https://doi.org/10.1093/bioinformatics/btae353
[134] K. Soman, P. W. Rose, J. H. Morris, R. E. Akbas, B. Smith,
B. Peetoom, C. Villouta-Reyes, G. Cerono, Y . Shi, A. Rizk-Jackson,
S. Israni, C. A. Nelson, S. Huang, and S. E. Baranzini, “Biomedical
knowledge graph-optimized prompt generation for large language
models,”Bioinformatics, vol. 40, no. 9, 2024. [Online]. Available:
https://doi.org/10.1093/bioinformatics/btae560
[135] D. Soong, S. Sridhar, H. Si, J. S. Wagner, A. C. C. Sá, C. Y . Yu,
K. Karagoz, M. Guan, S. Kumar, H. Hamadeh, and B. W. Higgs,
“Improving accuracy of gpt-3/4 results on biomedical data using a
retrieval-augmented language model,”PLOS Digit Health, vol. 3, no. 8,
p. e0000568, 2024.
[136] A. T. Neumann, Y . Yin, S. Sowe, S. Decker, and M. Jarke, “An
llm-driven chatbot in higher education for databases and information
systems,”IEEE Transactions on Education, vol. 68, no. 1, pp. 103–116,
2025.

51
[137] Y . Guo, W. Qiu, G. Leroy, S. Wang, and T. Cohen, “Retrieval
augmentation of large language models for lay language generation,”
Journal of Biomedical Informatics, vol. 149, p. 104580, 2024.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S1532046423003015
[138] B. Alsafari, E. Atwell, A. Walker, and M. Callaghan, “Towards
effective teaching assistants: From intent-based chatbots to llm-
powered teaching assistants,”Natural Language Processing Journal,
vol. 8, p. 100101, 2024. [Online]. Available: https://www.sciencedirect.
com/science/article/pii/S2949719124000499
[139] Z. Levonian, C. Li, W. Zhu, A. Gade, O. Henkel, M.-
E. Postle, and W. Xing, “Retrieval-augmented generation to
improve math question-answering: Trade-offs between groundedness
and human preference,” p. arXiv:2310.03184, October 01,
2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv231003184Lhttps://arxiv.org/pdf/2310.03184.pdf
[140] X. Du and H. Ji, “Retrieval-augmented generative question answering
for event argument extraction,” ser. Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 2022, Conference Proceedings,
pp. 4649–4666. [Online]. Available: https://aclanthology.org/2022.
emnlp-main.307https://doi.org/10.18653/v1/2022.emnlp-main.307
[141] M. Alkhalaf, P. Yu, M. Yin, and C. Deng, “Applying generative
ai with retrieval augmented generation to summarize and extract
key clinical information from electronic health records,”Journal of
Biomedical Informatics, vol. 156, p. 104662, 2024. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1532046424000807
[142] Y . Ren, Y . Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-
and-sample: Document-level event argument extraction via hybrid
retrieval augmentation,” ser. Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (V olume 1: Long
Papers). Association for Computational Linguistics, 2023, Conference
Proceedings, pp. 293–306. [Online]. Available: https://aclanthology.
org/2023.acl-long.17https://doi.org/10.18653/v1/2023.acl-long.17
[143] X. Li, Z. Li, C. Shi, Y . Xu, Q. Du, M. Tan, and J. Huang, “Alphafin:
Benchmarking financial analysis with retrieval-augmented stock-
chain framework,” ser. Proceedings of the 2024 Joint International
Conference on Computational Linguistics, Language Resources
and Evaluation (LREC-COLING 2024). ELRA and ICCL, 2024,
Conference Proceedings, pp. 773–783. [Online]. Available: https:
//aclanthology.org/2024.lrec-main.69/
[144] A. Jimeno Yepes, Y . You, J. Milczek, S. Laverde, and R. Li, “Financial
report chunking for effective retrieval augmented generation,” p.
arXiv:2402.05131, February 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240205131J
[145] G. Izacard and E. Grave, “Leveraging passage retrieval with
generative models for open domain question answering,” ser.
Proceedings of the 16th Conference of the European Chapter of the
Association for Computational Linguistics: Main V olume. Association
for Computational Linguistics, 2021, Conference Proceedings, pp.
874–880. [Online]. Available: https://aclanthology.org/2021.eacl-main.
74https://doi.org/10.18653/v1/2021.eacl-main.74
[146] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford,
K. Millican, G. van den Driessche, J.-B. Lespiau, B. Damoc, A. Clark,
D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang,
L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving,
O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and
L. Sifre, “Improving language models by retrieving from trillions of
tokens,” p. arXiv:2112.04426, December 01, 2021 2021. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2021arXiv211204426B
[147] R. Luo, L. Sun, Y . Xia, T. Qin, S. Zhang, H. Poon, and T.-Y .
Liu, “Biogpt: generative pre-trained transformer for biomedical text
generation and mining,”Briefings in Bioinformatics, vol. 23, no. 6,
2022. [Online]. Available: https://doi.org/10.1093/bib/bbac409
[148] Y . Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware
unified pre-trained encoder-decoder models for code understanding
and generation,” ser. Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2021, Conference Proceedings, pp. 8696–
8708. [Online]. Available: https://aclanthology.org/2021.emnlp-main.
685https://doi.org/10.18653/v1/2021.emnlp-main.685
[149] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri, “Asleep
at the keyboard? assessing the security of github copilot’s code
contributions,” p. arXiv:2108.09293, August 01, 2021 2021. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2021arXiv210809293P
[150] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-
4: Enhancing vision-language understanding with advanced largelanguage models,” p. arXiv:2304.10592, April 01, 2023 2023. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2023arXiv230410592Z
[151] H. Liu, C. Li, Q. Wu, and Y . J. Lee, “Visual instruction tuning,”
p. arXiv:2304.08485, April 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230408485L
[152] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen,
X. Liu, J. Wang, W. Ge, Y . Fan, K. Dang, M. Du, X. Ren,
R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin, “Qwen2-vl: Enhancing
vision-language model’s perception of the world at any resolution,”
p. arXiv:2409.12191, September 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240912191W
[153] Anthropic, “Chat with claude,” 2024. [Online]. Available: https:
//claude.ai/chats
[154] B. Workshop, T. Le Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c,
D. Hesslow, R. Castagné, A. Sasha Luccioni, F. Yvon, M. Gallé, J. Tow,
A. M. Rush, S. Biderman, A. Webson, P. Sasanka Ammanamanchi,
T. Wang, B. Sagot, N. Muennighoff, A. Villanova del Moral,
O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy,
H. Nguyen, L. Saulnier, S. Tan, P. Ortiz Suarez, V . Sanh, H. Laurençon,
Y . Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi,
A. Soroa, A. Fikri Aji, A. Alfassy, A. Rogers, A. Kreisberg Nitzav,
C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien,
D. Ifeoluwa Adelani, D. Radev, E. González Ponferrada, E. Levkovizh,
E. Kim, E. Bar Natan, F. De Toni, G. Dupont, G. Kruszewski,
G. Pistilli, H. Elsahar, H. Benyamina, H. Tran, I. Yu, I. Abdulmumin,
I. Johnson, I. Gonzalez-Dios, J. de la Rosa, J. Chim, J. Dodge, J. Zhu,
J. Chang, J. Frohberg, J. Tobing, J. Bhattacharjee, K. Almubarak,
K. Chen, K. Lo, L. V on Werra, L. Weber, L. Phan, L. Ben allal,
L. Tanguy, M. Dey, M. Romero Muñoz, M. Masoud, M. Grandury,
M. Šaško, M. Huang, M. Coavoux, M. Singh, M. Tian-Jian Jiang,
M. Chien Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner,
N. Khamis, O. Nguyen, O. Espejel, O. de Gibert, P. Villegaset al.,
“Bloom: A 176b-parameter open-access multilingual language model,”
p. arXiv:2211.05100, November 01, 2022 2022. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2022arXiv221105100W
[155] DeepSeek-AI, A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao,
C. Dengr, C. Ruan, D. Dai, D. Guo, D. Yang, D. Chen, D. Ji,
E. Li, F. Lin, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Xu,
H. Yang, H. Zhang, H. Ding, H. Xin, H. Gao, H. Li, H. Qu,
J. L. Cai, J. Liang, J. Guo, J. Ni, J. Li, J. Chen, J. Yuan, J. Qiu,
J. Song, K. Dong, K. Gao, K. Guan, L. Wang, L. Zhang, L. Xu,
L. Xia, L. Zhao, L. Zhang, M. Li, M. Wang, M. Zhang, M. Zhang,
M. Tang, M. Li, N. Tian, P. Huang, P. Wang, P. Zhang, Q. Zhu,
Q. Chen, Q. Du, R. J. Chen, R. L. Jin, R. Ge, R. Pan, R. Xu,
R. Chen, S. S. Li, S. Lu, S. Zhou, S. Chen, S. Wu, S. Ye, S. Ma,
S. Wang, S. Zhou, S. Yu, S. Zhou, S. Zheng, T. Wang, T. Pei,
T. Yuan, T. Sun, W. L. Xiao, W. Zeng, W. An, W. Liu, W. Liang,
W. Gao, W. Zhang, X. Q. Li, X. Jin, X. Wang, X. Bi, X. Liu,
X. Wang, X. Shen, X. Chen, X. Chen, X. Nie, X. Sunet al.,
“Deepseek-v2: A strong, economical, and efficient mixture-of-experts
language model,” p. arXiv:2405.04434, May 01, 2024 2024. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2024arXiv240504434D
[156] B. Wang and A. Komatsuzaki, “Gpt-j-6b: A 6 billion parameter
autoregressive language model,” 2021. [Online]. Available: https:
//github.com/kingoflolz/mesh-transformer-jax
[157] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus,
Y . Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S.
Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-
Ros, M. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra,
A. Yu, V . Zhao, Y . Huang, A. Dai, H. Yu, S. Petrov, E. H.
Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V . Le,
and J. Wei, “Scaling instruction-finetuned language models,” p.
arXiv:2210.11416, October 01, 2022 2022. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2022arXiv221011416C
[158] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,
S. Shakeri, E. Taropa, P. Bailey, Z. Chen, E. Chu, J. H. Clark,
L. El Shafey, Y . Huang, K. Meier-Hellstern, G. Mishra, E. Moreira,
M. Omernick, K. Robinson, S. Ruder, Y . Tay, K. Xiao, Y . Xu,
Y . Zhang, G. Hernandez Abrego, J. Ahn, J. Austin, P. Barham,
J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y . Cheng,
C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave,
M. Dehghani, S. Dev, J. Devlin, M. Díaz, N. Du, E. Dyer, V . Feinberg,
F. Feng, V . Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez,
G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu,
J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah, M. Jagielski, W. Jia,
K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee,
E. Li, M. Li, W. Li, Y . Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu,

52
M. Maggioni, A. Mahendru, J. Maynez, V . Misra, M. Moussalem,
Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat,
M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter,
P. Riley, A. Castro Ros, A. Roy, B. Saetaet al., “Palm 2 technical
report,” p. arXiv:2305.10403, May 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230510403A
[159] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,
O. Levy, V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-
to-sequence pre-training for natural language generation, translation,
and comprehension,” ser. Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics. Association
for Computational Linguistics, 2020, Conference Proceedings,
pp. 7871–7880. [Online]. Available: https://aclanthology.org/2020.
acl-main.703https://doi.org/10.18653/v1/2020.acl-main.703
[160] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,
N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher,
C. Canton Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes,
J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn,
S. Hosseini, R. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa,
I. Kloumann, A. Korenev, P. Singh Koura, M.-A. Lachaux, T. Lavril,
J. Lee, D. Liskovich, Y . Lu, Y . Mao, X. Martinet, T. Mihaylov,
P. Mishra, I. Molybog, Y . Nie, A. Poulton, J. Reizenstein, R. Rungta,
K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian,
X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu,
Z. Yan, I. Zarov, Y . Zhang, A. Fan, M. Kambadur, S. Narang,
A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, “Llama 2: Open
foundation and fine-tuned chat models,” p. arXiv:2307.09288, July
01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv230709288Thttps://arxiv.org/pdf/2307.09288.pdf
[161] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,
A. Joulin, E. Grave, and G. Lample, “Llama: Open and efficient
foundation language models,” p. arXiv:2302.13971, February 01,
2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv230213971T
[162] TheBloke, “Llama 2 70b chat - awq,” 2023. [Online]. Avail-
able: https://huggingface.co/TheBloke/Llama-2-70B-Chat-AWQhttps:
//arxiv.org/abs/2307.09288
[163] Ai@Meta, “Llama 3 model card,” 2024. [Online]. Available:
https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md
[164] A. I. Meta, “Introducing llama 3.1: Our most capable models to date,”
2024. [Online]. Available: https://ai.meta.com/blog/meta-llama-3-1/
[165] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford,
D. Singh Chaplot, D. de las Casas, F. Bressand, G. Lengyel,
G. Lample, L. Saulnier, L. Renard Lavaud, M.-A. Lachaux,
P. Stock, T. Le Scao, T. Lavril, T. Wang, T. Lacroix, and
W. El Sayed, “Mistral 7b,” p. arXiv:2310.06825, October 01,
2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv231006825Jhttps://arxiv.org/pdf/2310.06825.pdf
[166] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
C. Bamford, D. Singh Chaplot, D. de las Casas, E. B. Hanna,
F. Bressand, G. Lengyel, G. Bour, G. Lample, L. Renard Lavaud,
L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang,
S. Antoniak, T. Le Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix,
and W. El Sayed, “Mixtral of experts,” p. arXiv:2401.04088, January
01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240104088J
[167] A. I. Nomic, “Gpt4all: Private, local ai chatbot platform by nomic,”
2025. [Online]. Available: https://www.nomic.ai/gpt4all
[168] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
“Language models are unsupervised multitask learners,” 2019, Confer-
ence Proceedings.
[169] “Openai product.” [Online]. Available: https://openai.com/product
[170] OpenAI, J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,
F. Leoni Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat,
R. Avila, I. Babuschkin, S. Balaji, V . Balcom, P. Baltescu, H. Bao,
M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro,
C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman,
G. Brockman, T. Brooks, M. Brundage, K. Button, T. Cai,
R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan,
C. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen,
M. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung, D. Cummings,
J. Currier, Y . Dai, C. Decareaux, T. Degry, N. Deutsch, D. Deville,
A. Dhar, D. Dohan, S. Dowling, S. Dunning, A. Ecoffet, A. Eleti,
T. Eloundou, D. Farhi, L. Fedus, N. Felix, S. Posada Fishman,
J. Forte, I. Fulford, L. Gao, E. Georges, C. Gibson, V . Goel,
T. Gogineni, G. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein,S. Gray, R. Greene, J. Gross, S. S. Gu, Y . Guo, C. Hallacy,
J. Han, J. Harris, Y . He, M. Heaton, J. Heidecke, C. Hesse,
A. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu, S. Hu,
X. Hu, J. Huizinga, S. Jain, S. Jainet al., “Gpt-4 technical report,”
p. arXiv:2303.08774, March 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230308774O
[171] OpenAI, A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh,
A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, A. M ˛ adry,
A. Baker-Whitcomb, A. Beutel, A. Borzunov, A. Carney, A. Chow,
A. Kirillov, A. Nichol, A. Paino, A. Renzin, A. Tachard Passos,
A. Kirillov, A. Christakis, A. Conneau, A. Kamali, A. Jabri, A. Moyer,
A. Tam, A. Crookes, A. Tootoochian, A. Tootoonchian, A. Kumar,
A. Vallone, A. Karpathy, A. Braunstein, A. Cann, A. Codispoti,
A. Galu, A. Kondrich, A. Tulloch, A. Mishchenko, A. Baek, A. Jiang,
A. Pelisse, A. Woodford, A. Gosalia, A. Dhar, A. Pantuliano,
A. Nayak, A. Oliver, B. Zoph, B. Ghorbani, B. Leimberger,
B. Rossen, B. Sokolowsky, B. Wang, B. Zweig, B. Hoover, B. Samic,
B. McGrew, B. Spero, B. Giertler, B. Cheng, B. Lightcap, B. Walkin,
B. Quinn, B. Guarraci, B. Hsu, B. Kellogg, B. Eastman, C. Lugaresi,
C. Wainwright, C. Bassin, C. Hudson, C. Chu, C. Nelson, C. Li, C. J.
Shern, C. Conger, C. Barette, C. V oss, C. Ding, C. Lu, C. Zhang,
C. Beaumont, C. Hallacy, C. Koch, C. Gibson, C. Kim, C. Choi,
C. McLeavey, C. Hesse, C. Fischer, C. Winter, C. Czarnecki, C. Jarvis,
C. Wei, C. Koumouzelis, D. Sherburnet al., “Gpt-4o system card,”
p. arXiv:2410.21276, October 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv241021276O
[172] J. Bai, S. Bai, Y . Chu, Z. Cui, K. Dang, X. Deng, Y . Fan,
W. Ge, Y . Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin,
D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren,
C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang, S. Wu,
B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y . Yao,
B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y . Zhang, Z. Zhang,
C. Zhou, J. Zhou, X. Zhou, and T. Zhu, “Qwen technical report,”
p. arXiv:2309.16609, September 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230916609B
[173] A. Salemi and H. Zamani, “Evaluating retrieval quality in retrieval-
augmented generation,” p. 2395–2400, 2024. [Online]. Available:
https://doi.org/10.1145/3626772.3657957
[174] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu,
H. Liu, T. Xu, E. Chen, Y . Luo, P. Cheng, H. Deng, Z. Wang, and
Z. Lu, “Crud-rag: A comprehensive chinese benchmark for retrieval-
augmented generation of large language models,” p. arXiv:2401.17043,
January 01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2024arXiv240117043Lhttps://arxiv.org/pdf/2401.17043.pdf
[175] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,
C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova,
L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit,
Q. Le, and S. Petrov, “Natural questions: A benchmark for question
answering research,”Transactions of the Association for Computational
Linguistics, vol. 7, pp. 452–466, 2019. [Online]. Available: https:
//aclanthology.org/Q19-1026https://doi.org/10.1162/tacl_a_00276
[176] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,
and L. Deng, “Ms marco: A human generated machine reading
comprehension dataset,” 2016. [Online]. Available: http://dblp.uni-trier.
de/db/conf/nips/coco2016.html#NguyenRSGTMD16
[177] U. Butler, “Open australian legal corpus,” 2025. [Online]. Available:
https://huggingface.co/datasets/isaacus/open-australian-legal-corpus
[178] D. Tuggener, P. von Däniken, T. Peetz, and M. Cieliebak, “Ledgar: A
large-scale multi-label corpus for text classification of legal provisions
in contracts,” ser. Proceedings of the Twelfth Language Resources and
Evaluation Conference. European Language Resources Association,
2020, Conference Proceedings, pp. 1235–1241. [Online]. Available:
https://aclanthology.org/2020.lrec-1.155/
[179] L. L. Wang, K. Lo, Y . Chandrasekhar, R. Reas, J. Yang, D. Burdick,
D. Eide, K. Funk, Y . Katsis, R. M. Kinney, Y . Li, Z. Liu, W. Merrill,
P. Mooney, D. A. Murdick, D. Rishi, J. Sheehan, Z. Shen, B. Stilson,
A. D. Wade, K. Wang, N. X. R. Wang, C. Wilhelm, B. Xie, D. M.
Raymond, D. S. Weld, O. Etzioni, and S. Kohlmeier, “Cord-19:
The covid-19 open research dataset,” ser. Proceedings of the 1st
Workshop on NLP for COVID-19 at ACL 2020. Association for
Computational Linguistics, 2020, Conference Proceedings. [Online].
Available: https://aclanthology.org/2020.nlpcovid19-acl.1
[180] Q. Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu, “Pubmedqa: A
dataset for biomedical research question answering,” ser. Proceedings
of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP). Association for

53
Computational Linguistics, 2019, Conference Proceedings, pp. 2567–
2577. [Online]. Available: https://aclanthology.org/D19-1259/https:
//doi.org/10.18653/v1/D19-1259
[181] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. Cohen, R. Salakhutdinov, and
C. D. Manning, “Hotpotqa: A dataset for diverse, explainable multi-
hop question answering,” ser. Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2018, Conference Proceedings, pp. 2369–
2380. [Online]. Available: https://aclanthology.org/D18-1259https:
//doi.org/10.18653/v1/D18-1259
[182] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa,
“Constructing a multi-hop qa dataset for comprehensive evaluation
of reasoning steps,” ser. Proceedings of the 28th International
Conference on Computational Linguistics. International Committee on
Computational Linguistics, 2020, Conference Proceedings, pp. 6609–
6625. [Online]. Available: https://aclanthology.org/2020.coling-main.
580https://doi.org/10.18653/v1/2020.coling-main.580
[183] X. Chen, H. Fang, T.-Y . Lin, R. Vedantam, S. Gupta, P. Dollar, and
C. L. Zitnick, “Microsoft coco captions: Data collection and evaluation
server,” p. arXiv:1504.00325, April 01, 2015 2015. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2015arXiv150400325C
[184] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
“Codesearchnet challenge: Evaluating the state of semantic code
search,” p. arXiv:1909.09436, September 01, 2019 2019. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2019arXiv190909436H
[185] S. Xu, L. Pang, J. Xu, H. Shen, and X. Cheng, “List-
aware reranking-truncation joint model for search and retrieval-
augmented generation,” p. arXiv:2402.02764, February 01,
2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240202764Xhttps://arxiv.org/pdf/2402.02764.pdf
[186] D. Wilmot and F. Keller, “Memory and knowledge augmented
language models for inferring salience in long-form stories,” ser.
Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing. Association for Computational
Linguistics, 2021, Conference Proceedings, pp. 851–865.
[Online]. Available: https://aclanthology.org/2021.emnlp-main.65https:
//doi.org/10.18653/v1/2021.emnlp-main.65https://aclanthology.org/
2021.emnlp-main.65.pdf
[187] H. Abdulrahman Alawwad, A. Alhothali, U. Naseem,
A. Alkhathlan, and A. Jamal, “Enhancing textbook question
answering task with large language models and retrieval
augmented generation,” p. arXiv:2402.05128, February 01,
2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240205128Ahttps://arxiv.org/pdf/2402.05128.pdf
[188] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “Triviaqa:
A large scale distantly supervised challenge dataset for reading
comprehension,” ser. Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics (V olume 1:
Long Papers). Association for Computational Linguistics, 2017,
Conference Proceedings, pp. 1601–1611. [Online]. Available: https:
//aclanthology.org/P17-1147https://doi.org/10.18653/v1/P17-1147
[189] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique:
Multihop questions via single-hop question composition,”Transactions
of the Association for Computational Linguistics, vol. 10, pp. 539–554,
2022. [Online]. Available: https://doi.org/10.1162/tacl_a_00475
[190] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal,
“Fever: a large-scale dataset for fact extraction and verification,”
ser. Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human
Language Technologies, V olume 1 (Long Papers). Association
for Computational Linguistics, 2018, Conference Proceedings, pp.
809–819. [Online]. Available: https://aclanthology.org/N18-1074https:
//doi.org/10.18653/v1/N18-1074
[191] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and
J. Berant, “Did aristotle use a laptop? a question answering
benchmark with implicit reasoning strategies,”Transactions of the
Association for Computational Linguistics, vol. 9, pp. 346–361,
2021. [Online]. Available: https://aclanthology.org/2021.tacl-1.21https:
//doi.org/10.1162/tacl_a_00370
[192] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,
“Wizard of wikipedia: Knowledge-powered conversational agents,”
p. arXiv:1811.01241, November 01, 2018 2018. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2018arXiv181101241D
[193] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing
on freebase from question-answer pairs,” ser. Proceedings of the
2013 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, 2013,Conference Proceedings, pp. 1533–1544. [Online]. Available:
https://aclanthology.org/D13-1160
[194] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
and O. Tafjord, “Think you have solved question answering? try
arc, the ai2 reasoning challenge,” p. arXiv:1803.05457, March
01, 2018 2018. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2018arXiv180305457C
[195] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, and
M. Auli, “Eli5: Long form question answering,” ser. Proceedings
of the 57th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 2019,
Conference Proceedings, pp. 3558–3567. [Online]. Available: https:
//aclanthology.org/P19-1346https://doi.org/10.18653/v1/P19-1346
[196] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and
J. Steinhardt, “Measuring massive multitask language understanding,”
p. arXiv:2009.03300, September 01, 2020 2020. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2020arXiv200903300H
[197] T. Ko ˇciský, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann,
G. Melis, and E. Grefenstette, “The narrativeqa reading comprehension
challenge,” p. arXiv:1712.07040, December 01, 2017 2017. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2017arXiv171207040K
[198] A. Mallen, A. Asai, V . Zhong, R. Das, D. Khashabi, and
H. Hajishirzi, “When not to trust language models: Investigating
effectiveness of parametric and non-parametric memories,” p.
arXiv:2212.10511, December 01, 2022 2022. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2022arXiv221210511M
[199] S. W.-t. Yih, M. Richardson, C. Meek, M.-W. Chang, and
J. Suh, “The value of semantic parse labeling for knowledge
base question answering,” pp. 201–206, August 2016. [Online].
Available: https://www.microsoft.com/en-us/research/publication/
the-value-of-semantic-parse-labeling-for-knowledge-base-question-answering-2/
[200] V . Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,
and W.-t. Yih, “Dense passage retrieval for open-domain question
answering,” ser. Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP). Association for
Computational Linguistics, 2020, Conference Proceedings, pp. 6769–
6781. [Online]. Available: https://aclanthology.org/2020.emnlp-main.
550https://doi.org/10.18653/v1/2020.emnlp-main.550
[201] I. Stelmakh, Y . Luan, B. Dhingra, and M.-W. Chang,
“Asqa: Factoid questions meet long-form answers,” ser.
Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing. Association for Computational
Linguistics, 2022, Conference Proceedings, pp. 8273–8288. [On-
line]. Available: https://aclanthology.org/2022.emnlp-main.566https:
//doi.org/10.18653/v1/2022.emnlp-main.566
[202] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of
armor conduct electricity? a new dataset for open book question
answering,” p. arXiv:1809.02789, September 01, 2018 2018. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2018arXiv180902789M
[203] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+
questions for machine comprehension of text,” ser. Proceedings of the
2016 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, 2016,
Conference Proceedings, pp. 2383–2392. [Online]. Available: https:
//aclanthology.org/D16-1264/https://doi.org/10.18653/v1/D16-1264
[204] H. Elsahar, P. V ougiouklis, A. Remaci, C. Gravier, J. Hare,
F. Laforest, and E. Simperl, “T-rex: A large scale alignment of
natural language with knowledge base triples,” ser. Proceedings of
the Eleventh International Conference on Language Resources and
Evaluation (LREC 2018). European Language Resources Association
(ELRA), 2018, Conference Proceedings. [Online]. Available: https:
//aclanthology.org/L18-1544
[205] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models
mimic human falsehoods,” ser. Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (V olume 1: Long
Papers). Association for Computational Linguistics, 2022, Conference
Proceedings, pp. 3214–3252. [Online]. Available: https://aclanthology.
org/2022.acl-long.229/https://doi.org/10.18653/v1/2022.acl-long.229
[206] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation
extraction via reading comprehension,” ser. Proceedings of the
21st Conference on Computational Natural Language Learning
(CoNLL 2017). Association for Computational Linguistics, 2017,
Conference Proceedings, pp. 333–342. [Online]. Available: https:
//aclanthology.org/K17-1034https://doi.org/10.18653/v1/K17-1034
[207] S. Reddy, D. Chen, and C. D. Manning, “Coqa: A conversational
question answering challenge,” p. arXiv:1808.07042, August 01,

54
2018 2018. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2018arXiv180807042R
[208] Y . Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du,
X. Liu, A. Zeng, L. Hou, Y . Dong, J. Tang, and J. Li, “Longbench:
A bilingual, multitask benchmark for long context understanding,”
p. arXiv:2308.14508, August 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230814508B
[209] Y . Bisk, R. Zellers, R. Le bras, J. Gao, and Y . Choi, “Piqa:
Reasoning about physical commonsense in natural language,”
Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 34, no. 05, pp. 7432–7439, 2020. [Online]. Available:
https://ojs.aaai.org/index.php/AAAI/article/view/6239
[210] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,
“A dataset of information-seeking questions and answers anchored in
research papers,” p. arXiv:2105.03011, May 01, 2021 2021. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2021arXiv210503011D
[211] Q. Guo, S. Cao, and Z. Yi, “A medical question answering system using
large language models and knowledge graphs,”International Journal
of Intelligent Systems, vol. 37, no. 11, pp. 8548–8564, 2022. [Online].
Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22955
[212] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,
and G. Neubig, “Wikiasp: A dataset for multi-domain aspect-based
summarization,”Transactions of the Association for Computational
Linguistics, vol. 9, pp. 211–225, 2021. [Online]. Available: https:
//doi.org/10.1162/tacl_a_00362
[213] Y . Y . W.-t. Y . C. Meek, “Wikiqa: A challenge dataset for open-domain
question answering,” pp. 2013–2018, September 17-21, 2015 2015.
[214] O. Press, M. Zhang, S. Min, L. Schmidt, N. A. Smith, and M. Lewis,
“Measuring and narrowing the compositionality gap in language
models,” p. arXiv:2210.03350, October 01, 2022 2022. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2022arXiv221003350P
[215] A. Krithara, A. Nentidis, K. Bougiatiotis, and G. Paliouras, “Bioasq-
qa: A manually curated corpus for biomedical question answering,”
Scientific Data, vol. 10, no. 1, p. 170, 2023. [Online]. Available:
https://doi.org/10.1038/s41597-023-02068-4
[216] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and
K. Toutanova, “Boolq: Exploring the surprising difficulty of natural
yes/no questions,” ser. Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, V olume 1 (Long and
Short Papers). Association for Computational Linguistics, 2019,
Conference Proceedings, pp. 2924–2936. [Online]. Available: https:
//aclanthology.org/N19-1300/https://doi.org/10.18653/v1/N19-1300
[217] S. Liu, Y . Chen, X. Xie, J. K. Siow, and Y . Liu, “Retrieval-augmented
generation for code summarization via hybrid gnn,” 2021 2021.
[Online]. Available: https://openreview.net/forum?id=zv-typ1gPxA
[218] A. See, P. J. Liu, and C. D. Manning, “Get to the point: Summarization
with pointer-generator networks,” ser. Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics (V olume
1: Long Papers). Association for Computational Linguistics, 2017,
Conference Proceedings, pp. 1073–1083. [Online]. Available: https:
//aclanthology.org/P17-1099/https://doi.org/10.18653/v1/P17-1099
[219] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou,
L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan,
N. Sundaresan, S. K. Deng, S. Fu, and S. Liu, “Codexglue: A machine
learning benchmark dataset for code understanding and generation,”
p. arXiv:2102.04664, February 01, 2021 2021. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2021arXiv210204664L
[220] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer
learning with a unified text-to-text transformer,” p. arXiv:1910.10683,
October 01, 2019 2019. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2019arXiv191010683R
[221] G. Wenzek, M.-A. Lachaux, A. Conneau, V . Chaudhary, F. Guzmán,
A. Joulin, and E. Grave, “Ccnet: Extracting high quality monolingual
datasets from web crawl data,” ser. Proceedings of the Twelfth
Language Resources and Evaluation Conference. European Language
Resources Association, 2020, Conference Proceedings, pp. 4003–4012.
[Online]. Available: https://aclanthology.org/2020.lrec-1.494
[222] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa: A
question answering challenge targeting commonsense knowledge,” ser.
Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, V olume 1 (Long and Short Papers). Association for
Computational Linguistics, 2019, Conference Proceedings, pp. 4149–4158. [Online]. Available: https://aclanthology.org/N19-1421/https:
//doi.org/10.18653/v1/N19-1421
[223] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual
captions: A cleaned, hypernymed, image alt-text dataset for automatic
image captioning,” ser. Proceedings of the 56th Annual Meeting
of the Association for Computational Linguistics (V olume 1:
Long Papers). Association for Computational Linguistics, 2018,
Conference Proceedings, pp. 2556–2565. [Online]. Available: https:
//aclanthology.org/P18-1238https://doi.org/10.18653/v1/P18-1238
[224] A. M. J. X. J. W. S. S. A. G. P. W. M. Z.
Mike Conover, Matt Hayes and R. Xin, “Databricks-dolly-15k,”
2023. [Online]. Available: https://www.databricks.com/blog/2023/04/
12/dolly-first-open-commercially-viable-instruction-tuned-llm
[225] S. Saha, P. Yadav, L. Bauer, and M. Bansal, “Explagraphs:
An explanation graph generation task for structured commonsense
reasoning,” ser. Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing. Association for
Computational Linguistics, 2021, Conference Proceedings, pp. 7716–
7740. [Online]. Available: https://aclanthology.org/2021.emnlp-main.
609/https://doi.org/10.18653/v1/2021.emnlp-main.609
[226] X. Jia, E. Gavves, B. Fernando, and T. Tuytelaars, “Guiding long-short
term memory for image caption generation,” p. arXiv:1509.04942,
September 01, 2015 2015. [Online]. Available: https://ui.adsabs.
harvard.edu/abs/2015arXiv150904942J
[227] M. Luo, Y . Zeng, P. Banerjee, and C. Baral, “Weakly-
supervised visual-retriever-reader for knowledge-based question
answering,” ser. Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing. Association for
Computational Linguistics, 2021, Conference Proceedings, pp. 6417–
6431. [Online]. Available: https://aclanthology.org/2021.emnlp-main.
517https://doi.org/10.18653/v1/2021.emnlp-main.517
[228] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hellaswag:
Can a machine really finish your sentence?” ser. Proceedings of
the 57th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 2019,
Conference Proceedings, pp. 4791–4800. [Online]. Available: https:
//aclanthology.org/P19-1472/https://doi.org/10.18653/v1/P19-1472
[229] J. Ferguson, M. Gardner, H. Hajishirzi, T. Khot, and P. Dasigi,
“Iirc: A dataset of incomplete information reading comprehension
questions,” ser. Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP). Association for
Computational Linguistics, 2020, Conference Proceedings, pp. 1137–
1147. [Online]. Available: https://aclanthology.org/2020.emnlp-main.
86/https://doi.org/10.18653/v1/2020.emnlp-main.86
[230] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis,
A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki, “Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs,” p.
arXiv:2111.02114, November 01, 2021 2021. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2021arXiv211102114S
[231] A. Talmor, O. Yoran, A. Catav, D. Lahav, Y . Wang, A. Asai,
G. Ilharco, H. Hajishirzi, and J. Berant, “Multimodalqa: Complex
question answering over text, tables and images,” p. arXiv:2104.06039,
April 01, 2021 2021. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2021arXiv210406039T
[232] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi, “Ok-vqa: A
visual question answering benchmark requiring external knowledge,”
p. arXiv:1906.00067, May 01, 2019 2019. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2019arXiv190600067M
[233] T. Zhang, H. Luo, Y .-S. Chuang, W. Fang, L. Gaitskell, T. Hartvigsen,
X. Wu, D. Fox, H. Meng, and J. Glass, “Interpretable unified language
checking,” p. arXiv:2304.03728, April 01, 2023 2023. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2023arXiv230403728Z
[234] “Pubmed database,” 1996. [Online]. Available: https://pubmed.ncbi.
nlm.nih.gov/
[235] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha,
A. H. Awadallah, A. Celikyilmaz, Y . Liu, X. Qiu, and D. Radev,
“Qmsum: A new benchmark for query-based multi-domain meeting
summarization,” ser. Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies. Association for Computational
Linguistics, 2021, Conference Proceedings, pp. 5905–5921. [Online].
Available: https://aclanthology.org/2021.naacl-main.472/https://doi.org/
10.18653/v1/2021.naacl-main.472
[236] R. Zellers, A. Holtzman, H. Rashkin, Y . Bisk, A. Farhadi, F. Roesner,
and Y . Choi,Defending against neural fake news. Curran Associates
Inc., 2019, p. Article 812.

55
[237] F. Zhang, B. Chen, Y . Zhang, J. Keung, J. Liu, D. Zan,
Y . Mao, J.-G. Lou, and W. Chen, “Repocoder: Repository-
level code completion through iterative retrieval and generation,”
ser. Proceedings of the 2023 Conference on Empirical Methods
in Natural Language Processing. Association for Computational
Linguistics, 2023, Conference Proceedings, pp. 2471–2484.
[Online]. Available: https://aclanthology.org/2023.emnlp-main.151/
https://doi.org/10.18653/v1/2023.emnlp-main.151
[238] “Wikidata.” [Online]. Available: https://www.wikipedia.org/
[239] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,
J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Atlas:
Few-shot learning with retrieval augmented language models,” p.
arXiv:2208.03299, August 01, 2022 2022. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2022arXiv220803299I
[240] S. Li, H. Ji, and J. Han, “Document-level event argument extraction
by conditional generation,” ser. Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies. Association for
Computational Linguistics, 2021, Conference Proceedings, pp. 894–
908. [Online]. Available: https://aclanthology.org/2021.naacl-main.
69https://doi.org/10.18653/v1/2021.naacl-main.69
[241] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer
sentinel mixture models,” p. arXiv:1609.07843, September 01,
2016 2016. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2016arXiv160907843M
[242] J. Baek, N. Chandrasekaran, S. Cucerzan, A. herring, and S. K.
Jauhar, “Knowledge-augmented large language models for personalized
contextual query suggestion,” p. arXiv:2311.06318, November 01,
2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv231106318B
[243] Y . Ke, L. Jin, K. Elangovan, H. Rizal Abdullah, N. Liu,
A. T. H. Sia, C. R. Soh, J. Y . M. Tung, J. C. L. Ong, and
D. S. W. Ting, “Development and testing of retrieval augmented
generation in large language models – a case study report,” p.
arXiv:2402.01733, January 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240201733K
[244] N. Craswell, B. Mitra, E. Yilmaz, D. Campos, and E. M.
V oorhees, “Overview of the trec 2019 deep learning track,”
p. arXiv:2003.07820, March 01, 2020 2020. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2020arXiv200307820C
[245] N. Craswell, B. Mitra, E. Yilmaz, D. F. Campos, and E. M.
V oorhees, “Overview of the trec 2020 deep learning track,”ArXiv, vol.
abs/2102.07662, 2021.
[246] G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel,
and R. Weischedel, “The automatic content extraction (ace)
program – tasks, data, and evaluation,” ser. Proceedings of
the Fourth International Conference on Language Resources and
Evaluation (LREC’04). European Language Resources Association
(ELRA), 2004, Conference Proceedings. [Online]. Available: http:
//www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf
[247] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles,
“Dense-captioning events in videos,” p. arXiv:1705.00754, May
01, 2017 2017. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2017arXiv170500754K
[248] H. Gurulingappa, A. M. Rajput, A. Roberts, J. Fluck, M. Hofmann-
Apitius, and L. Toldo, “Development of a benchmark corpus to support
the automatic extraction of drug-related adverse effects from medical
case reports,”J Biomed Inform, vol. 45, no. 5, pp. 885–92, 2012.
[249] W. Lu, Z. Zeng, J. Wang, Z. Lu, Z. Chen, H. Zhuang, and
C. Chen, “Eraser: Jailbreaking defense in large language models
via unlearning harmful knowledge,” p. arXiv:2404.05880, April
01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240405880L
[250] Y . Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and
D. Kiela, “Adversarial nli: A new benchmark for natural language
understanding,” ser. Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics. Association for
Computational Linguistics, 2020, Conference Proceedings, pp. 4885–
4901. [Online]. Available: https://aclanthology.org/2020.acl-main.441/
https://doi.org/10.18653/v1/2020.acl-main.441
[251] H. Gurulingappa, A. M. Rajput, A. Roberts, J. Fluck, M. Hofmann-
Apitius, and L. Toldo, “Development of a benchmark corpus to
support the automatic extraction of drug-related adverse effects
from medical case reports,”Journal of Biomedical Informatics,
vol. 45, no. 5, pp. 885–892, 2012. [Online]. Available: https:
//www.sciencedirect.com/science/article/pii/S1532046412000615[252] J. Mao, J. Ye, Y . Qian, M. Pavone, and Y . Wang, “A language
agent for autonomous driving,” p. arXiv:2311.10813, November
01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv231110813M
[253] X. Zhang, J. Zhao, and Y . LeCun, “Character-level
convolutional networks for text classification,” 2015. [Online].
Available: https://proceedings.neurips.cc/paper_files/paper/2015/file/
250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf
[254] S. Barnett, S. Kurniawan, S. Thudumu, Z. Brannelly, and
M. Abdelrazek, “Seven failure points when engineering a retrieval
augmented generation system,” p. 194–199, 2024. [Online]. Available:
https://doi.org/10.1145/3644815.3644945
[255] J. Hoffart, M. A. Yosef, I. Bordino, H. Fürstenau, M. Pinkal,
M. Spaniol, B. Taneva, S. Thater, and G. Weikum, “Robust
disambiguation of named entities in text,” ser. Proceedings
of the 2011 Conference on Empirical Methods in Natural
Language Processing. Association for Computational Linguistics,
2011, Conference Proceedings, pp. 782–792. [Online]. Available:
https://aclanthology.org/D11-1072/
[256] Y . Xiao, Y . Hou, H. Zhou, G. Diallo, M. Fiszman, J. Wolfson,
H. Kilicoglu, Y . Chen, C. Su, H. Xu, W. G. Mantyh, and R. Zhang, “Re-
purposing non-pharmacological interventions for alzheimer’s diseases
through link prediction on biomedical literature,”medRxiv, 2023.
[257] J. D. Romano, V . Truong, R. Kumar, M. Venkatesan, B. E. Graham,
Y . Hao, N. Matsumoto, X. Li, Z. Wang, M. D. Ritchie, L. Shen, and
J. H. Moore, “The alzheimer’s knowledge base: A knowledge graph
for alzheimer disease research,”J Med Internet Res, vol. 26, p. e46777,
2024.
[258] L. Dong, S. Huang, F. Wei, M. Lapata, M. Zhou, and K. Xu, “Learning
to generate product reviews from attributes,” ser. Proceedings of
the 15th Conference of the European Chapter of the Association
for Computational Linguistics: V olume 1, Long Papers. Association
for Computational Linguistics, 2017, Conference Proceedings, pp.
623–632. [Online]. Available: https://aclanthology.org/E17-1059/
[259] J. McAuley and J. Leskovec, “Hidden factors and hidden topics:
understanding rating dimensions with review text,” p. 165–172, 2013.
[Online]. Available: https://doi.org/10.1145/2507157.2507163
[260] S. Min, J. Michael, H. Hajishirzi, and L. Zettlemoyer,
“Ambigqa: Answering ambiguous open-domain questions,” ser.
Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP). Association for
Computational Linguistics, 2020, Conference Proceedings, pp. 5783–
5797. [Online]. Available: https://aclanthology.org/2020.emnlp-main.
466/https://doi.org/10.18653/v1/2020.emnlp-main.466
[261] J. Ge, S. Sun, J. Owens, V . Galvez, O. Gologorskaya, J. C. Lai, M. J.
Pletcher, and K. Lai, “Development of a liver disease-specific large
language model chat interface using retrieval augmented generation,”
medRxiv, 2023.
[262] T. Penzel, G. B. Moody, R. G. Mark, A. L. Goldberger, and J. H. Peter,
“The apnea-ecg database,”Computers in Cardiology 2000. Vol.27 (Cat.
00CH37163), pp. 255–258, 2000.
[263] D. Oard, W. Webber, D. Kirsch, and S. Golitsynskiy,Avocado research
email collection. Philadelphia: Linguistic Data Consortium, 2015.
[264] A. Parrish, A. Chen, N. Nangia, V . Padmakumar, J. Phang,
J. Thompson, P. M. Htut, and S. Bowman, “Bbq: A hand-
built bias benchmark for question answering,” ser. Findings of the
Association for Computational Linguistics: ACL 2022. Association for
Computational Linguistics, 2022, Conference Proceedings, pp. 2086–
2105. [Online]. Available: https://aclanthology.org/2022.findings-acl.
165/https://doi.org/10.18653/v1/2022.findings-acl.165
[265] E. Sharma, C. Li, and L. Wang, “Bigpatent: A large-scale dataset
for abstractive and coherent summarization,” ser. Proceedings of
the 57th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 2019,
Conference Proceedings, pp. 2204–2213. [Online]. Available: https:
//aclanthology.org/P19-1212https://doi.org/10.18653/v1/P19-1212
[266] Microsoft, “Bing.”
[267] S. Min, K. Krishna, X. Lyu, M. Lewis, W.-t. Yih, P. Koh,
M. Iyyer, L. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-
grained atomic evaluation of factual precision in long form
text generation,” ser. Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2023, Conference Proceedings, pp. 12 076–
12 100. [Online]. Available: https://aclanthology.org/2023.emnlp-main.
741/https://doi.org/10.18653/v1/2023.emnlp-main.741
[268] C. J. Mungall, J. A. McMurry, S. Köhler, J. P. Balhoff, C. Borromeo,
M. Brush, S. Carbon, T. Conlin, N. Dunn, M. Engelstad, E. Foster,

56
J. P. Gourdine, J. O. Jacobsen, D. Keith, B. Laraway, S. E. Lewis,
J. NguyenXuan, K. Shefchek, N. Vasilevsky, Z. Yuan, N. Washington,
H. Hochheiser, T. Groza, D. Smedley, P. N. Robinson, and M. A.
Haendel, “The monarch initiative: an integrative data and analytic
platform connecting phenotypes to genotypes across species,”Nucleic
Acids Res, vol. 45, no. D1, pp. D712–d722, 2017.
[269] I. Chalkidis, A. Jana, D. Hartung, M. Bommarito, I. Androutsopoulos,
D. Katz, and N. Aletras, “Lexglue: A benchmark dataset for
legal language understanding in english,” ser. Proceedings of
the 60th Annual Meeting of the Association for Computational
Linguistics (V olume 1: Long Papers). Association for Computational
Linguistics, 2022, Conference Proceedings, pp. 4310–4330.
[Online]. Available: https://aclanthology.org/2022.acl-long.297/https:
//doi.org/10.18653/v1/2022.acl-long.297
[270] S. A. Bondarenko M., Kerr D. and A. Tatem., “Census/projection-
disaggregated gridded population datasets, adjusted to match the
corresponding unpd 2020 estimates, for 183 countries in 2020
using built-settlement growth model (bsgm) outputs,” 2020. [Online].
Available: www.worldpop.com
[271] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia,
E. Chi, Q. Le, and D. Zhou, “Chain-of-thought prompting elicits
reasoning in large language models,” p. arXiv:2201.11903, January
01, 2022 2022. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2022arXiv220111903W
[272] C. Edwards, C. Zhai, and H. Ji, “Text2mol: Cross-
modal molecule retrieval with natural language queries,” ser.
Proceedings of the 2021 Conference on Empirical Methods
in Natural Language Processing. Association for Computational
Linguistics, 2021, Conference Proceedings, pp. 595–607. [Online].
Available: https://aclanthology.org/2021.emnlp-main.47/https://doi.org/
10.18653/v1/2021.emnlp-main.47
[273] O. Taboureau, S. Nielsen, K. Audouze, N. Weinhold, D. Edsgärd,
F. Roque, I. Kouskoumvekaki, A. Bora, R. Curpan, T. Jensen,
S. Brunak, and T. Oprea, “Chemprot: A disease chemical biology
database,”Nucleic acids research, vol. 39, pp. D367–72, 2010.
[274] Z. Chen, A. Hernández Cano, A. Romanou, A. Bonnet, K. Matoba,
F. Salvi, M. Pagliardini, S. Fan, A. Köpf, A. Mohtashami, A. Sallinen,
A. Sakhaeirad, V . Swamy, I. Krawczuk, D. Bayazit, A. Marmet,
S. Montariol, M.-A. Hartley, M. Jaggi, and A. Bosselut, “Meditron-
70b: Scaling medical pretraining for large language models,” p.
arXiv:2311.16079, November 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv231116079C
[275] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White,
and D. Poshyvanyk, “An empirical study on learning bug-
fixing patches in the wild via neural machine translation,” p.
arXiv:1812.08693, December 01, 2018 2018. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2018arXiv181208693T
[276] C. Liu, X. Xia, D. Lo, Z. Liu, A. E. Hassan, and S. Li, “Codematcher:
Searching code based on sequential semantics of important query
words,”ACM Trans. Softw. Eng. Methodol., vol. 31, no. 1, p. Article
12, 2021. [Online]. Available: https://doi.org/10.1145/3465403
[277] CodeParrot, “github-jupyter.” [Online]. Available: https://huggingface.
co/datasets/codeparrot/github-jupyter
[278] R. Speer, J. Chin, and C. Havasi, “Conceptnet 5.5: An open
multilingual graph of general knowledge,”Proceedings of the AAAI
Conference on Artificial Intelligence, vol. 31, no. 1, 2017. [Online].
Available: https://ojs.aaai.org/index.php/AAAI/article/view/11164
[279] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12m:
Pushing web-scale image-text pre-training to recognize long-tail visual
concepts,” p. arXiv:2102.08981, February 01, 2021 2021. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2021arXiv210208981C
[280] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Mapping
language to code in programmatic context,” ser. Proceedings of the
2018 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, 2018,
Conference Proceedings, pp. 1643–1652. [Online]. Available: https:
//aclanthology.org/D18-1192https://doi.org/10.18653/v1/D18-1192
[281] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the conll-
2003 shared task: Language-independent named entity recognition,”
ser. Proceedings of the Seventh Conference on Natural Language
Learning at HLT-NAACL 2003, 2003, Conference Proceedings, pp.
142–147. [Online]. Available: https://aclanthology.org/W03-0419
[282] D. Roth and W.-t. Yih, “A linear programming formulation for
global inference in natural language tasks,” ser. Proceedings of the
Eighth Conference on Computational Natural Language Learning
(CoNLL-2004) at HLT-NAACL 2004. Association for ComputationalLinguistics, 2004, Conference Proceedings, pp. 1–8. [Online].
Available: https://aclanthology.org/W04-2401
[283] C.-S. Wu, A. Madotto, W. Liu, P. Fung, and C. Xiong,
“Qaconv: Question answering on informative conversations,” p.
arXiv:2105.06912, May 01, 2021 2021. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2021arXiv210506912W
[284] Z. Chen, S. Li, C. Smiley, Z. Ma, S. Shah, and W. Y . Wang, “Convfinqa:
Exploring the chain of numerical reasoning in conversational finance
question answering,” ser. Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2022, Conference Proceedings, pp. 6279–
6292. [Online]. Available: https://aclanthology.org/2022.emnlp-main.
421/https://doi.org/10.18653/v1/2022.emnlp-main.421
[285] M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim,
“Coyo-700m: Image-text pair dataset,” 2022. [Online]. Available:
https://github.com/kakaobrain/coyo-dataset
[286] Y . Onoe, M. J. Q. Zhang, E. Choi, and G. Durrett, “Creak:
A dataset for commonsense reasoning over entity knowledge,” p.
arXiv:2109.01653, September 01, 2021 2021. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2021arXiv210901653O
[287] Y . Ding, Z. Wang, W. U. Ahmad, H. Ding, M. Tan, N. Jain,
M. Krishna Ramanathan, R. Nallapati, P. Bhatia, D. Roth, and
B. Xiang, “Crosscodeeval: A diverse and multilingual benchmark
for cross-file code completion,” p. arXiv:2310.11248, October 01,
2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv231011248D
[288] A. Talmor, O. Yoran, R. Le Bras, C. Bhagavatula, Y . Goldberg, Y . Choi,
and J. Berant, “Commonsenseqa 2.0: Exposing the limits of ai through
gamification,” p. arXiv:2201.05320, January 01, 2022 2022. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2022arXiv220105320T
[289] P. Baudiš and J. Šedivý, “Modeling of the question answering task
in the yodaqa system,” inExperimental IR Meets Multilinguality,
Multimodality, and Interaction, J. Mothe, J. Savoy, J. Kamps, K. Pinel-
Sauvagnat, G. Jones, E. San Juan, L. Capellato, and N. Ferro, Eds.
Springer International Publishing, 2015, Conference Proceedings, pp.
222–228.
[290] C. N. Ramesh, Vignav and P. Rajpurkar, “Cxr-pro: Mimic-cxr with
prior references omitted (version 1.0.0),” 2022. [Online]. Available:
https://doi.org/10.13026/frag-yn96.
[291] T. Satyapanich, F. Ferraro, and T. Finin, “Casie: Extracting
cybersecurity event information from text,”Proceedings of the AAAI
Conference on Artificial Intelligence, vol. 34, no. 05, pp. 8749–8757,
2020. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/
view/6401
[292] Y . Li, H. Su, X. Shen, W. Li, Z. Cao, and S. Niu, “Dailydialog:
A manually labelled multi-turn dialogue dataset,” ser. Proceedings
of the Eighth International Joint Conference on Natural Language
Processing (V olume 1: Long Papers). Asian Federation of Natural
Language Processing, 2017, Conference Proceedings, pp. 986–995.
[Online]. Available: https://aclanthology.org/I17-1099
[293] M. Alkhalaf, P. Yu, M. Yin, and C. Deng, “Applying generative
ai with retrieval augmented generation to summarize and extract
key clinical information from electronic health records,”Journal of
Biomedical Informatics, vol. 156, p. 104662, 2024. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1532046424000807
[294] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: a database of existing
faults to enable controlled testing studies for java programs,” p.
437–440, 2014. [Online]. Available: https://doi.org/10.1145/2610384.
2628055
[295] “Dig minecraft.” [Online]. Available: https://www.digminecraft.com/
[296] D. Dua, Y . Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner,
“Drop: A reading comprehension benchmark requiring discrete
reasoning over paragraphs,” ser. Proceedings of the 2019 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, V olume 1 (Long and
Short Papers). Association for Computational Linguistics, 2019,
Conference Proceedings, pp. 2368–2378. [Online]. Available: https:
//aclanthology.org/N19-1246/https://doi.org/10.18653/v1/N19-1246
[297] Y . Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, and
S. Nakamura, “Learning to generate pseudo-code from source code
using statistical machine translation,” in2015 30th IEEE/ACM Inter-
national Conference on Automated Software Engineering (ASE), 2015,
Conference Proceedings, pp. 574–584.
[298] S. Feng, H. Wan, C. Gunasekara, S. Patel, S. Joshi, and
L. Lastras, “doc2dial: A goal-oriented document-grounded dialogue
dataset,” ser. Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP). Association for

57
Computational Linguistics, 2020, Conference Proceedings, pp. 8118–
8128. [Online]. Available: https://aclanthology.org/2020.emnlp-main.
652/https://doi.org/10.18653/v1/2020.emnlp-main.652
[299] S. Wang, J. Liu, S. Song, J. Cheng, Y . Fu, P. Guo, K. Fang,
Y . Zhu, and Z. Dou, “Domainrag: A chinese benchmark for evaluating
domain-specific retrieval-augmented generation,” p. arXiv:2406.05654,
June 01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/
abs/2024arXiv240605654W
[300] J. A. Campos, A. Otegi, A. Soroa, J. Deriu, M. Cieliebak,
and E. Agirre, “Doqa - accessing domain-specific faqs via
conversational qa,” ser. Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics. Association for
Computational Linguistics, 2020, Conference Proceedings, pp. 7302–
7314. [Online]. Available: https://aclanthology.org/2020.acl-main.652/
https://doi.org/10.18653/v1/2020.acl-main.652
[301] I. Segura-Bedmar, P. Martínez, and M. Herrero-Zazo, “Semeval-2013
task 9 : Extraction of drug-drug interactions from biomedical texts
(ddiextraction 2013),” ser. Second Joint Conference on Lexical
and Computational Semantics (*SEM), V olume 2: Proceedings
of the Seventh International Workshop on Semantic Evaluation
(SemEval 2013). Association for Computational Linguistics, 2013,
Conference Proceedings, pp. 341–350. [Online]. Available: https:
//aclanthology.org/S13-2056/
[302] “Dynamed.” [Online]. Available: https://www.dynamed.com/
[303] W. Shi, R. Xu, Y . Zhuang, Y . Yu, J. Zhang, H. Wu, Y . Zhu, J. Ho,
C. Yang, and M. D. Wang, “Ehragent: Code empowers large language
models for few-shot complex tabular reasoning on electronic health
records,”Proc Conf Empir Methods Nat Lang Process, vol. 2024, pp.
22 315–22 339, 2024.
[304] H. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu, “Emotional
chatting machine: Emotional conversation generation with internal
and external memory,”Proceedings of the AAAI Conference on
Artificial Intelligence, vol. 32, no. 1, 2018. [Online]. Available:
https://ojs.aaai.org/index.php/AAAI/article/view/11325
[305] X. Zhang, Y . Chen, S. Hu, Z. Xu, J. Chen, M. Khai Hao, X. Han,
Z. Leng Thai, S. Wang, Z. Liu, and M. Sun, “∞bench: Extending
long context evaluation beyond 100k tokens,” p. arXiv:2402.13718,
February 01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2024arXiv240213718Z
[306] T. Mensink, J. Uijlings, L. Castrejon, A. Goel, F. Cadar, H. Zhou,
F. Sha, A. Araujo, and V . Ferrari, “Encyclopedic vqa: Visual
questions about detailed properties of fine-grained categories,”
p. arXiv:2306.09224, June 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230609224M
[307] C. Sciavolino, Z. Zhong, J. Lee, and D. Chen, “Simple entity-
centric questions challenge dense retrievers,” ser. Proceedings
of the 2021 Conference on Empirical Methods in Natural
Language Processing. Association for Computational Linguistics,
2021, Conference Proceedings, pp. 6138–6148. [Online]. Avail-
able: https://aclanthology.org/2021.emnlp-main.496/https://doi.org/10.
18653/v1/2021.emnlp-main.496
[308] “Easl recommendations on treatment of hepatitis c: Final update of the
series,”J Hepatol, vol. 73, no. 5, pp. 1170–1218, 2020.
[309] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details,
just the summary! topic-aware convolutional neural networks for
extreme summarization,” ser. Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2018, Conference Proceedings, pp. 1797–
1807. [Online]. Available: https://aclanthology.org/D18-1206https:
//doi.org/10.18653/v1/D18-1206
[310] “Facebook books dataset.” [Online]. Available: https://github.com/
sisinflab/LinkedDatasets/tree/master/facebook_book
[311] R. Aly, Z. Guo, M. Schlichtkrull, J. Thorne, A. Vlachos,
C. Christodoulopoulos, O. Cocarascu, and A. Mittal, “Feverous:
Fact extraction and verification over unstructured and structured
information,” p. arXiv:2106.05707, June 01, 2021 2021. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2021arXiv210605707A
[312] J. Park, S. Min, J. Kang, L. Zettlemoyer, and H. Hajishirzi,
“Faviq: Fact verification from information-seeking questions,” p.
arXiv:2107.02153, July 01, 2021 2021. [Online]. Available: https:
//ui.adsabs.harvard.edu/abs/2021arXiv210702153P
[313] J. Kim, S. Park, Y . Kwon, Y . Jo, J. Thorne, and E. Choi,
“Factkg: Fact verification via reasoning on knowledge graphs,”
p. arXiv:2305.06590, May 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230506590K
[314] N. Lee, W. Ping, P. Xu, M. Patwary, P. N. Fung,
M. Shoeybi, and B. Catanzaro, “Factuality enhanced languagemodels for open-ended text generation,” pp. 34 586–34 599, 2022.
[Online]. Available: https://proceedings.neurips.cc/paper_files/paper/
2022/file/df438caa36714f69277daa92d608dd63-Paper-Conference.pdf
[315] A. Kalyan, A. Kumar, A. Chandrasekaran, A. Sabharwal,
and P. Clark, “How much coffee was consumed during
emnlp 2019? fermi problems: A new reasoning challenge for
ai,” ser. Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing. Association for
Computational Linguistics, 2021, Conference Proceedings, pp. 7318–
7328. [Online]. Available: https://aclanthology.org/2021.emnlp-main.
582/https://doi.org/10.18653/v1/2021.emnlp-main.582
[316] P. Islam, A. Kannappan, D. Kiela, R. Qian, N. Scherrer, and
B. Vidgen, “Financebench: A new benchmark for financial question
answering,” p. arXiv:2311.11944, November 01, 2023 2023. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2023arXiv231111944I
[317] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus,
Y . Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S.
Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-
Ros, M. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra,
A. Yu, V . Zhao, Y . Huang, A. Dai, H. Yu, S. Petrov, E. H.
Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V . Le,
and J. Wei, “Scaling instruction-finetuned language models,” p.
arXiv:2210.11416, October 01, 2022 2022. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2022arXiv221011416C
[318] K. Jiang, D. Wu, and H. Jiang, “Freebaseqa: A new factoid qa data
set matching trivia-style question-answer pairs with freebase,” ser.
Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, V olume 1 (Long and Short Papers). Association
for Computational Linguistics, 2019, Conference Proceedings, pp.
318–323. [Online]. Available: https://aclanthology.org/N19-1028/https:
//doi.org/10.18653/v1/N19-1028
[319] T. Vu, M. Iyyer, X. Wang, N. Constant, J. Wei, J. Wei, C. Tar, Y .-H.
Sung, D. Zhou, Q. Le, and T. Luong, “Freshllms: Refreshing large
language models with search engine augmentation,” ser. Findings of the
Association for Computational Linguistics: ACL 2024. Association for
Computational Linguistics, 2024, Conference Proceedings, pp. 13 697–
13 720. [Online]. Available: https://aclanthology.org/2024.findings-acl.
813/https://doi.org/10.18653/v1/2024.findings-acl.813
[320] Y . Zong and X. Qiu, “Gaokao-mm: A chinese human-level
benchmark for multimodal models evaluation,” ser. Findings of the
Association for Computational Linguistics: ACL 2024. Association for
Computational Linguistics, 2024, Conference Proceedings, pp. 8817–
8825. [Online]. Available: https://aclanthology.org/2024.findings-acl.
521/https://doi.org/10.18653/v1/2024.findings-acl.521
[321] Y . Su, D. Cai, Y . Wang, S. Baker, A. Korhonen, N. Collier, and X. Liu,
“Stylistic dialogue generation via information-guided reinforcement
learning strategy,” p. arXiv:2004.02202, April 01, 2020 2020. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2020arXiv200402202S
[322] M. Li, H. Zhou, and R. Zhang, “Benchingmaking large langage
models in biomedical triple extraction,” p. arXiv:2310.18463, October
01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2023arXiv231018463L
[323] X. He, Y . Tian, Y . Sun, N. V . Chawla, T. Laurent, Y . LeCun,
X. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation
for textual graph understanding and question answering,” p.
arXiv:2402.07630, February 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240207630H
[324] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y . Yang,
J. Callan, and G. Neubig, “Pal: Program-aided language models,”
p. arXiv:2211.10435, November 01, 2022 2022. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2022arXiv221110435G
[325] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,
M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and
J. Schulman, “Training verifiers to solve math word problems,”
p. arXiv:2110.14168, October 01, 2021 2021. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2021arXiv211014168C
[326] Y . Zhou and C. Tan, “Investigating the effect of natural language
explanations on out-of-distribution generalization in few-shot nli,” ser.
Proceedings of the Second Workshop on Insights from Negative Results
in NLP. Association for Computational Linguistics, 2021, Conference
Proceedings, pp. 117–124. [Online]. Available: https://aclanthology.
org/2021.insights-1.17/https://doi.org/10.18653/v1/2021.insights-1.17
[327] S. Presser, “Books3,” 2020.
[328] “Harvard law case corpus.” [Online]. Available: https://case.law/
[329] Y . Luo, M. Shi, M. Osama Khan, M. Muneeb Afzal, H. Huang,
S. Yuan, Y . Tian, L. Song, A. Kouhana, T. Elze, Y . Fang, and

58
M. Wang, “Fairclip: Harnessing fairness in vision-language learning,”
p. arXiv:2403.19949, March 01, 2024 2024. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2024arXiv240319949L
[330] Y . Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y . Zhang,
“Chatdoctor: A medical chat model fine-tuned on a large language
model meta-ai (llama) using medical domain knowledge,” p.
arXiv:2303.14070, March 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230314070L
[331] W. Ling, P. Blunsom, E. Grefenstette, K. M. Hermann, T. Ko ˇciský,
F. Wang, and A. Senior, “Latent predictor networks for code
generation,” ser. Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (V olume 1: Long
Papers). Association for Computational Linguistics, 2016, Conference
Proceedings, pp. 599–609. [Online]. Available: https://aclanthology.
org/P16-1057/https://doi.org/10.18653/v1/P16-1057
[332] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde de Oliveira Pinto,
J. Kaplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman, A. Ray,
R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,
B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser,
M. Bavarian, C. Winter, P. Tillet, F. Petroski Such, D. Cummings,
M. Plappert, F. Chantzis, E. Barnes, A. Herbert-V oss, W. Hebgen Guss,
A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain,
W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V . Misra,
E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati,
K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
I. Sutskever, and W. Zaremba, “Evaluating large language models
trained on code,” p. arXiv:2107.03374, July 01, 2021 2021. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2021arXiv210703374C
[333] J. Liu, C. S. Xia, Y . Wang, and L. Zhang, “Is your code generated by
chatgpt really correct? rigorous evaluation of large language models
for code generation,” p. Article 943, 2023.
[334] K. Nakamura, S. Levy, Y .-L. Tuan, W. Chen, and W. Y . Wang,
“Hybridialogue: An information-seeking dialogue dataset grounded
on tabular and textual data,” ser. Findings of the Association for
Computational Linguistics: ACL 2022. Association for Computational
Linguistics, 2022, Conference Proceedings, pp. 481–492. [Online].
Available: https://aclanthology.org/2022.findings-acl.41/https://doi.org/
10.18653/v1/2022.findings-acl.41
[335] IMDb, “Imdb non-commercial datasets,” 2024. [Online]. Available:
https://developer.imdb.com/non-commercial-datasets/
[336] I. D. Community, “Developer community forum questions.” [Online].
Available: https://community.infineon.com/
[337] I. P. Documents, “Xensiv™– sensing the world sensor solutions
for automotive, industrial, consumer and iot applications.”
[Online]. Available: https://www.infineon.com/cms/en/product/sensor/
mems-microphones/
[338] Y . Chen, H. Hu, Y . Luan, H. Sun, S. Changpinyo,
A. Ritter, and M.-W. Chang, “Can pre-trained vision and
language models answer visual information-seeking questions?”
ser. Proceedings of the 2023 Conference on Empirical Methods
in Natural Language Processing. Association for Computational
Linguistics, 2023, Conference Proceedings, pp. 14 948–14 968.
[Online]. Available: https://aclanthology.org/2023.emnlp-main.925/
https://doi.org/10.18653/v1/2023.emnlp-main.925
[339] Z. Wu, R. Parish, H. Cheng, S. Min, P. Ammanabrolu,
M. Ostendorf, and H. Hajishirzi, “Inscit: Information-seeking
conversations with mixed-initiative interactions,”Transactions of the
Association for Computational Linguistics, vol. 11, pp. 453–468,
2023. [Online]. Available: https://aclanthology.org/2023.tacl-1.27/https:
//doi.org/10.1162/tacl_a_00559
[340] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan,
L. Rodriguez, S. Antani, G. R. Thoma, and C. J. McDonald, “Preparing
a collection of radiology examinations for distribution and retrieval,”J
Am Med Inform Assoc, vol. 23, no. 2, pp. 304–10, 2016.
[341] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec,
D. Tufi¸ s, and D. Varga, “The jrc-acquis: A multilingual aligned
parallel corpus with 20+ languages,” ser. Proceedings of the Fifth
International Conference on Language Resources and Evaluation
(LREC’06). European Language Resources Association (ELRA),
2006, Conference Proceedings. [Online]. Available: http://www.
lrec-conf.org/proceedings/lrec2006/pdf/340_pdf.pdf
[342] F. Petroni, A. Piktus, A. Fan, P. Lewis, M. Yazdani, N. De Cao,
J. Thorne, Y . Jernite, V . Karpukhin, J. Maillard, V . Plachouras,
T. Rocktäschel, and S. Riedel, “Kilt: a benchmark for knowledge
intensive language tasks,” ser. Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies. Association forComputational Linguistics, 2021, Conference Proceedings, pp. 2523–
2544. [Online]. Available: https://aclanthology.org/2021.naacl-main.
200/https://doi.org/10.18653/v1/2021.naacl-main.200
[343] D. Paperno, G. Kruszewski, A. Lazaridou, N. Q. Pham, R. Bernardi,
S. Pezzelle, M. Baroni, G. Boleda, and R. Fernández, “The lambada
dataset: Word prediction requiring a broad discourse context,” ser.
Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (V olume 1: Long Papers). Association for
Computational Linguistics, 2016, Conference Proceedings, pp. 1525–
1534. [Online]. Available: https://aclanthology.org/P16-1144/https:
//doi.org/10.18653/v1/P16-1144
[344] A. Salemi, S. Mysore, M. Bendersky, and H. Zamani, “Lamp: When
large language models meet personalization,” p. arXiv:2304.11406,
April 01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.
edu/abs/2023arXiv230411406S
[345] N. Guha, J. Nyarko, D. E. Ho, C. Ré, A. Chilton, A. Narayana,
A. Chohlas-Wood, A. Peters, B. Waldon, D. N. Rockmore,
D. Zambrano, D. Talisman, E. Hoque, F. Surani, F. Fagan,
G. Sarfaty, G. M. Dickinson, H. Porat, J. Hegland, J. Wu,
J. Nudell, J. Niklaus, J. Nay, J. H. Choi, K. Tobia, M. Hagan,
M. Ma, M. Livermore, N. Rasumov-Rahe, N. Holzenberger, N. Kolt,
P. Henderson, S. Rehaag, S. Goel, S. Gao, S. Williams, S. Gandhi,
T. Zur, V . Iyer, and Z. Li, “Legalbench: A collaboratively built
benchmark for measuring legal reasoning in large language models,”
p. arXiv:2308.11462, August 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230811462G
[346] K. Shuster, J. Urbanek, E. Dinan, A. Szlam, and J. Weston, “Deploying
lifelong open-domain dialogue learning,” p. arXiv:2008.08076, August
01, 2020 2020. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2020arXiv200808076S
[347] A. Ben Abacha, E. Agichtein, Y . Pinter, and D. Demner-Fushman,
Overview of the Medical Question Answering Task at TREC 2017
LiveQA, 2018.
[348] “Lyft_2021,” 2021. [Online]. Available: https://raw.githubusercontent.
com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_
2021.pdf
[349] X. Yue, Y . Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang,
S. Stevens, D. Jiang, W. Ren, Y . Sun, C. Wei, B. Yu, R. Yuan,
R. Sun, M. Yin, B. Zheng, Z. Yang, Y . Liu, W. Huang,
H. Sun, Y . Su, and W. Chen, “Mmmu: A massive multi-discipline
multimodal understanding and reasoning benchmark for expert agi,”
p. arXiv:2311.16502, November 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv231116502Y
[350] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng,
K.-W. Chang, M. Galley, and J. Gao, “Mathvista: Evaluating
mathematical reasoning of foundation models in visual contexts,”
p. arXiv:2310.02255, October 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv231002255L
[351] “Mtsample.” [Online]. Available: https://mtsamples.com/
[352] A. B. Abacha, Y . Mrabet, M. Sharp, T. R. Goodwin, S. E. Shooshan,
and D. Demner-Fushman, “Bridging the gap between consumers’
medication questions and trusted answers,”Stud Health Technol Inform,
vol. 264, pp. 25–29, 2019.
[353] X. Zhang, C. Tian, X. Yang, L. Chen, Z. Li, and L. R.
Petzold, “Alpacare:instruction-tuned large language models for medical
application,” p. arXiv:2310.14558, October 01, 2023 2023. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2023arXiv231014558Z
[354] A. Pal, L. K. Umapathi, and M. Sankarasubbu, “Medmcqa: A
large-scale multi-subject multi-choice dataset for medical domain
question answering,” pp. 248–260, 2022. [Online]. Available:
https://proceedings.mlr.press/v174/pal22a.html
[355] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and
P. Szolovits, “What disease does this patient have? a large-scale
open domain question answering dataset from medical exams,”
Applied Sciences, vol. 11, no. 14, p. 6421, 2021. [Online]. Available:
https://www.mdpi.com/2076-3417/11/14/6421
[356] Y . Zhang, H. Dai, Z. Kozareva, A. Smola, and L. Song, “Variational
reasoning for question answering with knowledge graph,”Proceedings
of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1,
2018. [Online]. Available: https://ojs.aaai.org/index.php/AAAI/article/
view/12057
[357] T.-Y . Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays,
P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollár, “Microsoft coco:
Common objects in context,” p. arXiv:1405.0312, May 01, 2014 2014.
[Online]. Available: https://ui.adsabs.harvard.edu/abs/2014arXiv1405.
0312L

59
[358] B. Dolan, C. Quirk, and C. Brockett, “Unsupervised construction
of large paraphrase corpora: Exploiting massively parallel
news sources,” ser. COLING 2004: Proceedings of the 20th
International Conference on Computational Linguistics. COLING,
2004, Conference Proceedings, pp. 350–356. [Online]. Available:
https://aclanthology.org/C04-1051/
[359] D. Chen and W. Dolan, “Collecting highly parallel data for
paraphrase evaluation,” ser. Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics: Human
Language Technologies. Association for Computational Linguistics,
2011, Conference Proceedings, pp. 190–200. [Online]. Available:
https://aclanthology.org/P11-1020/
[360] J. Xu, T. Mei, T. Yao, and Y . Rui, “Msr-vtt: A large video description
dataset for bridging video and language,”2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 5288–5296,
2016.
[361] A. E. W. Johnson, T. J. Pollard, N. R. Greenbaum, M. P. Lungren,
C.-y. Deng, Y . Peng, Z. Lu, R. G. Mark, S. J. Berkowitz, and S. Horng,
“Mimic-cxr-jpg, a large publicly available database of labeled chest
radiographs,” p. arXiv:1901.07042, January 01, 2019 2019. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2019arXiv190107042J
[362] “Minecraft wiki.” [Online]. Available: https://minecraft.wiki/
[363] P. Sen, A. F. Aji, and A. Saffari, “Mintaka: A complex, natural,
and multilingual dataset for end-to-end question answering,” ser.
Proceedings of the 29th International Conference on Computational
Linguistics. International Committee on Computational Linguistics,
2022, Conference Proceedings, pp. 1604–1619. [Online]. Available:
https://aclanthology.org/2022.coling-1.138/
[364] Y . Liu, H. Duan, Y . Zhang, B. Li, S. Zhang, W. Zhao,
Y . Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin,
“Mmbench: Is your multi-modal model an all-around player?”
p. arXiv:2307.06281, July 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230706281L
[365] Y . Fang, X. Liang, N. Zhang, K. Liu, R. Huang, Z. Chen,
X. Fan, and H. Chen, “Mol-instructions: A large-scale biomolecular
instruction dataset for large language models,” p. arXiv:2306.08018,
June 01, 2023 2023. [Online]. Available: https://ui.adsabs.harvard.edu/
abs/2023arXiv230608018F
[366] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,
E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton, “Program
synthesis with large language models,” p. arXiv:2108.07732, August
01, 2021 2021. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2021arXiv210807732A
[367] “Movielens,” 1998. [Online]. Available: https://grouplens.org/datasets/
movielens/
[368] B. Boecking, N. Usuyama, S. Bannur, D. C. Castro, A. Schwaighofer,
S. Hyland, M. Wetscherek, T. Naumann, A. Nori, J. Alvarez-
Valle, H. Poon, and O. Oktay, “Making the most of text
semantics to improve biomedical vision–language processing,” p.
arXiv:2204.09817, April 01, 2022 2022. [Online]. Available: https:
//ui.adsabs.harvard.edu/abs/2022arXiv220409817B
[369] M. Eric, R. Goel, S. Paul, A. Sethi, S. Agarwal, S. Gao, A. Kumar,
A. Goyal, P. Ku, and D. Hakkani-Tur, “Multiwoz 2.1: A consolidated
multi-domain dialogue dataset with state corrections and state tracking
baselines,” ser. Proceedings of the Twelfth Language Resources and
Evaluation Conference. European Language Resources Association,
2020, Conference Proceedings, pp. 422–428. [Online]. Available:
https://aclanthology.org/2020.lrec-1.53
[370] A. Williams, N. Nangia, and S. Bowman, “A broad-coverage
challenge corpus for sentence understanding through inference,”
ser. Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human
Language Technologies, V olume 1 (Long Papers). Association for
Computational Linguistics, 2018, Conference Proceedings, pp. 1112–
1122. [Online]. Available: https://aclanthology.org/N18-1101https:
//doi.org/10.18653/v1/N18-1101
[371] W. Tao, Y . Wang, E. Shi, L. Du, S. Han, H. Zhang, D. Zhang,
and W. Zhang, “On the evaluation of commit message generation
models: An experimental study,” p. arXiv:2107.05373, July 01,
2021 2021. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2021arXiv210705373T
[372] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,
“Looking beyond the surface: A challenge set for reading
comprehension over multiple sentences,” ser. Proceedings of the
2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies,
V olume 1 (Long Papers). Association for Computational Linguistics,2018, Conference Proceedings, pp. 252–262. [Online]. Available: https:
//aclanthology.org/N18-1023https://doi.org/10.18653/v1/N18-1023
[373] C. Fu, P. Chen, Y . Shen, Y . Qin, M. Zhang, X. Lin, J. Yang,
X. Zheng, K. Li, X. Sun, Y . Wu, and R. Ji, “Mme: A comprehensive
evaluation benchmark for multimodal large language models,”
p. arXiv:2306.13394, June 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230613394F
[374] X. V . Lin, C. Wang, L. Zettlemoyer, and M. D. Ernst, “Nl2bash:
A corpus and semantic parser for natural language interface to the
linux operating system,” ser. Proceedings of the Eleventh International
Conference on Language Resources and Evaluation (LREC 2018).
European Language Resources Association (ELRA), 2018, Conference
Proceedings. [Online]. Available: https://aclanthology.org/L18-1491
[375] M. Agarwal, T. Chakraborti, Q. Fu, D. Gros, X. V . Lin, J. Maene,
K. Talamadupula, Z. Teng, and J. White, “Neurips 2020 nlc2cmd
competition: Translating natural language to bash commands,”
p. arXiv:2103.02523, March 01, 2021 2021. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2021arXiv210302523A
[376] S. Riedel, L. Yao, and A. McCallum, “Modeling relations and their
mentions without labeled text,” p. 148–163, 2010.
[377] A. Trischler, T. Wang, X. Yuan, J. Harris, A. Sordoni, P. Bachman,
and K. Suleman, “Newsqa: A machine comprehension dataset,” ser.
Proceedings of the 2nd Workshop on Representation Learning for
NLP. Association for Computational Linguistics, 2017, Conference
Proceedings, pp. 191–200. [Online]. Available: https://aclanthology.
org/W17-2623https://doi.org/10.18653/v1/W17-2623
[378] H. Agrawal, K. Desai, Y . Wang, X. Chen, R. Jain, M. Johnson, D. Batra,
D. Parikh, S. Lee, and P. Anderson, “nocaps: novel object captioning
at scale,” p. arXiv:1812.08658, December 01, 2018 2018. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2018arXiv181208658A
[379] D. Bhattacharya, A. Aronsohn, J. Price, and V . Lo Re, “Hepatitis
c guidance 2023 update: Aasld-idsa recommendations for testing,
managing, and treating hepatitis c virus infection,”Clin Infect Dis,
2023.
[380] K. Lee, M.-W. Chang, and K. Toutanova, “Latent retrieval for
weakly supervised open domain question answering,” ser. Proceedings
of the 57th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 2019,
Conference Proceedings, pp. 6086–6096. [Online]. Available: https:
//aclanthology.org/P19-1612https://doi.org/10.18653/v1/P19-1612
[381] A. H. M. Lynn Marecek, MaryAnne Anthony-Smith,Prealgebra 2e,
2020. [Online]. Available: https://openstax.org/books/prealgebra-2e/
pages/1-introduction
[382] O. contributors, “Planet dump retrieved from https://planet.osm.org,”
2017. [Online]. Available: https://www.openstreetmap.org
[383] Q. Dong, X. Wan, and Y . Cao, “Parasci: A large scientific paraphrase
dataset for longer paraphrase generation,” ser. Proceedings of the
16th Conference of the European Chapter of the Association
for Computational Linguistics: Main V olume. Association for
Computational Linguistics, 2021, Conference Proceedings, pp. 424–
434. [Online]. Available: https://aclanthology.org/2021.eacl-main.33/
https://doi.org/10.18653/v1/2021.eacl-main.33
[384] “Pubmed central (pmc) full-text articles.” [Online]. Available:
https://www.ncbi.nlm.nih.gov/pmc/
[385] Y . Li, Y . Du, K. Zhou, J. Wang, X. Zhao, and J.-R. Wen,
“Evaluating object hallucination in large vision-language models,”
ser. Proceedings of the 2023 Conference on Empirical Methods
in Natural Language Processing. Association for Computational
Linguistics, 2023, Conference Proceedings, pp. 292–305. [Online].
Available: https://aclanthology.org/2023.emnlp-main.20/https://doi.org/
10.18653/v1/2023.emnlp-main.20
[386] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,
J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti,
E. Zhang, R. Child, R. Yazdani Aminabadi, J. Bernauer, X. Song,
M. Shoeybi, Y . He, M. Houston, S. Tiwary, and B. Catanzaro,
“Using deepspeed and megatron to train megatron-turing nlg 530b, a
large-scale generative language model,” p. arXiv:2201.11990, January
01, 2022 2022. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2022arXiv220111990S
[387] P. Lewis, Y . Wu, L. Liu, P. Minervini, H. Küttler, A. Piktus,
P. Stenetorp, and S. Riedel, “Paq: 65 million probably-asked
questions and what you can do with them,”Transactions of the
Association for Computational Linguistics, vol. 9, pp. 1098–1115,
2021. [Online]. Available: https://aclanthology.org/2021.tacl-1.65https:
//doi.org/10.1162/tacl_a_00415

60
[388] P. Wagner, N. Strodthoff, R. D. Bousseljot, D. Kreiseler, F. I. Lunze,
W. Samek, and T. Schaeffter, “Ptb-xl, a large publicly available
electrocardiography dataset,”Sci Data, vol. 7, no. 1, p. 154, 2020.
[389] N. Strodthoff, T. Mehari, C. Nagel, P. J. Aston, A. Sundar, C. Graff,
J. K. Kanters, W. Haverkamp, O. Dössel, A. Loewe, M. Bär, and
T. Schaeffter, “Ptb-xl+, a comprehensive electrocardiographic feature
dataset,”Scientific Data, vol. 10, no. 1, p. 279, 2023. [Online].
Available: https://doi.org/10.1038/s41597-023-02153-8
[390] “Pubmed abstracts.” [Online]. Available: https://pubmed.ncbi.nlm.nih.
gov/
[391] T. Ge, J. Hu, L. Wang, X. Wang, S.-Q. Chen, and F. Wei,
“In-context autoencoder for context compression in a large language
model,” p. arXiv:2307.06945, July 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230706945G
[392] A. Valerio Miceli Barone and R. Sennrich, “A parallel corpus of
python functions and documentation strings for automated code
documentation and code generation,” p. arXiv:1707.02275, July
01, 2017 2017. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2017arXiv170702275V
[393] M. Bahrami, N. C. Shrikanth, S. Ruangwan, L. Liu, Y . Mizobuchi,
M. Fukuyori, W.-P. Chen, K. Munakata, and T. Menzies, “Pytorrent:
A python library corpus for large-scale language models,” p.
arXiv:2110.01710, October 01, 2021 2021. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2021arXiv211001710B
[394] R. Anantha, S. Vakulenko, Z. Tu, S. Longpre, S. Pulman, and
S. Chappidi, “Open-domain question answering goes conversational
via question rewriting,” ser. Proceedings of the 2021 Conference of
the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies. Association for
Computational Linguistics, 2021, Conference Proceedings, pp. 520–
534. [Online]. Available: https://aclanthology.org/2021.naacl-main.44/
https://doi.org/10.18653/v1/2021.naacl-main.44
[395] A. Rogers, O. Kovaleva, M. Downey, and A. Rumshisky, “Getting
closer to ai complete question answering: A set of prerequisite
real tasks,”Proceedings of the AAAI Conference on Artificial
Intelligence, vol. 34, no. 05, pp. 8722–8731, 2020. [Online].
Available: https://ojs.aaai.org/index.php/AAAI/article/view/6398
[396] R. Y . Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen,
V . Padmakumar, J. Ma, J. Thompson, H. He, and S. R. Bowman,
“Quality: Question answering with long input texts, yes!” p.
arXiv:2112.08608, December 01, 2021 2021. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2021arXiv211208608P
[397] O. Tafjord, M. Gardner, K. Lin, and P. Clark, “Quartz: An
open-domain dataset of qualitative relationship questions,” ser.
Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP). Association for
Computational Linguistics, 2019, Conference Proceedings, pp. 5941–
5946. [Online]. Available: https://aclanthology.org/D19-1608/https:
//doi.org/10.18653/v1/D19-1608
[398] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y . Choi,
P. Liang, and L. Zettlemoyer, “Quac: Question answering in context,”
ser. Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing. Association for Computational
Linguistics, 2018, Conference Proceedings, pp. 2174–2184. [Online].
Available: https://aclanthology.org/D18-1241/https://doi.org/10.18653/
v1/D18-1241
[399] T. Hosking and M. Lapata, “Factorising meaning and form
for intent-preserving paraphrasing,” ser. Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics
and the 11th International Joint Conference on Natural Language
Processing (V olume 1: Long Papers). Association for Computational
Linguistics, 2021, Conference Proceedings, pp. 1405–1418.
[Online]. Available: https://aclanthology.org/2021.acl-long.112/https:
//doi.org/10.18653/v1/2021.acl-long.112
[400] A. Gupta, A. Agarwal, P. Singh, and P. Rai, “A deep generative
framework for paraphrase generation,”Proceedings of the AAAI
Conference on Artificial Intelligence, vol. 32, no. 1, 2018. [Online].
Available: https://ojs.aaai.org/index.php/AAAI/article/view/11956
[401] G. Lai, Q. Xie, H. Liu, Y . Yang, and E. Hovy, “Race:
Large-scale reading comprehension dataset from examinations,” ser.
Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing. Association for Computational Linguistics,
2017, Conference Proceedings, pp. 785–794. [Online]. Available: https:
//aclanthology.org/D17-1082/https://doi.org/10.18653/v1/D17-1082
[402] ParticleMedia, “Ragtruth.” [Online]. Available: https://github.com/
ParticleMedia/RAGTruth[403] S. Zhang, X. Liu, J. Liu, J. Gao, K. Duh, and
B. Van Durme, “Record: Bridging the gap between
human and machine commonsense reading comprehension,” p.
arXiv:1810.12885, October 01, 2018 2018. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2018arXiv181012885Z
[404] S. Gehman, S. Gururangan, M. Sap, Y . Choi, and N. A.
Smith, “Realtoxicityprompts: Evaluating neural toxic degeneration in
language models,” ser. Findings of the Association for Computational
Linguistics: EMNLP 2020. Association for Computational
Linguistics, 2020, Conference Proceedings, pp. 3356–3369.
[Online]. Available: https://aclanthology.org/2020.findings-emnlp.301/
https://doi.org/10.18653/v1/2020.findings-emnlp.301
[405] M. Völske, M. Potthast, S. Syed, and B. Stein, “Tl;dr: Mining
reddit to learn automatic summarization,” ser. Proceedings of
the Workshop on New Frontiers in Summarization. Association
for Computational Linguistics, 2017, Conference Proceedings, pp.
59–63. [Online]. Available: https://aclanthology.org/W17-4508/https:
//doi.org/10.18653/v1/W17-4508
[406] B. Y . Lin, Z. Wu, Y . Yang, D.-H. Lee, and X. Ren, “Riddlesense:
Reasoning about riddle questions featuring linguistic creativity and
commonsense knowledge,” ser. Findings of the Association for
Computational Linguistics: ACL-IJCNLP 2021. Association for
Computational Linguistics, 2021, Conference Proceedings, pp. 1504–
1515. [Online]. Available: https://aclanthology.org/2021.findings-acl.
131/https://doi.org/10.18653/v1/2021.findings-acl.131
[407] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, “Multi-
sentence argument linking,” ser. Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics. Association
for Computational Linguistics, 2020, Conference Proceedings,
pp. 8057–8077. [Online]. Available: https://aclanthology.org/2020.
acl-main.718https://doi.org/10.18653/v1/2020.acl-main.718
[408] Y . Lu, S. Liu, Q. Zhang, and Z. Xie, “Rtllm: An open-source
benchmark for design rtl generation with large language model,”
p. arXiv:2308.05345, August 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230805345L
[409] B. Gliwa, I. Mochol, M. Biesek, and A. Wawer, “Samsum corpus:
A human-annotated dialogue dataset for abstractive summarization,”
ser. Proceedings of the 2nd Workshop on New Frontiers in
Summarization. Association for Computational Linguistics, 2019,
Conference Proceedings, pp. 70–79. [Online]. Available: https:
//aclanthology.org/D19-5409/https://doi.org/10.18653/v1/D19-5409
[410] V . Ordonez, G. Kulkarni, and T. Berg, “Im2text: Describing
images using 1 million captioned photographs,” 2011. [Online].
Available: https://proceedings.neurips.cc/paper_files/paper/2011/file/
5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf
[411] D. A. Hudson and C. D. Manning, “Gqa: A new dataset for
real-world visual reasoning and compositional question answering,”
p. arXiv:1902.09506, February 01, 2019 2019. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2019arXiv190209506H
[412] “Scoliosis research society,” 1966. [Online]. Available: https://www.
srs.org/
[413] M. Dunn, L. Sagun, M. Higgins, V . Ugur Guney, V . Cirik, and
K. Cho, “Searchqa: A new q&a dataset augmented with context from
a search engine,” p. arXiv:1704.05179, April 01, 2017 2017. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2017arXiv170405179D
[414] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,
and H. Hajishirzi, “Self-instruct: Aligning language models with
self-generated instructions,” p. arXiv:2212.10560, December 01,
2022 2022. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2022arXiv221210560W
[415] M. Sap, H. Rashkin, D. Chen, R. Le Bras, and Y . Choi,
“Social iqa: Commonsense reasoning about social interactions,” ser.
Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP). Association for
Computational Linguistics, 2019, Conference Proceedings, pp. 4463–
4473. [Online]. Available: https://aclanthology.org/D19-1454/https:
//doi.org/10.18653/v1/D19-1454
[416] H. Kim, J. Hessel, L. Jiang, P. West, X. Lu, Y . Yu, P. Zhou, R. Bras,
M. Alikhani, G. Kim, M. Sap, and Y . Choi, “Soda: Million-scale
dialogue distillation with social commonsense contextualization,”
ser. Proceedings of the 2023 Conference on Empirical Methods
in Natural Language Processing. Association for Computational
Linguistics, 2023, Conference Proceedings, pp. 12 930–12 949.
[Online]. Available: https://aclanthology.org/2023.emnlp-main.799/
https://doi.org/10.18653/v1/2023.emnlp-main.799

61
[417] P. Pasupat and P. Liang, “Compositional semantic parsing
on semi-structured tables,” p. arXiv:1508.00305, August 01,
2015 2015. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2015arXiv150800305P
[418] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng,
and C. Potts, “Recursive deep models for semantic compositionality
over a sentiment treebank,” ser. Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, 2013, Conference Proceedings, pp.
1631–1642. [Online]. Available: https://aclanthology.org/D13-1170/
[419] C. Alt, A. Gabryszak, and L. Hennig, “Tacred revisited: A
thorough evaluation of the tacred relation extraction task,” ser.
Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics. Association for Computational
Linguistics, 2020, Conference Proceedings, pp. 1558–1569.
[Online]. Available: https://aclanthology.org/2020.acl-main.142/https:
//doi.org/10.18653/v1/2020.acl-main.142
[420] B. Berabi, J. He, V . Raychev, and M. T. Vechev, “Tfix: Learning to fix
coding errors with a text-to-text transformer,” 2021 2021.
[421] C. f. R. o. t. E. o. D. C. (UNISDR) and U. N. O.
for Disaster Risk Reduction, “The human cost of disasters
(2000–2019),” 2020. [Online]. Available: https://www.undrr.org/
publication/human-cost-disasters-overview-last-20-years-2000-2019
[422] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,
J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy,
“The pile: An 800gb dataset of diverse text for language modeling,”
p. arXiv:2101.00027, December 01, 2020 2020. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2021arXiv210100027G
[423] D. Kocetkov, R. Li, L. Ben Allal, J. Li, C. Mou, C. Muñoz Ferrandis,
Y . Jernite, M. Mitchell, S. Hughes, T. Wolf, D. Bahdanau, L. von
Werra, and H. de Vries, “The stack: 3 tb of permissively licensed
source code,” p. arXiv:2211.15533, November 01, 2022 2022. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2022arXiv221115533K
[424] Y . Zhuang, Y . Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa:
A dataset for llm question answering with external tools,” p.
arXiv:2306.13304, June 01, 2023 2023. [Online]. Available: https:
//ui.adsabs.harvard.edu/abs/2023arXiv230613304Z
[425] V . Adlakha, S. Dhuliawala, K. Suleman, H. de Vries, and S. Reddy,
“Topiocqa: Open-domain conversational question answering with
topic switching,”Transactions of the Association for Computational
Linguistics, vol. 10, pp. 468–483, 2022. [Online]. Available: https:
//aclanthology.org/2022.tacl-1.27/https://doi.org/10.1162/tacl_a_00471
[426] E. V oorhees, T. Alam, S. Bedrick, D. Demner-Fushman, W. R.
Hersh, K. Lo, K. Roberts, I. Soboroff, and L. L. Wang, “Trec-
covid: Constructing a pandemic information retrieval test collection,”
p. arXiv:2005.04474, May 01, 2020 2020. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2020arXiv200504474V
[427] H. Qian, Z. Liu, P. Zhang, K. Mao, D. Lian, Z. Dou, and T. Huang,
“Memorag: Boosting long context processing with global memory-
enhanced retrieval augmentation,” p. arXiv:2409.05591, September
01, 2024 2024. [Online]. Available: https://ui.adsabs.harvard.edu/abs/
2024arXiv240905591Q
[428] O. Honovich, T. Scialom, O. Levy, and T. Schick, “Unnatural
instructions: Tuning language models with (almost) no human labor,”
ser. Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (V olume 1: Long Papers). Association for
Computational Linguistics, 2023, Conference Proceedings, pp. 14 409–
14 428. [Online]. Available: https://aclanthology.org/2023.acl-long.806/
https://doi.org/10.18653/v1/2023.acl-long.806
[429] X. Wang, J. Wu, J. Chen, L. Li, Y .-F. Wang, and W. Y . Wang, “Vatex:
A large-scale, high-quality multilingual dataset for video-and-language
research,” p. arXiv:1904.03493, April 01, 2019 2019. [Online].
Available: https://ui.adsabs.harvard.edu/abs/2019arXiv190403493W
[430] M. Liu, N. Pinckney, B. Khailany, and H. Ren, “Verilogeval:
Evaluating large language models for verilog code generation,” p.
arXiv:2309.07544, September 01, 2023 2023. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2023arXiv230907544L
[431] A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick,
D. Batra, and D. Parikh, “Vqa: Visual question answering,”
p. arXiv:1505.00468, May 01, 2015 2015. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2015arXiv150500468A
[432] Y . Chang, M. Narang, H. Suzuki, G. Cao, J. Gao, and Y . Bisk, “Webqa:
Multihop and multimodal qa,” pp. 16 495–16 504, 2022/6 2022.
[433] L. Shang, Z. Lu, and H. Li, “Neural responding machine for
short-text conversation,” ser. Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics and the 7th
International Joint Conference on Natural Language Processing(V olume 1: Long Papers). Association for Computational Linguistics,
2015, Conference Proceedings, pp. 1577–1586. [Online]. Available:
https://aclanthology.org/P15-1152/https://doi.org/10.3115/v1/P15-1152
[434] D. Cohen, L. Yang, and W. B. Croft, “Wikipassageqa: A benchmark
collection for research on non-factoid answer passage retrieval,”
p. arXiv:1805.03797, May 01, 2018 2018. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2018arXiv180503797C
[435] “Wikieval,” 2023. [Online]. Available: https://huggingface.co/datasets/
explodinggradients/WikiEval
[436] A. Asai, X. Yu, J. Kasai, and H. Hajishirzi, “One
question answering model for many languages with cross-
lingual dense passage retrieval,” pp. 7547–7560, 2021 2021.
[Online]. Available: https://proceedings.neurips.cc/paper_files/paper/
2021/file/3df07fdae1ab273a967aaa1d355b8bb6-Paper.pdf
[437] K. Sakaguchi, R. Le Bras, C. Bhagavatula, and Y . Choi, “Winogrande:
An adversarial winograd schema challenge at scale,”Proceedings of
the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, pp.
8732–8740, 2020. [Online]. Available: https://ojs.aaai.org/index.php/
AAAI/article/view/6399
[438] S. Maekawa, H. Iso, S. Gurajada, and N. Bhutani, “Retrieval
helps or hurts? a deeper dive into the efficacy of retrieval
augmentation to language models,” ser. Proceedings of the 2024
Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies
(V olume 1: Long Papers). Association for Computational
Linguistics, 2024, Conference Proceedings, pp. 5506–5521.
[Online]. Available: https://aclanthology.org/2024.naacl-long.308/https:
//doi.org/10.18653/v1/2024.naacl-long.308
[439] S. Tedeschi, S. Conia, F. Cecconi, and R. Navigli, “Named
entity recognition for entity linking: What works and what‘s
next,” ser. Findings of the Association for Computational
Linguistics: EMNLP 2021. Association for Computational
Linguistics, 2021, Conference Proceedings, pp. 2584–2596.
[Online]. Available: https://aclanthology.org/2021.findings-emnlp.220/
https://doi.org/10.18653/v1/2021.findings-emnlp.220
[440] M. T. Pilehvar and J. Camacho-Collados, “Wic: the word-in-context
dataset for evaluating context-sensitive meaning representations,” ser.
Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, V olume 1 (Long and Short Papers). Association for
Computational Linguistics, 2019, Conference Proceedings, pp. 1267–
1273. [Online]. Available: https://aclanthology.org/N19-1128/https:
//doi.org/10.18653/v1/N19-1128
[441] A. Liu, S. Swayamdipta, N. A. Smith, and Y . Choi, “Wanli:
Worker and ai collaboration for natural language inference dataset
creation,” ser. Findings of the Association for Computational
Linguistics: EMNLP 2022. Association for Computational
Linguistics, 2022, Conference Proceedings, pp. 6826–6847. [Online].
Available: https://aclanthology.org/2022.findings-emnlp.508https://doi.
org/10.18653/v1/2022.findings-emnlp.508
[442] N. Asghar, “Yelp dataset challenge: Review rating prediction,”
p. arXiv:1605.05362, May 01, 2016 2016. [Online]. Available:
https://ui.adsabs.harvard.edu/abs/2016arXiv160505362A
[443] Yelp, “Yelp dataset.” [Online]. Available: https://www.yelp.com/dataset
[444] J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G.
Coleman, “Zinc: A free tool to discover chemistry for biology,”Journal
of Chemical Information and Modeling, vol. 52, no. 7, pp. 1757–1768,
2012. [Online]. Available: https://doi.org/10.1021/ci3001277

62
Andrew Brownreceived the BSc degree in Com-
puter Science (First Class Honours) from Queen’s
University Belfast, UK, in 2022, and is currently
pursuing the PhD degree in Computer Science at
Queen’s University Belfast. His research focuses on
natural language processing for document under-
standing and business information extraction. He has
served as a Demonstrator in video analytics and ma-
chine learning, cloud computing, and AI for health
at Queen’s University Belfast (2021–2024), and pre-
viously worked as a Junior Software Engineer with
Congruity360 (2022–2023). He received the Associate Fellowship of the
Higher Education Academy in 2021. His research interests include information
extraction, applied machine learning, and AI systems for documents.
Muhammad Romanreceived the Ph.D. degree in
Computer Science from Kohat University of Sci-
ence and Technology (KUST), Pakistan, in 2021,
specializing in Natural Language Processing, Infor-
mation Retrieval, and Large Language Models. He
has over 16 years of experience in AI research and
development, with expertise in Retrieval-Augmented
Generation, multimodal AI, and AI-driven orches-
tration. His current work focuses on LLM-based
multi-agent systems for energy flexibility services,
digital product passports, dataspaces, cross-sector
data sharing, and complete lifecycle analysis data, including renewable energy
integration, energy attribute certificates, and carbon footprint tracking. He
has authored multiple journal publications, and his research interests include
LLMs, document AI, compliance automation, and AI-native networking.
Barry Devereuxis a Senior Lecturer in the School
of Electronics, Electrical Engineering and Com-
puter Science at Queen’s University Belfast. His
research spans computational cognitive neuroscience
and natural language processing, with interests in
semantics, LLM analysis and interpretability, text
mining, and biomedical and clinical text data. He
has published in venues such as Computational Lin-
guistics, COLING, EMNLP, and Cognitive Science,
including work on the representation of compound
semantics in LLMs, retrieval-augmented generation
with knowledge graphs, and modelling human neuroimaging data in vision and
language processing. He is programme director of the QUB MSc programme
in Artificial Intelligence and serves as an Area Chair for the ACL Rolling
Review.